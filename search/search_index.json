{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Spark Structured Streaming (Apache Spark 3.1.1) \u00b6 Welcome to The Internals of Spark Structured Streaming online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Spark Structured Streaming as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Spark Structured Streaming \ud83d\udd25","title":"Home"},{"location":"#the-internals-of-spark-structured-streaming-apache-spark-311","text":"Welcome to The Internals of Spark Structured Streaming online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Spark Structured Streaming as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Spark Structured Streaming \ud83d\udd25","title":"The Internals of Spark Structured Streaming (Apache Spark 3.1.1)"},{"location":"CheckpointFileManager/","text":"CheckpointFileManager \u00b6 CheckpointFileManager is the < > of < > that manage checkpoint files (metadata of streaming batches) on Hadoop DFS-compatible file systems. CheckpointFileManager is < > per < > configuration property if defined before reverting to the available < >. CheckpointFileManager is used exclusively by HDFSMetadataLog , StreamMetadata and HDFSBackedStateStoreProvider . [[contract]] .CheckpointFileManager Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | createAtomic a| [[createAtomic]] [source, scala] \u00b6 createAtomic( path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream Used when: HDFSMetadataLog is requested to store metadata for a batch (that writeBatchToFile ) StreamMetadata helper object is requested to persist metadata HDFSBackedStateStore is requested for the deltaFileStream HDFSBackedStateStoreProvider is requested to writeSnapshotFile | delete a| [[delete]] [source, scala] \u00b6 delete(path: Path): Unit \u00b6 Deletes the given path recursively (if exists) Used when: RenameBasedFSDataOutputStream is requested to cancel CompactibleFileStreamLog is requested to store metadata for a batch (that deleteExpiredLog ) HDFSMetadataLog is requested to remove expired metadata and purgeAfter HDFSBackedStateStoreProvider is requested to do maintenance (that cleans up ) | exists a| [[exists]] [source, scala] \u00b6 exists(path: Path): Boolean \u00b6 Used when HDFSMetadataLog is created (to create the metadata directory ) and requested for metadata of a batch | isLocal a| [[isLocal]] [source, scala] \u00b6 isLocal: Boolean \u00b6 Does not seem to be used. | list a| [[list]] [source, scala] \u00b6 list( path: Path): Array[FileStatus] // <1> list( path: Path, filter: PathFilter): Array[FileStatus] <1> Uses PathFilter that accepts all files in the path Lists all files in the given path Used when: HDFSBackedStateStoreProvider is requested for all delta and snapshot files CompactibleFileStreamLog is requested for the compact interval and to deleteExpiredLog HDFSMetadataLog is requested for metadata of one or more batches , the latest committed batch , ordered batch metadata files , to remove expired metadata and purgeAfter | mkdirs a| [[mkdirs]] [source, scala] \u00b6 mkdirs(path: Path): Unit \u00b6 Used when: HDFSMetadataLog is created HDFSBackedStateStoreProvider is requested to initialize | open a| [[open]] [source, scala] \u00b6 open(path: Path): FSDataInputStream \u00b6 Opens a file (by the given path) for reading Used when: HDFSMetadataLog is requested for metadata of a batch HDFSBackedStateStoreProvider is requested to retrieve the state store for a specified version (that updateFromDeltaFile ), and readSnapshotFile |=== [[implementations]] .CheckpointFileManagers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | CheckpointFileManager | Description | FileContextBasedCheckpointFileManager | [[FileContextBasedCheckpointFileManager]] Default CheckpointFileManager that uses Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileContext.html[FileContext ] API for managing checkpoint files (unless < >) | FileSystemBasedCheckpointFileManager | [[FileSystemBasedCheckpointFileManager]] Basic CheckpointFileManager that uses Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem ] API for managing checkpoint files (that < > that the implementation of FileSystem.rename() is atomic or the correctness and fault-tolerance of Structured Streaming is not guaranteed) |=== === [[create]] Creating CheckpointFileManager Instance -- create Object Method [source, scala] \u00b6 create( path: Path, hadoopConf: Configuration): CheckpointFileManager create finds spark.sql.streaming.checkpointFileManagerClass configuration property in the hadoopConf configuration. If found, create simply instantiates whatever CheckpointFileManager implementation is defined. If not found, create creates a FileContextBasedCheckpointFileManager . In case of UnsupportedFileSystemException , create prints out the following WARN message to the logs and creates ( falls back on ) a FileSystemBasedCheckpointFileManager . Could not use FileContext API for managing Structured Streaming checkpoint files at [path]. Using FileSystem API instead for managing log files. If the implementation of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of your Structured Streaming is not guaranteed. create is used when: HDFSMetadataLog is created StreamMetadata utility is used to write metadata to a file (when StreamExecution is created) HDFSBackedStateStoreProvider is requested for a CheckpointFileManager","title":"CheckpointFileManager"},{"location":"CheckpointFileManager/#checkpointfilemanager","text":"CheckpointFileManager is the < > of < > that manage checkpoint files (metadata of streaming batches) on Hadoop DFS-compatible file systems. CheckpointFileManager is < > per < > configuration property if defined before reverting to the available < >. CheckpointFileManager is used exclusively by HDFSMetadataLog , StreamMetadata and HDFSBackedStateStoreProvider . [[contract]] .CheckpointFileManager Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | createAtomic a| [[createAtomic]]","title":"CheckpointFileManager"},{"location":"CheckpointFileManager/#source-scala","text":"createAtomic( path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream Used when: HDFSMetadataLog is requested to store metadata for a batch (that writeBatchToFile ) StreamMetadata helper object is requested to persist metadata HDFSBackedStateStore is requested for the deltaFileStream HDFSBackedStateStoreProvider is requested to writeSnapshotFile | delete a| [[delete]]","title":"[source, scala]"},{"location":"CheckpointFileManager/#source-scala_1","text":"","title":"[source, scala]"},{"location":"CheckpointFileManager/#deletepath-path-unit","text":"Deletes the given path recursively (if exists) Used when: RenameBasedFSDataOutputStream is requested to cancel CompactibleFileStreamLog is requested to store metadata for a batch (that deleteExpiredLog ) HDFSMetadataLog is requested to remove expired metadata and purgeAfter HDFSBackedStateStoreProvider is requested to do maintenance (that cleans up ) | exists a| [[exists]]","title":"delete(path: Path): Unit"},{"location":"CheckpointFileManager/#source-scala_2","text":"","title":"[source, scala]"},{"location":"CheckpointFileManager/#existspath-path-boolean","text":"Used when HDFSMetadataLog is created (to create the metadata directory ) and requested for metadata of a batch | isLocal a| [[isLocal]]","title":"exists(path: Path): Boolean"},{"location":"CheckpointFileManager/#source-scala_3","text":"","title":"[source, scala]"},{"location":"CheckpointFileManager/#islocal-boolean","text":"Does not seem to be used. | list a| [[list]]","title":"isLocal: Boolean"},{"location":"CheckpointFileManager/#source-scala_4","text":"list( path: Path): Array[FileStatus] // <1> list( path: Path, filter: PathFilter): Array[FileStatus] <1> Uses PathFilter that accepts all files in the path Lists all files in the given path Used when: HDFSBackedStateStoreProvider is requested for all delta and snapshot files CompactibleFileStreamLog is requested for the compact interval and to deleteExpiredLog HDFSMetadataLog is requested for metadata of one or more batches , the latest committed batch , ordered batch metadata files , to remove expired metadata and purgeAfter | mkdirs a| [[mkdirs]]","title":"[source, scala]"},{"location":"CheckpointFileManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"CheckpointFileManager/#mkdirspath-path-unit","text":"Used when: HDFSMetadataLog is created HDFSBackedStateStoreProvider is requested to initialize | open a| [[open]]","title":"mkdirs(path: Path): Unit"},{"location":"CheckpointFileManager/#source-scala_6","text":"","title":"[source, scala]"},{"location":"CheckpointFileManager/#openpath-path-fsdatainputstream","text":"Opens a file (by the given path) for reading Used when: HDFSMetadataLog is requested for metadata of a batch HDFSBackedStateStoreProvider is requested to retrieve the state store for a specified version (that updateFromDeltaFile ), and readSnapshotFile |=== [[implementations]] .CheckpointFileManagers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | CheckpointFileManager | Description | FileContextBasedCheckpointFileManager | [[FileContextBasedCheckpointFileManager]] Default CheckpointFileManager that uses Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileContext.html[FileContext ] API for managing checkpoint files (unless < >) | FileSystemBasedCheckpointFileManager | [[FileSystemBasedCheckpointFileManager]] Basic CheckpointFileManager that uses Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem ] API for managing checkpoint files (that < > that the implementation of FileSystem.rename() is atomic or the correctness and fault-tolerance of Structured Streaming is not guaranteed) |=== === [[create]] Creating CheckpointFileManager Instance -- create Object Method","title":"open(path: Path): FSDataInputStream"},{"location":"CheckpointFileManager/#source-scala_7","text":"create( path: Path, hadoopConf: Configuration): CheckpointFileManager create finds spark.sql.streaming.checkpointFileManagerClass configuration property in the hadoopConf configuration. If found, create simply instantiates whatever CheckpointFileManager implementation is defined. If not found, create creates a FileContextBasedCheckpointFileManager . In case of UnsupportedFileSystemException , create prints out the following WARN message to the logs and creates ( falls back on ) a FileSystemBasedCheckpointFileManager . Could not use FileContext API for managing Structured Streaming checkpoint files at [path]. Using FileSystem API instead for managing log files. If the implementation of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of your Structured Streaming is not guaranteed. create is used when: HDFSMetadataLog is created StreamMetadata utility is used to write metadata to a file (when StreamExecution is created) HDFSBackedStateStoreProvider is requested for a CheckpointFileManager","title":"[source, scala]"},{"location":"CommitLog/","text":"CommitLog \u2014 HDFSMetadataLog for Offset Commit Log \u00b6 CommitLog is an HDFSMetadataLog with CommitMetadata metadata. CommitLog is the offset commit log of streaming query execution engines . [[CommitMetadata]][[nextBatchWatermarkMs]] CommitLog uses CommitMetadata for the metadata with nextBatchWatermarkMs attribute (of type Long and the default 0 ). CommitLog < > commit metadata to files with names that are offsets. $ ls -tr [checkpoint-directory]/commits 0 1 2 3 4 5 6 7 8 9 $ cat [checkpoint-directory]/commits/8 v1 {\"nextBatchWatermarkMs\": 0} [[VERSION]] CommitLog uses 1 for the version. [[creating-instance]] CommitLog (like the parent HDFSMetadataLog ) takes the following to be created: [[sparkSession]] SparkSession [[path]] Path of the metadata log directory === [[serialize]] Serializing Metadata (Writing Metadata to Persistent Storage) -- serialize Method [source, scala] \u00b6 serialize( metadata: CommitMetadata, out: OutputStream): Unit serialize writes out the < > prefixed with v on a single line (e.g. v1 ) followed by the given CommitMetadata in JSON format. serialize is part of HDFSMetadataLog abstraction. === [[deserialize]] Deserializing Metadata -- deserialize Method [source, scala] \u00b6 deserialize(in: InputStream): CommitMetadata \u00b6 deserialize simply reads ( deserializes ) two lines from the given InputStream for version and the < > attribute. deserialize is part of HDFSMetadataLog abstraction. === [[add-batchId]] add Method [source, scala] \u00b6 add(batchId: Long): Unit \u00b6 add ...FIXME NOTE: add is used when...FIXME === [[add-batchId-metadata]] add Method [source, scala] \u00b6 add(batchId: Long, metadata: String): Boolean \u00b6 add ...FIXME add is part of MetadataLog abstraction.","title":"CommitLog"},{"location":"CommitLog/#commitlog-hdfsmetadatalog-for-offset-commit-log","text":"CommitLog is an HDFSMetadataLog with CommitMetadata metadata. CommitLog is the offset commit log of streaming query execution engines . [[CommitMetadata]][[nextBatchWatermarkMs]] CommitLog uses CommitMetadata for the metadata with nextBatchWatermarkMs attribute (of type Long and the default 0 ). CommitLog < > commit metadata to files with names that are offsets. $ ls -tr [checkpoint-directory]/commits 0 1 2 3 4 5 6 7 8 9 $ cat [checkpoint-directory]/commits/8 v1 {\"nextBatchWatermarkMs\": 0} [[VERSION]] CommitLog uses 1 for the version. [[creating-instance]] CommitLog (like the parent HDFSMetadataLog ) takes the following to be created: [[sparkSession]] SparkSession [[path]] Path of the metadata log directory === [[serialize]] Serializing Metadata (Writing Metadata to Persistent Storage) -- serialize Method","title":"CommitLog &mdash; HDFSMetadataLog for Offset Commit Log"},{"location":"CommitLog/#source-scala","text":"serialize( metadata: CommitMetadata, out: OutputStream): Unit serialize writes out the < > prefixed with v on a single line (e.g. v1 ) followed by the given CommitMetadata in JSON format. serialize is part of HDFSMetadataLog abstraction. === [[deserialize]] Deserializing Metadata -- deserialize Method","title":"[source, scala]"},{"location":"CommitLog/#source-scala_1","text":"","title":"[source, scala]"},{"location":"CommitLog/#deserializein-inputstream-commitmetadata","text":"deserialize simply reads ( deserializes ) two lines from the given InputStream for version and the < > attribute. deserialize is part of HDFSMetadataLog abstraction. === [[add-batchId]] add Method","title":"deserialize(in: InputStream): CommitMetadata"},{"location":"CommitLog/#source-scala_2","text":"","title":"[source, scala]"},{"location":"CommitLog/#addbatchid-long-unit","text":"add ...FIXME NOTE: add is used when...FIXME === [[add-batchId-metadata]] add Method","title":"add(batchId: Long): Unit"},{"location":"CommitLog/#source-scala_3","text":"","title":"[source, scala]"},{"location":"CommitLog/#addbatchid-long-metadata-string-boolean","text":"add ...FIXME add is part of MetadataLog abstraction.","title":"add(batchId: Long, metadata: String): Boolean"},{"location":"CommitMetadata/","text":"== [[CommitMetadata]] CommitMetadata CommitMetadata is...FIXME","title":"CommitMetadata"},{"location":"ContinuousDataSourceRDD/","text":"ContinuousDataSourceRDD \u00b6 ContinuousDataSourceRDD is a specialized RDD ( RDD[InternalRow] ) that is used exclusively for the only input RDD (with the input rows) of DataSourceV2ScanExec leaf physical operator with a ContinuousReader . ContinuousDataSourceRDD is < > exclusively when DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (which there is only one actually). [[spark.sql.streaming.continuous.executorQueueSize]] ContinuousDataSourceRDD uses spark.sql.streaming.continuous.executorQueueSize configuration property for the < >. [[spark.sql.streaming.continuous.executorPollIntervalMs]] ContinuousDataSourceRDD uses spark.sql.streaming.continuous.executorPollIntervalMs configuration property for the < >. [[creating-instance]] ContinuousDataSourceRDD takes the following to be created: [[sc]] SparkContext [[dataQueueSize]] Size of the data queue [[epochPollIntervalMs]] epochPollIntervalMs [[readerInputPartitions]] InputPartition[InternalRow] s [[getPreferredLocations]] ContinuousDataSourceRDD uses InputPartition (of a ContinuousDataSourceRDDPartition ) for preferred host locations (where the input partition reader can run faster). === [[compute]] Computing Partition -- compute Method [source, scala] \u00b6 compute( split: Partition, context: TaskContext): Iterator[InternalRow] NOTE: compute is part of the RDD Contract to compute a given partition. compute ...FIXME === [[getPartitions]] getPartitions Method [source, scala] \u00b6 getPartitions: Array[Partition] \u00b6 NOTE: getPartitions is part of the RDD Contract to specify the partitions to < >. getPartitions ...FIXME","title":"ContinuousDataSourceRDD"},{"location":"ContinuousDataSourceRDD/#continuousdatasourcerdd","text":"ContinuousDataSourceRDD is a specialized RDD ( RDD[InternalRow] ) that is used exclusively for the only input RDD (with the input rows) of DataSourceV2ScanExec leaf physical operator with a ContinuousReader . ContinuousDataSourceRDD is < > exclusively when DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (which there is only one actually). [[spark.sql.streaming.continuous.executorQueueSize]] ContinuousDataSourceRDD uses spark.sql.streaming.continuous.executorQueueSize configuration property for the < >. [[spark.sql.streaming.continuous.executorPollIntervalMs]] ContinuousDataSourceRDD uses spark.sql.streaming.continuous.executorPollIntervalMs configuration property for the < >. [[creating-instance]] ContinuousDataSourceRDD takes the following to be created: [[sc]] SparkContext [[dataQueueSize]] Size of the data queue [[epochPollIntervalMs]] epochPollIntervalMs [[readerInputPartitions]] InputPartition[InternalRow] s [[getPreferredLocations]] ContinuousDataSourceRDD uses InputPartition (of a ContinuousDataSourceRDDPartition ) for preferred host locations (where the input partition reader can run faster). === [[compute]] Computing Partition -- compute Method","title":"ContinuousDataSourceRDD"},{"location":"ContinuousDataSourceRDD/#source-scala","text":"compute( split: Partition, context: TaskContext): Iterator[InternalRow] NOTE: compute is part of the RDD Contract to compute a given partition. compute ...FIXME === [[getPartitions]] getPartitions Method","title":"[source, scala]"},{"location":"ContinuousDataSourceRDD/#source-scala_1","text":"","title":"[source, scala]"},{"location":"ContinuousDataSourceRDD/#getpartitions-arraypartition","text":"NOTE: getPartitions is part of the RDD Contract to specify the partitions to < >. getPartitions ...FIXME","title":"getPartitions: Array[Partition]"},{"location":"ContinuousQueuedDataReader/","text":"== [[ContinuousQueuedDataReader]] ContinuousQueuedDataReader ContinuousQueuedDataReader is < > exclusively when ContinuousDataSourceRDD is requested to < >. [[ContinuousRecord]] ContinuousQueuedDataReader uses two types of continuous records : [[EpochMarker]] EpochMarker [[ContinuousRow]] ContinuousRow (with the InternalRow at PartitionOffset ) === [[next]] Fetching Next Row -- next Method [source, scala] \u00b6 next(): InternalRow \u00b6 next ...FIXME NOTE: next is used when...FIXME === [[close]] Closing ContinuousQueuedDataReader -- close Method [source, scala] \u00b6 close(): Unit \u00b6 NOTE: close is part of the https://docs.oracle.com/javase/8/docs/api/java/io/Closeable.html[java.io.Closeable ] to close this stream and release any system resources associated with it. close ...FIXME === [[creating-instance]] Creating ContinuousQueuedDataReader Instance ContinuousQueuedDataReader takes the following to be created: [[partition]] ContinuousDataSourceRDDPartition [[context]] TaskContext [[dataQueueSize]] Size of the < > [[epochPollIntervalMs]] epochPollIntervalMs ContinuousQueuedDataReader initializes the < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | coordinatorId a| [[coordinatorId]] Epoch Coordinator Identifier Used when...FIXME | currentOffset a| [[currentOffset]] PartitionOffset Used when...FIXME | dataReaderThread a| [[dataReaderThread]] < > daemon thread that is created and started immediately when ContinuousQueuedDataReader is < > Used when...FIXME | epochCoordEndpoint a| [[epochCoordEndpoint]] RpcEndpointRef of the < > per < > Used when...FIXME | epochMarkerExecutor a| [[epochMarkerExecutor]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledExecutorService.html[java.util.concurrent.ScheduledExecutorService ] Used when...FIXME | epochMarkerGenerator a| [[epochMarkerGenerator]] < > Used when...FIXME | reader a| [[reader]] InputPartitionReader Used when...FIXME | queue a| [[queue]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ArrayBlockingQueue.html[java.util.concurrent.ArrayBlockingQueue ] of < > (of the given < >) Used when...FIXME |===","title":"ContinuousQueuedDataReader"},{"location":"ContinuousQueuedDataReader/#source-scala","text":"","title":"[source, scala]"},{"location":"ContinuousQueuedDataReader/#next-internalrow","text":"next ...FIXME NOTE: next is used when...FIXME === [[close]] Closing ContinuousQueuedDataReader -- close Method","title":"next(): InternalRow"},{"location":"ContinuousQueuedDataReader/#source-scala_1","text":"","title":"[source, scala]"},{"location":"ContinuousQueuedDataReader/#close-unit","text":"NOTE: close is part of the https://docs.oracle.com/javase/8/docs/api/java/io/Closeable.html[java.io.Closeable ] to close this stream and release any system resources associated with it. close ...FIXME === [[creating-instance]] Creating ContinuousQueuedDataReader Instance ContinuousQueuedDataReader takes the following to be created: [[partition]] ContinuousDataSourceRDDPartition [[context]] TaskContext [[dataQueueSize]] Size of the < > [[epochPollIntervalMs]] epochPollIntervalMs ContinuousQueuedDataReader initializes the < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | coordinatorId a| [[coordinatorId]] Epoch Coordinator Identifier Used when...FIXME | currentOffset a| [[currentOffset]] PartitionOffset Used when...FIXME | dataReaderThread a| [[dataReaderThread]] < > daemon thread that is created and started immediately when ContinuousQueuedDataReader is < > Used when...FIXME | epochCoordEndpoint a| [[epochCoordEndpoint]] RpcEndpointRef of the < > per < > Used when...FIXME | epochMarkerExecutor a| [[epochMarkerExecutor]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledExecutorService.html[java.util.concurrent.ScheduledExecutorService ] Used when...FIXME | epochMarkerGenerator a| [[epochMarkerGenerator]] < > Used when...FIXME | reader a| [[reader]] InputPartitionReader Used when...FIXME | queue a| [[queue]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ArrayBlockingQueue.html[java.util.concurrent.ArrayBlockingQueue ] of < > (of the given < >) Used when...FIXME |===","title":"close(): Unit"},{"location":"ContinuousStream/","text":"ContinuousStream \u00b6 ContinuousStream is...FIXME","title":"ContinuousStream"},{"location":"ContinuousStream/#continuousstream","text":"ContinuousStream is...FIXME","title":"ContinuousStream"},{"location":"ContinuousWriteRDD/","text":"ContinuousWriteRDD -- RDD of WriteToContinuousDataSourceExec Unary Physical Operator \u00b6 ContinuousWriteRDD is a specialized RDD ( RDD[Unit] ) that is used exclusively as the underlying RDD of WriteToContinuousDataSourceExec unary physical operator to < >. ContinuousWriteRDD is < > exclusively when WriteToContinuousDataSourceExec unary physical operator is requested to < >. [[partitioner]] [[getPartitions]] ContinuousWriteRDD uses the < > for the partitions and the partitioner. [[creating-instance]] ContinuousWriteRDD takes the following to be created: [[prev]] Parent RDD ( RDD[InternalRow] ) [[writeTask]] Write task ( DataWriterFactory[InternalRow] ) === [[compute]] Computing Partition -- compute Method [source, scala] \u00b6 compute( split: Partition, context: TaskContext): Iterator[Unit] NOTE: compute is part of the RDD Contract to compute a partition. compute requests the EpochCoordinatorRef helper for a < > (using the < >). NOTE: The < > runs on the driver as the single point to coordinate epochs across partition tasks. compute uses the EpochTracker helper to < > (using the < > local property). [[compute-loop]] compute then executes the following steps (in a loop) until the task (as the given TaskContext ) is killed or completed. compute requests the < > to compute the given partition (that gives an Iterator[InternalRow] ). compute requests the < > to create a DataWriter (for the partition and the task attempt IDs from the given TaskContext and the < > from the EpochTracker helper) and requests it to write all records (from the Iterator[InternalRow] ). compute prints out the following INFO message to the logs: Writer for partition [partitionId] in epoch [epoch] is committing. compute requests the DataWriter to commit (that gives a WriterCommitMessage ). compute requests the EpochCoordinator RPC endpoint reference to send out a < > message (with the WriterCommitMessage ). compute prints out the following INFO message to the logs: Writer for partition [partitionId] in epoch [epoch] is committed. In the end (of the loop), compute uses the EpochTracker helper to < >. In case of an error, compute prints out the following ERROR message to the logs and requests the DataWriter to abort. Writer for partition [partitionId] is aborting. In the end, compute prints out the following ERROR message to the logs: Writer for partition [partitionId] aborted.","title":"ContinuousWriteRDD"},{"location":"ContinuousWriteRDD/#continuouswriterdd-rdd-of-writetocontinuousdatasourceexec-unary-physical-operator","text":"ContinuousWriteRDD is a specialized RDD ( RDD[Unit] ) that is used exclusively as the underlying RDD of WriteToContinuousDataSourceExec unary physical operator to < >. ContinuousWriteRDD is < > exclusively when WriteToContinuousDataSourceExec unary physical operator is requested to < >. [[partitioner]] [[getPartitions]] ContinuousWriteRDD uses the < > for the partitions and the partitioner. [[creating-instance]] ContinuousWriteRDD takes the following to be created: [[prev]] Parent RDD ( RDD[InternalRow] ) [[writeTask]] Write task ( DataWriterFactory[InternalRow] ) === [[compute]] Computing Partition -- compute Method","title":"ContinuousWriteRDD -- RDD of WriteToContinuousDataSourceExec Unary Physical Operator"},{"location":"ContinuousWriteRDD/#source-scala","text":"compute( split: Partition, context: TaskContext): Iterator[Unit] NOTE: compute is part of the RDD Contract to compute a partition. compute requests the EpochCoordinatorRef helper for a < > (using the < >). NOTE: The < > runs on the driver as the single point to coordinate epochs across partition tasks. compute uses the EpochTracker helper to < > (using the < > local property). [[compute-loop]] compute then executes the following steps (in a loop) until the task (as the given TaskContext ) is killed or completed. compute requests the < > to compute the given partition (that gives an Iterator[InternalRow] ). compute requests the < > to create a DataWriter (for the partition and the task attempt IDs from the given TaskContext and the < > from the EpochTracker helper) and requests it to write all records (from the Iterator[InternalRow] ). compute prints out the following INFO message to the logs: Writer for partition [partitionId] in epoch [epoch] is committing. compute requests the DataWriter to commit (that gives a WriterCommitMessage ). compute requests the EpochCoordinator RPC endpoint reference to send out a < > message (with the WriterCommitMessage ). compute prints out the following INFO message to the logs: Writer for partition [partitionId] in epoch [epoch] is committed. In the end (of the loop), compute uses the EpochTracker helper to < >. In case of an error, compute prints out the following ERROR message to the logs and requests the DataWriter to abort. Writer for partition [partitionId] is aborting. In the end, compute prints out the following ERROR message to the logs: Writer for partition [partitionId] aborted.","title":"[source, scala]"},{"location":"DataReaderThread/","text":"== [[DataReaderThread]] DataReaderThread DataReaderThread is...FIXME","title":"DataReaderThread"},{"location":"DataSource/","text":"DataSource \u00b6 Tip Learn more about DataSource in The Internals of Spark SQL online book. Creating Streaming Source (Data Source V1) \u00b6 createSource ( metadataPath : String ): Source createSource creates a new instance of the data source class and branches off per the type: StreamSourceProvider FileFormat other types createSource is used when MicroBatchExecution is requested for an analyzed logical plan . StreamSourceProvider \u00b6 For a StreamSourceProvider , createSource requests the StreamSourceProvider to create a source . FileFormat \u00b6 For a FileFormat , createSource creates a new FileStreamSource . createSource throws an IllegalArgumentException when path option was not specified for a FileFormat data source: 'path' is not specified Other Types \u00b6 For any other data source type, createSource simply throws an UnsupportedOperationException : Data source [className] does not support streamed reading SourceInfo \u00b6 sourceInfo : SourceInfo Metadata of a Source with the following: Name (alias) Schema Partitioning columns sourceInfo is initialized (lazily) using sourceSchema . Lazy Value sourceInfo is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards. Used when: DataSource is requested to create a streaming source for a File-Based Data Source (when MicroBatchExecution is requested to initialize the analyzed logical plan ) StreamingRelation utility is used to create a StreamingRelation (when DataStreamReader is requested for a streaming query ) Generating Metadata of Streaming Source \u00b6 sourceSchema (): SourceInfo sourceSchema creates a new instance of the data source class and branches off per the type: StreamSourceProvider FileFormat other types sourceSchema is used when DataSource is requested for the SourceInfo . StreamSourceProvider \u00b6 For a StreamSourceProvider , sourceSchema requests the StreamSourceProvider for the name and schema (of the streaming source ). In the end, sourceSchema returns the name and the schema as part of SourceInfo (with partition columns unspecified). FileFormat \u00b6 For a FileFormat , sourceSchema ...FIXME Other Types \u00b6 For any other data source type, sourceSchema simply throws an UnsupportedOperationException : Data source [className] does not support streamed reading Creating Streaming Sink \u00b6 createSink ( outputMode : OutputMode ): Sink createSink creates a streaming sink for StreamSinkProvider or FileFormat data sources. Tip Learn more about FileFormat in The Internals of Spark SQL online book. createSink creates a new instance of the data source class and branches off per the type: For a StreamSinkProvider , createSink simply delegates the call and requests it to create a streaming sink For a FileFormat , createSink creates a FileStreamSink when path option is specified and the output mode is Append . createSink throws a IllegalArgumentException when path option is not specified for a FileFormat data source: 'path' is not specified createSink throws an AnalysisException when the given OutputMode is different from Append for a FileFormat data source: Data source [className] does not support [outputMode] output mode createSink throws an UnsupportedOperationException for unsupported data source formats: Data source [className] does not support streamed writing createSink is used when DataStreamWriter is requested to start a streaming query .","title":"DataSource"},{"location":"DataSource/#datasource","text":"Tip Learn more about DataSource in The Internals of Spark SQL online book.","title":"DataSource"},{"location":"DataSource/#creating-streaming-source-data-source-v1","text":"createSource ( metadataPath : String ): Source createSource creates a new instance of the data source class and branches off per the type: StreamSourceProvider FileFormat other types createSource is used when MicroBatchExecution is requested for an analyzed logical plan .","title":" Creating Streaming Source (Data Source V1)"},{"location":"DataSource/#streamsourceprovider","text":"For a StreamSourceProvider , createSource requests the StreamSourceProvider to create a source .","title":" StreamSourceProvider"},{"location":"DataSource/#fileformat","text":"For a FileFormat , createSource creates a new FileStreamSource . createSource throws an IllegalArgumentException when path option was not specified for a FileFormat data source: 'path' is not specified","title":" FileFormat"},{"location":"DataSource/#other-types","text":"For any other data source type, createSource simply throws an UnsupportedOperationException : Data source [className] does not support streamed reading","title":" Other Types"},{"location":"DataSource/#sourceinfo","text":"sourceInfo : SourceInfo Metadata of a Source with the following: Name (alias) Schema Partitioning columns sourceInfo is initialized (lazily) using sourceSchema . Lazy Value sourceInfo is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards. Used when: DataSource is requested to create a streaming source for a File-Based Data Source (when MicroBatchExecution is requested to initialize the analyzed logical plan ) StreamingRelation utility is used to create a StreamingRelation (when DataStreamReader is requested for a streaming query )","title":" SourceInfo"},{"location":"DataSource/#generating-metadata-of-streaming-source","text":"sourceSchema (): SourceInfo sourceSchema creates a new instance of the data source class and branches off per the type: StreamSourceProvider FileFormat other types sourceSchema is used when DataSource is requested for the SourceInfo .","title":" Generating Metadata of Streaming Source"},{"location":"DataSource/#streamsourceprovider_1","text":"For a StreamSourceProvider , sourceSchema requests the StreamSourceProvider for the name and schema (of the streaming source ). In the end, sourceSchema returns the name and the schema as part of SourceInfo (with partition columns unspecified).","title":" StreamSourceProvider"},{"location":"DataSource/#fileformat_1","text":"For a FileFormat , sourceSchema ...FIXME","title":" FileFormat"},{"location":"DataSource/#other-types_1","text":"For any other data source type, sourceSchema simply throws an UnsupportedOperationException : Data source [className] does not support streamed reading","title":" Other Types"},{"location":"DataSource/#creating-streaming-sink","text":"createSink ( outputMode : OutputMode ): Sink createSink creates a streaming sink for StreamSinkProvider or FileFormat data sources. Tip Learn more about FileFormat in The Internals of Spark SQL online book. createSink creates a new instance of the data source class and branches off per the type: For a StreamSinkProvider , createSink simply delegates the call and requests it to create a streaming sink For a FileFormat , createSink creates a FileStreamSink when path option is specified and the output mode is Append . createSink throws a IllegalArgumentException when path option is not specified for a FileFormat data source: 'path' is not specified createSink throws an AnalysisException when the given OutputMode is different from Append for a FileFormat data source: Data source [className] does not support [outputMode] output mode createSink throws an UnsupportedOperationException for unsupported data source formats: Data source [className] does not support streamed writing createSink is used when DataStreamWriter is requested to start a streaming query .","title":" Creating Streaming Sink"},{"location":"DataStreamReader/","text":"DataStreamReader \u00b6 DataStreamReader is an interface that Spark developers use to describe how to load data from a streaming data source . Accessing DataStreamReader \u00b6 DataStreamReader is available using SparkSession.readStream method. import org.apache.spark.sql.SparkSession val spark: SparkSession = ... val streamReader = spark.readStream Built-in Formats \u00b6 DataStreamReader supports loading streaming data from the following built-in data sources : csv json orc parquet (default) text textFile Tip Use spark.sql.sources.default configuration property to change the default data source. Note Hive data source can only be used with tables, and it is an AnalysisException when specified explicitly. Loading Data \u00b6 load (): DataFrame load ( path : String ): DataFrame DataStreamReader gives specialized methods for built-in data systems and formats . In order to plug in a custom data source, DataStreamReader gives format and load methods. load creates a streaming DataFrame that represents a \"loading\" streaming data node (and is internally a logical plan with a StreamingRelationV2 or StreamingRelation leaf logical operators). load uses DataSource.lookupDataSource utility to look up the data source by source alias. Tip Learn more about DataSource.lookupDataSource utility in The Internals of Spark SQL online book. SupportsRead Tables with MICRO_BATCH_READ or CONTINUOUS_READ \u00b6 For a TableProvider (that is not a FileDataSourceV2 ), load requests it for a Table . Tip Learn more about TableProvider in The Internals of Spark SQL online book. For a Table with SupportsRead with MICRO_BATCH_READ or CONTINUOUS_READ capabilities, load creates a DataFrame with StreamingRelationV2 leaf logical operator. Tip Learn more about Table , SupportsRead and capabilities in The Internals of Spark SQL online book. If the DataSource is a StreamSourceProvider , load creates the StreamingRelationV2 with a StreamingRelation leaf logical operator. For other Table s, load creates a DataFrame with a StreamingRelation leaf logical operator. Other Data Sources \u00b6 load creates a DataFrame with a StreamingRelation leaf logical operator.","title":"DataStreamReader"},{"location":"DataStreamReader/#datastreamreader","text":"DataStreamReader is an interface that Spark developers use to describe how to load data from a streaming data source .","title":"DataStreamReader"},{"location":"DataStreamReader/#accessing-datastreamreader","text":"DataStreamReader is available using SparkSession.readStream method. import org.apache.spark.sql.SparkSession val spark: SparkSession = ... val streamReader = spark.readStream","title":" Accessing DataStreamReader"},{"location":"DataStreamReader/#built-in-formats","text":"DataStreamReader supports loading streaming data from the following built-in data sources : csv json orc parquet (default) text textFile Tip Use spark.sql.sources.default configuration property to change the default data source. Note Hive data source can only be used with tables, and it is an AnalysisException when specified explicitly.","title":"Built-in Formats"},{"location":"DataStreamReader/#loading-data","text":"load (): DataFrame load ( path : String ): DataFrame DataStreamReader gives specialized methods for built-in data systems and formats . In order to plug in a custom data source, DataStreamReader gives format and load methods. load creates a streaming DataFrame that represents a \"loading\" streaming data node (and is internally a logical plan with a StreamingRelationV2 or StreamingRelation leaf logical operators). load uses DataSource.lookupDataSource utility to look up the data source by source alias. Tip Learn more about DataSource.lookupDataSource utility in The Internals of Spark SQL online book.","title":" Loading Data"},{"location":"DataStreamReader/#supportsread-tables-with-micro_batch_read-or-continuous_read","text":"For a TableProvider (that is not a FileDataSourceV2 ), load requests it for a Table . Tip Learn more about TableProvider in The Internals of Spark SQL online book. For a Table with SupportsRead with MICRO_BATCH_READ or CONTINUOUS_READ capabilities, load creates a DataFrame with StreamingRelationV2 leaf logical operator. Tip Learn more about Table , SupportsRead and capabilities in The Internals of Spark SQL online book. If the DataSource is a StreamSourceProvider , load creates the StreamingRelationV2 with a StreamingRelation leaf logical operator. For other Table s, load creates a DataFrame with a StreamingRelation leaf logical operator.","title":"SupportsRead Tables with MICRO_BATCH_READ or CONTINUOUS_READ"},{"location":"DataStreamReader/#other-data-sources","text":"load creates a DataFrame with a StreamingRelation leaf logical operator.","title":"Other Data Sources"},{"location":"DataStreamWriter/","text":"DataStreamWriter \u00b6 DataStreamWriter is an interface that Spark developers use to describe when the result of executing a streaming query is sent out to a streaming data source . Accessing DataStreamWriter \u00b6 DataStreamWriter is available using Dataset.writeStream method. import org.apache.spark.sql.streaming.DataStreamWriter import org.apache.spark.sql.Row val streamingQuery: Dataset[Long] = ... assert(streamingQuery.isStreaming) val writer: DataStreamWriter[Row] = streamingQuery.writeStream Writing to ForeachWriter \u00b6 foreach ( writer : ForeachWriter [ T ]): DataStreamWriter [ T ] Sets ForeachWriter as responsible for streaming writes Writing Micro-Batches to ForeachBatchSink \u00b6 foreachBatch ( function : ( Dataset [ T ], Long ) => Unit ): DataStreamWriter [ T ] Sets the source as foreachBatch and creates a ForeachBatchSink to be responsible for streaming writes. SPARK-24565 As per SPARK-24565 Add API for in Structured Streaming for exposing output rows of each microbatch as a DataFrame , the purpose of the method is to expose the micro-batch output as a dataframe for the following: Pass the output rows of each batch to a library that is designed for the batch jobs only Reuse batch data sources for output whose streaming version does not exist Multi-writes where the output rows are written to multiple outputs by writing twice for every batch Streaming Sink by Name \u00b6 format ( source : String ): DataStreamWriter [ T ] Specifies the streaming sink by name ( alias ) Output Mode \u00b6 outputMode ( outputMode : OutputMode ): DataStreamWriter [ T ] outputMode ( outputMode : String ): DataStreamWriter [ T ] Specifies the OutputMode of the streaming query (what data is sent out to a streaming sink when there is new data available in streaming data sources ) Default: Append Partitioning Streaming Writes \u00b6 partitionBy ( colNames : String * ): DataStreamWriter [ T ] Query Name \u00b6 queryName ( queryName : String ): DataStreamWriter [ T ] Assigns the name of a query that is just an additional option with the key queryName . Starting Streaming Query (Streaming Writes) \u00b6 start (): StreamingQuery // Explicit `path` (that could also be specified as an option) start ( path : String ): StreamingQuery Creates and immediately starts a StreamingQuery that is returned as a handle to control the execution of the query Internally, start branches off per source . memory foreach other formats ...FIXME start throws an AnalysisException for source to be hive . val q = spark. readStream. text(\"server-logs/*\"). writeStream. format(\"hive\") <-- hive format used as a streaming sink scala> q.start org.apache.spark.sql.AnalysisException: Hive data source can only be used with tables, you can not write files of Hive data source directly.; at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:234) ... 48 elided Trigger \u00b6 trigger ( trigger : Trigger ): DataStreamWriter [ T ] Sets the Trigger for how often the streaming query should be executed Default: ProcessingTime(0L) that runs a streaming query as often as possible.","title":"DataStreamWriter"},{"location":"DataStreamWriter/#datastreamwriter","text":"DataStreamWriter is an interface that Spark developers use to describe when the result of executing a streaming query is sent out to a streaming data source .","title":"DataStreamWriter"},{"location":"DataStreamWriter/#accessing-datastreamwriter","text":"DataStreamWriter is available using Dataset.writeStream method. import org.apache.spark.sql.streaming.DataStreamWriter import org.apache.spark.sql.Row val streamingQuery: Dataset[Long] = ... assert(streamingQuery.isStreaming) val writer: DataStreamWriter[Row] = streamingQuery.writeStream","title":" Accessing DataStreamWriter"},{"location":"DataStreamWriter/#writing-to-foreachwriter","text":"foreach ( writer : ForeachWriter [ T ]): DataStreamWriter [ T ] Sets ForeachWriter as responsible for streaming writes","title":" Writing to ForeachWriter"},{"location":"DataStreamWriter/#writing-micro-batches-to-foreachbatchsink","text":"foreachBatch ( function : ( Dataset [ T ], Long ) => Unit ): DataStreamWriter [ T ] Sets the source as foreachBatch and creates a ForeachBatchSink to be responsible for streaming writes. SPARK-24565 As per SPARK-24565 Add API for in Structured Streaming for exposing output rows of each microbatch as a DataFrame , the purpose of the method is to expose the micro-batch output as a dataframe for the following: Pass the output rows of each batch to a library that is designed for the batch jobs only Reuse batch data sources for output whose streaming version does not exist Multi-writes where the output rows are written to multiple outputs by writing twice for every batch","title":" Writing Micro-Batches to ForeachBatchSink"},{"location":"DataStreamWriter/#streaming-sink-by-name","text":"format ( source : String ): DataStreamWriter [ T ] Specifies the streaming sink by name ( alias )","title":" Streaming Sink by Name"},{"location":"DataStreamWriter/#output-mode","text":"outputMode ( outputMode : OutputMode ): DataStreamWriter [ T ] outputMode ( outputMode : String ): DataStreamWriter [ T ] Specifies the OutputMode of the streaming query (what data is sent out to a streaming sink when there is new data available in streaming data sources ) Default: Append","title":" Output Mode"},{"location":"DataStreamWriter/#partitioning-streaming-writes","text":"partitionBy ( colNames : String * ): DataStreamWriter [ T ]","title":" Partitioning Streaming Writes"},{"location":"DataStreamWriter/#query-name","text":"queryName ( queryName : String ): DataStreamWriter [ T ] Assigns the name of a query that is just an additional option with the key queryName .","title":" Query Name"},{"location":"DataStreamWriter/#starting-streaming-query-streaming-writes","text":"start (): StreamingQuery // Explicit `path` (that could also be specified as an option) start ( path : String ): StreamingQuery Creates and immediately starts a StreamingQuery that is returned as a handle to control the execution of the query Internally, start branches off per source . memory foreach other formats ...FIXME start throws an AnalysisException for source to be hive . val q = spark. readStream. text(\"server-logs/*\"). writeStream. format(\"hive\") <-- hive format used as a streaming sink scala> q.start org.apache.spark.sql.AnalysisException: Hive data source can only be used with tables, you can not write files of Hive data source directly.; at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:234) ... 48 elided","title":" Starting Streaming Query (Streaming Writes)"},{"location":"DataStreamWriter/#trigger","text":"trigger ( trigger : Trigger ): DataStreamWriter [ T ] Sets the Trigger for how often the streaming query should be executed Default: ProcessingTime(0L) that runs a streaming query as often as possible.","title":" Trigger"},{"location":"EpochCoordinator/","text":"EpochCoordinator RPC Endpoint \u00b6 EpochCoordinator is a ThreadSafeRpcEndpoint that tracks offsets and epochs ( coordinates epochs ) by handling < > (in < > and < > modes) from...FIXME EpochCoordinator is < > (using < > factory method) when ContinuousExecution is requested to < >. [[messages]] [[EpochCoordinatorMessage]] .EpochCoordinator RPC Endpoint's Messages [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Message | Description a| CommitPartitionEpoch [[CommitPartitionEpoch-partitionId]] Partition ID [[CommitPartitionEpoch-epoch]] Epoch [[CommitPartitionEpoch-message]] DataSource API V2's WriterCommitMessage | [[CommitPartitionEpoch]] Sent out (in one-way asynchronous mode) exclusively when ContinuousWriteRDD is requested to < > (after all rows were written down to a streaming sink) | GetCurrentEpoch | [[GetCurrentEpoch]] Sent out (in request-response synchronous mode) exclusively when EpochMarkerGenerator thread is requested to < > | IncrementAndGetEpoch | [[IncrementAndGetEpoch]] Sent out (in request-response synchronous mode) exclusively when ContinuousExecution is requested to < > (and start a separate epoch update thread) a| ReportPartitionOffset [[ReportPartitionOffset-partitionId]] Partition ID [[ReportPartitionOffset-epoch]] Epoch [[ReportPartitionOffset-offset]] PartitionOffset | [[ReportPartitionOffset]] Sent out (in one-way asynchronous mode) exclusively when ContinuousQueuedDataReader is requested for the < > to be read in the current epoch, and the epoch is done a| SetReaderPartitions [[SetReaderPartitions-numPartitions]] Number of partitions | [[SetReaderPartitions]] Sent out (in request-response synchronous mode) exclusively when DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (for a ContinuousReader and is about to create a < >) The < > is exactly the number of InputPartitions from the ContinuousReader . a| SetWriterPartitions [[SetWriterPartitions-numPartitions]] Number of partitions | [[SetWriterPartitions]] Sent out (in request-response synchronous mode) exclusively when WriteToContinuousDataSourceExec leaf physical operator is requested to < > (and requests a < > to collect that simply never finishes...and that's the trick of continuous mode) a| StopContinuousExecutionWrites | [[StopContinuousExecutionWrites]] Sent out (in request-response synchronous mode) exclusively when ContinuousExecution is requested to < > (and it finishes successfully or not) |=== [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.continuous.EpochCoordinatorRef* logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.continuous.EpochCoordinatorRef*=ALL Refer to < >. \u00b6 === [[receive]] Receiving Messages (Fire-And-Forget One-Way Mode) -- receive Method [source, scala] \u00b6 receive: PartialFunction[Any, Unit] \u00b6 NOTE: receive is part of the RpcEndpoint Contract in Apache Spark to receive messages in fire-and-forget one-way mode. receive handles the following messages: < > < > With the < > turned on, receive simply swallows messages and does nothing. === [[receiveAndReply]] Receiving Messages (Request-Response Two-Way Mode) -- receiveAndReply Method [source, scala] \u00b6 receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] \u00b6 NOTE: receiveAndReply is part of the RpcEndpoint Contract in Apache Spark to receive and reply to messages in request-response two-way mode. receiveAndReply handles the following messages: < > < > < > < > < > ==== [[resolveCommitsAtEpoch]] resolveCommitsAtEpoch Internal Method [source, scala] \u00b6 resolveCommitsAtEpoch(epoch: Long): Unit \u00b6 resolveCommitsAtEpoch ...FIXME NOTE: resolveCommitsAtEpoch is used exclusively when EpochCoordinator is requested to handle < > and < > messages. ==== [[commitEpoch]] commitEpoch Internal Method [source, scala] \u00b6 commitEpoch( epoch: Long, messages: Iterable[WriterCommitMessage]): Unit commitEpoch ...FIXME NOTE: commitEpoch is used exclusively when EpochCoordinator is requested to < >. Creating Instance \u00b6 EpochCoordinator takes the following to be created: [[reader]] ContinuousReader [[query]] ContinuousExecution [[startEpoch]] Start epoch [[session]] SparkSession [[rpcEnv]] RpcEnv === [[create]] Registering EpochCoordinator RPC Endpoint -- create Factory Method [source, scala] \u00b6 create( writer: StreamWriter, reader: ContinuousReader, query: ContinuousExecution, epochCoordinatorId: String, startEpoch: Long, session: SparkSession, env: SparkEnv): RpcEndpointRef create simply < > and requests the RpcEnv to register a RPC endpoint as EpochCoordinator-[id] (where id is the given epochCoordinatorId ). create prints out the following INFO message to the logs: Registered EpochCoordinator endpoint NOTE: create is used exclusively when ContinuousExecution is requested to < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | queryWritesStopped | [[queryWritesStopped]] Flag that indicates whether to drop messages ( true ) or not ( false ) when requested to < > Default: false Turned on ( true ) when requested to < > |===","title":"EpochCoordinator RPC Endpoint"},{"location":"EpochCoordinator/#epochcoordinator-rpc-endpoint","text":"EpochCoordinator is a ThreadSafeRpcEndpoint that tracks offsets and epochs ( coordinates epochs ) by handling < > (in < > and < > modes) from...FIXME EpochCoordinator is < > (using < > factory method) when ContinuousExecution is requested to < >. [[messages]] [[EpochCoordinatorMessage]] .EpochCoordinator RPC Endpoint's Messages [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Message | Description a| CommitPartitionEpoch [[CommitPartitionEpoch-partitionId]] Partition ID [[CommitPartitionEpoch-epoch]] Epoch [[CommitPartitionEpoch-message]] DataSource API V2's WriterCommitMessage | [[CommitPartitionEpoch]] Sent out (in one-way asynchronous mode) exclusively when ContinuousWriteRDD is requested to < > (after all rows were written down to a streaming sink) | GetCurrentEpoch | [[GetCurrentEpoch]] Sent out (in request-response synchronous mode) exclusively when EpochMarkerGenerator thread is requested to < > | IncrementAndGetEpoch | [[IncrementAndGetEpoch]] Sent out (in request-response synchronous mode) exclusively when ContinuousExecution is requested to < > (and start a separate epoch update thread) a| ReportPartitionOffset [[ReportPartitionOffset-partitionId]] Partition ID [[ReportPartitionOffset-epoch]] Epoch [[ReportPartitionOffset-offset]] PartitionOffset | [[ReportPartitionOffset]] Sent out (in one-way asynchronous mode) exclusively when ContinuousQueuedDataReader is requested for the < > to be read in the current epoch, and the epoch is done a| SetReaderPartitions [[SetReaderPartitions-numPartitions]] Number of partitions | [[SetReaderPartitions]] Sent out (in request-response synchronous mode) exclusively when DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (for a ContinuousReader and is about to create a < >) The < > is exactly the number of InputPartitions from the ContinuousReader . a| SetWriterPartitions [[SetWriterPartitions-numPartitions]] Number of partitions | [[SetWriterPartitions]] Sent out (in request-response synchronous mode) exclusively when WriteToContinuousDataSourceExec leaf physical operator is requested to < > (and requests a < > to collect that simply never finishes...and that's the trick of continuous mode) a| StopContinuousExecutionWrites | [[StopContinuousExecutionWrites]] Sent out (in request-response synchronous mode) exclusively when ContinuousExecution is requested to < > (and it finishes successfully or not) |=== [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.continuous.EpochCoordinatorRef* logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.continuous.EpochCoordinatorRef*=ALL","title":"EpochCoordinator RPC Endpoint"},{"location":"EpochCoordinator/#refer-to","text":"=== [[receive]] Receiving Messages (Fire-And-Forget One-Way Mode) -- receive Method","title":"Refer to &lt;&gt;."},{"location":"EpochCoordinator/#source-scala","text":"","title":"[source, scala]"},{"location":"EpochCoordinator/#receive-partialfunctionany-unit","text":"NOTE: receive is part of the RpcEndpoint Contract in Apache Spark to receive messages in fire-and-forget one-way mode. receive handles the following messages: < > < > With the < > turned on, receive simply swallows messages and does nothing. === [[receiveAndReply]] Receiving Messages (Request-Response Two-Way Mode) -- receiveAndReply Method","title":"receive: PartialFunction[Any, Unit]"},{"location":"EpochCoordinator/#source-scala_1","text":"","title":"[source, scala]"},{"location":"EpochCoordinator/#receiveandreplycontext-rpccallcontext-partialfunctionany-unit","text":"NOTE: receiveAndReply is part of the RpcEndpoint Contract in Apache Spark to receive and reply to messages in request-response two-way mode. receiveAndReply handles the following messages: < > < > < > < > < > ==== [[resolveCommitsAtEpoch]] resolveCommitsAtEpoch Internal Method","title":"receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit]"},{"location":"EpochCoordinator/#source-scala_2","text":"","title":"[source, scala]"},{"location":"EpochCoordinator/#resolvecommitsatepochepoch-long-unit","text":"resolveCommitsAtEpoch ...FIXME NOTE: resolveCommitsAtEpoch is used exclusively when EpochCoordinator is requested to handle < > and < > messages. ==== [[commitEpoch]] commitEpoch Internal Method","title":"resolveCommitsAtEpoch(epoch: Long): Unit"},{"location":"EpochCoordinator/#source-scala_3","text":"commitEpoch( epoch: Long, messages: Iterable[WriterCommitMessage]): Unit commitEpoch ...FIXME NOTE: commitEpoch is used exclusively when EpochCoordinator is requested to < >.","title":"[source, scala]"},{"location":"EpochCoordinator/#creating-instance","text":"EpochCoordinator takes the following to be created: [[reader]] ContinuousReader [[query]] ContinuousExecution [[startEpoch]] Start epoch [[session]] SparkSession [[rpcEnv]] RpcEnv === [[create]] Registering EpochCoordinator RPC Endpoint -- create Factory Method","title":"Creating Instance"},{"location":"EpochCoordinator/#source-scala_4","text":"create( writer: StreamWriter, reader: ContinuousReader, query: ContinuousExecution, epochCoordinatorId: String, startEpoch: Long, session: SparkSession, env: SparkEnv): RpcEndpointRef create simply < > and requests the RpcEnv to register a RPC endpoint as EpochCoordinator-[id] (where id is the given epochCoordinatorId ). create prints out the following INFO message to the logs: Registered EpochCoordinator endpoint NOTE: create is used exclusively when ContinuousExecution is requested to < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | queryWritesStopped | [[queryWritesStopped]] Flag that indicates whether to drop messages ( true ) or not ( false ) when requested to < > Default: false Turned on ( true ) when requested to < > |===","title":"[source, scala]"},{"location":"EpochCoordinatorRef/","text":"== [[EpochCoordinatorRef]] EpochCoordinatorRef EpochCoordinatorRef is...FIXME === [[create]] Creating Remote Reference to EpochCoordinator RPC Endpoint -- create Factory Method [source, scala] \u00b6 create( writer: StreamWriter, reader: ContinuousReader, query: ContinuousExecution, epochCoordinatorId: String, startEpoch: Long, session: SparkSession, env: SparkEnv): RpcEndpointRef create ...FIXME NOTE: create is used exclusively when ContinuousExecution is requested to < >. === [[get]] Getting Remote Reference to EpochCoordinator RPC Endpoint -- get Factory Method [source, scala] \u00b6 get(id: String, env: SparkEnv): RpcEndpointRef \u00b6 get ...FIXME [NOTE] \u00b6 get is used when: DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (and creates a < > for a ContinuousReader ) ContinuousQueuedDataReader is created (and initializes the < >) EpochMarkerGenerator is created (and initializes the < >) ContinuousWriteRDD is requested to < > * WriteToContinuousDataSourceExec is requested to < > \u00b6","title":"EpochCoordinatorRef"},{"location":"EpochCoordinatorRef/#source-scala","text":"create( writer: StreamWriter, reader: ContinuousReader, query: ContinuousExecution, epochCoordinatorId: String, startEpoch: Long, session: SparkSession, env: SparkEnv): RpcEndpointRef create ...FIXME NOTE: create is used exclusively when ContinuousExecution is requested to < >. === [[get]] Getting Remote Reference to EpochCoordinator RPC Endpoint -- get Factory Method","title":"[source, scala]"},{"location":"EpochCoordinatorRef/#source-scala_1","text":"","title":"[source, scala]"},{"location":"EpochCoordinatorRef/#getid-string-env-sparkenv-rpcendpointref","text":"get ...FIXME","title":"get(id: String, env: SparkEnv): RpcEndpointRef"},{"location":"EpochCoordinatorRef/#note","text":"get is used when: DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (and creates a < > for a ContinuousReader ) ContinuousQueuedDataReader is created (and initializes the < >) EpochMarkerGenerator is created (and initializes the < >) ContinuousWriteRDD is requested to < >","title":"[NOTE]"},{"location":"EpochCoordinatorRef/#writetocontinuousdatasourceexec-is-requested-to","text":"","title":"* WriteToContinuousDataSourceExec is requested to &lt;&gt;"},{"location":"EpochMarkerGenerator/","text":"== [[EpochMarkerGenerator]] EpochMarkerGenerator Thread EpochMarkerGenerator is...FIXME === [[run]] run Method [source, scala] \u00b6 run(): Unit \u00b6 NOTE: run is part of the https://docs.oracle.com/javase/8/docs/api/java/lang/Runnable.html[java.lang.Runnable ] Contract to be executed upon starting a thread. run ...FIXME","title":"EpochMarkerGenerator"},{"location":"EpochMarkerGenerator/#source-scala","text":"","title":"[source, scala]"},{"location":"EpochMarkerGenerator/#run-unit","text":"NOTE: run is part of the https://docs.oracle.com/javase/8/docs/api/java/lang/Runnable.html[java.lang.Runnable ] Contract to be executed upon starting a thread. run ...FIXME","title":"run(): Unit"},{"location":"EpochTracker/","text":"== [[EpochTracker]] EpochTracker EpochTracker is...FIXME === [[getCurrentEpoch]] Current Epoch -- getCurrentEpoch Method [source, scala] \u00b6 getCurrentEpoch: Option[Long] \u00b6 getCurrentEpoch ...FIXME NOTE: getCurrentEpoch is used when...FIXME === [[incrementCurrentEpoch]] Advancing (Incrementing) Epoch -- incrementCurrentEpoch Method [source, scala] \u00b6 incrementCurrentEpoch(): Unit \u00b6 incrementCurrentEpoch ...FIXME NOTE: incrementCurrentEpoch is used when...FIXME","title":"EpochTracker"},{"location":"EpochTracker/#source-scala","text":"","title":"[source, scala]"},{"location":"EpochTracker/#getcurrentepoch-optionlong","text":"getCurrentEpoch ...FIXME NOTE: getCurrentEpoch is used when...FIXME === [[incrementCurrentEpoch]] Advancing (Incrementing) Epoch -- incrementCurrentEpoch Method","title":"getCurrentEpoch: Option[Long]"},{"location":"EpochTracker/#source-scala_1","text":"","title":"[source, scala]"},{"location":"EpochTracker/#incrementcurrentepoch-unit","text":"incrementCurrentEpoch ...FIXME NOTE: incrementCurrentEpoch is used when...FIXME","title":"incrementCurrentEpoch(): Unit"},{"location":"EventTimeStats/","text":"EventTimeStats \u00b6 EventTimeStats is used to help calculate event-time column statistics (statistics of the values of an event-time column): Maximum Minimum Average Count EventTimeStats is used by EventTimeStatsAccum accumulator. Zero Value \u00b6 EventTimeStats defines a special value zero with the following values: Long.MinValue for the max Long.MaxValue for the min 0.0 for the avg 0L for the count Adding Event-Time Value \u00b6 add ( eventTime : Long ): Unit add updates the statistics given the eventTime value. Merging EventTimeStats \u00b6 merge ( that : EventTimeStats ): Unit merge ...FIXME","title":"EventTimeStats"},{"location":"EventTimeStats/#eventtimestats","text":"EventTimeStats is used to help calculate event-time column statistics (statistics of the values of an event-time column): Maximum Minimum Average Count EventTimeStats is used by EventTimeStatsAccum accumulator.","title":"EventTimeStats"},{"location":"EventTimeStats/#zero-value","text":"EventTimeStats defines a special value zero with the following values: Long.MinValue for the max Long.MaxValue for the min 0.0 for the avg 0L for the count","title":" Zero Value"},{"location":"EventTimeStats/#adding-event-time-value","text":"add ( eventTime : Long ): Unit add updates the statistics given the eventTime value.","title":" Adding Event-Time Value"},{"location":"EventTimeStats/#merging-eventtimestats","text":"merge ( that : EventTimeStats ): Unit merge ...FIXME","title":" Merging EventTimeStats"},{"location":"EventTimeStatsAccum/","text":"EventTimeStatsAccum Accumulator \u00b6 EventTimeStatsAccum is an AccumulatorV2 ( Spark Core ) that accumulates Long values and produces an EventTimeStats . AccumulatorV2 [ Long , EventTimeStats ] Creating Instance \u00b6 EventTimeStatsAccum takes the following to be created: EventTimeStats (default: EventTimeStats.zero ) EventTimeStatsAccum is created when: EventTimeWatermarkExec unary physical operator is created (and initializes eventTimeStats ) EventTimeWatermarkExec Physical Operator When EventTimeWatermarkExec physical operator is requested to execute, every task simply adds the values of the event-time watermark column expression to the EventTimeStatsAccum accumulator. As per design of Spark accumulators in Apache Spark, accumulator updates are automatically sent out ( propagated ) from tasks to the driver every heartbeat and then they are accumulated together. EventTimeStats \u00b6 EventTimeStatsAccum is given an EventTimeStats when created . Every time AccumulatorV2 methods are called, EventTimeStatsAccum simply relays them to the EventTimeStats (that is responsible for event-time statistics, i.e. max, min, avg, count). Adding Value \u00b6 add ( v : Long ): Unit add is part of the AccumulatorV2 ( Spark Core ) abstraction. add simply requests the EventTimeStats to add the given v value. add is used when EventTimeWatermarkExec physical operator is executed.","title":"EventTimeStatsAccum"},{"location":"EventTimeStatsAccum/#eventtimestatsaccum-accumulator","text":"EventTimeStatsAccum is an AccumulatorV2 ( Spark Core ) that accumulates Long values and produces an EventTimeStats . AccumulatorV2 [ Long , EventTimeStats ]","title":"EventTimeStatsAccum Accumulator"},{"location":"EventTimeStatsAccum/#creating-instance","text":"EventTimeStatsAccum takes the following to be created: EventTimeStats (default: EventTimeStats.zero ) EventTimeStatsAccum is created when: EventTimeWatermarkExec unary physical operator is created (and initializes eventTimeStats ) EventTimeWatermarkExec Physical Operator When EventTimeWatermarkExec physical operator is requested to execute, every task simply adds the values of the event-time watermark column expression to the EventTimeStatsAccum accumulator. As per design of Spark accumulators in Apache Spark, accumulator updates are automatically sent out ( propagated ) from tasks to the driver every heartbeat and then they are accumulated together.","title":"Creating Instance"},{"location":"EventTimeStatsAccum/#eventtimestats","text":"EventTimeStatsAccum is given an EventTimeStats when created . Every time AccumulatorV2 methods are called, EventTimeStatsAccum simply relays them to the EventTimeStats (that is responsible for event-time statistics, i.e. max, min, avg, count).","title":" EventTimeStats"},{"location":"EventTimeStatsAccum/#adding-value","text":"add ( v : Long ): Unit add is part of the AccumulatorV2 ( Spark Core ) abstraction. add simply requests the EventTimeStats to add the given v value. add is used when EventTimeWatermarkExec physical operator is executed.","title":" Adding Value"},{"location":"FileContextBasedCheckpointFileManager/","text":"== [[FileContextBasedCheckpointFileManager]] FileContextBasedCheckpointFileManager FileContextBasedCheckpointFileManager is...FIXME","title":"FileContextBasedCheckpointFileManager"},{"location":"FileSystemBasedCheckpointFileManager/","text":"FileSystemBasedCheckpointFileManager \u00b6 [[CheckpointFileManager]] FileSystemBasedCheckpointFileManager is a CheckpointFileManager that uses Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem ] API for managing checkpoint files: list uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#listStatus(org.apache.hadoop.fs.Path[],%20org.apache.hadoop.fs.PathFilter)++[FileSystem.listStatus ] mkdirs uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#mkdirs(org.apache.hadoop.fs.Path,%20org.apache.hadoop.fs.permission.FsPermission)++[FileSystem.mkdirs ] createTempFile uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#create(org.apache.hadoop.fs.Path,%20boolean)++[FileSystem.create ] (with overwrite enabled) [[createAtomic]] createAtomic uses RenameBasedFSDataOutputStream open uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#open(org.apache.hadoop.fs.Path)++[FileSystem.open ] exists uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#getFileStatus(org.apache.hadoop.fs.Path)++[FileSystem.getFileStatus ] renameTempFile uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#rename(org.apache.hadoop.fs.Path,%20org.apache.hadoop.fs.Path)++[FileSystem.rename ] delete uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#delete(org.apache.hadoop.fs.Path,%20boolean)++[FileSystem.delete ] (with recursive enabled) isLocal is true for the < > being LocalFileSystem or RawLocalFileSystem FileSystemBasedCheckpointFileManager is < > exclusively when CheckpointFileManager helper object is requested for a CheckpointFileManager (for HDFSMetadataLog , StreamMetadata and HDFSBackedStateStoreProvider ). [[RenameHelperMethods]] FileSystemBasedCheckpointFileManager is a RenameHelperMethods for < > by \"write-to-temp-file-and-rename\". === [[creating-instance]] Creating FileSystemBasedCheckpointFileManager Instance FileSystemBasedCheckpointFileManager takes the following to be created: [[path]] Checkpoint directory (Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/Path.html[Path ]) [[hadoopConf]] Configuration (Hadoop's http://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ]) FileSystemBasedCheckpointFileManager initializes the < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | fs a| [[fs]] Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem ] of the < > |===","title":"FileSystemBasedCheckpointFileManager"},{"location":"FileSystemBasedCheckpointFileManager/#filesystembasedcheckpointfilemanager","text":"[[CheckpointFileManager]] FileSystemBasedCheckpointFileManager is a CheckpointFileManager that uses Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem ] API for managing checkpoint files: list uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#listStatus(org.apache.hadoop.fs.Path[],%20org.apache.hadoop.fs.PathFilter)++[FileSystem.listStatus ] mkdirs uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#mkdirs(org.apache.hadoop.fs.Path,%20org.apache.hadoop.fs.permission.FsPermission)++[FileSystem.mkdirs ] createTempFile uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#create(org.apache.hadoop.fs.Path,%20boolean)++[FileSystem.create ] (with overwrite enabled) [[createAtomic]] createAtomic uses RenameBasedFSDataOutputStream open uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#open(org.apache.hadoop.fs.Path)++[FileSystem.open ] exists uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#getFileStatus(org.apache.hadoop.fs.Path)++[FileSystem.getFileStatus ] renameTempFile uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#rename(org.apache.hadoop.fs.Path,%20org.apache.hadoop.fs.Path)++[FileSystem.rename ] delete uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#delete(org.apache.hadoop.fs.Path,%20boolean)++[FileSystem.delete ] (with recursive enabled) isLocal is true for the < > being LocalFileSystem or RawLocalFileSystem FileSystemBasedCheckpointFileManager is < > exclusively when CheckpointFileManager helper object is requested for a CheckpointFileManager (for HDFSMetadataLog , StreamMetadata and HDFSBackedStateStoreProvider ). [[RenameHelperMethods]] FileSystemBasedCheckpointFileManager is a RenameHelperMethods for < > by \"write-to-temp-file-and-rename\". === [[creating-instance]] Creating FileSystemBasedCheckpointFileManager Instance FileSystemBasedCheckpointFileManager takes the following to be created: [[path]] Checkpoint directory (Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/Path.html[Path ]) [[hadoopConf]] Configuration (Hadoop's http://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ]) FileSystemBasedCheckpointFileManager initializes the < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | fs a| [[fs]] Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem ] of the < > |===","title":"FileSystemBasedCheckpointFileManager"},{"location":"FlatMapGroupsWithStateStrategy/","text":"FlatMapGroupsWithStateStrategy Execution Planning Strategy \u00b6 FlatMapGroupsWithStateStrategy is an execution planning strategy that can plan streaming queries with FlatMapGroupsWithState unary logical operators to FlatMapGroupsWithStateExec physical operator (with undefined StatefulOperatorStateInfo , batchTimestampMs , and eventTimeWatermark ). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy.html[Execution Planning Strategies] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. FlatMapGroupsWithStateStrategy is used exclusively when IncrementalExecution is requested to plan a streaming query. Demo \u00b6 import org.apache.spark.sql.streaming.GroupState val stateFunc = (key: Long, values: Iterator[(Timestamp, Long)], state: GroupState[Long]) => { Iterator((key, values.size)) } import java.sql.Timestamp import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode} val numGroups = spark. readStream. format(\"rate\"). load. as[(Timestamp, Long)]. groupByKey { case (time, value) => value % 2 }. flatMapGroupsWithState(OutputMode.Update, GroupStateTimeout.NoTimeout)(stateFunc) scala> numGroups.explain(true) == Parsed Logical Plan == 'SerializeFromObject [assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#267L, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2 AS _2#268] +- 'FlatMapGroupsWithState <function3>, unresolveddeserializer(upcast(getcolumnbyordinal(0, LongType), LongType, - root class: \"scala.Long\"), value#262L), unresolveddeserializer(newInstance(class scala.Tuple2), timestamp#253, value#254L), [value#262L], [timestamp#253, value#254L], obj#266: scala.Tuple2, class[value[0]: bigint], Update, false, NoTimeout +- AppendColumns <function1>, class scala.Tuple2, [StructField(_1,TimestampType,true), StructField(_2,LongType,false)], newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#262L] +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@38bcac50,rate,List(),None,List(),None,Map(),None), rate, [timestamp#253, value#254L] ... == Physical Plan == *SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#267L, assertnotnull(input[0, scala.Tuple2, true])._2 AS _2#268] +- FlatMapGroupsWithState <function3>, value#262: bigint, newInstance(class scala.Tuple2), [value#262L], [timestamp#253, value#254L], obj#266: scala.Tuple2, StatefulOperatorStateInfo(<unknown>,84b5dccb-3fa6-4343-a99c-6fa5490c9b33,0,0), class[value[0]: bigint], Update, NoTimeout, 0, 0 +- *Sort [value#262L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(value#262L, 200) +- AppendColumns <function1>, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#262L] +- StreamingRelation rate, [timestamp#253, value#254L]","title":"FlatMapGroupsWithStateStrategy"},{"location":"FlatMapGroupsWithStateStrategy/#flatmapgroupswithstatestrategy-execution-planning-strategy","text":"FlatMapGroupsWithStateStrategy is an execution planning strategy that can plan streaming queries with FlatMapGroupsWithState unary logical operators to FlatMapGroupsWithStateExec physical operator (with undefined StatefulOperatorStateInfo , batchTimestampMs , and eventTimeWatermark ). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy.html[Execution Planning Strategies] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. FlatMapGroupsWithStateStrategy is used exclusively when IncrementalExecution is requested to plan a streaming query.","title":"FlatMapGroupsWithStateStrategy Execution Planning Strategy"},{"location":"FlatMapGroupsWithStateStrategy/#demo","text":"import org.apache.spark.sql.streaming.GroupState val stateFunc = (key: Long, values: Iterator[(Timestamp, Long)], state: GroupState[Long]) => { Iterator((key, values.size)) } import java.sql.Timestamp import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode} val numGroups = spark. readStream. format(\"rate\"). load. as[(Timestamp, Long)]. groupByKey { case (time, value) => value % 2 }. flatMapGroupsWithState(OutputMode.Update, GroupStateTimeout.NoTimeout)(stateFunc) scala> numGroups.explain(true) == Parsed Logical Plan == 'SerializeFromObject [assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#267L, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2 AS _2#268] +- 'FlatMapGroupsWithState <function3>, unresolveddeserializer(upcast(getcolumnbyordinal(0, LongType), LongType, - root class: \"scala.Long\"), value#262L), unresolveddeserializer(newInstance(class scala.Tuple2), timestamp#253, value#254L), [value#262L], [timestamp#253, value#254L], obj#266: scala.Tuple2, class[value[0]: bigint], Update, false, NoTimeout +- AppendColumns <function1>, class scala.Tuple2, [StructField(_1,TimestampType,true), StructField(_2,LongType,false)], newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#262L] +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@38bcac50,rate,List(),None,List(),None,Map(),None), rate, [timestamp#253, value#254L] ... == Physical Plan == *SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#267L, assertnotnull(input[0, scala.Tuple2, true])._2 AS _2#268] +- FlatMapGroupsWithState <function3>, value#262: bigint, newInstance(class scala.Tuple2), [value#262L], [timestamp#253, value#254L], obj#266: scala.Tuple2, StatefulOperatorStateInfo(<unknown>,84b5dccb-3fa6-4343-a99c-6fa5490c9b33,0,0), class[value[0]: bigint], Update, NoTimeout, 0, 0 +- *Sort [value#262L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(value#262L, 200) +- AppendColumns <function1>, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#262L] +- StreamingRelation rate, [timestamp#253, value#254L]","title":"Demo"},{"location":"GroupState/","text":"GroupState \u00b6 GroupState is an < > of < > (of type S ) in Arbitrary Stateful Streaming Aggregation . GroupState is used with the following KeyValueGroupedDataset operations: mapGroupsWithState flatMapGroupsWithState GroupState is created separately for every aggregation key to hold a state as an aggregation state value . [[contract]] .GroupState Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | exists a| [[exists]] [source, scala] \u00b6 exists: Boolean \u00b6 Checks whether the state value exists or not If not exists, < > throws a NoSuchElementException . Use < > instead. | get a| [[get]] [source, scala] \u00b6 get: S \u00b6 Gets the state value if it < > or throws a NoSuchElementException | getCurrentProcessingTimeMs a| [[getCurrentProcessingTimeMs]] [source, scala] \u00b6 getCurrentProcessingTimeMs(): Long \u00b6 Gets the current processing time (as milliseconds in epoch time) | getCurrentWatermarkMs a| [[getCurrentWatermarkMs]] [source, scala] \u00b6 getCurrentWatermarkMs(): Long \u00b6 Gets the current event time watermark (as milliseconds in epoch time) | getOption a| [[getOption]] [source, scala] \u00b6 getOption: Option[S] \u00b6 Gets the state value as a Scala Option (regardless whether it < > or not) Used when: InputProcessor is requested to callFunctionAndUpdateState (when the row iterator is consumed and a state value has been updated, removed or timeout changed) GroupStateImpl is requested for the textual representation | hasTimedOut a| [[hasTimedOut]] [source, scala] \u00b6 hasTimedOut: Boolean \u00b6 Whether the state (for a given key) has timed out or not. Can only be true when timeouts are enabled using < > | remove a| [[remove]] [source, scala] \u00b6 remove(): Unit \u00b6 Removes the state | setTimeoutDuration a| [[setTimeoutDuration]] [source, scala] \u00b6 setTimeoutDuration(durationMs: Long): Unit setTimeoutDuration(duration: String): Unit Specifies the timeout duration for the state key (in millis or as a string, e.g. \"10 seconds\", \"1 hour\") for GroupStateTimeout.ProcessingTimeTimeout | setTimeoutTimestamp a| [[setTimeoutTimestamp]] [source, scala] \u00b6 setTimeoutTimestamp(timestamp: java.sql.Date): Unit setTimeoutTimestamp( timestamp: java.sql.Date, additionalDuration: String): Unit setTimeoutTimestamp(timestampMs: Long): Unit setTimeoutTimestamp( timestampMs: Long, additionalDuration: String): Unit Specifies the timeout timestamp for the state key for GroupStateTimeout.EventTimeTimeout | update a| [[update]] [source, scala] \u00b6 update(newState: S): Unit \u00b6 Updates the state (sets the state to a new value) |=== [[implementations]] GroupStateImpl is the default and only known implementation of the < > in Spark Structured Streaming.","title":"GroupState"},{"location":"GroupState/#groupstate","text":"GroupState is an < > of < > (of type S ) in Arbitrary Stateful Streaming Aggregation . GroupState is used with the following KeyValueGroupedDataset operations: mapGroupsWithState flatMapGroupsWithState GroupState is created separately for every aggregation key to hold a state as an aggregation state value . [[contract]] .GroupState Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | exists a| [[exists]]","title":"GroupState"},{"location":"GroupState/#source-scala","text":"","title":"[source, scala]"},{"location":"GroupState/#exists-boolean","text":"Checks whether the state value exists or not If not exists, < > throws a NoSuchElementException . Use < > instead. | get a| [[get]]","title":"exists: Boolean"},{"location":"GroupState/#source-scala_1","text":"","title":"[source, scala]"},{"location":"GroupState/#get-s","text":"Gets the state value if it < > or throws a NoSuchElementException | getCurrentProcessingTimeMs a| [[getCurrentProcessingTimeMs]]","title":"get: S"},{"location":"GroupState/#source-scala_2","text":"","title":"[source, scala]"},{"location":"GroupState/#getcurrentprocessingtimems-long","text":"Gets the current processing time (as milliseconds in epoch time) | getCurrentWatermarkMs a| [[getCurrentWatermarkMs]]","title":"getCurrentProcessingTimeMs(): Long"},{"location":"GroupState/#source-scala_3","text":"","title":"[source, scala]"},{"location":"GroupState/#getcurrentwatermarkms-long","text":"Gets the current event time watermark (as milliseconds in epoch time) | getOption a| [[getOption]]","title":"getCurrentWatermarkMs(): Long"},{"location":"GroupState/#source-scala_4","text":"","title":"[source, scala]"},{"location":"GroupState/#getoption-options","text":"Gets the state value as a Scala Option (regardless whether it < > or not) Used when: InputProcessor is requested to callFunctionAndUpdateState (when the row iterator is consumed and a state value has been updated, removed or timeout changed) GroupStateImpl is requested for the textual representation | hasTimedOut a| [[hasTimedOut]]","title":"getOption: Option[S]"},{"location":"GroupState/#source-scala_5","text":"","title":"[source, scala]"},{"location":"GroupState/#hastimedout-boolean","text":"Whether the state (for a given key) has timed out or not. Can only be true when timeouts are enabled using < > | remove a| [[remove]]","title":"hasTimedOut: Boolean"},{"location":"GroupState/#source-scala_6","text":"","title":"[source, scala]"},{"location":"GroupState/#remove-unit","text":"Removes the state | setTimeoutDuration a| [[setTimeoutDuration]]","title":"remove(): Unit"},{"location":"GroupState/#source-scala_7","text":"setTimeoutDuration(durationMs: Long): Unit setTimeoutDuration(duration: String): Unit Specifies the timeout duration for the state key (in millis or as a string, e.g. \"10 seconds\", \"1 hour\") for GroupStateTimeout.ProcessingTimeTimeout | setTimeoutTimestamp a| [[setTimeoutTimestamp]]","title":"[source, scala]"},{"location":"GroupState/#source-scala_8","text":"setTimeoutTimestamp(timestamp: java.sql.Date): Unit setTimeoutTimestamp( timestamp: java.sql.Date, additionalDuration: String): Unit setTimeoutTimestamp(timestampMs: Long): Unit setTimeoutTimestamp( timestampMs: Long, additionalDuration: String): Unit Specifies the timeout timestamp for the state key for GroupStateTimeout.EventTimeTimeout | update a| [[update]]","title":"[source, scala]"},{"location":"GroupState/#source-scala_9","text":"","title":"[source, scala]"},{"location":"GroupState/#updatenewstate-s-unit","text":"Updates the state (sets the state to a new value) |=== [[implementations]] GroupStateImpl is the default and only known implementation of the < > in Spark Structured Streaming.","title":"update(newState: S): Unit"},{"location":"GroupStateImpl/","text":"GroupStateImpl \u00b6 GroupStateImpl is the default and only known GroupState in Spark Structured Streaming. GroupStateImpl holds per-group < > of type S per group key. GroupStateImpl is < > when GroupStateImpl helper object is requested for the following: < > < > Creating Instance \u00b6 GroupStateImpl takes the following to be created: [[optionalValue]] State value (of type S ) [[batchProcessingTimeMs]] Batch processing time [[eventTimeWatermarkMs]] eventTimeWatermarkMs [[timeoutConf]] GroupStateTimeout [[hasTimedOut]] hasTimedOut flag [[watermarkPresent]] watermarkPresent flag === [[createForStreaming]] Creating GroupStateImpl for Streaming Query -- createForStreaming Factory Method [source, scala] \u00b6 createForStreaming S : GroupStateImpl[S] createForStreaming simply creates a < > with the given input arguments. NOTE: createForStreaming is used exclusively when InputProcessor is requested to callFunctionAndUpdateState (when InputProcessor is requested to processNewData and processTimedOutState ). === [[createForBatch]] Creating GroupStateImpl for Batch Query -- createForBatch Factory Method [source, scala] \u00b6 createForBatch( timeoutConf: GroupStateTimeout, watermarkPresent: Boolean): GroupStateImpl[Any] createForBatch ...FIXME NOTE: createForBatch is used when...FIXME === [[toString]] Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString ...FIXME === [[setTimeoutDuration]] Specifying Timeout Duration for ProcessingTimeTimeout -- setTimeoutDuration Method [source, scala] \u00b6 setTimeoutDuration(durationMs: Long): Unit \u00b6 setTimeoutDuration ...FIXME setTimeoutDuration is part of the GroupState abstraction. === [[setTimeoutTimestamp]] Specifying Timeout Timestamp for EventTimeTimeout -- setTimeoutTimestamp Method [source, scala] \u00b6 setTimeoutTimestamp(durationMs: Long): Unit \u00b6 setTimeoutTimestamp ...FIXME setTimeoutTimestamp is part of the GroupState abstraction. === [[getCurrentProcessingTimeMs]] Getting Processing Time -- getCurrentProcessingTimeMs Method [source, scala] \u00b6 getCurrentProcessingTimeMs(): Long \u00b6 getCurrentProcessingTimeMs simply returns the < >. getCurrentProcessingTimeMs is part of the GroupState abstraction. === [[update]] Updating State -- update Method [source, scala] \u00b6 update(newValue: S): Unit \u00b6 update ...FIXME update is part of the GroupState abstraction. === [[remove]] Removing State -- remove Method [source, scala] \u00b6 remove(): Unit \u00b6 remove ...FIXME remove is part of the GroupState abstraction. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | value a| [[value]] FIXME Used when...FIXME | defined a| [[defined]] FIXME Used when...FIXME | updated a| [[updated]][[hasUpdated]] Updated flag that says whether the state has been < > or not Default: false Disabled ( false ) when GroupStateImpl is requested to < > Enabled ( true ) when GroupStateImpl is requested to < > | removed a| [[removed]][[hasRemoved]] Removed flag that says whether the state is marked < > or not Default: false Disabled ( false ) when GroupStateImpl is requested to < > Enabled ( true ) when GroupStateImpl is requested to < > | timeoutTimestamp a| [[timeoutTimestamp]][[getTimeoutTimestamp]] Current timeout timestamp (in millis) for GroupStateTimeout.EventTimeTimeout or GroupStateTimeout.ProcessingTimeTimeout [[NO_TIMESTAMP]] Default: -1 Defined using < > (for EventTimeTimeout ) and < > (for ProcessingTimeTimeout ) |===","title":"GroupStateImpl"},{"location":"GroupStateImpl/#groupstateimpl","text":"GroupStateImpl is the default and only known GroupState in Spark Structured Streaming. GroupStateImpl holds per-group < > of type S per group key. GroupStateImpl is < > when GroupStateImpl helper object is requested for the following: < > < >","title":"GroupStateImpl"},{"location":"GroupStateImpl/#creating-instance","text":"GroupStateImpl takes the following to be created: [[optionalValue]] State value (of type S ) [[batchProcessingTimeMs]] Batch processing time [[eventTimeWatermarkMs]] eventTimeWatermarkMs [[timeoutConf]] GroupStateTimeout [[hasTimedOut]] hasTimedOut flag [[watermarkPresent]] watermarkPresent flag === [[createForStreaming]] Creating GroupStateImpl for Streaming Query -- createForStreaming Factory Method","title":"Creating Instance"},{"location":"GroupStateImpl/#source-scala","text":"createForStreaming S : GroupStateImpl[S] createForStreaming simply creates a < > with the given input arguments. NOTE: createForStreaming is used exclusively when InputProcessor is requested to callFunctionAndUpdateState (when InputProcessor is requested to processNewData and processTimedOutState ). === [[createForBatch]] Creating GroupStateImpl for Batch Query -- createForBatch Factory Method","title":"[source, scala]"},{"location":"GroupStateImpl/#source-scala_1","text":"createForBatch( timeoutConf: GroupStateTimeout, watermarkPresent: Boolean): GroupStateImpl[Any] createForBatch ...FIXME NOTE: createForBatch is used when...FIXME === [[toString]] Textual Representation -- toString Method","title":"[source, scala]"},{"location":"GroupStateImpl/#source-scala_2","text":"","title":"[source, scala]"},{"location":"GroupStateImpl/#tostring-string","text":"NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString ...FIXME === [[setTimeoutDuration]] Specifying Timeout Duration for ProcessingTimeTimeout -- setTimeoutDuration Method","title":"toString: String"},{"location":"GroupStateImpl/#source-scala_3","text":"","title":"[source, scala]"},{"location":"GroupStateImpl/#settimeoutdurationdurationms-long-unit","text":"setTimeoutDuration ...FIXME setTimeoutDuration is part of the GroupState abstraction. === [[setTimeoutTimestamp]] Specifying Timeout Timestamp for EventTimeTimeout -- setTimeoutTimestamp Method","title":"setTimeoutDuration(durationMs: Long): Unit"},{"location":"GroupStateImpl/#source-scala_4","text":"","title":"[source, scala]"},{"location":"GroupStateImpl/#settimeouttimestampdurationms-long-unit","text":"setTimeoutTimestamp ...FIXME setTimeoutTimestamp is part of the GroupState abstraction. === [[getCurrentProcessingTimeMs]] Getting Processing Time -- getCurrentProcessingTimeMs Method","title":"setTimeoutTimestamp(durationMs: Long): Unit"},{"location":"GroupStateImpl/#source-scala_5","text":"","title":"[source, scala]"},{"location":"GroupStateImpl/#getcurrentprocessingtimems-long","text":"getCurrentProcessingTimeMs simply returns the < >. getCurrentProcessingTimeMs is part of the GroupState abstraction. === [[update]] Updating State -- update Method","title":"getCurrentProcessingTimeMs(): Long"},{"location":"GroupStateImpl/#source-scala_6","text":"","title":"[source, scala]"},{"location":"GroupStateImpl/#updatenewvalue-s-unit","text":"update ...FIXME update is part of the GroupState abstraction. === [[remove]] Removing State -- remove Method","title":"update(newValue: S): Unit"},{"location":"GroupStateImpl/#source-scala_7","text":"","title":"[source, scala]"},{"location":"GroupStateImpl/#remove-unit","text":"remove ...FIXME remove is part of the GroupState abstraction. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | value a| [[value]] FIXME Used when...FIXME | defined a| [[defined]] FIXME Used when...FIXME | updated a| [[updated]][[hasUpdated]] Updated flag that says whether the state has been < > or not Default: false Disabled ( false ) when GroupStateImpl is requested to < > Enabled ( true ) when GroupStateImpl is requested to < > | removed a| [[removed]][[hasRemoved]] Removed flag that says whether the state is marked < > or not Default: false Disabled ( false ) when GroupStateImpl is requested to < > Enabled ( true ) when GroupStateImpl is requested to < > | timeoutTimestamp a| [[timeoutTimestamp]][[getTimeoutTimestamp]] Current timeout timestamp (in millis) for GroupStateTimeout.EventTimeTimeout or GroupStateTimeout.ProcessingTimeTimeout [[NO_TIMESTAMP]] Default: -1 Defined using < > (for EventTimeTimeout ) and < > (for ProcessingTimeTimeout ) |===","title":"remove(): Unit"},{"location":"GroupStateTimeout/","text":"GroupStateTimeout \u00b6 GroupStateTimeout represents an aggregation state timeout that defines when a GroupState can be considered timed-out ( expired ) in Arbitrary Stateful Streaming Aggregation . GroupStateTimeout is used with the following KeyValueGroupedDataset operations: mapGroupsWithState flatMapGroupsWithState [[extensions]] .GroupStateTimeouts [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | GroupStateTimeout | Description | EventTimeTimeout | [[EventTimeTimeout]] Timeout based on event time Used when...FIXME | NoTimeout | [[NoTimeout]] No timeout Used when...FIXME | ProcessingTimeTimeout a| [[ProcessingTimeTimeout]] Timeout based on processing time FlatMapGroupsWithStateExec physical operator requires that batchTimestampMs is specified when ProcessingTimeTimeout is used. batchTimestampMs is defined when IncrementalExecution is created (with the state ). IncrementalExecution is given OffsetSeqMetadata when StreamExecution is requested to run a streaming batch . |===","title":"GroupStateTimeout"},{"location":"GroupStateTimeout/#groupstatetimeout","text":"GroupStateTimeout represents an aggregation state timeout that defines when a GroupState can be considered timed-out ( expired ) in Arbitrary Stateful Streaming Aggregation . GroupStateTimeout is used with the following KeyValueGroupedDataset operations: mapGroupsWithState flatMapGroupsWithState [[extensions]] .GroupStateTimeouts [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | GroupStateTimeout | Description | EventTimeTimeout | [[EventTimeTimeout]] Timeout based on event time Used when...FIXME | NoTimeout | [[NoTimeout]] No timeout Used when...FIXME | ProcessingTimeTimeout a| [[ProcessingTimeTimeout]] Timeout based on processing time FlatMapGroupsWithStateExec physical operator requires that batchTimestampMs is specified when ProcessingTimeTimeout is used. batchTimestampMs is defined when IncrementalExecution is created (with the state ). IncrementalExecution is given OffsetSeqMetadata when StreamExecution is requested to run a streaming batch . |===","title":"GroupStateTimeout"},{"location":"HDFSBackedStateStore/","text":"HDFSBackedStateStore \u00b6 HDFSBackedStateStore is a concrete StateStore that uses a Hadoop DFS-compatible file system for versioned state persistence. HDFSBackedStateStore is < > exclusively when HDFSBackedStateStoreProvider is requested for the specified version of state (store) for update (when StateStore utility is requested to look up a StateStore by provider id ). [[id]] HDFSBackedStateStore uses the StateStoreId of the owning HDFSBackedStateStoreProvider . [[toString]] When requested for the textual representation, HDFSBackedStateStore gives HDFSStateStore[id=(op=[operatorId],part=[partitionId]),dir=[baseDir]] . [[logging]] [TIP] ==== HDFSBackedStateStore is an internal class of HDFSBackedStateStoreProvider and uses its logger . ==== === [[creating-instance]] Creating HDFSBackedStateStore Instance HDFSBackedStateStore takes the following to be created: [[version]] Version [[mapToUpdate]] State Map ( ConcurrentHashMap[UnsafeRow, UnsafeRow] ) HDFSBackedStateStore initializes the < >. === [[state]] Internal State -- state Internal Property [source, scala] \u00b6 state: STATE \u00b6 state is the current state of HDFSBackedStateStore and can be in one of the three possible states: < >, < >, and < >. State changes (to the internal < > registry) are allowed as long as HDFSBackedStateStore is in the default < > state. Right after a HDFSBackedStateStore transitions to either < > or < > state, no further state changes are allowed. NOTE: Don't get confused with the term \"state\" as there are two states: the internal < > of HDFSBackedStateStore and the state of a streaming query (that HDFSBackedStateStore is responsible for). [[states]] .Internal States [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | ABORTED a| [[ABORTED]] After < > | COMMITTED a| [[COMMITTED]] After < > < > flag indicates whether HDFSBackedStateStore is in this state or not. | UPDATING a| [[UPDATING]] (default) Initial state after the HDFSBackedStateStore was < > Allows for state changes (e.g. < >, < >, < >) and eventually < > or < > them |=== === [[writeUpdateToDeltaFile]] writeUpdateToDeltaFile Internal Method [source, scala] \u00b6 writeUpdateToDeltaFile( output: DataOutputStream, key: UnsafeRow, value: UnsafeRow): Unit CAUTION: FIXME === [[put]] put Method [source, scala] \u00b6 put( key: UnsafeRow, value: UnsafeRow): Unit NOTE: put is a part of StateStore.md#put[StateStore Contract] to...FIXME put stores the copies of the key and value in < > internal registry followed by < > (using < >). put reports an IllegalStateException when HDFSBackedStateStore is not in < > state: Cannot put after already committed or aborted === [[commit]] Committing State Changes -- commit Method [source, scala] \u00b6 commit(): Long \u00b6 commit is part of the StateStore abstraction. commit requests the parent HDFSBackedStateStoreProvider to commit state changes (as a new version of state) (with the < >, the < > and the < >). commit transitions HDFSBackedStateStore to < > state. commit prints out the following INFO message to the logs: Committed version [newVersion] for [this] to file [finalDeltaFile] commit returns a < >. commit throws an IllegalStateException when HDFSBackedStateStore is not in < > state: Cannot commit after already committed or aborted commit throws an IllegalStateException for any NonFatal exception: Error committing version [newVersion] into [this] === [[abort]] Aborting State Changes -- abort Method [source, scala] \u00b6 abort(): Unit \u00b6 abort is part of the StateStore abstraction. abort ...FIXME === [[metrics]] Performance Metrics -- metrics Method [source, scala] \u00b6 metrics: StateStoreMetrics \u00b6 metrics is part of the StateStore abstraction. metrics requests the performance metrics of the parent HDFSBackedStateStoreProvider . The performance metrics of the provider used are only the ones listed in supportedCustomMetrics . In the end, metrics returns a new StateStoreMetrics with the following: Total number of keys as the size of < > < > as the memoryUsedBytes metric (of the parent provider) < > as the supportedCustomMetrics and the metricStateOnCurrentVersionSizeBytes metric of the parent provider === [[hasCommitted]] Are State Changes Committed? -- hasCommitted Method [source, scala] \u00b6 hasCommitted: Boolean \u00b6 hasCommitted is part of the StateStore abstraction. hasCommitted returns true when HDFSBackedStateStore is in < > state and false otherwise. Internal Properties \u00b6 [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | compressedStream a| [[compressedStream]] [source, scala] \u00b6 compressedStream: DataOutputStream \u00b6 The compressed https://docs.oracle.com/javase/8/docs/api/java/io/DataOutputStream.html[java.io.DataOutputStream ] for the < > | deltaFileStream a| [[deltaFileStream]] [source, scala] \u00b6 deltaFileStream: CheckpointFileManager.CancellableFSDataOutputStream \u00b6 | finalDeltaFile a| [[finalDeltaFile]] [source, scala] \u00b6 finalDeltaFile: Path \u00b6 The Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the deltaFile for the version | newVersion a| [[newVersion]] [source, scala] \u00b6 newVersion: Long \u00b6 Used exclusively when HDFSBackedStateStore is requested for the < >, to < > and < > |===","title":"HDFSBackedStateStore"},{"location":"HDFSBackedStateStore/#hdfsbackedstatestore","text":"HDFSBackedStateStore is a concrete StateStore that uses a Hadoop DFS-compatible file system for versioned state persistence. HDFSBackedStateStore is < > exclusively when HDFSBackedStateStoreProvider is requested for the specified version of state (store) for update (when StateStore utility is requested to look up a StateStore by provider id ). [[id]] HDFSBackedStateStore uses the StateStoreId of the owning HDFSBackedStateStoreProvider . [[toString]] When requested for the textual representation, HDFSBackedStateStore gives HDFSStateStore[id=(op=[operatorId],part=[partitionId]),dir=[baseDir]] . [[logging]] [TIP] ==== HDFSBackedStateStore is an internal class of HDFSBackedStateStoreProvider and uses its logger . ==== === [[creating-instance]] Creating HDFSBackedStateStore Instance HDFSBackedStateStore takes the following to be created: [[version]] Version [[mapToUpdate]] State Map ( ConcurrentHashMap[UnsafeRow, UnsafeRow] ) HDFSBackedStateStore initializes the < >. === [[state]] Internal State -- state Internal Property","title":"HDFSBackedStateStore"},{"location":"HDFSBackedStateStore/#source-scala","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStore/#state-state","text":"state is the current state of HDFSBackedStateStore and can be in one of the three possible states: < >, < >, and < >. State changes (to the internal < > registry) are allowed as long as HDFSBackedStateStore is in the default < > state. Right after a HDFSBackedStateStore transitions to either < > or < > state, no further state changes are allowed. NOTE: Don't get confused with the term \"state\" as there are two states: the internal < > of HDFSBackedStateStore and the state of a streaming query (that HDFSBackedStateStore is responsible for). [[states]] .Internal States [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | ABORTED a| [[ABORTED]] After < > | COMMITTED a| [[COMMITTED]] After < > < > flag indicates whether HDFSBackedStateStore is in this state or not. | UPDATING a| [[UPDATING]] (default) Initial state after the HDFSBackedStateStore was < > Allows for state changes (e.g. < >, < >, < >) and eventually < > or < > them |=== === [[writeUpdateToDeltaFile]] writeUpdateToDeltaFile Internal Method","title":"state: STATE"},{"location":"HDFSBackedStateStore/#source-scala_1","text":"writeUpdateToDeltaFile( output: DataOutputStream, key: UnsafeRow, value: UnsafeRow): Unit CAUTION: FIXME === [[put]] put Method","title":"[source, scala]"},{"location":"HDFSBackedStateStore/#source-scala_2","text":"put( key: UnsafeRow, value: UnsafeRow): Unit NOTE: put is a part of StateStore.md#put[StateStore Contract] to...FIXME put stores the copies of the key and value in < > internal registry followed by < > (using < >). put reports an IllegalStateException when HDFSBackedStateStore is not in < > state: Cannot put after already committed or aborted === [[commit]] Committing State Changes -- commit Method","title":"[source, scala]"},{"location":"HDFSBackedStateStore/#source-scala_3","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStore/#commit-long","text":"commit is part of the StateStore abstraction. commit requests the parent HDFSBackedStateStoreProvider to commit state changes (as a new version of state) (with the < >, the < > and the < >). commit transitions HDFSBackedStateStore to < > state. commit prints out the following INFO message to the logs: Committed version [newVersion] for [this] to file [finalDeltaFile] commit returns a < >. commit throws an IllegalStateException when HDFSBackedStateStore is not in < > state: Cannot commit after already committed or aborted commit throws an IllegalStateException for any NonFatal exception: Error committing version [newVersion] into [this] === [[abort]] Aborting State Changes -- abort Method","title":"commit(): Long"},{"location":"HDFSBackedStateStore/#source-scala_4","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStore/#abort-unit","text":"abort is part of the StateStore abstraction. abort ...FIXME === [[metrics]] Performance Metrics -- metrics Method","title":"abort(): Unit"},{"location":"HDFSBackedStateStore/#source-scala_5","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStore/#metrics-statestoremetrics","text":"metrics is part of the StateStore abstraction. metrics requests the performance metrics of the parent HDFSBackedStateStoreProvider . The performance metrics of the provider used are only the ones listed in supportedCustomMetrics . In the end, metrics returns a new StateStoreMetrics with the following: Total number of keys as the size of < > < > as the memoryUsedBytes metric (of the parent provider) < > as the supportedCustomMetrics and the metricStateOnCurrentVersionSizeBytes metric of the parent provider === [[hasCommitted]] Are State Changes Committed? -- hasCommitted Method","title":"metrics: StateStoreMetrics"},{"location":"HDFSBackedStateStore/#source-scala_6","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStore/#hascommitted-boolean","text":"hasCommitted is part of the StateStore abstraction. hasCommitted returns true when HDFSBackedStateStore is in < > state and false otherwise.","title":"hasCommitted: Boolean"},{"location":"HDFSBackedStateStore/#internal-properties","text":"[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | compressedStream a| [[compressedStream]]","title":"Internal Properties"},{"location":"HDFSBackedStateStore/#source-scala_7","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStore/#compressedstream-dataoutputstream","text":"The compressed https://docs.oracle.com/javase/8/docs/api/java/io/DataOutputStream.html[java.io.DataOutputStream ] for the < > | deltaFileStream a| [[deltaFileStream]]","title":"compressedStream: DataOutputStream"},{"location":"HDFSBackedStateStore/#source-scala_8","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStore/#deltafilestream-checkpointfilemanagercancellablefsdataoutputstream","text":"| finalDeltaFile a| [[finalDeltaFile]]","title":"deltaFileStream: CheckpointFileManager.CancellableFSDataOutputStream"},{"location":"HDFSBackedStateStore/#source-scala_9","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStore/#finaldeltafile-path","text":"The Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the deltaFile for the version | newVersion a| [[newVersion]]","title":"finalDeltaFile: Path"},{"location":"HDFSBackedStateStore/#source-scala_10","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStore/#newversion-long","text":"Used exclusively when HDFSBackedStateStore is requested for the < >, to < > and < > |===","title":"newVersion: Long"},{"location":"HDFSBackedStateStoreProvider/","text":"HDFSBackedStateStoreProvider \u00b6 HDFSBackedStateStoreProvider is a StateStoreProvider that uses a Hadoop DFS-compatible file system for versioned state checkpointing . HDFSBackedStateStoreProvider is the default StateStoreProvider per the spark.sql.streaming.stateStore.providerClass internal configuration property. HDFSBackedStateStoreProvider is < > and immediately requested to < > when StateStoreProvider utility is requested to < >. That is when HDFSBackedStateStoreProvider is given the < > that uniquely identifies the state store to use for a stateful operator and a partition. HDFSStateStoreProvider uses HDFSBackedStateStores to manage state (< >). HDFSBackedStateStoreProvider manages versioned state in delta and snapshot files (and uses a < > internally for faster access to state versions). [[creating-instance]] HDFSBackedStateStoreProvider takes no arguments to be created. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider=ALL Refer to < >. \u00b6 Performance Metrics \u00b6 [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Name (in web UI) | Description | memoryUsedBytes a| [[memoryUsedBytes]] Estimated size of the < > internal registry | count of cache hit on states cache in provider a| [[metricLoadedMapCacheHit]][[loadedMapCacheHitCount]] The number of times < > was successful and found ( hit ) the requested state version in the < > internal cache | count of cache miss on states cache in provider a| [[metricLoadedMapCacheMiss]][[loadedMapCacheMissCount]] The number of times < > could not find ( missed ) the requested state version in the < > internal cache | estimated size of state only on current version a| [[metricStateOnCurrentVersionSizeBytes]][[stateOnCurrentVersionSizeBytes]] Estimated size of the current state (of the HDFSBackedStateStore ) |=== === [[baseDir]] State Checkpoint Base Directory -- baseDir Lazy Internal Property [source,scala] \u00b6 baseDir: Path \u00b6 baseDir is the base directory (as a Hadoop Path ) for state checkpointing (for < > and < > state files). baseDir is initialized lazily since it is not yet known when HDFSBackedStateStoreProvider is < >. baseDir is initialized and created based on the < > of the < > when HDFSBackedStateStoreProvider is requested to < >. === [[stateStoreId]][[stateStoreId_]] StateStoreId -- Unique Identifier of State Store As a < >, HDFSBackedStateStoreProvider is associated with a < > (which is a unique identifier of the state store for a stateful operator and a partition). HDFSBackedStateStoreProvider is given the < > at < > (as requested by the < > contract). The < > is then used for the following: HDFSBackedStateStore is requested for the id HDFSBackedStateStoreProvider is requested for the < > and the < > === [[toString]] Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. HDFSBackedStateStoreProvider uses the < > and the < > for the textual representation: HDFSStateStoreProvider[id = (op=[operatorId],part=[partitionId]),dir = [baseDir]] === [[getStore]] Loading Specified Version of State (Store) For Update -- getStore Method [source, scala] \u00b6 getStore( version: Long): StateStore getStore is part of the StateStoreProvider abstraction. getStore creates a new empty state ( ConcurrentHashMap[UnsafeRow, UnsafeRow] ) and < > for versions greater than 0 . In the end, getStore creates a new HDFSBackedStateStore for the specified version with the new state and prints out the following INFO message to the logs: Retrieved version [version] of [this] for update getStore throws an IllegalArgumentException when the specified version is less than 0 (negative): Version cannot be less than 0 === [[deltaFile]] deltaFile Internal Method [source, scala] \u00b6 deltaFile(version: Long): Path \u00b6 deltaFile simply returns the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the [version].delta file in the < >. deltaFile is used when: HDFSBackedStateStore is created (and creates the < >) HDFSBackedStateStoreProvider is requested to updateFromDeltaFile === [[snapshotFile]] snapshotFile Internal Method [source, scala] \u00b6 snapshotFile(version: Long): Path \u00b6 snapshotFile simply returns the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the [version].snapshot file in the < >. NOTE: snapshotFile is used when HDFSBackedStateStoreProvider is requested to < > or < >. === [[fetchFiles]] Listing All Delta And Snapshot Files In State Checkpoint Directory -- fetchFiles Internal Method [source, scala] \u00b6 fetchFiles(): Seq[StoreFile] \u00b6 fetchFiles requests the < > for all the files in the < >. For every file, fetchFiles splits the name into two parts with . (dot) as a separator (files with more or less than two parts are simply ignored) and registers a new StoreFile for snapshot and delta files: For snapshot files, fetchFiles creates a new StoreFile with isSnapshot flag on ( true ) For delta files, fetchFiles creates a new StoreFile with isSnapshot flag off ( false ) NOTE: delta files are only registered if there was no snapshot file for the version. fetchFiles prints out the following WARN message to the logs for any other files: Could not identify file [path] for [this] In the end, fetchFiles sorts the StoreFiles based on their version, prints out the following DEBUG message to the logs, and returns the files. Current set of files for [this]: [storeFiles] NOTE: fetchFiles is used when HDFSBackedStateStoreProvider is requested to < > and < >. === [[init]] Initializing StateStoreProvider -- init Method [source, scala] \u00b6 init( stateStoreId: StateStoreId, keySchema: StructType, valueSchema: StructType, indexOrdinal: Option[Int], storeConf: StateStoreConf, hadoopConf: Configuration): Unit NOTE: init is part of the < > to initialize itself. init records the values of the input arguments as the < >, < >, < >, < >, and < > internal properties. init requests the given StateStoreConf for the spark.sql.streaming.maxBatchesToRetainInMemory configuration property (that is then recorded in the < > internal property). In the end, init requests the < > to create the < > directory (with parent directories). === [[filesForVersion]] Finding Snapshot File and Delta Files For Version -- filesForVersion Internal Method [source, scala] \u00b6 filesForVersion( allFiles: Seq[StoreFile], version: Long): Seq[StoreFile] filesForVersion finds the latest snapshot version among the given allFiles files up to and including the given version (it may or may not be available). If a snapshot file was found (among the given file up to and including the given version), filesForVersion takes all delta files between the version of the snapshot file (exclusive) and the given version (inclusive) from the given allFiles files. NOTE: The number of delta files should be the given version minus the snapshot version. If a snapshot file was not found, filesForVersion takes all delta files up to the given version (inclusive) from the given allFiles files. In the end, filesForVersion returns a snapshot version (if available) and all delta files up to the given version (inclusive). NOTE: filesForVersion is used when HDFSBackedStateStoreProvider is requested to < > and < >. === [[doMaintenance]] State Maintenance (Snapshotting and Cleaning Up) -- doMaintenance Method [source, scala] \u00b6 doMaintenance(): Unit \u00b6 NOTE: doMaintenance is part of the < > for optional state maintenance. doMaintenance simply does < > followed by < >. In case of any non-fatal errors, doMaintenance simply prints out the following WARN message to the logs: Error performing snapshot and cleaning up [this] ==== [[doSnapshot]] State Snapshoting (Rolling Up Delta Files into Snapshot File) -- doSnapshot Internal Method [source, scala] \u00b6 doSnapshot(): Unit \u00b6 doSnapshot < > ( files ) and prints out the following DEBUG message to the logs: fetchFiles() took [time] ms. doSnapshot returns immediately (and does nothing) when there are no delta and snapshot files. doSnapshot takes the version of the latest file ( lastVersion ). doSnapshot < > (among the files and for the last version). doSnapshot looks up the last version in the < >. When the last version was found in the cache and the number of delta files is above spark.sql.streaming.stateStore.minDeltasForSnapshot internal threshold, doSnapshot < >. In the end, doSnapshot prints out the following DEBUG message to the logs: writeSnapshotFile() took [time] ms. In case of non-fatal errors, doSnapshot simply prints out the following WARN message to the logs: Error doing snapshots for [this] NOTE: doSnapshot is used exclusively when HDFSBackedStateStoreProvider is requested to < >. ==== [[cleanup]] Cleaning Up (Removing Old State Files) -- cleanup Internal Method [source, scala] \u00b6 cleanup(): Unit \u00b6 cleanup < > ( files ) and prints out the following DEBUG message to the logs: fetchFiles() took [time] ms. cleanup returns immediately (and does nothing) when there are no delta and snapshot files. cleanup takes the version of the latest state file ( lastVersion ) and decrements it by spark.sql.streaming.minBatchesToRetain configuration property that gives the earliest version to retain (and all older state files to be removed). cleanup requests the < > to delete the path of every old state file. cleanup prints out the following DEBUG message to the logs: deleting files took [time] ms. In the end, cleanup prints out the following INFO message to the logs: Deleted files older than [version] for [this]: [filesToDelete] In case of a non-fatal exception, cleanup prints out the following WARN message to the logs: Error cleaning up files for [this] NOTE: cleanup is used exclusively when HDFSBackedStateStoreProvider is requested for < >. === [[close]] Closing State Store Provider -- close Method [source, scala] \u00b6 close(): Unit \u00b6 NOTE: close is part of the < > to close the state store provider. close ...FIXME === [[getMetricsForProvider]] getMetricsForProvider Method [source, scala] \u00b6 getMetricsForProvider(): Map[String, Long] \u00b6 getMetricsForProvider returns the following < >: < > < > < > getMetricsForProvider is used when HDFSBackedStateStore is requested for performance metrics . === [[supportedCustomMetrics]] Supported StateStoreCustomMetrics -- supportedCustomMetrics Method [source, scala] \u00b6 supportedCustomMetrics: Seq[StateStoreCustomMetric] \u00b6 NOTE: supportedCustomMetrics is part of the < > for the < > of a state store provider. supportedCustomMetrics includes the following < >: < > < > < > === [[commitUpdates]] Committing State Changes (As New Version of State) -- commitUpdates Internal Method [source, scala] \u00b6 commitUpdates( newVersion: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow], output: DataOutputStream): Unit commitUpdates < > (with the given DataOutputStream ) followed by < > (with the given newVersion and the map state). commitUpdates is used when HDFSBackedStateStore is requested to commit state changes . === [[loadMap]] Loading Specified Version of State (from Internal Cache or Snapshot and Delta Files) -- loadMap Internal Method [source, scala] \u00b6 loadMap( version: Long): ConcurrentHashMap[UnsafeRow, UnsafeRow] loadMap firstly tries to find the state version in the < > internal cache and, if found, returns it immediately and increments the < > metric. If the requested state version could not be found in the < > internal cache, loadMap prints out the following WARN message to the logs: [options=\"wrap\"] \u00b6 The state for version [version] doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query. \u00b6 loadMap increments the < > metric. loadMap < > and, if found, < > and returns it. If not found, loadMap tries to find the most recent state version by decrementing the requested version until one is found in the < > internal cache or < >. loadMap < > for all the remaining versions (from the snapshot version up to the requested one). loadMap < > (the closest snapshot and the remaining delta versions) and returns it. In the end, loadMap prints out the following DEBUG message to the logs: Loading state for [version] takes [elapsedMs] ms. NOTE: loadMap is used exclusively when HDFSBackedStateStoreProvider is requested for the < >. === [[readSnapshotFile]] Loading State Snapshot File For Specified Version -- readSnapshotFile Internal Method [source, scala] \u00b6 readSnapshotFile( version: Long): Option[ConcurrentHashMap[UnsafeRow, UnsafeRow]] readSnapshotFile < > for the given version . readSnapshotFile requests the < > to open the snapshot file for reading and < > ( input ). readSnapshotFile reads the decompressed input stream until an EOF (that is marked as the integer -1 in the stream) and inserts key and value rows in a state map ( ConcurrentHashMap[UnsafeRow, UnsafeRow] ): First integer is the size of a key (buffer) followed by the key itself (of the size). readSnapshotFile creates an UnsafeRow for the key (with the number of fields as indicated by the number of fields of the < >). Next integer is the size of a value (buffer) followed by the value itself (of the size). readSnapshotFile creates an UnsafeRow for the value (with the number of fields as indicated by the number of fields of the < >). In the end, readSnapshotFile prints out the following INFO message to the logs and returns the key-value map. Read snapshot file for version [version] of [this] from [fileToRead] In case of FileNotFoundException readSnapshotFile simply returns None (to indicate no snapshot state file was available and so no state for the version). readSnapshotFile throws an IOException for the size of a key or a value below 0 : Error reading snapshot file [fileToRead] of [this]: [key|value] size cannot be [keySize|valueSize] NOTE: readSnapshotFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[updateFromDeltaFile]] Updating State with State Changes For Specified Version (per Delta File) -- updateFromDeltaFile Internal Method [source, scala] \u00b6 updateFromDeltaFile( version: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit [NOTE] \u00b6 updateFromDeltaFile is very similar code-wise to < > with the two main differences: updateFromDeltaFile is given the state map to update (while < > loads the state from a snapshot file) updateFromDeltaFile removes a key from the state map when the value (size) is -1 (while < > throws an IOException ) The following description is almost an exact copy of < > just for completeness. \u00b6 updateFromDeltaFile < > for the requested version . updateFromDeltaFile requests the < > to open the delta file for reading and < > ( input ). updateFromDeltaFile reads the decompressed input stream until an EOF (that is marked as the integer -1 in the stream) and inserts key and value rows in the given state map: First integer is the size of a key (buffer) followed by the key itself (of the size). updateFromDeltaFile creates an UnsafeRow for the key (with the number of fields as indicated by the number of fields of the < >). Next integer is the size of a value (buffer) followed by the value itself (of the size). updateFromDeltaFile creates an UnsafeRow for the value (with the number of fields as indicated by the number of fields of the < >) or removes the corresponding key from the state map (if the value size is -1 ) NOTE: updateFromDeltaFile removes the key-value entry from the state map if the value (size) is -1 . In the end, updateFromDeltaFile prints out the following INFO message to the logs and returns the key-value map. Read delta file for version [version] of [this] from [fileToRead] updateFromDeltaFile throws an IllegalStateException in case of FileNotFoundException while opening the delta file for the specified version: Error reading delta file [fileToRead] of [this]: [fileToRead] does not exist NOTE: updateFromDeltaFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[putStateIntoStateCacheMap]] Caching New Version of State -- putStateIntoStateCacheMap Internal Method [source, scala] \u00b6 putStateIntoStateCacheMap( newVersion: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit putStateIntoStateCacheMap registers state for a given version, i.e. adds the map state under the newVersion key in the < > internal registry. With the < > threshold as 0 or below, putStateIntoStateCacheMap simply removes all entries from the < > internal registry and returns. putStateIntoStateCacheMap removes the oldest state version(s) in the < > internal registry until its size is at the < > threshold. With the size of the < > internal registry is at the < > threshold, putStateIntoStateCacheMap does two more optimizations per newVersion It does not add the given state when the version of the oldest state is earlier (larger) than the given newVersion It removes the oldest state when older (smaller) than the given newVersion NOTE: putStateIntoStateCacheMap is used when HDFSBackedStateStoreProvider is requested to < > and < >. === [[writeSnapshotFile]] Writing Compressed Snapshot File for Specified Version -- writeSnapshotFile Internal Method [source, scala] \u00b6 writeSnapshotFile( version: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit writeSnapshotFile < > for the given version. writeSnapshotFile requests the < > to create the snapshot file (with overwriting enabled) and < >. For every key-value UnsafeRow pair in the given map, writeSnapshotFile writes the size of the key followed by the key itself (as bytes). writeSnapshotFile then writes the size of the value followed by the value itself (as bytes). In the end, writeSnapshotFile prints out the following INFO message to the logs: Written snapshot file for version [version] of [this] at [targetFile] In case of any Throwable exception, writeSnapshotFile < > and re-throws the exception. NOTE: writeSnapshotFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[compressStream]] compressStream Internal Method [source, scala] \u00b6 compressStream( outputStream: DataOutputStream): DataOutputStream compressStream creates a new LZ4CompressionCodec (based on the < >) and requests it to create a LZ4BlockOutputStream with the given DataOutputStream . In the end, compressStream creates a new DataOutputStream with the LZ4BlockOutputStream . NOTE: compressStream is used when...FIXME === [[cancelDeltaFile]] cancelDeltaFile Internal Method [source, scala] \u00b6 cancelDeltaFile( compressedStream: DataOutputStream, rawStream: CancellableFSDataOutputStream): Unit cancelDeltaFile ...FIXME NOTE: cancelDeltaFile is used when...FIXME === [[finalizeDeltaFile]] finalizeDeltaFile Internal Method [source, scala] \u00b6 finalizeDeltaFile( output: DataOutputStream): Unit finalizeDeltaFile simply writes -1 to the given DataOutputStream (to indicate end of file) and closes it. NOTE: finalizeDeltaFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[loadedMaps]] Lookup Table (Cache) of States By Version -- loadedMaps Internal Method [source, scala] \u00b6 loadedMaps: TreeMap[ Long, // version ConcurrentHashMap[UnsafeRow, UnsafeRow]] // state (as keys and values) loadedMaps is a https://docs.oracle.com/javase/8/docs/api/java/util/TreeMap.html[java.util.TreeMap ] of state versions sorted according to the reversed ordering of the versions (i.e. long numbers). A new entry (a version and the state updates) can only be added when HDFSBackedStateStoreProvider is requested to < > (and only when the spark.sql.streaming.maxBatchesToRetainInMemory internal configuration is above 0 ). loadedMaps is mainly used when HDFSBackedStateStoreProvider is requested to < >. Positive hits (when a version could be found in the cache) is available as the < > performance metric while misses are counted in the < > performance metric. NOTE: With no or missing versions in cache < > metric should be above 0 while < > always 0 (or smaller than the other metric). The estimated size of loadedMaps is available as the < > performance metric. The spark.sql.streaming.maxBatchesToRetainInMemory internal configuration is used as the threshold of the number of elements in loadedMaps . When 0 or negative, every < > removes all elements in ( clears ) loadedMaps . NOTE: It is possible to change the configuration at restart of a structured query. The state deltas (the values) in loadedMaps are cleared (all entries removed) when HDFSBackedStateStoreProvider is requested to < >. Used when HDFSBackedStateStoreProvider is requested for the following: < > < > === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | fm a| [[fm]] CheckpointFileManager for the < > (and the < >) Used when: Creating a new HDFSBackedStateStore (to create the CancellableFSDataOutputStream for the finalDeltaFile ) HDFSBackedStateStoreProvider is requested to < > (to create the < >), < >, < >, < >, < >, and < > | hadoopConf a| [[hadoopConf]] Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] of the < > Given when HDFSBackedStateStoreProvider is requested to < > | keySchema a| [[keySchema]] [source, scala] \u00b6 keySchema: StructType \u00b6 Schema of the state keys | valueSchema a| [[valueSchema]] [source, scala] \u00b6 valueSchema: StructType \u00b6 Schema of the state values | numberOfVersionsToRetainInMemory a| [[numberOfVersionsToRetainInMemory]] [source, scala] \u00b6 numberOfVersionsToRetainInMemory: Int \u00b6 numberOfVersionsToRetainInMemory is the maximum number of entries in the < > internal registry and is configured by the spark.sql.streaming.maxBatchesToRetainInMemory internal configuration. numberOfVersionsToRetainInMemory is a threshold when HDFSBackedStateStoreProvider removes the last key from the < > internal registry (per reverse ordering of state versions) when requested to < >. | sparkConf a| [[sparkConf]] SparkConf |===","title":"HDFSBackedStateStoreProvider"},{"location":"HDFSBackedStateStoreProvider/#hdfsbackedstatestoreprovider","text":"HDFSBackedStateStoreProvider is a StateStoreProvider that uses a Hadoop DFS-compatible file system for versioned state checkpointing . HDFSBackedStateStoreProvider is the default StateStoreProvider per the spark.sql.streaming.stateStore.providerClass internal configuration property. HDFSBackedStateStoreProvider is < > and immediately requested to < > when StateStoreProvider utility is requested to < >. That is when HDFSBackedStateStoreProvider is given the < > that uniquely identifies the state store to use for a stateful operator and a partition. HDFSStateStoreProvider uses HDFSBackedStateStores to manage state (< >). HDFSBackedStateStoreProvider manages versioned state in delta and snapshot files (and uses a < > internally for faster access to state versions). [[creating-instance]] HDFSBackedStateStoreProvider takes no arguments to be created. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider=ALL","title":"HDFSBackedStateStoreProvider"},{"location":"HDFSBackedStateStoreProvider/#refer-to","text":"","title":"Refer to &lt;&gt;."},{"location":"HDFSBackedStateStoreProvider/#performance-metrics","text":"[cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Name (in web UI) | Description | memoryUsedBytes a| [[memoryUsedBytes]] Estimated size of the < > internal registry | count of cache hit on states cache in provider a| [[metricLoadedMapCacheHit]][[loadedMapCacheHitCount]] The number of times < > was successful and found ( hit ) the requested state version in the < > internal cache | count of cache miss on states cache in provider a| [[metricLoadedMapCacheMiss]][[loadedMapCacheMissCount]] The number of times < > could not find ( missed ) the requested state version in the < > internal cache | estimated size of state only on current version a| [[metricStateOnCurrentVersionSizeBytes]][[stateOnCurrentVersionSizeBytes]] Estimated size of the current state (of the HDFSBackedStateStore ) |=== === [[baseDir]] State Checkpoint Base Directory -- baseDir Lazy Internal Property","title":"Performance Metrics"},{"location":"HDFSBackedStateStoreProvider/#sourcescala","text":"","title":"[source,scala]"},{"location":"HDFSBackedStateStoreProvider/#basedir-path","text":"baseDir is the base directory (as a Hadoop Path ) for state checkpointing (for < > and < > state files). baseDir is initialized lazily since it is not yet known when HDFSBackedStateStoreProvider is < >. baseDir is initialized and created based on the < > of the < > when HDFSBackedStateStoreProvider is requested to < >. === [[stateStoreId]][[stateStoreId_]] StateStoreId -- Unique Identifier of State Store As a < >, HDFSBackedStateStoreProvider is associated with a < > (which is a unique identifier of the state store for a stateful operator and a partition). HDFSBackedStateStoreProvider is given the < > at < > (as requested by the < > contract). The < > is then used for the following: HDFSBackedStateStore is requested for the id HDFSBackedStateStoreProvider is requested for the < > and the < > === [[toString]] Textual Representation -- toString Method","title":"baseDir: Path"},{"location":"HDFSBackedStateStoreProvider/#source-scala","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#tostring-string","text":"NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. HDFSBackedStateStoreProvider uses the < > and the < > for the textual representation: HDFSStateStoreProvider[id = (op=[operatorId],part=[partitionId]),dir = [baseDir]] === [[getStore]] Loading Specified Version of State (Store) For Update -- getStore Method","title":"toString: String"},{"location":"HDFSBackedStateStoreProvider/#source-scala_1","text":"getStore( version: Long): StateStore getStore is part of the StateStoreProvider abstraction. getStore creates a new empty state ( ConcurrentHashMap[UnsafeRow, UnsafeRow] ) and < > for versions greater than 0 . In the end, getStore creates a new HDFSBackedStateStore for the specified version with the new state and prints out the following INFO message to the logs: Retrieved version [version] of [this] for update getStore throws an IllegalArgumentException when the specified version is less than 0 (negative): Version cannot be less than 0 === [[deltaFile]] deltaFile Internal Method","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#source-scala_2","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#deltafileversion-long-path","text":"deltaFile simply returns the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the [version].delta file in the < >. deltaFile is used when: HDFSBackedStateStore is created (and creates the < >) HDFSBackedStateStoreProvider is requested to updateFromDeltaFile === [[snapshotFile]] snapshotFile Internal Method","title":"deltaFile(version: Long): Path"},{"location":"HDFSBackedStateStoreProvider/#source-scala_3","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#snapshotfileversion-long-path","text":"snapshotFile simply returns the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the [version].snapshot file in the < >. NOTE: snapshotFile is used when HDFSBackedStateStoreProvider is requested to < > or < >. === [[fetchFiles]] Listing All Delta And Snapshot Files In State Checkpoint Directory -- fetchFiles Internal Method","title":"snapshotFile(version: Long): Path"},{"location":"HDFSBackedStateStoreProvider/#source-scala_4","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#fetchfiles-seqstorefile","text":"fetchFiles requests the < > for all the files in the < >. For every file, fetchFiles splits the name into two parts with . (dot) as a separator (files with more or less than two parts are simply ignored) and registers a new StoreFile for snapshot and delta files: For snapshot files, fetchFiles creates a new StoreFile with isSnapshot flag on ( true ) For delta files, fetchFiles creates a new StoreFile with isSnapshot flag off ( false ) NOTE: delta files are only registered if there was no snapshot file for the version. fetchFiles prints out the following WARN message to the logs for any other files: Could not identify file [path] for [this] In the end, fetchFiles sorts the StoreFiles based on their version, prints out the following DEBUG message to the logs, and returns the files. Current set of files for [this]: [storeFiles] NOTE: fetchFiles is used when HDFSBackedStateStoreProvider is requested to < > and < >. === [[init]] Initializing StateStoreProvider -- init Method","title":"fetchFiles(): Seq[StoreFile]"},{"location":"HDFSBackedStateStoreProvider/#source-scala_5","text":"init( stateStoreId: StateStoreId, keySchema: StructType, valueSchema: StructType, indexOrdinal: Option[Int], storeConf: StateStoreConf, hadoopConf: Configuration): Unit NOTE: init is part of the < > to initialize itself. init records the values of the input arguments as the < >, < >, < >, < >, and < > internal properties. init requests the given StateStoreConf for the spark.sql.streaming.maxBatchesToRetainInMemory configuration property (that is then recorded in the < > internal property). In the end, init requests the < > to create the < > directory (with parent directories). === [[filesForVersion]] Finding Snapshot File and Delta Files For Version -- filesForVersion Internal Method","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#source-scala_6","text":"filesForVersion( allFiles: Seq[StoreFile], version: Long): Seq[StoreFile] filesForVersion finds the latest snapshot version among the given allFiles files up to and including the given version (it may or may not be available). If a snapshot file was found (among the given file up to and including the given version), filesForVersion takes all delta files between the version of the snapshot file (exclusive) and the given version (inclusive) from the given allFiles files. NOTE: The number of delta files should be the given version minus the snapshot version. If a snapshot file was not found, filesForVersion takes all delta files up to the given version (inclusive) from the given allFiles files. In the end, filesForVersion returns a snapshot version (if available) and all delta files up to the given version (inclusive). NOTE: filesForVersion is used when HDFSBackedStateStoreProvider is requested to < > and < >. === [[doMaintenance]] State Maintenance (Snapshotting and Cleaning Up) -- doMaintenance Method","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#source-scala_7","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#domaintenance-unit","text":"NOTE: doMaintenance is part of the < > for optional state maintenance. doMaintenance simply does < > followed by < >. In case of any non-fatal errors, doMaintenance simply prints out the following WARN message to the logs: Error performing snapshot and cleaning up [this] ==== [[doSnapshot]] State Snapshoting (Rolling Up Delta Files into Snapshot File) -- doSnapshot Internal Method","title":"doMaintenance(): Unit"},{"location":"HDFSBackedStateStoreProvider/#source-scala_8","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#dosnapshot-unit","text":"doSnapshot < > ( files ) and prints out the following DEBUG message to the logs: fetchFiles() took [time] ms. doSnapshot returns immediately (and does nothing) when there are no delta and snapshot files. doSnapshot takes the version of the latest file ( lastVersion ). doSnapshot < > (among the files and for the last version). doSnapshot looks up the last version in the < >. When the last version was found in the cache and the number of delta files is above spark.sql.streaming.stateStore.minDeltasForSnapshot internal threshold, doSnapshot < >. In the end, doSnapshot prints out the following DEBUG message to the logs: writeSnapshotFile() took [time] ms. In case of non-fatal errors, doSnapshot simply prints out the following WARN message to the logs: Error doing snapshots for [this] NOTE: doSnapshot is used exclusively when HDFSBackedStateStoreProvider is requested to < >. ==== [[cleanup]] Cleaning Up (Removing Old State Files) -- cleanup Internal Method","title":"doSnapshot(): Unit"},{"location":"HDFSBackedStateStoreProvider/#source-scala_9","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#cleanup-unit","text":"cleanup < > ( files ) and prints out the following DEBUG message to the logs: fetchFiles() took [time] ms. cleanup returns immediately (and does nothing) when there are no delta and snapshot files. cleanup takes the version of the latest state file ( lastVersion ) and decrements it by spark.sql.streaming.minBatchesToRetain configuration property that gives the earliest version to retain (and all older state files to be removed). cleanup requests the < > to delete the path of every old state file. cleanup prints out the following DEBUG message to the logs: deleting files took [time] ms. In the end, cleanup prints out the following INFO message to the logs: Deleted files older than [version] for [this]: [filesToDelete] In case of a non-fatal exception, cleanup prints out the following WARN message to the logs: Error cleaning up files for [this] NOTE: cleanup is used exclusively when HDFSBackedStateStoreProvider is requested for < >. === [[close]] Closing State Store Provider -- close Method","title":"cleanup(): Unit"},{"location":"HDFSBackedStateStoreProvider/#source-scala_10","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#close-unit","text":"NOTE: close is part of the < > to close the state store provider. close ...FIXME === [[getMetricsForProvider]] getMetricsForProvider Method","title":"close(): Unit"},{"location":"HDFSBackedStateStoreProvider/#source-scala_11","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#getmetricsforprovider-mapstring-long","text":"getMetricsForProvider returns the following < >: < > < > < > getMetricsForProvider is used when HDFSBackedStateStore is requested for performance metrics . === [[supportedCustomMetrics]] Supported StateStoreCustomMetrics -- supportedCustomMetrics Method","title":"getMetricsForProvider(): Map[String, Long]"},{"location":"HDFSBackedStateStoreProvider/#source-scala_12","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#supportedcustommetrics-seqstatestorecustommetric","text":"NOTE: supportedCustomMetrics is part of the < > for the < > of a state store provider. supportedCustomMetrics includes the following < >: < > < > < > === [[commitUpdates]] Committing State Changes (As New Version of State) -- commitUpdates Internal Method","title":"supportedCustomMetrics: Seq[StateStoreCustomMetric]"},{"location":"HDFSBackedStateStoreProvider/#source-scala_13","text":"commitUpdates( newVersion: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow], output: DataOutputStream): Unit commitUpdates < > (with the given DataOutputStream ) followed by < > (with the given newVersion and the map state). commitUpdates is used when HDFSBackedStateStore is requested to commit state changes . === [[loadMap]] Loading Specified Version of State (from Internal Cache or Snapshot and Delta Files) -- loadMap Internal Method","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#source-scala_14","text":"loadMap( version: Long): ConcurrentHashMap[UnsafeRow, UnsafeRow] loadMap firstly tries to find the state version in the < > internal cache and, if found, returns it immediately and increments the < > metric. If the requested state version could not be found in the < > internal cache, loadMap prints out the following WARN message to the logs:","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"HDFSBackedStateStoreProvider/#the-state-for-version-version-doesnt-exist-in-loadedmaps-reading-snapshot-file-and-delta-files-if-needednote-that-this-is-normal-for-the-first-batch-of-starting-query","text":"loadMap increments the < > metric. loadMap < > and, if found, < > and returns it. If not found, loadMap tries to find the most recent state version by decrementing the requested version until one is found in the < > internal cache or < >. loadMap < > for all the remaining versions (from the snapshot version up to the requested one). loadMap < > (the closest snapshot and the remaining delta versions) and returns it. In the end, loadMap prints out the following DEBUG message to the logs: Loading state for [version] takes [elapsedMs] ms. NOTE: loadMap is used exclusively when HDFSBackedStateStoreProvider is requested for the < >. === [[readSnapshotFile]] Loading State Snapshot File For Specified Version -- readSnapshotFile Internal Method","title":"The state for version [version] doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query."},{"location":"HDFSBackedStateStoreProvider/#source-scala_15","text":"readSnapshotFile( version: Long): Option[ConcurrentHashMap[UnsafeRow, UnsafeRow]] readSnapshotFile < > for the given version . readSnapshotFile requests the < > to open the snapshot file for reading and < > ( input ). readSnapshotFile reads the decompressed input stream until an EOF (that is marked as the integer -1 in the stream) and inserts key and value rows in a state map ( ConcurrentHashMap[UnsafeRow, UnsafeRow] ): First integer is the size of a key (buffer) followed by the key itself (of the size). readSnapshotFile creates an UnsafeRow for the key (with the number of fields as indicated by the number of fields of the < >). Next integer is the size of a value (buffer) followed by the value itself (of the size). readSnapshotFile creates an UnsafeRow for the value (with the number of fields as indicated by the number of fields of the < >). In the end, readSnapshotFile prints out the following INFO message to the logs and returns the key-value map. Read snapshot file for version [version] of [this] from [fileToRead] In case of FileNotFoundException readSnapshotFile simply returns None (to indicate no snapshot state file was available and so no state for the version). readSnapshotFile throws an IOException for the size of a key or a value below 0 : Error reading snapshot file [fileToRead] of [this]: [key|value] size cannot be [keySize|valueSize] NOTE: readSnapshotFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[updateFromDeltaFile]] Updating State with State Changes For Specified Version (per Delta File) -- updateFromDeltaFile Internal Method","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#source-scala_16","text":"updateFromDeltaFile( version: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#note","text":"updateFromDeltaFile is very similar code-wise to < > with the two main differences: updateFromDeltaFile is given the state map to update (while < > loads the state from a snapshot file) updateFromDeltaFile removes a key from the state map when the value (size) is -1 (while < > throws an IOException )","title":"[NOTE]"},{"location":"HDFSBackedStateStoreProvider/#the-following-description-is-almost-an-exact-copy-of-just-for-completeness","text":"updateFromDeltaFile < > for the requested version . updateFromDeltaFile requests the < > to open the delta file for reading and < > ( input ). updateFromDeltaFile reads the decompressed input stream until an EOF (that is marked as the integer -1 in the stream) and inserts key and value rows in the given state map: First integer is the size of a key (buffer) followed by the key itself (of the size). updateFromDeltaFile creates an UnsafeRow for the key (with the number of fields as indicated by the number of fields of the < >). Next integer is the size of a value (buffer) followed by the value itself (of the size). updateFromDeltaFile creates an UnsafeRow for the value (with the number of fields as indicated by the number of fields of the < >) or removes the corresponding key from the state map (if the value size is -1 ) NOTE: updateFromDeltaFile removes the key-value entry from the state map if the value (size) is -1 . In the end, updateFromDeltaFile prints out the following INFO message to the logs and returns the key-value map. Read delta file for version [version] of [this] from [fileToRead] updateFromDeltaFile throws an IllegalStateException in case of FileNotFoundException while opening the delta file for the specified version: Error reading delta file [fileToRead] of [this]: [fileToRead] does not exist NOTE: updateFromDeltaFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[putStateIntoStateCacheMap]] Caching New Version of State -- putStateIntoStateCacheMap Internal Method","title":"The following description is almost an exact copy of &lt;&gt; just for completeness."},{"location":"HDFSBackedStateStoreProvider/#source-scala_17","text":"putStateIntoStateCacheMap( newVersion: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit putStateIntoStateCacheMap registers state for a given version, i.e. adds the map state under the newVersion key in the < > internal registry. With the < > threshold as 0 or below, putStateIntoStateCacheMap simply removes all entries from the < > internal registry and returns. putStateIntoStateCacheMap removes the oldest state version(s) in the < > internal registry until its size is at the < > threshold. With the size of the < > internal registry is at the < > threshold, putStateIntoStateCacheMap does two more optimizations per newVersion It does not add the given state when the version of the oldest state is earlier (larger) than the given newVersion It removes the oldest state when older (smaller) than the given newVersion NOTE: putStateIntoStateCacheMap is used when HDFSBackedStateStoreProvider is requested to < > and < >. === [[writeSnapshotFile]] Writing Compressed Snapshot File for Specified Version -- writeSnapshotFile Internal Method","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#source-scala_18","text":"writeSnapshotFile( version: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit writeSnapshotFile < > for the given version. writeSnapshotFile requests the < > to create the snapshot file (with overwriting enabled) and < >. For every key-value UnsafeRow pair in the given map, writeSnapshotFile writes the size of the key followed by the key itself (as bytes). writeSnapshotFile then writes the size of the value followed by the value itself (as bytes). In the end, writeSnapshotFile prints out the following INFO message to the logs: Written snapshot file for version [version] of [this] at [targetFile] In case of any Throwable exception, writeSnapshotFile < > and re-throws the exception. NOTE: writeSnapshotFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[compressStream]] compressStream Internal Method","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#source-scala_19","text":"compressStream( outputStream: DataOutputStream): DataOutputStream compressStream creates a new LZ4CompressionCodec (based on the < >) and requests it to create a LZ4BlockOutputStream with the given DataOutputStream . In the end, compressStream creates a new DataOutputStream with the LZ4BlockOutputStream . NOTE: compressStream is used when...FIXME === [[cancelDeltaFile]] cancelDeltaFile Internal Method","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#source-scala_20","text":"cancelDeltaFile( compressedStream: DataOutputStream, rawStream: CancellableFSDataOutputStream): Unit cancelDeltaFile ...FIXME NOTE: cancelDeltaFile is used when...FIXME === [[finalizeDeltaFile]] finalizeDeltaFile Internal Method","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#source-scala_21","text":"finalizeDeltaFile( output: DataOutputStream): Unit finalizeDeltaFile simply writes -1 to the given DataOutputStream (to indicate end of file) and closes it. NOTE: finalizeDeltaFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[loadedMaps]] Lookup Table (Cache) of States By Version -- loadedMaps Internal Method","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#source-scala_22","text":"loadedMaps: TreeMap[ Long, // version ConcurrentHashMap[UnsafeRow, UnsafeRow]] // state (as keys and values) loadedMaps is a https://docs.oracle.com/javase/8/docs/api/java/util/TreeMap.html[java.util.TreeMap ] of state versions sorted according to the reversed ordering of the versions (i.e. long numbers). A new entry (a version and the state updates) can only be added when HDFSBackedStateStoreProvider is requested to < > (and only when the spark.sql.streaming.maxBatchesToRetainInMemory internal configuration is above 0 ). loadedMaps is mainly used when HDFSBackedStateStoreProvider is requested to < >. Positive hits (when a version could be found in the cache) is available as the < > performance metric while misses are counted in the < > performance metric. NOTE: With no or missing versions in cache < > metric should be above 0 while < > always 0 (or smaller than the other metric). The estimated size of loadedMaps is available as the < > performance metric. The spark.sql.streaming.maxBatchesToRetainInMemory internal configuration is used as the threshold of the number of elements in loadedMaps . When 0 or negative, every < > removes all elements in ( clears ) loadedMaps . NOTE: It is possible to change the configuration at restart of a structured query. The state deltas (the values) in loadedMaps are cleared (all entries removed) when HDFSBackedStateStoreProvider is requested to < >. Used when HDFSBackedStateStoreProvider is requested for the following: < > < > === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | fm a| [[fm]] CheckpointFileManager for the < > (and the < >) Used when: Creating a new HDFSBackedStateStore (to create the CancellableFSDataOutputStream for the finalDeltaFile ) HDFSBackedStateStoreProvider is requested to < > (to create the < >), < >, < >, < >, < >, and < > | hadoopConf a| [[hadoopConf]] Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] of the < > Given when HDFSBackedStateStoreProvider is requested to < > | keySchema a| [[keySchema]]","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#source-scala_23","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#keyschema-structtype","text":"Schema of the state keys | valueSchema a| [[valueSchema]]","title":"keySchema: StructType"},{"location":"HDFSBackedStateStoreProvider/#source-scala_24","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#valueschema-structtype","text":"Schema of the state values | numberOfVersionsToRetainInMemory a| [[numberOfVersionsToRetainInMemory]]","title":"valueSchema: StructType"},{"location":"HDFSBackedStateStoreProvider/#source-scala_25","text":"","title":"[source, scala]"},{"location":"HDFSBackedStateStoreProvider/#numberofversionstoretaininmemory-int","text":"numberOfVersionsToRetainInMemory is the maximum number of entries in the < > internal registry and is configured by the spark.sql.streaming.maxBatchesToRetainInMemory internal configuration. numberOfVersionsToRetainInMemory is a threshold when HDFSBackedStateStoreProvider removes the last key from the < > internal registry (per reverse ordering of state versions) when requested to < >. | sparkConf a| [[sparkConf]] SparkConf |===","title":"numberOfVersionsToRetainInMemory: Int"},{"location":"HDFSMetadataLog/","text":"HDFSMetadataLog \u00b6 HDFSMetadataLog is an extension of the MetadataLog abstraction for metadata storage to store batch files in a metadata log directory on Hadoop DFS (for fault-tolerance and reliability). Extensions \u00b6 CommitLog CompactibleFileStreamLog KafkaSourceInitialOffsetWriter OffsetSeqLog Creating Instance \u00b6 HDFSMetadataLog takes the following to be created: SparkSession Path of the metadata log directory While being created, HDFSMetadataLog makes sure that the path exists (and creates it if not). Metadata Log Directory \u00b6 HDFSMetadataLog uses the given path as the metadata log directory with metadata logs ( one per batch ). The path is immediately converted to a Hadoop Path for file management. CheckpointFileManager \u00b6 HDFSMetadataLog creates a CheckpointFileManager (with the metadata log directory ) when created . Implicit Json4s Formats \u00b6 HDFSMetadataLog uses Json4s with the Jackson binding for metadata serialization and deserialization (to and from JSON format). Latest Committed Batch Id with Metadata (If Available) \u00b6 getLatest (): Option [( Long , T )] getLatest is a part of MetadataLog abstraction. getLatest requests the internal < > for the files in < > that match < >. getLatest takes the batch ids (the batch files correspond to) and sorts the ids in reverse order. getLatest gives the first batch id with the metadata which < >. Note It is possible that the batch id could be in the metadata storage, but not available for retrieval. Retrieving Metadata of Streaming Batch (if Available) \u00b6 get ( batchId : Long ): Option [ T ] get is part of the MetadataLog abstraction. get ...FIXME Deserializing Metadata \u00b6 deserialize ( in : InputStream ): T deserialize deserializes a metadata (of type T ) from a given InputStream . deserialize is used to retrieve metadata of a batch . Retrieving Metadata of Streaming Batches (if Available) \u00b6 get ( startId : Option [ Long ], endId : Option [ Long ]): Array [( Long , T )] get is part of the MetadataLog abstraction. get ...FIXME Persisting Metadata of Streaming Micro-Batch \u00b6 add ( batchId : Long , metadata : T ): Boolean add is part of the MetadataLog abstraction. add return true when the metadata of the streaming batch was not available and persisted successfully. Otherwise, add returns false . Internally, add < > ( batchId ) and returns false when found. Otherwise, when not found, add < > for the given batchId and < >. add returns true if successful. Writing Batch Metadata to File (Metadata Log) \u00b6 writeBatchToFile ( metadata : T , path : Path ): Unit writeBatchToFile requests the < > to createAtomic (for the specified path and the overwriteIfPossible flag disabled). writeBatchToFile then < > (to the CancellableFSDataOutputStream output stream) and closes the stream. In case of an exception, writeBatchToFile simply requests the CancellableFSDataOutputStream output stream to cancel (so that the output file is not generated) and re-throws the exception. Serializing Metadata \u00b6 serialize ( metadata : T , out : OutputStream ): Unit serialize simply writes out the log data in a serialized format (using Json4s (with Jackson binding) library). Purging Expired Metadata \u00b6 purge ( thresholdBatchId : Long ): Unit purge is part of the MetadataLog abstraction. purge ...FIXME Batch Files \u00b6 HDFSMetadataLog considers a file a batch file when the name is simply a long number. HDFSMetadataLog uses a Hadoop PathFilter to list only batch files. Verifying Batch Ids \u00b6 verifyBatchIds ( batchIds : Seq [ Long ], startId : Option [ Long ], endId : Option [ Long ]): Unit verifyBatchIds ...FIXME verifyBatchIds is used when: FileStreamSourceLog is requested to get HDFSMetadataLog is requested to get Path of Metadata File by Batch Id \u00b6 batchIdToPath ( batchId : Long ): Path batchIdToPath simply creates a Hadoop Path for the file by the given batchId under the metadata log directory . Batch Id by Path of Metadata File \u00b6 pathToBatchId ( path : Path ): Long pathToBatchId ...FIXME","title":"HDFSMetadataLog"},{"location":"HDFSMetadataLog/#hdfsmetadatalog","text":"HDFSMetadataLog is an extension of the MetadataLog abstraction for metadata storage to store batch files in a metadata log directory on Hadoop DFS (for fault-tolerance and reliability).","title":"HDFSMetadataLog"},{"location":"HDFSMetadataLog/#extensions","text":"CommitLog CompactibleFileStreamLog KafkaSourceInitialOffsetWriter OffsetSeqLog","title":"Extensions"},{"location":"HDFSMetadataLog/#creating-instance","text":"HDFSMetadataLog takes the following to be created: SparkSession Path of the metadata log directory While being created, HDFSMetadataLog makes sure that the path exists (and creates it if not).","title":"Creating Instance"},{"location":"HDFSMetadataLog/#metadata-log-directory","text":"HDFSMetadataLog uses the given path as the metadata log directory with metadata logs ( one per batch ). The path is immediately converted to a Hadoop Path for file management.","title":" Metadata Log Directory"},{"location":"HDFSMetadataLog/#checkpointfilemanager","text":"HDFSMetadataLog creates a CheckpointFileManager (with the metadata log directory ) when created .","title":" CheckpointFileManager"},{"location":"HDFSMetadataLog/#implicit-json4s-formats","text":"HDFSMetadataLog uses Json4s with the Jackson binding for metadata serialization and deserialization (to and from JSON format).","title":" Implicit Json4s Formats"},{"location":"HDFSMetadataLog/#latest-committed-batch-id-with-metadata-if-available","text":"getLatest (): Option [( Long , T )] getLatest is a part of MetadataLog abstraction. getLatest requests the internal < > for the files in < > that match < >. getLatest takes the batch ids (the batch files correspond to) and sorts the ids in reverse order. getLatest gives the first batch id with the metadata which < >. Note It is possible that the batch id could be in the metadata storage, but not available for retrieval.","title":" Latest Committed Batch Id with Metadata (If Available)"},{"location":"HDFSMetadataLog/#retrieving-metadata-of-streaming-batch-if-available","text":"get ( batchId : Long ): Option [ T ] get is part of the MetadataLog abstraction. get ...FIXME","title":" Retrieving Metadata of Streaming Batch (if Available)"},{"location":"HDFSMetadataLog/#deserializing-metadata","text":"deserialize ( in : InputStream ): T deserialize deserializes a metadata (of type T ) from a given InputStream . deserialize is used to retrieve metadata of a batch .","title":" Deserializing Metadata"},{"location":"HDFSMetadataLog/#retrieving-metadata-of-streaming-batches-if-available","text":"get ( startId : Option [ Long ], endId : Option [ Long ]): Array [( Long , T )] get is part of the MetadataLog abstraction. get ...FIXME","title":" Retrieving Metadata of Streaming Batches (if Available)"},{"location":"HDFSMetadataLog/#persisting-metadata-of-streaming-micro-batch","text":"add ( batchId : Long , metadata : T ): Boolean add is part of the MetadataLog abstraction. add return true when the metadata of the streaming batch was not available and persisted successfully. Otherwise, add returns false . Internally, add < > ( batchId ) and returns false when found. Otherwise, when not found, add < > for the given batchId and < >. add returns true if successful.","title":" Persisting Metadata of Streaming Micro-Batch"},{"location":"HDFSMetadataLog/#writing-batch-metadata-to-file-metadata-log","text":"writeBatchToFile ( metadata : T , path : Path ): Unit writeBatchToFile requests the < > to createAtomic (for the specified path and the overwriteIfPossible flag disabled). writeBatchToFile then < > (to the CancellableFSDataOutputStream output stream) and closes the stream. In case of an exception, writeBatchToFile simply requests the CancellableFSDataOutputStream output stream to cancel (so that the output file is not generated) and re-throws the exception.","title":" Writing Batch Metadata to File (Metadata Log)"},{"location":"HDFSMetadataLog/#serializing-metadata","text":"serialize ( metadata : T , out : OutputStream ): Unit serialize simply writes out the log data in a serialized format (using Json4s (with Jackson binding) library).","title":" Serializing Metadata"},{"location":"HDFSMetadataLog/#purging-expired-metadata","text":"purge ( thresholdBatchId : Long ): Unit purge is part of the MetadataLog abstraction. purge ...FIXME","title":" Purging Expired Metadata"},{"location":"HDFSMetadataLog/#batch-files","text":"HDFSMetadataLog considers a file a batch file when the name is simply a long number. HDFSMetadataLog uses a Hadoop PathFilter to list only batch files.","title":" Batch Files"},{"location":"HDFSMetadataLog/#verifying-batch-ids","text":"verifyBatchIds ( batchIds : Seq [ Long ], startId : Option [ Long ], endId : Option [ Long ]): Unit verifyBatchIds ...FIXME verifyBatchIds is used when: FileStreamSourceLog is requested to get HDFSMetadataLog is requested to get","title":" Verifying Batch Ids"},{"location":"HDFSMetadataLog/#path-of-metadata-file-by-batch-id","text":"batchIdToPath ( batchId : Long ): Path batchIdToPath simply creates a Hadoop Path for the file by the given batchId under the metadata log directory .","title":" Path of Metadata File by Batch Id"},{"location":"HDFSMetadataLog/#batch-id-by-path-of-metadata-file","text":"pathToBatchId ( path : Path ): Long pathToBatchId ...FIXME","title":" Batch Id by Path of Metadata File"},{"location":"IncrementalExecution/","text":"IncrementalExecution \u00b6 IncrementalExecution is the QueryExecution of streaming queries. Tip Learn more about QueryExecution in The Internals of Spark SQL online book. statefulOperatorId \u00b6 IncrementalExecution uses the statefulOperatorId internal counter for the IDs of the stateful operators in the optimized logical plan (while applying the preparations rules) when requested to prepare the plan for execution (in executedPlan phase). Preparing Logical Plan (of Streaming Query) for Execution \u00b6 When requested for the optimized logical plan (of the logical plan ), IncrementalExecution transforms CurrentBatchTimestamp and ExpressionWithRandomSeed expressions with the timestamp literal and new random seeds, respectively. When transforming CurrentBatchTimestamp expressions, IncrementalExecution prints out the following INFO message to the logs: Current batch timestamp = [timestamp] Right after being created , IncrementalExecution is executed (in the queryPlanning phase by the MicroBatchExecution and ContinuousExecution stream execution engines) and so the entire query execution pipeline is executed up to and including executedPlan . That means that the extra planning strategies and the state preparation rule have been applied at this point and the streaming query is ready for execution. Creating Instance \u00b6 IncrementalExecution takes the following to be created: SparkSession LogicalPlan OutputMode (as specified using DataStreamWriter.outputMode method) State checkpoint location ID of a streaming query Run ID of a streaming query Batch ID OffsetSeqMetadata IncrementalExecution is created (and becomes the StreamExecution.lastExecution ) when: MicroBatchExecution is requested to run a single streaming micro-batch (in queryPlanning phase) ContinuousExecution is requested to run a streaming query in continuous mode (in queryPlanning phase) Dataset.explain operator is executed (on a streaming query) State Checkpoint Location \u00b6 IncrementalExecution is given the checkpoint location when created . For the two available execution engines ( MicroBatchExecution and ContinuousExecution ), the checkpoint location is actually state directory under the checkpoint root directory . val queryName = \"rate2memory\" val checkpointLocation = s\"file:/tmp/checkpoint-$queryName\" val query = spark .readStream .format(\"rate\") .load .writeStream .format(\"memory\") .queryName(queryName) .option(\"checkpointLocation\", checkpointLocation) .start // Give the streaming query a moment (one micro-batch) // So lastExecution is available for the checkpointLocation import scala.concurrent.duration._ query.awaitTermination(1.second.toMillis) import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val stateCheckpointDir = query .asInstanceOf[StreamingQueryWrapper] .streamingQuery .lastExecution .checkpointLocation val stateDir = s\"$checkpointLocation/state\" assert(stateCheckpointDir equals stateDir) State checkpoint location is used when IncrementalExecution is requested for the state info of the next stateful operator (when requested to optimize a streaming physical plan using the state preparation rule that creates the stateful physical operators: StateStoreSaveExec , StateStoreRestoreExec , StreamingDeduplicateExec , FlatMapGroupsWithStateExec , StreamingSymmetricHashJoinExec , and StreamingGlobalLimitExec ). Number of State Stores (spark.sql.shuffle.partitions) \u00b6 numStateStores : Int numStateStores is the number of state stores which corresponds to spark.sql.shuffle.partitions configuration property (default: 200 ). Tip Learn more about spark.sql.shuffle.partitions configuration property in The Internals of Spark SQL online book. Internally, numStateStores requests the OffsetSeqMetadata for the spark.sql.shuffle.partitions configuration property (using the streaming configuration ) or simply takes whatever was defined for the given SparkSession (default: 200 ). numStateStores is initialized right when IncrementalExecution is created . numStateStores is used when IncrementalExecution is requested for the state info of the next stateful operator (when requested to optimize a streaming physical plan using the state preparation rule that creates the stateful physical operators: StateStoreSaveExec , StateStoreRestoreExec , StreamingDeduplicateExec , FlatMapGroupsWithStateExec , StreamingSymmetricHashJoinExec , and StreamingGlobalLimitExec ). Extra Planning Strategies for Streaming Queries \u00b6 IncrementalExecution uses a custom SparkPlanner with the following extra planning strategies to plan the streaming query for execution: StreamingJoinStrategy StatefulAggregationStrategy FlatMapGroupsWithStateStrategy StreamingRelationStrategy StreamingDeduplicationStrategy StreamingGlobalLimitStrategy Tip Learn more about SparkPlanner in The Internals of Spark SQL online book. State Preparation Rule For Execution-Specific Configuration \u00b6 state : Rule [ SparkPlan ] state is a custom physical preparation rule ( Rule[SparkPlan] ) that can transform a streaming physical plan ( SparkPlan ) with the following physical operators: StateStoreSaveExec with any unary physical operator ( UnaryExecNode ) with a StateStoreRestoreExec StreamingDeduplicateExec FlatMapGroupsWithStateExec StreamingSymmetricHashJoinExec StreamingGlobalLimitExec state simply transforms the physical plan with the above physical operators and fills out the execution-specific configuration: nextStatefulOperationStateInfo for the state info OutputMode batchWatermarkMs (through the OffsetSeqMetadata ) for the event-time watermark batchTimestampMs (through the OffsetSeqMetadata ) for the current timestamp getStateWatermarkPredicates for the state watermark predicates (for StreamingSymmetricHashJoinExec ) state rule is used (as part of the physical query optimizations) when IncrementalExecution is requested to optimize (prepare) the physical plan of the streaming query (once for ContinuousExecution and every trigger for MicroBatchExecution in queryPlanning phase). Tip Learn more about Physical Query Optimizations in The Internals of Spark SQL online book. Next StatefulOperationStateInfo \u00b6 nextStatefulOperationStateInfo (): StatefulOperatorStateInfo nextStatefulOperationStateInfo simply creates a new StatefulOperatorStateInfo with the state checkpoint location , the run ID (of the streaming query), the next statefulOperator ID , the current batch ID , and the number of state stores . Note The only changing part of StatefulOperatorStateInfo across calls of the nextStatefulOperationStateInfo method is the the next statefulOperator ID . All the other properties (the state checkpoint location , the run ID , the current batch ID , and the number of state stores ) are the same within a single IncrementalExecution instance. The only two properties that may ever change are the run ID (after a streaming query is restarted from the checkpoint) and the current batch ID (every micro-batch in MicroBatchExecution execution engine). nextStatefulOperationStateInfo is used when IncrementalExecution is requested to optimize a streaming physical plan using the state preparation rule (and creates the stateful physical operators: StateStoreSaveExec , StateStoreRestoreExec , StreamingDeduplicateExec , FlatMapGroupsWithStateExec , StreamingSymmetricHashJoinExec , and StreamingGlobalLimitExec ). Checking Out Whether Last Execution Requires Another Non-Data Micro-Batch \u00b6 shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ): Boolean shouldRunAnotherBatch is positive ( true ) if there is at least one StateStoreWriter operator (in the executedPlan physical query plan ) that requires another non-data batch (per the given OffsetSeqMetadata with the event-time watermark and the batch timestamp). Otherwise, shouldRunAnotherBatch is negative ( false ). shouldRunAnotherBatch is used when MicroBatchExecution is requested to construct the next streaming micro-batch (and checks out whether the last batch execution requires another non-data batch). Demo: State Checkpoint Directory \u00b6 Using setConf(SHUFFLE_PARTITIONS, 1) will make for an easier debugging as the state is then only for one partition and makes monitoring easier. import org . apache . spark . sql . internal . SQLConf . SHUFFLE_PARTITIONS spark . sessionState . conf . setConf ( SHUFFLE_PARTITIONS , 1 ) assert ( spark . sessionState . conf . numShufflePartitions == 1 ) Using the rate source as an input. val counts = spark . readStream . format ( \"rate\" ) . load . groupBy ( window ( $ \"timestamp\" , \"5 seconds\" ) as \"group\" ) . agg ( count ( \"value\" ) as \"value_count\" ) // <-- creates an Aggregate logical operator . orderBy ( \"group\" ) // <-- makes for easier checking assert ( counts . isStreaming , \"This should be a streaming query\" ) Searching for checkpoint = <unknown> in the following output for StateStoreSaveExec and StateStoreRestoreExec physical operators. scala> counts.explain == Physical Plan == *(5) Sort [group#5 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(group#5 ASC NULLS FIRST, 1) +- *(4) HashAggregate(keys=[window#11], functions=[count(value#1L)]) +- StateStoreSave [window#11], state info [ checkpoint = <unknown>, runId = 558bf725-accb-487d-97eb-f790fa4a6138, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2 +- *(3) HashAggregate(keys=[window#11], functions=[merge_count(value#1L)]) +- StateStoreRestore [window#11], state info [ checkpoint = <unknown>, runId = 558bf725-accb-487d-97eb-f790fa4a6138, opId = 0, ver = 0, numPartitions = 1], 2 +- *(2) HashAggregate(keys=[window#11], functions=[merge_count(value#1L)]) +- Exchange hashpartitioning(window#11, 1) +- *(1) HashAggregate(keys=[window#11], functions=[partial_count(value#1L)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#11, value#1L] +- *(1) Filter isnotnull(timestamp#0) +- StreamingRelation rate, [timestamp#0, value#1L] Start the query with the checkpointLocation option. val checkpointLocation = \"/tmp/spark-streams-state-checkpoint-root\" import scala . concurrent . duration . _ import org . apache . spark . sql . streaming .{ OutputMode , Trigger } val t = Trigger . ProcessingTime ( 1 . hour ) // should be enough time for exploration val sq = counts . writeStream . format ( \"console\" ) . option ( \"truncate\" , false ) . option ( \"checkpointLocation\" , checkpointLocation ) . trigger ( t ) . outputMode ( OutputMode . Complete ) . start Wait till the first batch which should happen right after start and access lastExecution that has the checkpoint resolved. import org . apache . spark . sql . execution . streaming . _ val lastExecution = sq . asInstanceOf [ StreamingQueryWrapper ]. streamingQuery . lastExecution assert ( lastExecution . checkpointLocation == s\"file: ${ checkpointLocation } /state\" )","title":"IncrementalExecution"},{"location":"IncrementalExecution/#incrementalexecution","text":"IncrementalExecution is the QueryExecution of streaming queries. Tip Learn more about QueryExecution in The Internals of Spark SQL online book.","title":"IncrementalExecution"},{"location":"IncrementalExecution/#statefuloperatorid","text":"IncrementalExecution uses the statefulOperatorId internal counter for the IDs of the stateful operators in the optimized logical plan (while applying the preparations rules) when requested to prepare the plan for execution (in executedPlan phase).","title":" statefulOperatorId"},{"location":"IncrementalExecution/#preparing-logical-plan-of-streaming-query-for-execution","text":"When requested for the optimized logical plan (of the logical plan ), IncrementalExecution transforms CurrentBatchTimestamp and ExpressionWithRandomSeed expressions with the timestamp literal and new random seeds, respectively. When transforming CurrentBatchTimestamp expressions, IncrementalExecution prints out the following INFO message to the logs: Current batch timestamp = [timestamp] Right after being created , IncrementalExecution is executed (in the queryPlanning phase by the MicroBatchExecution and ContinuousExecution stream execution engines) and so the entire query execution pipeline is executed up to and including executedPlan . That means that the extra planning strategies and the state preparation rule have been applied at this point and the streaming query is ready for execution.","title":" Preparing Logical Plan (of Streaming Query) for Execution"},{"location":"IncrementalExecution/#creating-instance","text":"IncrementalExecution takes the following to be created: SparkSession LogicalPlan OutputMode (as specified using DataStreamWriter.outputMode method) State checkpoint location ID of a streaming query Run ID of a streaming query Batch ID OffsetSeqMetadata IncrementalExecution is created (and becomes the StreamExecution.lastExecution ) when: MicroBatchExecution is requested to run a single streaming micro-batch (in queryPlanning phase) ContinuousExecution is requested to run a streaming query in continuous mode (in queryPlanning phase) Dataset.explain operator is executed (on a streaming query)","title":"Creating Instance"},{"location":"IncrementalExecution/#state-checkpoint-location","text":"IncrementalExecution is given the checkpoint location when created . For the two available execution engines ( MicroBatchExecution and ContinuousExecution ), the checkpoint location is actually state directory under the checkpoint root directory . val queryName = \"rate2memory\" val checkpointLocation = s\"file:/tmp/checkpoint-$queryName\" val query = spark .readStream .format(\"rate\") .load .writeStream .format(\"memory\") .queryName(queryName) .option(\"checkpointLocation\", checkpointLocation) .start // Give the streaming query a moment (one micro-batch) // So lastExecution is available for the checkpointLocation import scala.concurrent.duration._ query.awaitTermination(1.second.toMillis) import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val stateCheckpointDir = query .asInstanceOf[StreamingQueryWrapper] .streamingQuery .lastExecution .checkpointLocation val stateDir = s\"$checkpointLocation/state\" assert(stateCheckpointDir equals stateDir) State checkpoint location is used when IncrementalExecution is requested for the state info of the next stateful operator (when requested to optimize a streaming physical plan using the state preparation rule that creates the stateful physical operators: StateStoreSaveExec , StateStoreRestoreExec , StreamingDeduplicateExec , FlatMapGroupsWithStateExec , StreamingSymmetricHashJoinExec , and StreamingGlobalLimitExec ).","title":"State Checkpoint Location"},{"location":"IncrementalExecution/#number-of-state-stores-sparksqlshufflepartitions","text":"numStateStores : Int numStateStores is the number of state stores which corresponds to spark.sql.shuffle.partitions configuration property (default: 200 ). Tip Learn more about spark.sql.shuffle.partitions configuration property in The Internals of Spark SQL online book. Internally, numStateStores requests the OffsetSeqMetadata for the spark.sql.shuffle.partitions configuration property (using the streaming configuration ) or simply takes whatever was defined for the given SparkSession (default: 200 ). numStateStores is initialized right when IncrementalExecution is created . numStateStores is used when IncrementalExecution is requested for the state info of the next stateful operator (when requested to optimize a streaming physical plan using the state preparation rule that creates the stateful physical operators: StateStoreSaveExec , StateStoreRestoreExec , StreamingDeduplicateExec , FlatMapGroupsWithStateExec , StreamingSymmetricHashJoinExec , and StreamingGlobalLimitExec ).","title":" Number of State Stores (spark.sql.shuffle.partitions)"},{"location":"IncrementalExecution/#extra-planning-strategies-for-streaming-queries","text":"IncrementalExecution uses a custom SparkPlanner with the following extra planning strategies to plan the streaming query for execution: StreamingJoinStrategy StatefulAggregationStrategy FlatMapGroupsWithStateStrategy StreamingRelationStrategy StreamingDeduplicationStrategy StreamingGlobalLimitStrategy Tip Learn more about SparkPlanner in The Internals of Spark SQL online book.","title":" Extra Planning Strategies for Streaming Queries"},{"location":"IncrementalExecution/#state-preparation-rule-for-execution-specific-configuration","text":"state : Rule [ SparkPlan ] state is a custom physical preparation rule ( Rule[SparkPlan] ) that can transform a streaming physical plan ( SparkPlan ) with the following physical operators: StateStoreSaveExec with any unary physical operator ( UnaryExecNode ) with a StateStoreRestoreExec StreamingDeduplicateExec FlatMapGroupsWithStateExec StreamingSymmetricHashJoinExec StreamingGlobalLimitExec state simply transforms the physical plan with the above physical operators and fills out the execution-specific configuration: nextStatefulOperationStateInfo for the state info OutputMode batchWatermarkMs (through the OffsetSeqMetadata ) for the event-time watermark batchTimestampMs (through the OffsetSeqMetadata ) for the current timestamp getStateWatermarkPredicates for the state watermark predicates (for StreamingSymmetricHashJoinExec ) state rule is used (as part of the physical query optimizations) when IncrementalExecution is requested to optimize (prepare) the physical plan of the streaming query (once for ContinuousExecution and every trigger for MicroBatchExecution in queryPlanning phase). Tip Learn more about Physical Query Optimizations in The Internals of Spark SQL online book.","title":" State Preparation Rule For Execution-Specific Configuration"},{"location":"IncrementalExecution/#next-statefuloperationstateinfo","text":"nextStatefulOperationStateInfo (): StatefulOperatorStateInfo nextStatefulOperationStateInfo simply creates a new StatefulOperatorStateInfo with the state checkpoint location , the run ID (of the streaming query), the next statefulOperator ID , the current batch ID , and the number of state stores . Note The only changing part of StatefulOperatorStateInfo across calls of the nextStatefulOperationStateInfo method is the the next statefulOperator ID . All the other properties (the state checkpoint location , the run ID , the current batch ID , and the number of state stores ) are the same within a single IncrementalExecution instance. The only two properties that may ever change are the run ID (after a streaming query is restarted from the checkpoint) and the current batch ID (every micro-batch in MicroBatchExecution execution engine). nextStatefulOperationStateInfo is used when IncrementalExecution is requested to optimize a streaming physical plan using the state preparation rule (and creates the stateful physical operators: StateStoreSaveExec , StateStoreRestoreExec , StreamingDeduplicateExec , FlatMapGroupsWithStateExec , StreamingSymmetricHashJoinExec , and StreamingGlobalLimitExec ).","title":" Next StatefulOperationStateInfo"},{"location":"IncrementalExecution/#checking-out-whether-last-execution-requires-another-non-data-micro-batch","text":"shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ): Boolean shouldRunAnotherBatch is positive ( true ) if there is at least one StateStoreWriter operator (in the executedPlan physical query plan ) that requires another non-data batch (per the given OffsetSeqMetadata with the event-time watermark and the batch timestamp). Otherwise, shouldRunAnotherBatch is negative ( false ). shouldRunAnotherBatch is used when MicroBatchExecution is requested to construct the next streaming micro-batch (and checks out whether the last batch execution requires another non-data batch).","title":" Checking Out Whether Last Execution Requires Another Non-Data Micro-Batch"},{"location":"IncrementalExecution/#demo-state-checkpoint-directory","text":"Using setConf(SHUFFLE_PARTITIONS, 1) will make for an easier debugging as the state is then only for one partition and makes monitoring easier. import org . apache . spark . sql . internal . SQLConf . SHUFFLE_PARTITIONS spark . sessionState . conf . setConf ( SHUFFLE_PARTITIONS , 1 ) assert ( spark . sessionState . conf . numShufflePartitions == 1 ) Using the rate source as an input. val counts = spark . readStream . format ( \"rate\" ) . load . groupBy ( window ( $ \"timestamp\" , \"5 seconds\" ) as \"group\" ) . agg ( count ( \"value\" ) as \"value_count\" ) // <-- creates an Aggregate logical operator . orderBy ( \"group\" ) // <-- makes for easier checking assert ( counts . isStreaming , \"This should be a streaming query\" ) Searching for checkpoint = <unknown> in the following output for StateStoreSaveExec and StateStoreRestoreExec physical operators. scala> counts.explain == Physical Plan == *(5) Sort [group#5 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(group#5 ASC NULLS FIRST, 1) +- *(4) HashAggregate(keys=[window#11], functions=[count(value#1L)]) +- StateStoreSave [window#11], state info [ checkpoint = <unknown>, runId = 558bf725-accb-487d-97eb-f790fa4a6138, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2 +- *(3) HashAggregate(keys=[window#11], functions=[merge_count(value#1L)]) +- StateStoreRestore [window#11], state info [ checkpoint = <unknown>, runId = 558bf725-accb-487d-97eb-f790fa4a6138, opId = 0, ver = 0, numPartitions = 1], 2 +- *(2) HashAggregate(keys=[window#11], functions=[merge_count(value#1L)]) +- Exchange hashpartitioning(window#11, 1) +- *(1) HashAggregate(keys=[window#11], functions=[partial_count(value#1L)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#11, value#1L] +- *(1) Filter isnotnull(timestamp#0) +- StreamingRelation rate, [timestamp#0, value#1L] Start the query with the checkpointLocation option. val checkpointLocation = \"/tmp/spark-streams-state-checkpoint-root\" import scala . concurrent . duration . _ import org . apache . spark . sql . streaming .{ OutputMode , Trigger } val t = Trigger . ProcessingTime ( 1 . hour ) // should be enough time for exploration val sq = counts . writeStream . format ( \"console\" ) . option ( \"truncate\" , false ) . option ( \"checkpointLocation\" , checkpointLocation ) . trigger ( t ) . outputMode ( OutputMode . Complete ) . start Wait till the first batch which should happen right after start and access lastExecution that has the checkpoint resolved. import org . apache . spark . sql . execution . streaming . _ val lastExecution = sq . asInstanceOf [ StreamingQueryWrapper ]. streamingQuery . lastExecution assert ( lastExecution . checkpointLocation == s\"file: ${ checkpointLocation } /state\" )","title":"Demo: State Checkpoint Directory"},{"location":"InputProcessor/","text":"InputProcessor \u00b6 InputProcessor is a helper class that is used to update state (in the state store ) of a single partition of a FlatMapGroupsWithStateExec physical operator. Creating Instance \u00b6 InputProcessor takes the following to be created: StateStore InputProcessor is created when FlatMapGroupsWithStateExec physical operator is executed (for storeUpdateFunction while processing rows per partition with a corresponding per-partition state store). StateStore \u00b6 InputProcessor is given a StateStore when created . The StateStore manages the per-group state (and is used when processing new data and timed-out state data , and in the \"all rows processed\" callback ). Processing New Data \u00b6 processNewData ( dataIter : Iterator [ InternalRow ]): Iterator [ InternalRow ] processNewData creates a grouped iterator of (of pairs of) per-group state keys and the row values from the given data iterator ( dataIter ) with the grouping attributes and the output schema of the child operator (of the parent FlatMapGroupsWithStateExec physical operator). For every per-group state key (in the grouped iterator), processNewData requests the StateManager (of the parent FlatMapGroupsWithStateExec physical operator) to get the state (from the StateStore ) and callFunctionAndUpdateState (with the hasTimedOut flag off). processNewData is used when FlatMapGroupsWithStateExec physical operator is executed. Processing Timed-Out State Data \u00b6 processTimedOutState (): Iterator [ InternalRow ] processTimedOutState does nothing and simply returns an empty iterator for GroupStateTimeout.NoTimeout . With timeout enabled , processTimedOutState gets the current timeout threshold per GroupStateTimeout : batchTimestampMs for ProcessingTimeTimeout eventTimeWatermark for EventTimeTimeout processTimedOutState creates an iterator of timed-out state data by requesting the StateManager for all the available state data (in the StateStore ) and takes only the state data with timeout defined and below the current timeout threshold. In the end, for every timed-out state data, processTimedOutState callFunctionAndUpdateState (with the hasTimedOut flag on). processTimedOutState is used when FlatMapGroupsWithStateExec physical operator is executed. callFunctionAndUpdateState Internal Method \u00b6 callFunctionAndUpdateState ( stateData : StateData , valueRowIter : Iterator [ InternalRow ], hasTimedOut : Boolean ): Iterator [ InternalRow ] callFunctionAndUpdateState is used when InputProcessor is requested to process new data and timed-out state data with the given hasTimedOut flag is off and on, respectively. callFunctionAndUpdateState creates a key object by requesting the given StateData for the UnsafeRow of the key ( keyRow ) and converts it to an object (using the internal state key converter ). callFunctionAndUpdateState creates value objects by taking every value row (from the given valueRowIter iterator) and converts them to objects (using the internal state value converter ). callFunctionAndUpdateState creates a new GroupStateImpl with the following: The current state value (of the given StateData ) that could possibly be null The batchTimestampMs of the parent FlatMapGroupsWithStateExec operator (that could possibly be -1 ) The event-time watermark of the parent FlatMapGroupsWithStateExec operator (that could possibly be -1 ) The GroupStateTimeout of the parent FlatMapGroupsWithStateExec operator The watermarkPresent flag of the parent FlatMapGroupsWithStateExec operator The given hasTimedOut flag callFunctionAndUpdateState then executes the user-defined state function (of the parent FlatMapGroupsWithStateExec operator) on the key object, value objects, and the newly-created GroupStateImpl . For every output value from the user-defined state function, callFunctionAndUpdateState updates numOutputRows performance metric and wraps the values to an internal row (using the internal output value converter ). In the end, callFunctionAndUpdateState returns a Iterator[InternalRow] which calls the completion function right after rows have been processed (so the iterator is considered fully consumed). \"All Rows Processed\" Callback \u00b6 onIteratorCompletion : Unit onIteratorCompletion branches off per whether the GroupStateImpl has been marked removed and no timeout timestamp is specified or not. When the GroupStateImpl has been marked removed and no timeout timestamp is specified, onIteratorCompletion does the following: . Requests the StateManager (of the parent FlatMapGroupsWithStateExec operator) to remove the state (from the StateStore for the key row of the given StateData ) . Increments the numUpdatedStateRows performance metric Otherwise, when the GroupStateImpl has not been marked removed or the timeout timestamp is specified, onIteratorCompletion checks whether the timeout timestamp has changed by comparing the timeout timestamps of the GroupStateImpl and the given StateData . (only when the GroupStateImpl has been updated , removed or the timeout timestamp changed) onIteratorCompletion does the following: . Requests the StateManager (of the parent FlatMapGroupsWithStateExec operator) to persist the state (in the StateStore with the key row, updated state object, and the timeout timestamp of the given StateData ) . Increments the numUpdatedStateRows performance metrics onIteratorCompletion is used when InputProcessor is requested to callFunctionAndUpdateState (right after rows have been processed) Converters \u00b6 Output Value Converter \u00b6 An output value converter (of type Any => InternalRow ) to wrap a given output value (from the user-defined state function) to a row The data type of the row is specified as the data type of the output object attribute when the parent FlatMapGroupsWithStateExec operator is created Used when InputProcessor is requested to callFunctionAndUpdateState . State Key Converter \u00b6 A state key converter (of type InternalRow => Any ) to deserialize a given row (for a per-group state key) to the current state value The deserialization expression for keys is specified as the key deserializer expression when the parent FlatMapGroupsWithStateExec operator is created The data type of state keys is specified as the grouping attributes when the parent FlatMapGroupsWithStateExec operator is created Used when InputProcessor is requested to callFunctionAndUpdateState . State Value Converter \u00b6 A state value converter (of type InternalRow => Any ) to deserialize a given row (for a per-group state value) to a Scala value The deserialization expression for values is specified as the value deserializer expression when the parent FlatMapGroupsWithStateExec operator is created The data type of state values is specified as the data attributes when the parent FlatMapGroupsWithStateExec operator is created Used when InputProcessor is requested to callFunctionAndUpdateState .","title":"InputProcessor"},{"location":"InputProcessor/#inputprocessor","text":"InputProcessor is a helper class that is used to update state (in the state store ) of a single partition of a FlatMapGroupsWithStateExec physical operator.","title":"InputProcessor"},{"location":"InputProcessor/#creating-instance","text":"InputProcessor takes the following to be created: StateStore InputProcessor is created when FlatMapGroupsWithStateExec physical operator is executed (for storeUpdateFunction while processing rows per partition with a corresponding per-partition state store).","title":"Creating Instance"},{"location":"InputProcessor/#statestore","text":"InputProcessor is given a StateStore when created . The StateStore manages the per-group state (and is used when processing new data and timed-out state data , and in the \"all rows processed\" callback ).","title":" StateStore"},{"location":"InputProcessor/#processing-new-data","text":"processNewData ( dataIter : Iterator [ InternalRow ]): Iterator [ InternalRow ] processNewData creates a grouped iterator of (of pairs of) per-group state keys and the row values from the given data iterator ( dataIter ) with the grouping attributes and the output schema of the child operator (of the parent FlatMapGroupsWithStateExec physical operator). For every per-group state key (in the grouped iterator), processNewData requests the StateManager (of the parent FlatMapGroupsWithStateExec physical operator) to get the state (from the StateStore ) and callFunctionAndUpdateState (with the hasTimedOut flag off). processNewData is used when FlatMapGroupsWithStateExec physical operator is executed.","title":" Processing New Data"},{"location":"InputProcessor/#processing-timed-out-state-data","text":"processTimedOutState (): Iterator [ InternalRow ] processTimedOutState does nothing and simply returns an empty iterator for GroupStateTimeout.NoTimeout . With timeout enabled , processTimedOutState gets the current timeout threshold per GroupStateTimeout : batchTimestampMs for ProcessingTimeTimeout eventTimeWatermark for EventTimeTimeout processTimedOutState creates an iterator of timed-out state data by requesting the StateManager for all the available state data (in the StateStore ) and takes only the state data with timeout defined and below the current timeout threshold. In the end, for every timed-out state data, processTimedOutState callFunctionAndUpdateState (with the hasTimedOut flag on). processTimedOutState is used when FlatMapGroupsWithStateExec physical operator is executed.","title":" Processing Timed-Out State Data"},{"location":"InputProcessor/#callfunctionandupdatestate-internal-method","text":"callFunctionAndUpdateState ( stateData : StateData , valueRowIter : Iterator [ InternalRow ], hasTimedOut : Boolean ): Iterator [ InternalRow ] callFunctionAndUpdateState is used when InputProcessor is requested to process new data and timed-out state data with the given hasTimedOut flag is off and on, respectively. callFunctionAndUpdateState creates a key object by requesting the given StateData for the UnsafeRow of the key ( keyRow ) and converts it to an object (using the internal state key converter ). callFunctionAndUpdateState creates value objects by taking every value row (from the given valueRowIter iterator) and converts them to objects (using the internal state value converter ). callFunctionAndUpdateState creates a new GroupStateImpl with the following: The current state value (of the given StateData ) that could possibly be null The batchTimestampMs of the parent FlatMapGroupsWithStateExec operator (that could possibly be -1 ) The event-time watermark of the parent FlatMapGroupsWithStateExec operator (that could possibly be -1 ) The GroupStateTimeout of the parent FlatMapGroupsWithStateExec operator The watermarkPresent flag of the parent FlatMapGroupsWithStateExec operator The given hasTimedOut flag callFunctionAndUpdateState then executes the user-defined state function (of the parent FlatMapGroupsWithStateExec operator) on the key object, value objects, and the newly-created GroupStateImpl . For every output value from the user-defined state function, callFunctionAndUpdateState updates numOutputRows performance metric and wraps the values to an internal row (using the internal output value converter ). In the end, callFunctionAndUpdateState returns a Iterator[InternalRow] which calls the completion function right after rows have been processed (so the iterator is considered fully consumed).","title":" callFunctionAndUpdateState Internal Method"},{"location":"InputProcessor/#all-rows-processed-callback","text":"onIteratorCompletion : Unit onIteratorCompletion branches off per whether the GroupStateImpl has been marked removed and no timeout timestamp is specified or not. When the GroupStateImpl has been marked removed and no timeout timestamp is specified, onIteratorCompletion does the following: . Requests the StateManager (of the parent FlatMapGroupsWithStateExec operator) to remove the state (from the StateStore for the key row of the given StateData ) . Increments the numUpdatedStateRows performance metric Otherwise, when the GroupStateImpl has not been marked removed or the timeout timestamp is specified, onIteratorCompletion checks whether the timeout timestamp has changed by comparing the timeout timestamps of the GroupStateImpl and the given StateData . (only when the GroupStateImpl has been updated , removed or the timeout timestamp changed) onIteratorCompletion does the following: . Requests the StateManager (of the parent FlatMapGroupsWithStateExec operator) to persist the state (in the StateStore with the key row, updated state object, and the timeout timestamp of the given StateData ) . Increments the numUpdatedStateRows performance metrics onIteratorCompletion is used when InputProcessor is requested to callFunctionAndUpdateState (right after rows have been processed)","title":" \"All Rows Processed\" Callback"},{"location":"InputProcessor/#converters","text":"","title":"Converters"},{"location":"InputProcessor/#output-value-converter","text":"An output value converter (of type Any => InternalRow ) to wrap a given output value (from the user-defined state function) to a row The data type of the row is specified as the data type of the output object attribute when the parent FlatMapGroupsWithStateExec operator is created Used when InputProcessor is requested to callFunctionAndUpdateState .","title":" Output Value Converter"},{"location":"InputProcessor/#state-key-converter","text":"A state key converter (of type InternalRow => Any ) to deserialize a given row (for a per-group state key) to the current state value The deserialization expression for keys is specified as the key deserializer expression when the parent FlatMapGroupsWithStateExec operator is created The data type of state keys is specified as the grouping attributes when the parent FlatMapGroupsWithStateExec operator is created Used when InputProcessor is requested to callFunctionAndUpdateState .","title":" State Key Converter"},{"location":"InputProcessor/#state-value-converter","text":"A state value converter (of type InternalRow => Any ) to deserialize a given row (for a per-group state value) to a Scala value The deserialization expression for values is specified as the value deserializer expression when the parent FlatMapGroupsWithStateExec operator is created The data type of state values is specified as the data attributes when the parent FlatMapGroupsWithStateExec operator is created Used when InputProcessor is requested to callFunctionAndUpdateState .","title":" State Value Converter"},{"location":"KeyValueGroupedDataset/","text":"KeyValueGroupedDataset \u00b6 KeyValueGroupedDataset represents a grouped dataset as a result of Dataset.groupByKey operator (that aggregates records by a grouping function). // Dataset[T] groupByKey(func: T => K): KeyValueGroupedDataset[K, T] import java.sql.Timestamp val numGroups = spark. readStream. format(\"rate\"). load. as[(Timestamp, Long)]. groupByKey { case (time, value) => value % 2 } scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] KeyValueGroupedDataset is also < > for < > and < > operators. scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] scala> :type numGroups.keyAs[String] org.apache.spark.sql.KeyValueGroupedDataset[String,(java.sql.Timestamp, Long)] scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] val mapped = numGroups.mapValues { case (ts, n) => s\"($ts, $n)\" } scala> :type mapped org.apache.spark.sql.KeyValueGroupedDataset[Long,String] KeyValueGroupedDataset works for batch and streaming aggregations, but shines the most when used for Streaming Aggregation . scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ numGroups. mapGroups { case(group, values) => values.size }. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). start ------------------------------------------- Batch: 0 ------------------------------------------- +-----+ |value| +-----+ +-----+ ------------------------------------------- Batch: 1 ------------------------------------------- +-----+ |value| +-----+ | 3| | 2| +-----+ ------------------------------------------- Batch: 2 ------------------------------------------- +-----+ |value| +-----+ | 5| | 5| +-----+ // Eventually... spark.streams.active.foreach(_.stop) The most prestigious use case of KeyValueGroupedDataset however is Arbitrary Stateful Streaming Aggregation that allows for accumulating streaming state (by means of GroupState ) using < > and the more advanced < > operators. [[operators]] .KeyValueGroupedDataset's Operators [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Operator | Description | agg a| [[agg]] [source, scala] \u00b6 agg U1 : Dataset[(K, U1)] agg U1, U2 : Dataset[(K, U1, U2)] agg U1, U2, U3 : Dataset[(K, U1, U2, U3)] agg U1, U2, U3, U4 : Dataset[(K, U1, U2, U3, U4)] | cogroup a| [[cogroup]] [source, scala] \u00b6 cogroup U, R : Encoder ( f: (K, Iterator[V], Iterator[U]) => TraversableOnce[R]): Dataset[R] | count a| [[count]] [source, scala] \u00b6 count(): Dataset[(K, Long)] \u00b6 | flatMapGroups a| [[flatMapGroups]] [source, scala] \u00b6 flatMapGroups U : Encoder : Dataset[U] \u00b6 | flatMapGroupsWithState a| [[flatMapGroupsWithState]] flatMapGroupsWithState [ S : Encoder , U : Encoder ]( outputMode : OutputMode , timeoutConf : GroupStateTimeout )( func : ( K , Iterator [ V ], GroupState [ S ]) => Iterator [ U ]): Dataset [ U ] Arbitrary Stateful Streaming Aggregation - streaming aggregation with explicit state and state timeout Note The difference between this flatMapGroupsWithState and mapGroupsWithState operators is the state function that generates zero or more elements (that are in turn the rows in the result streaming Dataset ). | keyAs a| [[keyAs]] [source, scala] \u00b6 keys: Dataset[K] keyAs[L : Encoder]: KeyValueGroupedDataset[L, V] | mapGroups a| [[mapGroups]] [source, scala] \u00b6 mapGroups U : Encoder : Dataset[U] \u00b6 | spark-sql-streaming-KeyValueGroupedDataset-mapGroupsWithState.md[mapGroupsWithState] a| [[mapGroupsWithState]] mapGroupsWithState [ S : Encoder , U : Encoder ]( func : ( K , Iterator [ V ], GroupState [ S ]) => U ): Dataset [ U ] mapGroupsWithState [ S : Encoder , U : Encoder ]( timeoutConf : GroupStateTimeout )( func : ( K , Iterator [ V ], GroupState [ S ]) => U ): Dataset [ U ] Creates a new Dataset with FlatMapGroupsWithState logical operator Note The difference between mapGroupsWithState and flatMapGroupsWithState is the state function that generates exactly one element (that is in turn the row in the result Dataset ). | mapValues a| [[mapValues]] [source, scala] \u00b6 mapValues W : Encoder : KeyValueGroupedDataset[K, W] \u00b6 | reduceGroups a| [[reduceGroups]] [source, scala] \u00b6 reduceGroups(f: (V, V) => V): Dataset[(K, V)] \u00b6 |=== === [[creating-instance]] Creating KeyValueGroupedDataset Instance KeyValueGroupedDataset takes the following when created: [[kEncoder]] Encoder for keys [[vEncoder]] Encoder for values [[queryExecution]] QueryExecution [[dataAttributes]] Data attributes [[groupingAttributes]] Grouping attributes","title":"KeyValueGroupedDataset"},{"location":"KeyValueGroupedDataset/#keyvaluegroupeddataset","text":"KeyValueGroupedDataset represents a grouped dataset as a result of Dataset.groupByKey operator (that aggregates records by a grouping function). // Dataset[T] groupByKey(func: T => K): KeyValueGroupedDataset[K, T] import java.sql.Timestamp val numGroups = spark. readStream. format(\"rate\"). load. as[(Timestamp, Long)]. groupByKey { case (time, value) => value % 2 } scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] KeyValueGroupedDataset is also < > for < > and < > operators. scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] scala> :type numGroups.keyAs[String] org.apache.spark.sql.KeyValueGroupedDataset[String,(java.sql.Timestamp, Long)] scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] val mapped = numGroups.mapValues { case (ts, n) => s\"($ts, $n)\" } scala> :type mapped org.apache.spark.sql.KeyValueGroupedDataset[Long,String] KeyValueGroupedDataset works for batch and streaming aggregations, but shines the most when used for Streaming Aggregation . scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ numGroups. mapGroups { case(group, values) => values.size }. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). start ------------------------------------------- Batch: 0 ------------------------------------------- +-----+ |value| +-----+ +-----+ ------------------------------------------- Batch: 1 ------------------------------------------- +-----+ |value| +-----+ | 3| | 2| +-----+ ------------------------------------------- Batch: 2 ------------------------------------------- +-----+ |value| +-----+ | 5| | 5| +-----+ // Eventually... spark.streams.active.foreach(_.stop) The most prestigious use case of KeyValueGroupedDataset however is Arbitrary Stateful Streaming Aggregation that allows for accumulating streaming state (by means of GroupState ) using < > and the more advanced < > operators. [[operators]] .KeyValueGroupedDataset's Operators [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Operator | Description | agg a| [[agg]]","title":"KeyValueGroupedDataset"},{"location":"KeyValueGroupedDataset/#source-scala","text":"agg U1 : Dataset[(K, U1)] agg U1, U2 : Dataset[(K, U1, U2)] agg U1, U2, U3 : Dataset[(K, U1, U2, U3)] agg U1, U2, U3, U4 : Dataset[(K, U1, U2, U3, U4)] | cogroup a| [[cogroup]]","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#source-scala_1","text":"cogroup U, R : Encoder ( f: (K, Iterator[V], Iterator[U]) => TraversableOnce[R]): Dataset[R] | count a| [[count]]","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#source-scala_2","text":"","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#count-datasetk-long","text":"| flatMapGroups a| [[flatMapGroups]]","title":"count(): Dataset[(K, Long)]"},{"location":"KeyValueGroupedDataset/#source-scala_3","text":"","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#flatmapgroupsu-encoder-datasetu","text":"| flatMapGroupsWithState a| [[flatMapGroupsWithState]] flatMapGroupsWithState [ S : Encoder , U : Encoder ]( outputMode : OutputMode , timeoutConf : GroupStateTimeout )( func : ( K , Iterator [ V ], GroupState [ S ]) => Iterator [ U ]): Dataset [ U ] Arbitrary Stateful Streaming Aggregation - streaming aggregation with explicit state and state timeout Note The difference between this flatMapGroupsWithState and mapGroupsWithState operators is the state function that generates zero or more elements (that are in turn the rows in the result streaming Dataset ). | keyAs a| [[keyAs]]","title":"flatMapGroupsU : Encoder: Dataset[U]"},{"location":"KeyValueGroupedDataset/#source-scala_4","text":"keys: Dataset[K] keyAs[L : Encoder]: KeyValueGroupedDataset[L, V] | mapGroups a| [[mapGroups]]","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#source-scala_5","text":"","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#mapgroupsu-encoder-datasetu","text":"| spark-sql-streaming-KeyValueGroupedDataset-mapGroupsWithState.md[mapGroupsWithState] a| [[mapGroupsWithState]] mapGroupsWithState [ S : Encoder , U : Encoder ]( func : ( K , Iterator [ V ], GroupState [ S ]) => U ): Dataset [ U ] mapGroupsWithState [ S : Encoder , U : Encoder ]( timeoutConf : GroupStateTimeout )( func : ( K , Iterator [ V ], GroupState [ S ]) => U ): Dataset [ U ] Creates a new Dataset with FlatMapGroupsWithState logical operator Note The difference between mapGroupsWithState and flatMapGroupsWithState is the state function that generates exactly one element (that is in turn the row in the result Dataset ). | mapValues a| [[mapValues]]","title":"mapGroupsU : Encoder: Dataset[U]"},{"location":"KeyValueGroupedDataset/#source-scala_6","text":"","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#mapvaluesw-encoder-keyvaluegroupeddatasetk-w","text":"| reduceGroups a| [[reduceGroups]]","title":"mapValuesW : Encoder: KeyValueGroupedDataset[K, W]"},{"location":"KeyValueGroupedDataset/#source-scala_7","text":"","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#reducegroupsf-v-v-v-datasetk-v","text":"|=== === [[creating-instance]] Creating KeyValueGroupedDataset Instance KeyValueGroupedDataset takes the following when created: [[kEncoder]] Encoder for keys [[vEncoder]] Encoder for values [[queryExecution]] QueryExecution [[dataAttributes]] Data attributes [[groupingAttributes]] Grouping attributes","title":"reduceGroups(f: (V, V) =&gt; V): Dataset[(K, V)]"},{"location":"MetadataLog/","text":"MetadataLog \u00b6 MetadataLog is an abstraction of metadata logs that can add , get , getLatest and purge metadata (of type T ). Type Constructor MetadataLog[T] is a Scala type constructor with the type parameter T Contract \u00b6 Storing Metadata of Streaming Batch \u00b6 add ( batchId : Long , metadata : T ): Boolean Stores ( adds ) metadata of a streaming batch Used when: KafkaMicroBatchStream is requested to getOrCreateInitialPartitionOffsets KafkaSource is requested for the initialPartitionOffsets CompactibleFileStreamLog is requested for the store metadata of a streaming batch and to compact FileStreamSource is requested to fetchMaxOffset FileStreamSourceLog is requested to store (add) metadata of a streaming batch ManifestFileCommitProtocol is requested to commitJob MicroBatchExecution stream execution engine is requested to < > and < > ContinuousExecution stream execution engine is requested to < > and < > RateStreamMicroBatchReader is created ( creationTimeMs ) get \u00b6 get ( batchId : Long ): Option [ T ] get ( startId : Option [ Long ], endId : Option [ Long ]): Array [( Long , T )] Looks up ( gets ) metadata of one or more streaming batches Used when...FIXME getLatest \u00b6 getLatest (): Option [( Long , T )] Looks up the latest-committed metadata (if available) Used when...FIXME purge \u00b6 purge ( thresholdBatchId : Long ): Unit Purging ( removing ) metadata older than the given threshold Used when...FIXME Implementations \u00b6 HDFSMetadataLog","title":"MetadataLog"},{"location":"MetadataLog/#metadatalog","text":"MetadataLog is an abstraction of metadata logs that can add , get , getLatest and purge metadata (of type T ). Type Constructor MetadataLog[T] is a Scala type constructor with the type parameter T","title":"MetadataLog"},{"location":"MetadataLog/#contract","text":"","title":"Contract"},{"location":"MetadataLog/#storing-metadata-of-streaming-batch","text":"add ( batchId : Long , metadata : T ): Boolean Stores ( adds ) metadata of a streaming batch Used when: KafkaMicroBatchStream is requested to getOrCreateInitialPartitionOffsets KafkaSource is requested for the initialPartitionOffsets CompactibleFileStreamLog is requested for the store metadata of a streaming batch and to compact FileStreamSource is requested to fetchMaxOffset FileStreamSourceLog is requested to store (add) metadata of a streaming batch ManifestFileCommitProtocol is requested to commitJob MicroBatchExecution stream execution engine is requested to < > and < > ContinuousExecution stream execution engine is requested to < > and < > RateStreamMicroBatchReader is created ( creationTimeMs )","title":" Storing Metadata of Streaming Batch"},{"location":"MetadataLog/#get","text":"get ( batchId : Long ): Option [ T ] get ( startId : Option [ Long ], endId : Option [ Long ]): Array [( Long , T )] Looks up ( gets ) metadata of one or more streaming batches Used when...FIXME","title":" get"},{"location":"MetadataLog/#getlatest","text":"getLatest (): Option [( Long , T )] Looks up the latest-committed metadata (if available) Used when...FIXME","title":" getLatest"},{"location":"MetadataLog/#purge","text":"purge ( thresholdBatchId : Long ): Unit Purging ( removing ) metadata older than the given threshold Used when...FIXME","title":" purge"},{"location":"MetadataLog/#implementations","text":"HDFSMetadataLog","title":"Implementations"},{"location":"MicroBatchStream/","text":"MicroBatchStream \u00b6 MicroBatchStream is an extension of the SparkDataStream abstraction for streaming sources for Micro-Batch Stream Processing . Contract \u00b6 Creating PartitionReaderFactory \u00b6 PartitionReaderFactory createReaderFactory () Used when MicroBatchScanExec physical operator is requested for a PartitionReaderFactory Latest Offset \u00b6 Offset latestOffset () Used when MicroBatchExecution is requested to constructNextBatch Input Partitions \u00b6 InputPartition [] planInputPartitions ( Offset start , Offset end ) Used when MicroBatchScanExec physical operator is requested for partitions Implementations \u00b6 KafkaMicroBatchStream MemoryStream RateStreamMicroBatchStream TextSocketMicroBatchStream","title":"MicroBatchStream"},{"location":"MicroBatchStream/#microbatchstream","text":"MicroBatchStream is an extension of the SparkDataStream abstraction for streaming sources for Micro-Batch Stream Processing .","title":"MicroBatchStream"},{"location":"MicroBatchStream/#contract","text":"","title":"Contract"},{"location":"MicroBatchStream/#creating-partitionreaderfactory","text":"PartitionReaderFactory createReaderFactory () Used when MicroBatchScanExec physical operator is requested for a PartitionReaderFactory","title":" Creating PartitionReaderFactory"},{"location":"MicroBatchStream/#latest-offset","text":"Offset latestOffset () Used when MicroBatchExecution is requested to constructNextBatch","title":" Latest Offset"},{"location":"MicroBatchStream/#input-partitions","text":"InputPartition [] planInputPartitions ( Offset start , Offset end ) Used when MicroBatchScanExec physical operator is requested for partitions","title":" Input Partitions"},{"location":"MicroBatchStream/#implementations","text":"KafkaMicroBatchStream MemoryStream RateStreamMicroBatchStream TextSocketMicroBatchStream","title":"Implementations"},{"location":"Offset/","text":"Offset -- Read Position of Streaming Query \u00b6 Offset is the < > of < > that represent progress of a streaming query in < > format. [[contract]] .Offset Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | json a| [[json]] [source, java] \u00b6 String json() \u00b6 Converts the offset to JSON format (JSON-encoded offset) Used when: MicroBatchExecution stream execution engine is requested to construct the next streaming micro-batch and run a streaming micro-batch (with MicroBatchReader sources) OffsetSeq is requested for the textual representation OffsetSeqLog is requested to serialize metadata (write metadata in serialized format) ProgressReporter is requested to record trigger offsets ContinuousExecution stream execution engine is requested to run a streaming query in continuous mode and commit an epoch |=== [[extensions]] .Offsets [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Offset | Description | ContinuousMemoryStreamOffset | [[ContinuousMemoryStreamOffset]] | FileStreamSourceOffset | [[FileStreamSourceOffset]] | KafkaSourceOffset | [[KafkaSourceOffset]] | LongOffset | [[LongOffset]] | RateStreamOffset | [[RateStreamOffset]] | SerializedOffset | [[SerializedOffset]] JSON-encoded offset that is used when loading an offset from an external storage, e.g. from checkpoint after restart | TextSocketOffset | [[TextSocketOffset]] |===","title":"Offset"},{"location":"Offset/#offset-read-position-of-streaming-query","text":"Offset is the < > of < > that represent progress of a streaming query in < > format. [[contract]] .Offset Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | json a| [[json]]","title":"Offset -- Read Position of Streaming Query"},{"location":"Offset/#source-java","text":"","title":"[source, java]"},{"location":"Offset/#string-json","text":"Converts the offset to JSON format (JSON-encoded offset) Used when: MicroBatchExecution stream execution engine is requested to construct the next streaming micro-batch and run a streaming micro-batch (with MicroBatchReader sources) OffsetSeq is requested for the textual representation OffsetSeqLog is requested to serialize metadata (write metadata in serialized format) ProgressReporter is requested to record trigger offsets ContinuousExecution stream execution engine is requested to run a streaming query in continuous mode and commit an epoch |=== [[extensions]] .Offsets [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Offset | Description | ContinuousMemoryStreamOffset | [[ContinuousMemoryStreamOffset]] | FileStreamSourceOffset | [[FileStreamSourceOffset]] | KafkaSourceOffset | [[KafkaSourceOffset]] | LongOffset | [[LongOffset]] | RateStreamOffset | [[RateStreamOffset]] | SerializedOffset | [[SerializedOffset]] JSON-encoded offset that is used when loading an offset from an external storage, e.g. from checkpoint after restart | TextSocketOffset | [[TextSocketOffset]] |===","title":"String json()"},{"location":"OffsetSeq/","text":"OffsetSeq \u00b6 OffsetSeq is the metadata managed by Hadoop DFS-based metadata storage . OffsetSeq is < > (possibly using the < > factory methods) when: OffsetSeqLog is requested to deserialize metadata (retrieve metadata from a persistent storage) StreamProgress is requested to convert itself to OffsetSeq (most importantly when MicroBatchExecution stream execution engine is requested to construct the next streaming micro-batch to commit available offsets for a batch to the write-ahead log ) ContinuousExecution stream execution engine is requested to < > and < > Creating Instance \u00b6 OffsetSeq takes the following when created: [[offsets]] Collection of optional Offsets (with None for < >) [[metadata]] Optional OffsetSeqMetadata (default: None ) === [[toStreamProgress]] Converting to StreamProgress -- toStreamProgress Method [source, scala] \u00b6 toStreamProgress( sources: Seq[BaseStreamingSource]): StreamProgress toStreamProgress creates a new StreamProgress and adds the streaming sources for which there are new offsets available. NOTE: < > is a collection with holes (empty elements) for streaming sources with no new data available. toStreamProgress throws an AssertionError if the number of the input sources does not match the < >: There are [[offsets.size]] sources in the checkpoint offsets and now there are [[sources.size]] sources requested by the query. Cannot continue. toStreamProgress is used when: MicroBatchExecution is requested to < > and < > ContinuousExecution is requested for < > === [[toString]] Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString simply converts the < > to JSON (if an offset is available) or - (a dash if an offset is not available for a streaming source at that position). === [[fill]] Creating OffsetSeq Instance -- fill Factory Methods [source, scala] \u00b6 fill( offsets: Offset*): OffsetSeq // <1> fill( metadata: Option[String], offsets: Offset*): OffsetSeq <1> Uses no metadata ( None ) fill simply creates an < > for the given variable sequence of Offset s and the optional OffsetSeqMetadata (in JSON format). fill is used when: OffsetSeqLog is requested to deserialize metadata ContinuousExecution stream execution engine is requested to get start offsets and addOffset","title":"OffsetSeq"},{"location":"OffsetSeq/#offsetseq","text":"OffsetSeq is the metadata managed by Hadoop DFS-based metadata storage . OffsetSeq is < > (possibly using the < > factory methods) when: OffsetSeqLog is requested to deserialize metadata (retrieve metadata from a persistent storage) StreamProgress is requested to convert itself to OffsetSeq (most importantly when MicroBatchExecution stream execution engine is requested to construct the next streaming micro-batch to commit available offsets for a batch to the write-ahead log ) ContinuousExecution stream execution engine is requested to < > and < >","title":"OffsetSeq"},{"location":"OffsetSeq/#creating-instance","text":"OffsetSeq takes the following when created: [[offsets]] Collection of optional Offsets (with None for < >) [[metadata]] Optional OffsetSeqMetadata (default: None ) === [[toStreamProgress]] Converting to StreamProgress -- toStreamProgress Method","title":"Creating Instance"},{"location":"OffsetSeq/#source-scala","text":"toStreamProgress( sources: Seq[BaseStreamingSource]): StreamProgress toStreamProgress creates a new StreamProgress and adds the streaming sources for which there are new offsets available. NOTE: < > is a collection with holes (empty elements) for streaming sources with no new data available. toStreamProgress throws an AssertionError if the number of the input sources does not match the < >: There are [[offsets.size]] sources in the checkpoint offsets and now there are [[sources.size]] sources requested by the query. Cannot continue. toStreamProgress is used when: MicroBatchExecution is requested to < > and < > ContinuousExecution is requested for < > === [[toString]] Textual Representation -- toString Method","title":"[source, scala]"},{"location":"OffsetSeq/#source-scala_1","text":"","title":"[source, scala]"},{"location":"OffsetSeq/#tostring-string","text":"NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString simply converts the < > to JSON (if an offset is available) or - (a dash if an offset is not available for a streaming source at that position). === [[fill]] Creating OffsetSeq Instance -- fill Factory Methods","title":"toString: String"},{"location":"OffsetSeq/#source-scala_2","text":"fill( offsets: Offset*): OffsetSeq // <1> fill( metadata: Option[String], offsets: Offset*): OffsetSeq <1> Uses no metadata ( None ) fill simply creates an < > for the given variable sequence of Offset s and the optional OffsetSeqMetadata (in JSON format). fill is used when: OffsetSeqLog is requested to deserialize metadata ContinuousExecution stream execution engine is requested to get start offsets and addOffset","title":"[source, scala]"},{"location":"OffsetSeqLog/","text":"OffsetSeqLog \u2014 Hadoop DFS-based Metadata Storage of OffsetSeqs \u00b6 OffsetSeqLog is a Hadoop DFS-based metadata storage for OffsetSeq metadata. OffsetSeqLog is created as the write-ahead log (WAL) of offsets of streaming query execution engines . [[OffsetSeq]][[offsets]][[metadata]] OffsetSeqLog uses OffsetSeq for metadata which holds an ordered collection of offsets and optional metadata (as OffsetSeqMetadata for event-time watermark ). [[VERSION]] OffsetSeqLog uses 1 for the version when < > and < > metadata. Creating Instance \u00b6 OffsetSeqLog takes the following to be created: [[sparkSession]] SparkSession [[path]] Path of the metadata log directory === [[serialize]] Serializing Metadata (Writing Metadata in Serialized Format) -- serialize Method [source, scala] \u00b6 serialize( offsetSeq: OffsetSeq, out: OutputStream): Unit serialize firstly writes out the < > prefixed with v on a single line (e.g. v1 ) followed by the optional metadata in JSON format. serialize then writes out the offsets in JSON format, one per line. NOTE: No offsets to write in offsetSeq for a streaming source is marked as - (a dash) in the log. $ ls -tr [checkpoint-directory]/offsets 0 1 2 3 4 5 6 $ cat [checkpoint-directory]/offsets/6 v1 {\"batchWatermarkMs\":0,\"batchTimestampMs\":1502872590006,\"conf\":{\"spark.sql.shuffle.partitions\":\"200\",\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\"}} 51 serialize is part of HDFSMetadataLog abstraction. === [[deserialize]] Deserializing Metadata (Reading OffsetSeq from Serialized Format) -- deserialize Method [source, scala] \u00b6 deserialize(in: InputStream): OffsetSeq \u00b6 deserialize firstly parses the < > on the first line. deserialize reads the optional metadata (with an empty line for metadata not available). deserialize creates a SerializedOffset for every line left. In the end, deserialize creates a OffsetSeq for the optional metadata and the SerializedOffsets . When there are no lines in the InputStream , deserialize throws an IllegalStateException : Incomplete log file deserialize is part of HDFSMetadataLog abstraction.","title":"OffsetSeqLog"},{"location":"OffsetSeqLog/#offsetseqlog-hadoop-dfs-based-metadata-storage-of-offsetseqs","text":"OffsetSeqLog is a Hadoop DFS-based metadata storage for OffsetSeq metadata. OffsetSeqLog is created as the write-ahead log (WAL) of offsets of streaming query execution engines . [[OffsetSeq]][[offsets]][[metadata]] OffsetSeqLog uses OffsetSeq for metadata which holds an ordered collection of offsets and optional metadata (as OffsetSeqMetadata for event-time watermark ). [[VERSION]] OffsetSeqLog uses 1 for the version when < > and < > metadata.","title":"OffsetSeqLog &mdash; Hadoop DFS-based Metadata Storage of OffsetSeqs"},{"location":"OffsetSeqLog/#creating-instance","text":"OffsetSeqLog takes the following to be created: [[sparkSession]] SparkSession [[path]] Path of the metadata log directory === [[serialize]] Serializing Metadata (Writing Metadata in Serialized Format) -- serialize Method","title":"Creating Instance"},{"location":"OffsetSeqLog/#source-scala","text":"serialize( offsetSeq: OffsetSeq, out: OutputStream): Unit serialize firstly writes out the < > prefixed with v on a single line (e.g. v1 ) followed by the optional metadata in JSON format. serialize then writes out the offsets in JSON format, one per line. NOTE: No offsets to write in offsetSeq for a streaming source is marked as - (a dash) in the log. $ ls -tr [checkpoint-directory]/offsets 0 1 2 3 4 5 6 $ cat [checkpoint-directory]/offsets/6 v1 {\"batchWatermarkMs\":0,\"batchTimestampMs\":1502872590006,\"conf\":{\"spark.sql.shuffle.partitions\":\"200\",\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\"}} 51 serialize is part of HDFSMetadataLog abstraction. === [[deserialize]] Deserializing Metadata (Reading OffsetSeq from Serialized Format) -- deserialize Method","title":"[source, scala]"},{"location":"OffsetSeqLog/#source-scala_1","text":"","title":"[source, scala]"},{"location":"OffsetSeqLog/#deserializein-inputstream-offsetseq","text":"deserialize firstly parses the < > on the first line. deserialize reads the optional metadata (with an empty line for metadata not available). deserialize creates a SerializedOffset for every line left. In the end, deserialize creates a OffsetSeq for the optional metadata and the SerializedOffsets . When there are no lines in the InputStream , deserialize throws an IllegalStateException : Incomplete log file deserialize is part of HDFSMetadataLog abstraction.","title":"deserialize(in: InputStream): OffsetSeq"},{"location":"OffsetSeqMetadata/","text":"OffsetSeqMetadata \u2014 Metadata of Streaming Batch \u00b6 OffsetSeqMetadata holds the metadata for the current streaming batch: [[batchWatermarkMs]] Event-time watermark threshold [[batchTimestampMs]] Batch timestamp (in millis) [[conf]] Streaming configuration with spark.sql.shuffle.partitions and spark.sql.streaming.stateStore.providerClass configuration properties OffsetSeqMetadata is used mainly when IncrementalExecution is created. [[relevantSQLConfs]] OffsetSeqMetadata considers some configuration properties as relevantSQLConfs : SHUFFLE_PARTITIONS STATE_STORE_PROVIDER_CLASS STREAMING_MULTIPLE_WATERMARK_POLICY FLATMAPGROUPSWITHSTATE_STATE_FORMAT_VERSION STREAMING_AGGREGATION_STATE_FORMAT_VERSION relevantSQLConfs are used when OffsetSeqMetadata is created and is requested to setSessionConf . Creating OffsetSeqMetadata \u00b6 apply ( batchWatermarkMs : Long , batchTimestampMs : Long , sessionConf : RuntimeConfig ): OffsetSeqMetadata apply ...FIXME apply is used when...FIXME === [[setSessionConf]] setSessionConf Method [source, scala] \u00b6 setSessionConf(metadata: OffsetSeqMetadata, sessionConf: RuntimeConfig): Unit \u00b6 setSessionConf ...FIXME NOTE: setSessionConf is used when...FIXME","title":"OffsetSeqMetadata"},{"location":"OffsetSeqMetadata/#offsetseqmetadata-metadata-of-streaming-batch","text":"OffsetSeqMetadata holds the metadata for the current streaming batch: [[batchWatermarkMs]] Event-time watermark threshold [[batchTimestampMs]] Batch timestamp (in millis) [[conf]] Streaming configuration with spark.sql.shuffle.partitions and spark.sql.streaming.stateStore.providerClass configuration properties OffsetSeqMetadata is used mainly when IncrementalExecution is created. [[relevantSQLConfs]] OffsetSeqMetadata considers some configuration properties as relevantSQLConfs : SHUFFLE_PARTITIONS STATE_STORE_PROVIDER_CLASS STREAMING_MULTIPLE_WATERMARK_POLICY FLATMAPGROUPSWITHSTATE_STATE_FORMAT_VERSION STREAMING_AGGREGATION_STATE_FORMAT_VERSION relevantSQLConfs are used when OffsetSeqMetadata is created and is requested to setSessionConf .","title":"OffsetSeqMetadata &mdash; Metadata of Streaming Batch"},{"location":"OffsetSeqMetadata/#creating-offsetseqmetadata","text":"apply ( batchWatermarkMs : Long , batchTimestampMs : Long , sessionConf : RuntimeConfig ): OffsetSeqMetadata apply ...FIXME apply is used when...FIXME === [[setSessionConf]] setSessionConf Method","title":" Creating OffsetSeqMetadata"},{"location":"OffsetSeqMetadata/#source-scala","text":"","title":"[source, scala]"},{"location":"OffsetSeqMetadata/#setsessionconfmetadata-offsetseqmetadata-sessionconf-runtimeconfig-unit","text":"setSessionConf ...FIXME NOTE: setSessionConf is used when...FIXME","title":"setSessionConf(metadata: OffsetSeqMetadata, sessionConf: RuntimeConfig): Unit"},{"location":"OneSideHashJoiner/","text":"OneSideHashJoiner \u00b6 OneSideHashJoiner manages join state of one side of a < > (using < >). OneSideHashJoiner is < > exclusively for < > physical operator (when requested to < >). .OneSideHashJoiner and StreamingSymmetricHashJoinExec image::images/OneSideHashJoiner.png[align=\"center\"] StreamingSymmetricHashJoinExec physical operator uses two OneSideHashJoiners per side of the stream-stream join (< > and < > sides). OneSideHashJoiner uses an < > to < >. NOTE: OneSideHashJoiner is a Scala private internal class of < > and so has full access to StreamingSymmetricHashJoinExec properties. Creating OneSideHashJoiner Instance \u00b6 OneSideHashJoiner takes the following to be created: [[joinSide]] JoinSide [[inputAttributes]] Input attributes ( Seq[Attribute] ) [[joinKeys]] Join keys ( Seq[Expression] ) [[inputIter]] Input rows ( Iterator[InternalRow] ) [[preJoinFilterExpr]] Optional pre-join filter Catalyst expression [[postJoinFilter]] Post-join filter ( (InternalRow) => Boolean ) < > OneSideHashJoiner initializes the < >. === [[joinStateManager]] SymmetricHashJoinStateManager -- joinStateManager Internal Property [source, scala] \u00b6 joinStateManager: SymmetricHashJoinStateManager \u00b6 joinStateManager is a SymmetricHashJoinStateManager that is created for a OneSideHashJoiner (with the < >, the < >, the < >, and the < > of the owning < >). joinStateManager is used when OneSideHashJoiner is requested for the following: < > < > < > < > === [[updatedStateRowsCount]] Number of Updated State Rows -- updatedStateRowsCount Internal Counter updatedStateRowsCount is the number the join keys and associated rows that were persisted as a join state, i.e. how many times < > requested the < > to append the join key and the input row (to a join state). updatedStateRowsCount is then used (via < > method) for the < > performance metric. updatedStateRowsCount is available via numUpdatedStateRows method. [[numUpdatedStateRows]] [source, scala] numUpdatedStateRows: Long \u00b6 NOTE: numUpdatedStateRows is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < > (and < >). === [[stateWatermarkPredicate]] Optional Join State Watermark Predicate -- stateWatermarkPredicate Internal Property [source, scala] \u00b6 stateWatermarkPredicate: Option[JoinStateWatermarkPredicate] \u00b6 When < >, OneSideHashJoiner is given a < >. stateWatermarkPredicate is used for the < > (when a < >) and the < > (when a < >) that are both used when OneSideHashJoiner is requested to < >. === [[storeAndJoinWithOtherSide]] storeAndJoinWithOtherSide Method [source, scala] \u00b6 storeAndJoinWithOtherSide( otherSideJoiner: OneSideHashJoiner)( generateJoinedRow: (InternalRow, InternalRow) => JoinedRow): Iterator[InternalRow] storeAndJoinWithOtherSide tries to find the watermark attribute among the input attributes . storeAndJoinWithOtherSide creates a watermark expression (for the watermark attribute and the current event-time watermark ). [[storeAndJoinWithOtherSide-nonLateRows]] With the watermark attribute found, storeAndJoinWithOtherSide generates a new predicate for the watermark expression and the < > that is then used to filter out ( exclude ) late rows from the < >. Otherwise, the input rows are left unchanged (i.e. no rows are considered late and excluded). [[storeAndJoinWithOtherSide-nonLateRows-flatMap]] For every < > (possibly < >), storeAndJoinWithOtherSide applies the < > predicate and branches off per result (< > or < >). NOTE: storeAndJoinWithOtherSide is used when StreamingSymmetricHashJoinExec physical operator is requested to < >. ==== [[preJoinFilter-true]] preJoinFilter Predicate Positive ( true ) When the < > predicate succeeds on an input row, storeAndJoinWithOtherSide extracts the join key (using the < >) and requests the given OneSideHashJoiner ( otherSideJoiner ) for the < > that is in turn requested for the state values for the extracted join key. The values are then processed ( mapped over ) using the given generateJoinedRow function and then filtered by the < >. storeAndJoinWithOtherSide uses the < > (on the extracted join key) and the < > (on the current input row) to determine whether to request the < > to append the key and the input row (to a join state). If so, storeAndJoinWithOtherSide increments the < > counter. ==== [[preJoinFilter-false]] preJoinFilter Predicate Negative ( false ) When the < > predicate fails on an input row, storeAndJoinWithOtherSide creates a new Iterator[InternalRow] of joined rows per < > and < >: For LeftSide and LeftOuter , the join row is the current row with the values of the right side all null ( nullRight ) For RightSide and RightOuter , the join row is the current row with the values of the left side all null ( nullLeft ) For all other combinations, the iterator is simply empty (that will be removed from the output by the outer < >). === [[removeOldState]] Removing Old State -- removeOldState Method [source, scala] \u00b6 removeOldState(): Iterator[UnsafeRowPair] \u00b6 removeOldState branches off per the < >: For < >, removeOldState requests the < > to removeByKeyCondition (with the < >) For < >, removeOldState requests the < > to removeByValueCondition (with the < >) For any other predicates, removeOldState returns an empty iterator (no rows to process) NOTE: removeOldState is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >. === [[get]] Retrieving Value Rows For Key -- get Method [source, scala] \u00b6 get(key: UnsafeRow): Iterator[UnsafeRow] \u00b6 get simply requests the < > to retrieve value rows for the key . NOTE: get is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >. === [[commitStateAndGetMetrics]] Committing State (Changes) and Requesting Performance Metrics -- commitStateAndGetMetrics Method [source, scala] \u00b6 commitStateAndGetMetrics(): StateStoreMetrics \u00b6 commitStateAndGetMetrics simply requests the < > to commit followed by requesting for the performance metrics . commitStateAndGetMetrics is used when StreamingSymmetricHashJoinExec physical operator is requested to < >. Internal Properties \u00b6 [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | keyGenerator a| [[keyGenerator]] [source, scala] \u00b6 keyGenerator: UnsafeProjection \u00b6 Function to project ( extract ) join keys from an input row Used when...FIXME | preJoinFilter a| [[preJoinFilter]] [source, scala] \u00b6 preJoinFilter: InternalRow => Boolean \u00b6 Used when...FIXME | stateKeyWatermarkPredicateFunc a| [[stateKeyWatermarkPredicateFunc]] [source, scala] \u00b6 stateKeyWatermarkPredicateFunc: InternalRow => Boolean \u00b6 Predicate for late rows based on the < > Used for the following: < > (and check out whether to append a row to the SymmetricHashJoinStateManager ) < > | stateValueWatermarkPredicateFunc a| [[stateValueWatermarkPredicateFunc]] [source, scala] \u00b6 stateValueWatermarkPredicateFunc: InternalRow => Boolean \u00b6 Predicate for late rows based on the < > Used for the following: < > (and check out whether to append a row to the < >) < > |===","title":"OneSideHashJoiner"},{"location":"OneSideHashJoiner/#onesidehashjoiner","text":"OneSideHashJoiner manages join state of one side of a < > (using < >). OneSideHashJoiner is < > exclusively for < > physical operator (when requested to < >). .OneSideHashJoiner and StreamingSymmetricHashJoinExec image::images/OneSideHashJoiner.png[align=\"center\"] StreamingSymmetricHashJoinExec physical operator uses two OneSideHashJoiners per side of the stream-stream join (< > and < > sides). OneSideHashJoiner uses an < > to < >. NOTE: OneSideHashJoiner is a Scala private internal class of < > and so has full access to StreamingSymmetricHashJoinExec properties.","title":"OneSideHashJoiner"},{"location":"OneSideHashJoiner/#creating-onesidehashjoiner-instance","text":"OneSideHashJoiner takes the following to be created: [[joinSide]] JoinSide [[inputAttributes]] Input attributes ( Seq[Attribute] ) [[joinKeys]] Join keys ( Seq[Expression] ) [[inputIter]] Input rows ( Iterator[InternalRow] ) [[preJoinFilterExpr]] Optional pre-join filter Catalyst expression [[postJoinFilter]] Post-join filter ( (InternalRow) => Boolean ) < > OneSideHashJoiner initializes the < >. === [[joinStateManager]] SymmetricHashJoinStateManager -- joinStateManager Internal Property","title":"Creating OneSideHashJoiner Instance"},{"location":"OneSideHashJoiner/#source-scala","text":"","title":"[source, scala]"},{"location":"OneSideHashJoiner/#joinstatemanager-symmetrichashjoinstatemanager","text":"joinStateManager is a SymmetricHashJoinStateManager that is created for a OneSideHashJoiner (with the < >, the < >, the < >, and the < > of the owning < >). joinStateManager is used when OneSideHashJoiner is requested for the following: < > < > < > < > === [[updatedStateRowsCount]] Number of Updated State Rows -- updatedStateRowsCount Internal Counter updatedStateRowsCount is the number the join keys and associated rows that were persisted as a join state, i.e. how many times < > requested the < > to append the join key and the input row (to a join state). updatedStateRowsCount is then used (via < > method) for the < > performance metric. updatedStateRowsCount is available via numUpdatedStateRows method. [[numUpdatedStateRows]] [source, scala]","title":"joinStateManager: SymmetricHashJoinStateManager"},{"location":"OneSideHashJoiner/#numupdatedstaterows-long","text":"NOTE: numUpdatedStateRows is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < > (and < >). === [[stateWatermarkPredicate]] Optional Join State Watermark Predicate -- stateWatermarkPredicate Internal Property","title":"numUpdatedStateRows: Long"},{"location":"OneSideHashJoiner/#source-scala_1","text":"","title":"[source, scala]"},{"location":"OneSideHashJoiner/#statewatermarkpredicate-optionjoinstatewatermarkpredicate","text":"When < >, OneSideHashJoiner is given a < >. stateWatermarkPredicate is used for the < > (when a < >) and the < > (when a < >) that are both used when OneSideHashJoiner is requested to < >. === [[storeAndJoinWithOtherSide]] storeAndJoinWithOtherSide Method","title":"stateWatermarkPredicate: Option[JoinStateWatermarkPredicate]"},{"location":"OneSideHashJoiner/#source-scala_2","text":"storeAndJoinWithOtherSide( otherSideJoiner: OneSideHashJoiner)( generateJoinedRow: (InternalRow, InternalRow) => JoinedRow): Iterator[InternalRow] storeAndJoinWithOtherSide tries to find the watermark attribute among the input attributes . storeAndJoinWithOtherSide creates a watermark expression (for the watermark attribute and the current event-time watermark ). [[storeAndJoinWithOtherSide-nonLateRows]] With the watermark attribute found, storeAndJoinWithOtherSide generates a new predicate for the watermark expression and the < > that is then used to filter out ( exclude ) late rows from the < >. Otherwise, the input rows are left unchanged (i.e. no rows are considered late and excluded). [[storeAndJoinWithOtherSide-nonLateRows-flatMap]] For every < > (possibly < >), storeAndJoinWithOtherSide applies the < > predicate and branches off per result (< > or < >). NOTE: storeAndJoinWithOtherSide is used when StreamingSymmetricHashJoinExec physical operator is requested to < >. ==== [[preJoinFilter-true]] preJoinFilter Predicate Positive ( true ) When the < > predicate succeeds on an input row, storeAndJoinWithOtherSide extracts the join key (using the < >) and requests the given OneSideHashJoiner ( otherSideJoiner ) for the < > that is in turn requested for the state values for the extracted join key. The values are then processed ( mapped over ) using the given generateJoinedRow function and then filtered by the < >. storeAndJoinWithOtherSide uses the < > (on the extracted join key) and the < > (on the current input row) to determine whether to request the < > to append the key and the input row (to a join state). If so, storeAndJoinWithOtherSide increments the < > counter. ==== [[preJoinFilter-false]] preJoinFilter Predicate Negative ( false ) When the < > predicate fails on an input row, storeAndJoinWithOtherSide creates a new Iterator[InternalRow] of joined rows per < > and < >: For LeftSide and LeftOuter , the join row is the current row with the values of the right side all null ( nullRight ) For RightSide and RightOuter , the join row is the current row with the values of the left side all null ( nullLeft ) For all other combinations, the iterator is simply empty (that will be removed from the output by the outer < >). === [[removeOldState]] Removing Old State -- removeOldState Method","title":"[source, scala]"},{"location":"OneSideHashJoiner/#source-scala_3","text":"","title":"[source, scala]"},{"location":"OneSideHashJoiner/#removeoldstate-iteratorunsaferowpair","text":"removeOldState branches off per the < >: For < >, removeOldState requests the < > to removeByKeyCondition (with the < >) For < >, removeOldState requests the < > to removeByValueCondition (with the < >) For any other predicates, removeOldState returns an empty iterator (no rows to process) NOTE: removeOldState is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >. === [[get]] Retrieving Value Rows For Key -- get Method","title":"removeOldState(): Iterator[UnsafeRowPair]"},{"location":"OneSideHashJoiner/#source-scala_4","text":"","title":"[source, scala]"},{"location":"OneSideHashJoiner/#getkey-unsaferow-iteratorunsaferow","text":"get simply requests the < > to retrieve value rows for the key . NOTE: get is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >. === [[commitStateAndGetMetrics]] Committing State (Changes) and Requesting Performance Metrics -- commitStateAndGetMetrics Method","title":"get(key: UnsafeRow): Iterator[UnsafeRow]"},{"location":"OneSideHashJoiner/#source-scala_5","text":"","title":"[source, scala]"},{"location":"OneSideHashJoiner/#commitstateandgetmetrics-statestoremetrics","text":"commitStateAndGetMetrics simply requests the < > to commit followed by requesting for the performance metrics . commitStateAndGetMetrics is used when StreamingSymmetricHashJoinExec physical operator is requested to < >.","title":"commitStateAndGetMetrics(): StateStoreMetrics"},{"location":"OneSideHashJoiner/#internal-properties","text":"[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | keyGenerator a| [[keyGenerator]]","title":"Internal Properties"},{"location":"OneSideHashJoiner/#source-scala_6","text":"","title":"[source, scala]"},{"location":"OneSideHashJoiner/#keygenerator-unsafeprojection","text":"Function to project ( extract ) join keys from an input row Used when...FIXME | preJoinFilter a| [[preJoinFilter]]","title":"keyGenerator: UnsafeProjection"},{"location":"OneSideHashJoiner/#source-scala_7","text":"","title":"[source, scala]"},{"location":"OneSideHashJoiner/#prejoinfilter-internalrow-boolean","text":"Used when...FIXME | stateKeyWatermarkPredicateFunc a| [[stateKeyWatermarkPredicateFunc]]","title":"preJoinFilter: InternalRow =&gt; Boolean"},{"location":"OneSideHashJoiner/#source-scala_8","text":"","title":"[source, scala]"},{"location":"OneSideHashJoiner/#statekeywatermarkpredicatefunc-internalrow-boolean","text":"Predicate for late rows based on the < > Used for the following: < > (and check out whether to append a row to the SymmetricHashJoinStateManager ) < > | stateValueWatermarkPredicateFunc a| [[stateValueWatermarkPredicateFunc]]","title":"stateKeyWatermarkPredicateFunc: InternalRow =&gt; Boolean"},{"location":"OneSideHashJoiner/#source-scala_9","text":"","title":"[source, scala]"},{"location":"OneSideHashJoiner/#statevaluewatermarkpredicatefunc-internalrow-boolean","text":"Predicate for late rows based on the < > Used for the following: < > (and check out whether to append a row to the < >) < > |===","title":"stateValueWatermarkPredicateFunc: InternalRow =&gt; Boolean"},{"location":"OutputMode/","text":"OutputMode \u00b6 OutputMode of a streaming query describes what data is written to a streaming sink . DataStreamWriter \u00b6 The output mode is specified on the writing side of a streaming query using DataStreamWriter.outputMode method (by alias or a value of org.apache.spark.sql.streaming.OutputMode object). import org . apache . spark . sql . streaming . OutputMode . Update val inputStream = spark . readStream . format ( \"rate\" ) . load . writeStream . format ( \"console\" ) . outputMode ( Update ) // <-- update output mode . start Append Output Mode \u00b6 Append (alias: append ) is the default output mode that writes \"new\" rows only. In streaming aggregations , a \"new\" row is when the intermediate state becomes final, i.e. when new events for the grouping key can only be considered late which is when watermark moves past the event time of the key. Append output mode requires that a streaming query defines event-time watermark (using withWatermark operator) on the event time column that is used in aggregation (directly or using window standard function). Required for datasets with FileFormat format (to create FileStreamSink ) Append is mandatory when multiple flatMapGroupsWithState operators are used in a structured query. Complete Output Mode \u00b6 Complete (alias: complete ) writes all the rows of a Result Table (and corresponds to a traditional batch structured query). Complete mode does not drop old aggregation state and preserves all data in the Result Table. Supported only for streaming aggregations (as asserted by UnsupportedOperationChecker ). Update Output Mode \u00b6 Update (alias: update ) writes only the rows that were updated (every time there are updates). For queries that are not streaming aggregations , Update is equivalent to the Append output mode.","title":"OutputMode"},{"location":"OutputMode/#outputmode","text":"OutputMode of a streaming query describes what data is written to a streaming sink .","title":"OutputMode"},{"location":"OutputMode/#datastreamwriter","text":"The output mode is specified on the writing side of a streaming query using DataStreamWriter.outputMode method (by alias or a value of org.apache.spark.sql.streaming.OutputMode object). import org . apache . spark . sql . streaming . OutputMode . Update val inputStream = spark . readStream . format ( \"rate\" ) . load . writeStream . format ( \"console\" ) . outputMode ( Update ) // <-- update output mode . start","title":" DataStreamWriter"},{"location":"OutputMode/#append-output-mode","text":"Append (alias: append ) is the default output mode that writes \"new\" rows only. In streaming aggregations , a \"new\" row is when the intermediate state becomes final, i.e. when new events for the grouping key can only be considered late which is when watermark moves past the event time of the key. Append output mode requires that a streaming query defines event-time watermark (using withWatermark operator) on the event time column that is used in aggregation (directly or using window standard function). Required for datasets with FileFormat format (to create FileStreamSink ) Append is mandatory when multiple flatMapGroupsWithState operators are used in a structured query.","title":" Append Output Mode"},{"location":"OutputMode/#complete-output-mode","text":"Complete (alias: complete ) writes all the rows of a Result Table (and corresponds to a traditional batch structured query). Complete mode does not drop old aggregation state and preserves all data in the Result Table. Supported only for streaming aggregations (as asserted by UnsupportedOperationChecker ).","title":" Complete Output Mode"},{"location":"OutputMode/#update-output-mode","text":"Update (alias: update ) writes only the rows that were updated (every time there are updates). For queries that are not streaming aggregations , Update is equivalent to the Append output mode.","title":" Update Output Mode"},{"location":"PartitionOffset/","text":"== [[PartitionOffset]] PartitionOffset PartitionOffset is...FIXME","title":"PartitionOffset"},{"location":"RateStreamContinuousReader/","text":"RateStreamContinuousReader \u00b6 RateStreamContinuousReader is a ContinuousReader that...FIXME","title":"RateStreamContinuousReader"},{"location":"RateStreamContinuousReader/#ratestreamcontinuousreader","text":"RateStreamContinuousReader is a ContinuousReader that...FIXME","title":"RateStreamContinuousReader"},{"location":"RateStreamProvider/","text":"RateStreamProvider \u00b6 RateStreamProvider is...FIXME","title":"RateStreamProvider"},{"location":"RateStreamProvider/#ratestreamprovider","text":"RateStreamProvider is...FIXME","title":"RateStreamProvider"},{"location":"SQLConf/","text":"SQLConf \u2014 Internal Configuration Store \u00b6 SQLConf is an internal configuration store for parameters and hints used to configure a Spark Structured Streaming application (and Spark SQL applications in general). Tip Find out more on SQLConf in The Internals of Spark SQL streamingFileCommitProtocolClass \u00b6 spark.sql.streaming.commitProtocolClass configuration property Used when FileStreamSink is requested to \"add\" a batch of data streamingMetricsEnabled \u00b6 spark.sql.streaming.metricsEnabled configuration property Used when StreamExecution is requested to runStream fileSinkLogCleanupDelay \u00b6 spark.sql.streaming.fileSink.log.cleanupDelay configuration property Used when FileStreamSinkLog is created fileSinkLogDeletion \u00b6 spark.sql.streaming.fileSink.log.deletion configuration property Used when FileStreamSinkLog is created fileSinkLogCompactInterval \u00b6 spark.sql.streaming.fileSink.log.compactInterval configuration property Used when FileStreamSinkLog is created minBatchesToRetain \u00b6 spark.sql.streaming.minBatchesToRetain configuration property Used when: CompactibleFileStreamLog is created StreamExecution is created StateStoreConf is created [[accessor-methods]] .SQLConf's Property Accessor Methods [cols=\"1,1\",options=\"header\",width=\"100%\"] |=== | Method Name / Property | Description | continuousStreamingExecutorQueueSize spark.sql.streaming.continuous.executorQueueSize a| [[continuousStreamingExecutorQueueSize]] Used when: DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (and creates a < >) ContinuousCoalesceExec unary physical operator is requested to execute | continuousStreamingExecutorPollIntervalMs spark.sql.streaming.continuous.executorPollIntervalMs a| [[continuousStreamingExecutorPollIntervalMs]] Used exclusively when DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (and creates a < >) | disabledV2StreamingMicroBatchReaders spark.sql.streaming.disabledV2MicroBatchReaders a| [[disabledV2StreamingMicroBatchReaders]] Used exclusively when MicroBatchExecution is requested for the < > (of a streaming query) | fileSourceLogDeletion spark.sql.streaming.fileSource.log.deletion a| [[fileSourceLogDeletion]][[FILE_SOURCE_LOG_DELETION]] Used exclusively when FileStreamSourceLog is requested for the isDeletingExpiredLog | fileSourceLogCleanupDelay spark.sql.streaming.fileSource.log.cleanupDelay a| [[fileSourceLogCleanupDelay]][[FILE_SOURCE_LOG_CLEANUP_DELAY]] Used exclusively when FileStreamSourceLog is requested for the fileCleanupDelayMs | fileSourceLogCompactInterval spark.sql.streaming.fileSource.log.compactInterval a| [[fileSourceLogCompactInterval]][[FILE_SOURCE_LOG_COMPACT_INTERVAL]] Used exclusively when FileStreamSourceLog is requested for the default compaction interval | FLATMAPGROUPSWITHSTATE_STATE_FORMAT_VERSION spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion a| [[FLATMAPGROUPSWITHSTATE_STATE_FORMAT_VERSION]] Used when: FlatMapGroupsWithStateStrategy execution planning strategy is requested to plan a streaming query (and creates a FlatMapGroupsWithStateExec physical operator for every FlatMapGroupsWithState logical operator) Among the checkpointed properties | SHUFFLE_PARTITIONS spark.sql.shuffle.partitions a| [[SHUFFLE_PARTITIONS]] See https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-properties.html#spark.sql.shuffle.partitions[spark.sql.shuffle.partitions ] in The Internals of Spark SQL. | stateStoreMinDeltasForSnapshot spark.sql.streaming.stateStore.minDeltasForSnapshot a| [[stateStoreMinDeltasForSnapshot]] Used (as StateStoreConf.minDeltasForSnapshot ) exclusively when HDFSBackedStateStoreProvider is requested to doSnapshot | stateStoreProviderClass spark.sql.streaming.stateStore.providerClass a| [[stateStoreProviderClass]] Used when: StateStoreWriter is requested to stateStoreCustomMetrics (when StateStoreWriter is requested for the metrics and getProgress ) StateStoreConf is created | STREAMING_AGGREGATION_STATE_FORMAT_VERSION spark.sql.streaming.aggregation.stateFormatVersion a| [[STREAMING_AGGREGATION_STATE_FORMAT_VERSION]] Used when: StatefulAggregationStrategy execution planning strategy is executed OffsetSeqMetadata is requested for the relevantSQLConfs and the relevantSQLConfDefaultValues | STREAMING_CHECKPOINT_FILE_MANAGER_CLASS spark.sql.streaming.checkpointFileManagerClass a| [[STREAMING_CHECKPOINT_FILE_MANAGER_CLASS]] Used exclusively when CheckpointFileManager helper object is requested to create a CheckpointFileManager | streamingMetricsEnabled spark.sql.streaming.metricsEnabled a| [[streamingMetricsEnabled]] Used exclusively when StreamExecution is requested for runStream (to control whether to register a metrics reporter for a streaming query) | STREAMING_MULTIPLE_WATERMARK_POLICY spark.sql.streaming.multipleWatermarkPolicy a| [[STREAMING_MULTIPLE_WATERMARK_POLICY]] | streamingNoDataMicroBatchesEnabled spark.sql.streaming.noDataMicroBatches.enabled a| [[streamingNoDataMicroBatchesEnabled]][[STREAMING_NO_DATA_MICRO_BATCHES_ENABLED]] Used exclusively when MicroBatchExecution stream execution engine is requested to < > | streamingNoDataProgressEventInterval spark.sql.streaming.noDataProgressEventInterval a| [[streamingNoDataProgressEventInterval]] Used exclusively for ProgressReporter | streamingPollingDelay spark.sql.streaming.pollingDelay a| [[streamingPollingDelay]][[STREAMING_POLLING_DELAY]] Used exclusively when StreamExecution is created | streamingProgressRetention spark.sql.streaming.numRecentProgressUpdates a| [[streamingProgressRetention]][[STREAMING_PROGRESS_RETENTION]] Used exclusively when ProgressReporter is requested to update progress of streaming query (and possibly remove an excess) |===","title":"SQLConf"},{"location":"SQLConf/#sqlconf-internal-configuration-store","text":"SQLConf is an internal configuration store for parameters and hints used to configure a Spark Structured Streaming application (and Spark SQL applications in general). Tip Find out more on SQLConf in The Internals of Spark SQL","title":"SQLConf &mdash; Internal Configuration Store"},{"location":"SQLConf/#streamingfilecommitprotocolclass","text":"spark.sql.streaming.commitProtocolClass configuration property Used when FileStreamSink is requested to \"add\" a batch of data","title":" streamingFileCommitProtocolClass"},{"location":"SQLConf/#streamingmetricsenabled","text":"spark.sql.streaming.metricsEnabled configuration property Used when StreamExecution is requested to runStream","title":" streamingMetricsEnabled"},{"location":"SQLConf/#filesinklogcleanupdelay","text":"spark.sql.streaming.fileSink.log.cleanupDelay configuration property Used when FileStreamSinkLog is created","title":" fileSinkLogCleanupDelay"},{"location":"SQLConf/#filesinklogdeletion","text":"spark.sql.streaming.fileSink.log.deletion configuration property Used when FileStreamSinkLog is created","title":" fileSinkLogDeletion"},{"location":"SQLConf/#filesinklogcompactinterval","text":"spark.sql.streaming.fileSink.log.compactInterval configuration property Used when FileStreamSinkLog is created","title":" fileSinkLogCompactInterval"},{"location":"SQLConf/#minbatchestoretain","text":"spark.sql.streaming.minBatchesToRetain configuration property Used when: CompactibleFileStreamLog is created StreamExecution is created StateStoreConf is created [[accessor-methods]] .SQLConf's Property Accessor Methods [cols=\"1,1\",options=\"header\",width=\"100%\"] |=== | Method Name / Property | Description | continuousStreamingExecutorQueueSize spark.sql.streaming.continuous.executorQueueSize a| [[continuousStreamingExecutorQueueSize]] Used when: DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (and creates a < >) ContinuousCoalesceExec unary physical operator is requested to execute | continuousStreamingExecutorPollIntervalMs spark.sql.streaming.continuous.executorPollIntervalMs a| [[continuousStreamingExecutorPollIntervalMs]] Used exclusively when DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (and creates a < >) | disabledV2StreamingMicroBatchReaders spark.sql.streaming.disabledV2MicroBatchReaders a| [[disabledV2StreamingMicroBatchReaders]] Used exclusively when MicroBatchExecution is requested for the < > (of a streaming query) | fileSourceLogDeletion spark.sql.streaming.fileSource.log.deletion a| [[fileSourceLogDeletion]][[FILE_SOURCE_LOG_DELETION]] Used exclusively when FileStreamSourceLog is requested for the isDeletingExpiredLog | fileSourceLogCleanupDelay spark.sql.streaming.fileSource.log.cleanupDelay a| [[fileSourceLogCleanupDelay]][[FILE_SOURCE_LOG_CLEANUP_DELAY]] Used exclusively when FileStreamSourceLog is requested for the fileCleanupDelayMs | fileSourceLogCompactInterval spark.sql.streaming.fileSource.log.compactInterval a| [[fileSourceLogCompactInterval]][[FILE_SOURCE_LOG_COMPACT_INTERVAL]] Used exclusively when FileStreamSourceLog is requested for the default compaction interval | FLATMAPGROUPSWITHSTATE_STATE_FORMAT_VERSION spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion a| [[FLATMAPGROUPSWITHSTATE_STATE_FORMAT_VERSION]] Used when: FlatMapGroupsWithStateStrategy execution planning strategy is requested to plan a streaming query (and creates a FlatMapGroupsWithStateExec physical operator for every FlatMapGroupsWithState logical operator) Among the checkpointed properties | SHUFFLE_PARTITIONS spark.sql.shuffle.partitions a| [[SHUFFLE_PARTITIONS]] See https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-properties.html#spark.sql.shuffle.partitions[spark.sql.shuffle.partitions ] in The Internals of Spark SQL. | stateStoreMinDeltasForSnapshot spark.sql.streaming.stateStore.minDeltasForSnapshot a| [[stateStoreMinDeltasForSnapshot]] Used (as StateStoreConf.minDeltasForSnapshot ) exclusively when HDFSBackedStateStoreProvider is requested to doSnapshot | stateStoreProviderClass spark.sql.streaming.stateStore.providerClass a| [[stateStoreProviderClass]] Used when: StateStoreWriter is requested to stateStoreCustomMetrics (when StateStoreWriter is requested for the metrics and getProgress ) StateStoreConf is created | STREAMING_AGGREGATION_STATE_FORMAT_VERSION spark.sql.streaming.aggregation.stateFormatVersion a| [[STREAMING_AGGREGATION_STATE_FORMAT_VERSION]] Used when: StatefulAggregationStrategy execution planning strategy is executed OffsetSeqMetadata is requested for the relevantSQLConfs and the relevantSQLConfDefaultValues | STREAMING_CHECKPOINT_FILE_MANAGER_CLASS spark.sql.streaming.checkpointFileManagerClass a| [[STREAMING_CHECKPOINT_FILE_MANAGER_CLASS]] Used exclusively when CheckpointFileManager helper object is requested to create a CheckpointFileManager | streamingMetricsEnabled spark.sql.streaming.metricsEnabled a| [[streamingMetricsEnabled]] Used exclusively when StreamExecution is requested for runStream (to control whether to register a metrics reporter for a streaming query) | STREAMING_MULTIPLE_WATERMARK_POLICY spark.sql.streaming.multipleWatermarkPolicy a| [[STREAMING_MULTIPLE_WATERMARK_POLICY]] | streamingNoDataMicroBatchesEnabled spark.sql.streaming.noDataMicroBatches.enabled a| [[streamingNoDataMicroBatchesEnabled]][[STREAMING_NO_DATA_MICRO_BATCHES_ENABLED]] Used exclusively when MicroBatchExecution stream execution engine is requested to < > | streamingNoDataProgressEventInterval spark.sql.streaming.noDataProgressEventInterval a| [[streamingNoDataProgressEventInterval]] Used exclusively for ProgressReporter | streamingPollingDelay spark.sql.streaming.pollingDelay a| [[streamingPollingDelay]][[STREAMING_POLLING_DELAY]] Used exclusively when StreamExecution is created | streamingProgressRetention spark.sql.streaming.numRecentProgressUpdates a| [[streamingProgressRetention]][[STREAMING_PROGRESS_RETENTION]] Used exclusively when ProgressReporter is requested to update progress of streaming query (and possibly remove an excess) |===","title":" minBatchesToRetain"},{"location":"Sink/","text":"Sink \u00b6 Sink is an extension of the Table abstraction for streaming sinks that add the batch results of a streaming query in Micro-Batch Stream Processing . Note Sink extends Table interface for the only purpose of making it compatible with Data Source V2. All Table methods simply throw an IllegalStateException . Contract \u00b6 Adding Batch \u00b6 addBatch ( batchId : Long , data : DataFrame ): Unit Adds a batch of data to the sink Used when MicroBatchExecution stream execution engine is requested to add a batch to a sink (addBatch phase) (while running micro-batches of a streaming query ) Implementations \u00b6 FileStreamSink ForeachBatchSink KafkaSink","title":"Sink"},{"location":"Sink/#sink","text":"Sink is an extension of the Table abstraction for streaming sinks that add the batch results of a streaming query in Micro-Batch Stream Processing . Note Sink extends Table interface for the only purpose of making it compatible with Data Source V2. All Table methods simply throw an IllegalStateException .","title":"Sink"},{"location":"Sink/#contract","text":"","title":"Contract"},{"location":"Sink/#adding-batch","text":"addBatch ( batchId : Long , data : DataFrame ): Unit Adds a batch of data to the sink Used when MicroBatchExecution stream execution engine is requested to add a batch to a sink (addBatch phase) (while running micro-batches of a streaming query )","title":" Adding Batch"},{"location":"Sink/#implementations","text":"FileStreamSink ForeachBatchSink KafkaSink","title":"Implementations"},{"location":"Source/","text":"Source \u2014 Streaming Source in Micro-Batch Stream Processing \u00b6 Source is an extension of the SparkDataStream abstraction for streaming sources for \"streamed reading\" of continually arriving data in a streaming query (identified by offset ). Source is used in Micro-Batch Stream Processing . Source is created using StreamSourceProvider.createSource (and DataSource.createSource ). For fault tolerance, Source must be able to replay an arbitrary sequence of past data in a stream using a range of offsets. This is the assumption so Structured Streaming can achieve end-to-end exactly-once guarantees. Contract \u00b6 commit \u00b6 commit ( end : Offset ): Unit Commits data up to the given end offset (informs the source that Spark has completed processing all data for offsets less than or equal to the end offset and will only request offsets greater than the end offset in the future). Used when: MicroBatchExecution stream execution engine is requested to write offsets to a commit log (walCommit phase) while running an activated streaming query getBatch \u00b6 getBatch ( start : Option [ Offset ], end : Offset ): DataFrame Generating a streaming DataFrame with data between the start and end offsets Start offset can be undefined ( None ) to indicate that the batch should begin with the first record Used when MicroBatchExecution stream execution engine is requested to run an activated streaming query , namely: Populate start offsets from checkpoint (resuming from checkpoint) Request unprocessed data from all sources (getBatch phase) getOffset \u00b6 getOffset : Option [ Offset ] Latest (maximum) offset of the source (or None to denote no data) Used when: MicroBatchExecution stream execution engine ( Micro-Batch Stream Processing ) is requested for latest offsets of all sources (getOffset phase) while running activated streaming query schema \u00b6 schema : StructType Schema of the data from this source Implementations \u00b6 FileStreamSource KafkaSource initialOffset Method \u00b6 initialOffset (): OffsetV2 initialOffset throws an IllegalStateException . initialOffset is part of the SparkDataStream abstraction. deserializeOffset Method \u00b6 deserializeOffset ( json : String ): OffsetV2 deserializeOffset throws an IllegalStateException . deserializeOffset is part of the SparkDataStream abstraction.","title":"Source"},{"location":"Source/#source-streaming-source-in-micro-batch-stream-processing","text":"Source is an extension of the SparkDataStream abstraction for streaming sources for \"streamed reading\" of continually arriving data in a streaming query (identified by offset ). Source is used in Micro-Batch Stream Processing . Source is created using StreamSourceProvider.createSource (and DataSource.createSource ). For fault tolerance, Source must be able to replay an arbitrary sequence of past data in a stream using a range of offsets. This is the assumption so Structured Streaming can achieve end-to-end exactly-once guarantees.","title":"Source &mdash; Streaming Source in Micro-Batch Stream Processing"},{"location":"Source/#contract","text":"","title":"Contract"},{"location":"Source/#commit","text":"commit ( end : Offset ): Unit Commits data up to the given end offset (informs the source that Spark has completed processing all data for offsets less than or equal to the end offset and will only request offsets greater than the end offset in the future). Used when: MicroBatchExecution stream execution engine is requested to write offsets to a commit log (walCommit phase) while running an activated streaming query","title":" commit"},{"location":"Source/#getbatch","text":"getBatch ( start : Option [ Offset ], end : Offset ): DataFrame Generating a streaming DataFrame with data between the start and end offsets Start offset can be undefined ( None ) to indicate that the batch should begin with the first record Used when MicroBatchExecution stream execution engine is requested to run an activated streaming query , namely: Populate start offsets from checkpoint (resuming from checkpoint) Request unprocessed data from all sources (getBatch phase)","title":" getBatch"},{"location":"Source/#getoffset","text":"getOffset : Option [ Offset ] Latest (maximum) offset of the source (or None to denote no data) Used when: MicroBatchExecution stream execution engine ( Micro-Batch Stream Processing ) is requested for latest offsets of all sources (getOffset phase) while running activated streaming query","title":" getOffset"},{"location":"Source/#schema","text":"schema : StructType Schema of the data from this source","title":" schema"},{"location":"Source/#implementations","text":"FileStreamSource KafkaSource","title":"Implementations"},{"location":"Source/#initialoffset-method","text":"initialOffset (): OffsetV2 initialOffset throws an IllegalStateException . initialOffset is part of the SparkDataStream abstraction.","title":" initialOffset Method"},{"location":"Source/#deserializeoffset-method","text":"deserializeOffset ( json : String ): OffsetV2 deserializeOffset throws an IllegalStateException . deserializeOffset is part of the SparkDataStream abstraction.","title":" deserializeOffset Method"},{"location":"SparkDataStream/","text":"SparkDataStream \u00b6 SparkDataStream is an abstraction of readable data streams . Contract \u00b6 commit \u00b6 void commit ( Offset end ) Used when: ContinuousExecution stream execution engine is requested to commit MicroBatchExecution stream execution engine is requested to constructNextBatch deserializeOffset \u00b6 Offset deserializeOffset ( String json ) Used when: ContinuousExecution stream execution engine is requested to runContinuous and commit MicroBatchExecution stream execution engine is requested to constructNextBatch and runBatch initialOffset \u00b6 Offset initialOffset () Used when: ContinuousExecution stream execution engine is requested to runContinuous MicroBatchExecution stream execution engine is requested to constructNextBatch and runBatch stop \u00b6 void stop () Used when: StreamExecution is requested to stop sources Implementations \u00b6 ContinuousStream MemoryStreamBase MicroBatchStream Source SupportsAdmissionControl","title":"SparkDataStream"},{"location":"SparkDataStream/#sparkdatastream","text":"SparkDataStream is an abstraction of readable data streams .","title":"SparkDataStream"},{"location":"SparkDataStream/#contract","text":"","title":"Contract"},{"location":"SparkDataStream/#commit","text":"void commit ( Offset end ) Used when: ContinuousExecution stream execution engine is requested to commit MicroBatchExecution stream execution engine is requested to constructNextBatch","title":" commit"},{"location":"SparkDataStream/#deserializeoffset","text":"Offset deserializeOffset ( String json ) Used when: ContinuousExecution stream execution engine is requested to runContinuous and commit MicroBatchExecution stream execution engine is requested to constructNextBatch and runBatch","title":" deserializeOffset"},{"location":"SparkDataStream/#initialoffset","text":"Offset initialOffset () Used when: ContinuousExecution stream execution engine is requested to runContinuous MicroBatchExecution stream execution engine is requested to constructNextBatch and runBatch","title":" initialOffset"},{"location":"SparkDataStream/#stop","text":"void stop () Used when: StreamExecution is requested to stop sources","title":" stop"},{"location":"SparkDataStream/#implementations","text":"ContinuousStream MemoryStreamBase MicroBatchStream Source SupportsAdmissionControl","title":"Implementations"},{"location":"StateStore/","text":"StateStore \u00b6 StateStore is the < > of < > for managing state in Stateful Stream Processing (e.g. for persisting running aggregates in Streaming Aggregation ). StateStore supports incremental checkpointing in which only the key-value \"Row\" pairs that changed are < > or < > (without touching other key-value pairs). StateStore is identified with the < > (among other properties for identification). [[implementations]] NOTE: HDFSBackedStateStore is the default and only known implementation of the < > in Spark Structured Streaming. [[contract]] .StateStore Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | abort a| [[abort]] [source, scala] \u00b6 abort(): Unit \u00b6 Aborts ( discards ) changes to the state store Used when: StateStoreOps implicit class is requested to mapPartitionsWithStateStore (when the state store has not been < > for a task that finishes, possibly with an error) StateStoreHandler (of SymmetricHashJoinStateManager ) is requested to abortIfNeeded (when the state store has not been < > for a task that finishes, possibly with an error) | commit a| [[commit]] [source, scala] \u00b6 commit(): Long \u00b6 Commits the changes to the state store (and returns the current version) Used when: FlatMapGroupsWithStateExec , StreamingDeduplicateExec and StreamingGlobalLimitExec physical operators are executed (right after all rows in a partition have been processed) StreamingAggregationStateManagerBaseImpl is requested to commit (changes to) a state store (when StateStoreSaveExec physical operator is executed) StateStoreHandler (of SymmetricHashJoinStateManager ) is requested to commit changes to a state store | get a| [[get]] get ( key : UnsafeRow ): UnsafeRow Looks up ( gets ) the value of the given non- null key Used when: StreamingDeduplicateExec and StreamingGlobalLimitExec physical operators are executed StateManagerImplBase (of FlatMapGroupsWithStateExecHelper ) is requested to getState StreamingAggregationStateManagerImplV1 and StreamingAggregationStateManagerImplV2 are requested to get the value of a non-null key KeyToNumValuesStore is requested to get KeyWithIndexToValueStore` is requested to get and getAll | getRange a| [[getRange]] [source, scala] \u00b6 getRange( start: Option[UnsafeRow], end: Option[UnsafeRow]): Iterator[UnsafeRowPair] Gets the key-value pairs of UnsafeRows for the specified range (with optional approximate start and end extents) Used when: WatermarkSupport is requested to removeKeysOlderThanWatermark StateManagerImplBase is requested to getAllState StreamingAggregationStateManagerBaseImpl is requested for keys KeyToNumValuesStore and KeyWithIndexToValueStore are requested to iterator NOTE: All the uses above assume the start and end as None that basically is < >. | hasCommitted a| [[hasCommitted]] [source, scala] \u00b6 hasCommitted: Boolean \u00b6 Flag to indicate whether state changes have been committed ( true ) or not ( false ) Used when: RDD (via StateStoreOps implicit class) is requested to mapPartitionsWithStateStore (and a task finishes and may need to < >) SymmetricHashJoinStateManager is requested to abortIfNeeded (when a task finishes and may need to < >)) | id a| [[id]] [source, scala] \u00b6 id: StateStoreId \u00b6 The < > of the state store Used when: HDFSBackedStateStore state store is requested for the textual representation StateStoreHandler (of SymmetricHashJoinStateManager ) is requested to < > and < > | iterator a| [[iterator]] [source, scala] \u00b6 iterator(): Iterator[UnsafeRowPair] \u00b6 Returns an iterator with all the kay-value pairs in the state store Used when: StateStoreRestoreExec physical operator is requested to execute HDFSBackedStateStore state store in particular and any StateStore in general are requested to getRange StreamingAggregationStateManagerImplV1 state manager is requested for the iterator and values StreamingAggregationStateManagerImplV2 state manager is requested to iterator and values | metrics a| [[metrics]] [source, scala] \u00b6 metrics: StateStoreMetrics \u00b6 StateStoreMetrics of the state store Used when: StateStoreWriter stateful physical operator is requested to setStoreMetrics StateStoreHandler (of SymmetricHashJoinStateManager ) is requested to commit and for the metrics | put a| [[put]] [source, scala] \u00b6 put( key: UnsafeRow, value: UnsafeRow): Unit Stores ( puts ) the value for the (non-null) key Used when: StreamingDeduplicateExec and StreamingGlobalLimitExec physical operators are executed StateManagerImplBase is requested to putState StreamingAggregationStateManagerImplV1 and StreamingAggregationStateManagerImplV2 are requested to store a row in a state store KeyToNumValuesStore and KeyWithIndexToValueStore are requested to store a new value for a given key | remove a| [[remove]] [source, scala] \u00b6 remove(key: UnsafeRow): Unit \u00b6 Removes the (non-null) key from the state store Used when: Physical operators with WatermarkSupport are requested to removeKeysOlderThanWatermark StateManagerImplBase is requested to removeState StreamingAggregationStateManagerBaseImpl is requested to remove a key from a state store KeyToNumValuesStore is requested to remove a key KeyWithIndexToValueStore is requested to < > and < > | version a| [[version]] [source, scala] \u00b6 version: Long \u00b6 Version of the state store Used exclusively when HDFSBackedStateStore state store is requested for a new version (that simply the current version incremented) |=== [NOTE] \u00b6 StateStore was introduced in https://github.com/apache/spark/commit/8c826880f5eaa3221c4e9e7d3fece54e821a0b98[[SPARK-13809 ][SQL] State store for streaming aggregations]. Read the motivation and design in https://docs.google.com/document/d/1-ncawFx8JS5Zyfq1HAEGBx56RDet9wfVp_hDM8ZL254/edit[State Store for Streaming Aggregations]. \u00b6 [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.StateStore$ logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.StateStore$=ALL Refer to < >. \u00b6 === [[coordinatorRef]] Creating (and Caching) RPC Endpoint Reference to StateStoreCoordinator for Executors -- coordinatorRef Internal Object Method [source, scala] \u00b6 coordinatorRef: Option[StateStoreCoordinatorRef] \u00b6 coordinatorRef requests the SparkEnv helper object for the current SparkEnv . If the SparkEnv is available and the <<_coordRef, _coordRef>> is not assigned yet, coordinatorRef prints out the following DEBUG message to the logs followed by requesting the StateStoreCoordinatorRef for the StateStoreCoordinator endpoint . Getting StateStoreCoordinatorRef If the SparkEnv is available, coordinatorRef prints out the following INFO message to the logs: Retrieved reference to StateStoreCoordinator: [_coordRef] NOTE: coordinatorRef is used when StateStore helper object is requested to < > (when StateStore object helper is requested to < >) and < > (when StateStore object helper is requested to < >). === [[unload]] Unloading State Store Provider -- unload Method [source, scala] \u00b6 unload(storeProviderId: StateStoreProviderId): Unit \u00b6 unload ...FIXME NOTE: unload is used when StateStore helper object is requested to < > and < >. === [[stop]] stop Object Method [source, scala] \u00b6 stop(): Unit \u00b6 stop ...FIXME NOTE: stop seems only be used in tests. === [[reportActiveStoreInstance]] Announcing New StateStoreProvider -- reportActiveStoreInstance Internal Object Method [source, scala] \u00b6 reportActiveStoreInstance( storeProviderId: StateStoreProviderId): Unit reportActiveStoreInstance takes the current host and executorId (from the BlockManager on the Spark executor) and requests the < > to reportActiveInstance . NOTE: reportActiveStoreInstance uses SparkEnv to access the BlockManager . In the end, reportActiveStoreInstance prints out the following INFO message to the logs: Reported that the loaded instance [storeProviderId] is active NOTE: reportActiveStoreInstance is used exclusively when StateStore utility is requested to < >. === [[MaintenanceTask]] MaintenanceTask Daemon Thread MaintenanceTask is a daemon thread that < >. When an error occurs, MaintenanceTask clears < > internal registry. MaintenanceTask is scheduled on state-store-maintenance-task thread pool that runs periodically every spark.sql.streaming.stateStore.maintenanceInterval . Looking Up StateStore by Provider ID \u00b6 get ( storeProviderId : StateStoreProviderId , keySchema : StructType , valueSchema : StructType , indexOrdinal : Option [ Int ], version : Long , storeConf : StateStoreConf , hadoopConf : Configuration ): StateStore get finds StateStore for the specified StateStoreProviderId and version. NOTE: The version is either the < > (in Continuous Stream Processing ) or the current batch ID (in Micro-Batch Stream Processing ). Internally, get looks up the < > (by storeProviderId ) in the < > internal cache. If unavailable, get uses the StateStoreProvider utility to < >. get will also < > (unless already started) and < >. In the end, get requests the StateStoreProvider to < >. get is used when: StateStoreRDD is requested to compute a partition StateStoreHandler (of SymmetricHashJoinStateManager ) is requested to < > ==== [[startMaintenanceIfNeeded]] Starting Periodic Maintenance Task (Unless Already Started) -- startMaintenanceIfNeeded Internal Object Method [source, scala] \u00b6 startMaintenanceIfNeeded(): Unit \u00b6 startMaintenanceIfNeeded schedules < > to start after and every spark.sql.streaming.stateStore.maintenanceInterval (defaults to 60s ). NOTE: startMaintenanceIfNeeded does nothing when the maintenance task has already been started and is still running. NOTE: startMaintenanceIfNeeded is used exclusively when StateStore is requested to < >. ==== [[doMaintenance]] Doing State Maintenance of Registered State Store Providers -- doMaintenance Internal Object Method [source, scala] \u00b6 doMaintenance(): Unit \u00b6 Internally, doMaintenance prints the following DEBUG message to the logs: Doing maintenance doMaintenance then requests every spark-sql-streaming-StateStoreProvider.md[StateStoreProvider] (registered in < >) to spark-sql-streaming-StateStoreProvider.md#doMaintenance[do its own internal maintenance] (only when a StateStoreProvider < >). When a StateStoreProvider is < >, doMaintenance < > and prints the following INFO message to the logs: Unloaded [provider] NOTE: doMaintenance is used exclusively in < >. ==== [[verifyIfStoreInstanceActive]] verifyIfStoreInstanceActive Internal Object Method [source, scala] \u00b6 verifyIfStoreInstanceActive(storeProviderId: StateStoreProviderId): Boolean \u00b6 verifyIfStoreInstanceActive ...FIXME NOTE: verifyIfStoreInstanceActive is used exclusively when StateStore helper object is requested to < > (from a running < >). === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | loadedProviders | [[loadedProviders]] Loaded providers internal cache, i.e. < > per < > Used in...FIXME | _coordRef | [[_coordRef]] StateStoreCoordinator RPC endpoint (a RpcEndpointRef to StateStoreCoordinator ) Used in...FIXME |===","title":"StateStore"},{"location":"StateStore/#statestore","text":"StateStore is the < > of < > for managing state in Stateful Stream Processing (e.g. for persisting running aggregates in Streaming Aggregation ). StateStore supports incremental checkpointing in which only the key-value \"Row\" pairs that changed are < > or < > (without touching other key-value pairs). StateStore is identified with the < > (among other properties for identification). [[implementations]] NOTE: HDFSBackedStateStore is the default and only known implementation of the < > in Spark Structured Streaming. [[contract]] .StateStore Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | abort a| [[abort]]","title":"StateStore"},{"location":"StateStore/#source-scala","text":"","title":"[source, scala]"},{"location":"StateStore/#abort-unit","text":"Aborts ( discards ) changes to the state store Used when: StateStoreOps implicit class is requested to mapPartitionsWithStateStore (when the state store has not been < > for a task that finishes, possibly with an error) StateStoreHandler (of SymmetricHashJoinStateManager ) is requested to abortIfNeeded (when the state store has not been < > for a task that finishes, possibly with an error) | commit a| [[commit]]","title":"abort(): Unit"},{"location":"StateStore/#source-scala_1","text":"","title":"[source, scala]"},{"location":"StateStore/#commit-long","text":"Commits the changes to the state store (and returns the current version) Used when: FlatMapGroupsWithStateExec , StreamingDeduplicateExec and StreamingGlobalLimitExec physical operators are executed (right after all rows in a partition have been processed) StreamingAggregationStateManagerBaseImpl is requested to commit (changes to) a state store (when StateStoreSaveExec physical operator is executed) StateStoreHandler (of SymmetricHashJoinStateManager ) is requested to commit changes to a state store | get a| [[get]] get ( key : UnsafeRow ): UnsafeRow Looks up ( gets ) the value of the given non- null key Used when: StreamingDeduplicateExec and StreamingGlobalLimitExec physical operators are executed StateManagerImplBase (of FlatMapGroupsWithStateExecHelper ) is requested to getState StreamingAggregationStateManagerImplV1 and StreamingAggregationStateManagerImplV2 are requested to get the value of a non-null key KeyToNumValuesStore is requested to get KeyWithIndexToValueStore` is requested to get and getAll | getRange a| [[getRange]]","title":"commit(): Long"},{"location":"StateStore/#source-scala_2","text":"getRange( start: Option[UnsafeRow], end: Option[UnsafeRow]): Iterator[UnsafeRowPair] Gets the key-value pairs of UnsafeRows for the specified range (with optional approximate start and end extents) Used when: WatermarkSupport is requested to removeKeysOlderThanWatermark StateManagerImplBase is requested to getAllState StreamingAggregationStateManagerBaseImpl is requested for keys KeyToNumValuesStore and KeyWithIndexToValueStore are requested to iterator NOTE: All the uses above assume the start and end as None that basically is < >. | hasCommitted a| [[hasCommitted]]","title":"[source, scala]"},{"location":"StateStore/#source-scala_3","text":"","title":"[source, scala]"},{"location":"StateStore/#hascommitted-boolean","text":"Flag to indicate whether state changes have been committed ( true ) or not ( false ) Used when: RDD (via StateStoreOps implicit class) is requested to mapPartitionsWithStateStore (and a task finishes and may need to < >) SymmetricHashJoinStateManager is requested to abortIfNeeded (when a task finishes and may need to < >)) | id a| [[id]]","title":"hasCommitted: Boolean"},{"location":"StateStore/#source-scala_4","text":"","title":"[source, scala]"},{"location":"StateStore/#id-statestoreid","text":"The < > of the state store Used when: HDFSBackedStateStore state store is requested for the textual representation StateStoreHandler (of SymmetricHashJoinStateManager ) is requested to < > and < > | iterator a| [[iterator]]","title":"id: StateStoreId"},{"location":"StateStore/#source-scala_5","text":"","title":"[source, scala]"},{"location":"StateStore/#iterator-iteratorunsaferowpair","text":"Returns an iterator with all the kay-value pairs in the state store Used when: StateStoreRestoreExec physical operator is requested to execute HDFSBackedStateStore state store in particular and any StateStore in general are requested to getRange StreamingAggregationStateManagerImplV1 state manager is requested for the iterator and values StreamingAggregationStateManagerImplV2 state manager is requested to iterator and values | metrics a| [[metrics]]","title":"iterator(): Iterator[UnsafeRowPair]"},{"location":"StateStore/#source-scala_6","text":"","title":"[source, scala]"},{"location":"StateStore/#metrics-statestoremetrics","text":"StateStoreMetrics of the state store Used when: StateStoreWriter stateful physical operator is requested to setStoreMetrics StateStoreHandler (of SymmetricHashJoinStateManager ) is requested to commit and for the metrics | put a| [[put]]","title":"metrics: StateStoreMetrics"},{"location":"StateStore/#source-scala_7","text":"put( key: UnsafeRow, value: UnsafeRow): Unit Stores ( puts ) the value for the (non-null) key Used when: StreamingDeduplicateExec and StreamingGlobalLimitExec physical operators are executed StateManagerImplBase is requested to putState StreamingAggregationStateManagerImplV1 and StreamingAggregationStateManagerImplV2 are requested to store a row in a state store KeyToNumValuesStore and KeyWithIndexToValueStore are requested to store a new value for a given key | remove a| [[remove]]","title":"[source, scala]"},{"location":"StateStore/#source-scala_8","text":"","title":"[source, scala]"},{"location":"StateStore/#removekey-unsaferow-unit","text":"Removes the (non-null) key from the state store Used when: Physical operators with WatermarkSupport are requested to removeKeysOlderThanWatermark StateManagerImplBase is requested to removeState StreamingAggregationStateManagerBaseImpl is requested to remove a key from a state store KeyToNumValuesStore is requested to remove a key KeyWithIndexToValueStore is requested to < > and < > | version a| [[version]]","title":"remove(key: UnsafeRow): Unit"},{"location":"StateStore/#source-scala_9","text":"","title":"[source, scala]"},{"location":"StateStore/#version-long","text":"Version of the state store Used exclusively when HDFSBackedStateStore state store is requested for a new version (that simply the current version incremented) |===","title":"version: Long"},{"location":"StateStore/#note","text":"StateStore was introduced in https://github.com/apache/spark/commit/8c826880f5eaa3221c4e9e7d3fece54e821a0b98[[SPARK-13809 ][SQL] State store for streaming aggregations].","title":"[NOTE]"},{"location":"StateStore/#read-the-motivation-and-design-in-httpsdocsgooglecomdocumentd1-ncawfx8js5zyfq1haegbx56rdet9wfvp_hdm8zl254editstate-store-for-streaming-aggregations","text":"[[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.StateStore$ logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.StateStore$=ALL","title":"Read the motivation and design in https://docs.google.com/document/d/1-ncawFx8JS5Zyfq1HAEGBx56RDet9wfVp_hDM8ZL254/edit[State Store for Streaming Aggregations]."},{"location":"StateStore/#refer-to","text":"=== [[coordinatorRef]] Creating (and Caching) RPC Endpoint Reference to StateStoreCoordinator for Executors -- coordinatorRef Internal Object Method","title":"Refer to &lt;&gt;."},{"location":"StateStore/#source-scala_10","text":"","title":"[source, scala]"},{"location":"StateStore/#coordinatorref-optionstatestorecoordinatorref","text":"coordinatorRef requests the SparkEnv helper object for the current SparkEnv . If the SparkEnv is available and the <<_coordRef, _coordRef>> is not assigned yet, coordinatorRef prints out the following DEBUG message to the logs followed by requesting the StateStoreCoordinatorRef for the StateStoreCoordinator endpoint . Getting StateStoreCoordinatorRef If the SparkEnv is available, coordinatorRef prints out the following INFO message to the logs: Retrieved reference to StateStoreCoordinator: [_coordRef] NOTE: coordinatorRef is used when StateStore helper object is requested to < > (when StateStore object helper is requested to < >) and < > (when StateStore object helper is requested to < >). === [[unload]] Unloading State Store Provider -- unload Method","title":"coordinatorRef: Option[StateStoreCoordinatorRef]"},{"location":"StateStore/#source-scala_11","text":"","title":"[source, scala]"},{"location":"StateStore/#unloadstoreproviderid-statestoreproviderid-unit","text":"unload ...FIXME NOTE: unload is used when StateStore helper object is requested to < > and < >. === [[stop]] stop Object Method","title":"unload(storeProviderId: StateStoreProviderId): Unit"},{"location":"StateStore/#source-scala_12","text":"","title":"[source, scala]"},{"location":"StateStore/#stop-unit","text":"stop ...FIXME NOTE: stop seems only be used in tests. === [[reportActiveStoreInstance]] Announcing New StateStoreProvider -- reportActiveStoreInstance Internal Object Method","title":"stop(): Unit"},{"location":"StateStore/#source-scala_13","text":"reportActiveStoreInstance( storeProviderId: StateStoreProviderId): Unit reportActiveStoreInstance takes the current host and executorId (from the BlockManager on the Spark executor) and requests the < > to reportActiveInstance . NOTE: reportActiveStoreInstance uses SparkEnv to access the BlockManager . In the end, reportActiveStoreInstance prints out the following INFO message to the logs: Reported that the loaded instance [storeProviderId] is active NOTE: reportActiveStoreInstance is used exclusively when StateStore utility is requested to < >. === [[MaintenanceTask]] MaintenanceTask Daemon Thread MaintenanceTask is a daemon thread that < >. When an error occurs, MaintenanceTask clears < > internal registry. MaintenanceTask is scheduled on state-store-maintenance-task thread pool that runs periodically every spark.sql.streaming.stateStore.maintenanceInterval .","title":"[source, scala]"},{"location":"StateStore/#looking-up-statestore-by-provider-id","text":"get ( storeProviderId : StateStoreProviderId , keySchema : StructType , valueSchema : StructType , indexOrdinal : Option [ Int ], version : Long , storeConf : StateStoreConf , hadoopConf : Configuration ): StateStore get finds StateStore for the specified StateStoreProviderId and version. NOTE: The version is either the < > (in Continuous Stream Processing ) or the current batch ID (in Micro-Batch Stream Processing ). Internally, get looks up the < > (by storeProviderId ) in the < > internal cache. If unavailable, get uses the StateStoreProvider utility to < >. get will also < > (unless already started) and < >. In the end, get requests the StateStoreProvider to < >. get is used when: StateStoreRDD is requested to compute a partition StateStoreHandler (of SymmetricHashJoinStateManager ) is requested to < > ==== [[startMaintenanceIfNeeded]] Starting Periodic Maintenance Task (Unless Already Started) -- startMaintenanceIfNeeded Internal Object Method","title":" Looking Up StateStore by Provider ID"},{"location":"StateStore/#source-scala_14","text":"","title":"[source, scala]"},{"location":"StateStore/#startmaintenanceifneeded-unit","text":"startMaintenanceIfNeeded schedules < > to start after and every spark.sql.streaming.stateStore.maintenanceInterval (defaults to 60s ). NOTE: startMaintenanceIfNeeded does nothing when the maintenance task has already been started and is still running. NOTE: startMaintenanceIfNeeded is used exclusively when StateStore is requested to < >. ==== [[doMaintenance]] Doing State Maintenance of Registered State Store Providers -- doMaintenance Internal Object Method","title":"startMaintenanceIfNeeded(): Unit"},{"location":"StateStore/#source-scala_15","text":"","title":"[source, scala]"},{"location":"StateStore/#domaintenance-unit","text":"Internally, doMaintenance prints the following DEBUG message to the logs: Doing maintenance doMaintenance then requests every spark-sql-streaming-StateStoreProvider.md[StateStoreProvider] (registered in < >) to spark-sql-streaming-StateStoreProvider.md#doMaintenance[do its own internal maintenance] (only when a StateStoreProvider < >). When a StateStoreProvider is < >, doMaintenance < > and prints the following INFO message to the logs: Unloaded [provider] NOTE: doMaintenance is used exclusively in < >. ==== [[verifyIfStoreInstanceActive]] verifyIfStoreInstanceActive Internal Object Method","title":"doMaintenance(): Unit"},{"location":"StateStore/#source-scala_16","text":"","title":"[source, scala]"},{"location":"StateStore/#verifyifstoreinstanceactivestoreproviderid-statestoreproviderid-boolean","text":"verifyIfStoreInstanceActive ...FIXME NOTE: verifyIfStoreInstanceActive is used exclusively when StateStore helper object is requested to < > (from a running < >). === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | loadedProviders | [[loadedProviders]] Loaded providers internal cache, i.e. < > per < > Used in...FIXME | _coordRef | [[_coordRef]] StateStoreCoordinator RPC endpoint (a RpcEndpointRef to StateStoreCoordinator ) Used in...FIXME |===","title":"verifyIfStoreInstanceActive(storeProviderId: StateStoreProviderId): Boolean"},{"location":"StateStoreAwareZipPartitionsRDD/","text":"StateStoreAwareZipPartitionsRDD \u00b6 StateStoreAwareZipPartitionsRDD is a ZippedPartitionsRDD2 with the < > and < > parent RDDs. StateStoreAwareZipPartitionsRDD is < > exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < > (and requests < > for one). Creating Instance \u00b6 StateStoreAwareZipPartitionsRDD takes the following to be created: [[sc]] SparkContext [[f]] Function ( (Iterator[A], Iterator[B]) => Iterator[V] , e.g. processPartitions ) [[rdd1]] Left RDD - the RDD of the left side of a join ( RDD[A] ) [[rdd2]] Right RDD - the RDD of the right side of a join ( RDD[B] ) [[stateInfo]] StatefulOperatorStateInfo [[stateStoreNames]] Names of the state stores [[storeCoordinator]] StateStoreCoordinatorRef === [[getPreferredLocations]] Placement Preferences of Partition (Preferred Locations) -- getPreferredLocations Method [source, scala] \u00b6 getPreferredLocations(partition: Partition): Seq[String] \u00b6 NOTE: getPreferredLocations is a part of the RDD Contract to specify placement preferences (aka preferred task locations ), i.e. where tasks should be executed to be as close to the data as possible. getPreferredLocations simply requests the < > for the location of every < > (with the < > and the partition ID) and returns unique executor IDs (so that processing a partition happens on the executor with the proper state store for the operator and the partition).","title":"StateStoreAwareZipPartitionsRDD"},{"location":"StateStoreAwareZipPartitionsRDD/#statestoreawarezippartitionsrdd","text":"StateStoreAwareZipPartitionsRDD is a ZippedPartitionsRDD2 with the < > and < > parent RDDs. StateStoreAwareZipPartitionsRDD is < > exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < > (and requests < > for one).","title":"StateStoreAwareZipPartitionsRDD"},{"location":"StateStoreAwareZipPartitionsRDD/#creating-instance","text":"StateStoreAwareZipPartitionsRDD takes the following to be created: [[sc]] SparkContext [[f]] Function ( (Iterator[A], Iterator[B]) => Iterator[V] , e.g. processPartitions ) [[rdd1]] Left RDD - the RDD of the left side of a join ( RDD[A] ) [[rdd2]] Right RDD - the RDD of the right side of a join ( RDD[B] ) [[stateInfo]] StatefulOperatorStateInfo [[stateStoreNames]] Names of the state stores [[storeCoordinator]] StateStoreCoordinatorRef === [[getPreferredLocations]] Placement Preferences of Partition (Preferred Locations) -- getPreferredLocations Method","title":"Creating Instance"},{"location":"StateStoreAwareZipPartitionsRDD/#source-scala","text":"","title":"[source, scala]"},{"location":"StateStoreAwareZipPartitionsRDD/#getpreferredlocationspartition-partition-seqstring","text":"NOTE: getPreferredLocations is a part of the RDD Contract to specify placement preferences (aka preferred task locations ), i.e. where tasks should be executed to be as close to the data as possible. getPreferredLocations simply requests the < > for the location of every < > (with the < > and the partition ID) and returns unique executor IDs (so that processing a partition happens on the executor with the proper state store for the operator and the partition).","title":"getPreferredLocations(partition: Partition): Seq[String]"},{"location":"StateStoreConf/","text":"StateStoreConf \u00b6 minDeltasForSnapshot \u00b6 spark.sql.streaming.stateStore.minDeltasForSnapshot maxVersionsToRetainInMemory \u00b6 spark.sql.streaming.maxBatchesToRetainInMemory minVersionsToRetain \u00b6 spark.sql.streaming.minBatchesToRetain Used when HDFSBackedStateStoreProvider is requested for cleanup . providerClass \u00b6 spark.sql.streaming.stateStore.providerClass Used when StateStoreProvider helper object is requested to create and initialize the StateStoreProvider .","title":"StateStoreConf"},{"location":"StateStoreConf/#statestoreconf","text":"","title":"StateStoreConf"},{"location":"StateStoreConf/#mindeltasforsnapshot","text":"spark.sql.streaming.stateStore.minDeltasForSnapshot","title":" minDeltasForSnapshot"},{"location":"StateStoreConf/#maxversionstoretaininmemory","text":"spark.sql.streaming.maxBatchesToRetainInMemory","title":" maxVersionsToRetainInMemory"},{"location":"StateStoreConf/#minversionstoretain","text":"spark.sql.streaming.minBatchesToRetain Used when HDFSBackedStateStoreProvider is requested for cleanup .","title":" minVersionsToRetain"},{"location":"StateStoreConf/#providerclass","text":"spark.sql.streaming.stateStore.providerClass Used when StateStoreProvider helper object is requested to create and initialize the StateStoreProvider .","title":" providerClass"},{"location":"StateStoreCoordinator/","text":"StateStoreCoordinator RPC Endpoint \u00b6 StateStoreCoordinator keeps track of StateStore s on Spark executors (per host and executor ID). StateStoreCoordinator is used by StateStoreRDD when requested to get the location preferences of partitions (based on the location of the stores). StateStoreCoordinator is a ThreadSafeRpcEndpoint RPC endpoint that manipulates < > registry through < >. [[messages]] .StateStoreCoordinator RPC Endpoint's Messages and Message Handlers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Message | Message Handler | DeactivateInstances a| [[DeactivateInstances]] Removes < > of a streaming query (given runId ) Internally, StateStoreCoordinator finds the StateStoreProviderIds of the streaming query per queryRunId and the given runId and removes them from the < > internal registry. StateStoreCoordinator prints out the following DEBUG message to the logs: Deactivating instances related to checkpoint location [runId]: [storeIdsToRemove] | GetLocation a| [[GetLocation]] Gives the location of < > (from < >) with the host and an executor id on that host. You should see the following DEBUG message in the logs: Got location of the state store [id]: [executorId] | ReportActiveInstance a| [[ReportActiveInstance]] One-way asynchronous (fire-and-forget) message to register a new < > on an executor (given host and executorId ). Sent out exclusively when StateStoreCoordinatorRef RPC endpoint reference is requested to reportActiveInstance (when StateStore utility is requested to look up the StateStore by provider ID when the StateStore and a corresponding StateStoreProvider were just created and initialized). Internally, StateStoreCoordinator prints out the following DEBUG message to the logs: Reported state store [id] is active at [executorId] In the end, StateStoreCoordinator adds the StateStoreProviderId to the < > internal registry. | StopCoordinator a| [[StopCoordinator]] Stops StateStoreCoordinator RPC Endpoint You should see the following DEBUG message in the logs: StateStoreCoordinator stopped | VerifyIfInstanceActive a| [[VerifyIfInstanceActive]] Verifies if a given < > is registered (in < >) on executorId You should see the following DEBUG message in the logs: Verified that state store [id] is active: [response] |=== [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator=ALL Refer to < >. \u00b6 === [[instances]] instances Internal Registry [source,scala] \u00b6 instances: HashMap[StateStoreProviderId, ExecutorCacheTaskLocation] \u00b6 instances is an internal registry of < > by their < > and ExecutorCacheTaskLocations (with a host and a executorId ). A new StateStoreProviderId added when StateStoreCoordinator is requested to < > All StateStoreProviderIds of a streaming query are removed when StateStoreCoordinator is requested to < >","title":"StateStoreCoordinator"},{"location":"StateStoreCoordinator/#statestorecoordinator-rpc-endpoint","text":"StateStoreCoordinator keeps track of StateStore s on Spark executors (per host and executor ID). StateStoreCoordinator is used by StateStoreRDD when requested to get the location preferences of partitions (based on the location of the stores). StateStoreCoordinator is a ThreadSafeRpcEndpoint RPC endpoint that manipulates < > registry through < >. [[messages]] .StateStoreCoordinator RPC Endpoint's Messages and Message Handlers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Message | Message Handler | DeactivateInstances a| [[DeactivateInstances]] Removes < > of a streaming query (given runId ) Internally, StateStoreCoordinator finds the StateStoreProviderIds of the streaming query per queryRunId and the given runId and removes them from the < > internal registry. StateStoreCoordinator prints out the following DEBUG message to the logs: Deactivating instances related to checkpoint location [runId]: [storeIdsToRemove] | GetLocation a| [[GetLocation]] Gives the location of < > (from < >) with the host and an executor id on that host. You should see the following DEBUG message in the logs: Got location of the state store [id]: [executorId] | ReportActiveInstance a| [[ReportActiveInstance]] One-way asynchronous (fire-and-forget) message to register a new < > on an executor (given host and executorId ). Sent out exclusively when StateStoreCoordinatorRef RPC endpoint reference is requested to reportActiveInstance (when StateStore utility is requested to look up the StateStore by provider ID when the StateStore and a corresponding StateStoreProvider were just created and initialized). Internally, StateStoreCoordinator prints out the following DEBUG message to the logs: Reported state store [id] is active at [executorId] In the end, StateStoreCoordinator adds the StateStoreProviderId to the < > internal registry. | StopCoordinator a| [[StopCoordinator]] Stops StateStoreCoordinator RPC Endpoint You should see the following DEBUG message in the logs: StateStoreCoordinator stopped | VerifyIfInstanceActive a| [[VerifyIfInstanceActive]] Verifies if a given < > is registered (in < >) on executorId You should see the following DEBUG message in the logs: Verified that state store [id] is active: [response] |=== [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator=ALL","title":"StateStoreCoordinator RPC Endpoint"},{"location":"StateStoreCoordinator/#refer-to","text":"=== [[instances]] instances Internal Registry","title":"Refer to &lt;&gt;."},{"location":"StateStoreCoordinator/#sourcescala","text":"","title":"[source,scala]"},{"location":"StateStoreCoordinator/#instances-hashmapstatestoreproviderid-executorcachetasklocation","text":"instances is an internal registry of < > by their < > and ExecutorCacheTaskLocations (with a host and a executorId ). A new StateStoreProviderId added when StateStoreCoordinator is requested to < > All StateStoreProviderIds of a streaming query are removed when StateStoreCoordinator is requested to < >","title":"instances: HashMap[StateStoreProviderId, ExecutorCacheTaskLocation]"},{"location":"StateStoreCoordinatorRef/","text":"StateStoreCoordinatorRef \u00b6 StateStoreCoordinatorRef is used to (let the tasks on Spark executors to) send < > to the < > (that lives on the driver). [[creating-instance]] [[rpcEndpointRef]] StateStoreCoordinatorRef is given the RpcEndpointRef to the StateStoreCoordinator RPC endpoint when created. StateStoreCoordinatorRef is < > through StateStoreCoordinatorRef helper object when requested to create one for the < > (when StreamingQueryManager is created) or an < > (when StateStore helper object is requested for the RPC endpoint reference to StateStoreCoordinator for Executors ). [[messages]] .StateStoreCoordinatorRef's Methods and Underlying RPC Messages [width=\"100%\",cols=\"1m,3\",options=\"header\"] |=== | Method | Description | deactivateInstances a| [[deactivateInstances]] [source, scala] \u00b6 deactivateInstances(runId: UUID): Unit \u00b6 Requests the RpcEndpointRef to send a DeactivateInstances synchronous message with the given runId and waits for a true / false response Used exclusively when StreamingQueryManager is requested to handle termination of a streaming query (when StreamExecution is requested to run a streaming query and the query has finished (running streaming batches) ). | getLocation a| [[getLocation]] [source, scala] \u00b6 getLocation( stateStoreProviderId: StateStoreProviderId): Option[String] Requests the RpcEndpointRef to send a GetLocation synchronous message with the given StateStoreProviderId and waits for the location Used when: StateStoreAwareZipPartitionsRDD is requested for the preferred locations of a partition (when StreamingSymmetricHashJoinExec physical operator is executed StateStoreRDD is requested for preferred locations for a task for a partition | reportActiveInstance a| [[reportActiveInstance]] [source, scala] \u00b6 reportActiveInstance( stateStoreProviderId: StateStoreProviderId, host: String, executorId: String): Unit Requests the RpcEndpointRef to send a ReportActiveInstance one-way asynchronous (fire-and-forget) message with the given StateStoreProviderId , host and executorId Used when StateStore utility is requested for reportActiveStoreInstance (when StateStore utility is requested to look up the StateStore by StateStoreProviderId ) | stop a| [[stop]] [source, scala] \u00b6 stop(): Unit \u00b6 Requests the RpcEndpointRef to send a StopCoordinator synchronous message Used exclusively for unit testing | verifyIfInstanceActive a| [[verifyIfInstanceActive]] [source, scala] \u00b6 verifyIfInstanceActive( stateStoreProviderId: StateStoreProviderId, executorId: String): Boolean Requests the RpcEndpointRef to send a VerifyIfInstanceActive synchronous message with the given StateStoreProviderId and executorId , and waits for a true / false response Used when StateStore utility is requested for verifyIfStoreInstanceActive (when requested to doMaintenance from a running MaintenanceTask daemon thread ) |=== Creating StateStoreCoordinatorRef to StateStoreCoordinator RPC Endpoint for Driver \u00b6 forDriver ( env : SparkEnv ): StateStoreCoordinatorRef forDriver ...FIXME forDriver is used when StreamingQueryManager is created . Creating StateStoreCoordinatorRef to StateStoreCoordinator RPC Endpoint for Executor \u00b6 forExecutor ( env : SparkEnv ): StateStoreCoordinatorRef forExecutor ...FIXME forExecutor is used when StateStore utility is requested for the RPC endpoint reference to StateStoreCoordinator for Executors .","title":"StateStoreCoordinatorRef"},{"location":"StateStoreCoordinatorRef/#statestorecoordinatorref","text":"StateStoreCoordinatorRef is used to (let the tasks on Spark executors to) send < > to the < > (that lives on the driver). [[creating-instance]] [[rpcEndpointRef]] StateStoreCoordinatorRef is given the RpcEndpointRef to the StateStoreCoordinator RPC endpoint when created. StateStoreCoordinatorRef is < > through StateStoreCoordinatorRef helper object when requested to create one for the < > (when StreamingQueryManager is created) or an < > (when StateStore helper object is requested for the RPC endpoint reference to StateStoreCoordinator for Executors ). [[messages]] .StateStoreCoordinatorRef's Methods and Underlying RPC Messages [width=\"100%\",cols=\"1m,3\",options=\"header\"] |=== | Method | Description | deactivateInstances a| [[deactivateInstances]]","title":"StateStoreCoordinatorRef"},{"location":"StateStoreCoordinatorRef/#source-scala","text":"","title":"[source, scala]"},{"location":"StateStoreCoordinatorRef/#deactivateinstancesrunid-uuid-unit","text":"Requests the RpcEndpointRef to send a DeactivateInstances synchronous message with the given runId and waits for a true / false response Used exclusively when StreamingQueryManager is requested to handle termination of a streaming query (when StreamExecution is requested to run a streaming query and the query has finished (running streaming batches) ). | getLocation a| [[getLocation]]","title":"deactivateInstances(runId: UUID): Unit"},{"location":"StateStoreCoordinatorRef/#source-scala_1","text":"getLocation( stateStoreProviderId: StateStoreProviderId): Option[String] Requests the RpcEndpointRef to send a GetLocation synchronous message with the given StateStoreProviderId and waits for the location Used when: StateStoreAwareZipPartitionsRDD is requested for the preferred locations of a partition (when StreamingSymmetricHashJoinExec physical operator is executed StateStoreRDD is requested for preferred locations for a task for a partition | reportActiveInstance a| [[reportActiveInstance]]","title":"[source, scala]"},{"location":"StateStoreCoordinatorRef/#source-scala_2","text":"reportActiveInstance( stateStoreProviderId: StateStoreProviderId, host: String, executorId: String): Unit Requests the RpcEndpointRef to send a ReportActiveInstance one-way asynchronous (fire-and-forget) message with the given StateStoreProviderId , host and executorId Used when StateStore utility is requested for reportActiveStoreInstance (when StateStore utility is requested to look up the StateStore by StateStoreProviderId ) | stop a| [[stop]]","title":"[source, scala]"},{"location":"StateStoreCoordinatorRef/#source-scala_3","text":"","title":"[source, scala]"},{"location":"StateStoreCoordinatorRef/#stop-unit","text":"Requests the RpcEndpointRef to send a StopCoordinator synchronous message Used exclusively for unit testing | verifyIfInstanceActive a| [[verifyIfInstanceActive]]","title":"stop(): Unit"},{"location":"StateStoreCoordinatorRef/#source-scala_4","text":"verifyIfInstanceActive( stateStoreProviderId: StateStoreProviderId, executorId: String): Boolean Requests the RpcEndpointRef to send a VerifyIfInstanceActive synchronous message with the given StateStoreProviderId and executorId , and waits for a true / false response Used when StateStore utility is requested for verifyIfStoreInstanceActive (when requested to doMaintenance from a running MaintenanceTask daemon thread ) |===","title":"[source, scala]"},{"location":"StateStoreCoordinatorRef/#creating-statestorecoordinatorref-to-statestorecoordinator-rpc-endpoint-for-driver","text":"forDriver ( env : SparkEnv ): StateStoreCoordinatorRef forDriver ...FIXME forDriver is used when StreamingQueryManager is created .","title":" Creating StateStoreCoordinatorRef to StateStoreCoordinator RPC Endpoint for Driver"},{"location":"StateStoreCoordinatorRef/#creating-statestorecoordinatorref-to-statestorecoordinator-rpc-endpoint-for-executor","text":"forExecutor ( env : SparkEnv ): StateStoreCoordinatorRef forExecutor ...FIXME forExecutor is used when StateStore utility is requested for the RPC endpoint reference to StateStoreCoordinator for Executors .","title":" Creating StateStoreCoordinatorRef to StateStoreCoordinator RPC Endpoint for Executor"},{"location":"StateStoreOps/","text":"StateStoreOps \u00b6 [[dataRDD]] StateStoreOps is a Scala implicit class of a data RDD (of type RDD[T] ) to create a StateStoreRDD for the following physical operators: FlatMapGroupsWithStateExec StateStoreRestoreExec StateStoreSaveExec StreamingDeduplicateExec Note Implicit Classes are a language feature in Scala for implicit conversions with extension methods for existing types. Creating StateStoreRDD (with storeUpdateFunction Aborting StateStore When Task Fails) \u00b6 mapPartitionsWithStateStore [ U ]( stateInfo : StatefulOperatorStateInfo , keySchema : StructType , valueSchema : StructType , indexOrdinal : Option [ Int ], sessionState : SessionState , storeCoordinator : Option [ StateStoreCoordinatorRef ])( storeUpdateFunction : ( StateStore , Iterator [ T ]) => Iterator [ U ]): StateStoreRDD [ T , U ] Internally, mapPartitionsWithStateStore requests SparkContext to clean storeUpdateFunction function. NOTE: mapPartitionsWithStateStore uses the < > to access the current SparkContext . NOTE: Function Cleaning is to clean a closure from unreferenced variables before it is serialized and sent to tasks. SparkContext reports a SparkException when the closure is not serializable. mapPartitionsWithStateStore then creates a (wrapper) function to abort the StateStore if state updates had not been committed before a task finished (which is to make sure that the StateStore has been committed or aborted in the end to follow the contract of StateStore ). NOTE: mapPartitionsWithStateStore uses TaskCompletionListener to be notified when a task has finished. In the end, mapPartitionsWithStateStore creates a StateStoreRDD (with the wrapper function, SessionState and StateStoreCoordinatorRef ). mapPartitionsWithStateStore is used when the following physical operators are executed: FlatMapGroupsWithStateExec StateStoreRestoreExec StateStoreSaveExec StreamingDeduplicateExec StreamingGlobalLimitExec","title":"StateStoreOps"},{"location":"StateStoreOps/#statestoreops","text":"[[dataRDD]] StateStoreOps is a Scala implicit class of a data RDD (of type RDD[T] ) to create a StateStoreRDD for the following physical operators: FlatMapGroupsWithStateExec StateStoreRestoreExec StateStoreSaveExec StreamingDeduplicateExec Note Implicit Classes are a language feature in Scala for implicit conversions with extension methods for existing types.","title":"StateStoreOps"},{"location":"StateStoreOps/#creating-statestorerdd-with-storeupdatefunction-aborting-statestore-when-task-fails","text":"mapPartitionsWithStateStore [ U ]( stateInfo : StatefulOperatorStateInfo , keySchema : StructType , valueSchema : StructType , indexOrdinal : Option [ Int ], sessionState : SessionState , storeCoordinator : Option [ StateStoreCoordinatorRef ])( storeUpdateFunction : ( StateStore , Iterator [ T ]) => Iterator [ U ]): StateStoreRDD [ T , U ] Internally, mapPartitionsWithStateStore requests SparkContext to clean storeUpdateFunction function. NOTE: mapPartitionsWithStateStore uses the < > to access the current SparkContext . NOTE: Function Cleaning is to clean a closure from unreferenced variables before it is serialized and sent to tasks. SparkContext reports a SparkException when the closure is not serializable. mapPartitionsWithStateStore then creates a (wrapper) function to abort the StateStore if state updates had not been committed before a task finished (which is to make sure that the StateStore has been committed or aborted in the end to follow the contract of StateStore ). NOTE: mapPartitionsWithStateStore uses TaskCompletionListener to be notified when a task has finished. In the end, mapPartitionsWithStateStore creates a StateStoreRDD (with the wrapper function, SessionState and StateStoreCoordinatorRef ). mapPartitionsWithStateStore is used when the following physical operators are executed: FlatMapGroupsWithStateExec StateStoreRestoreExec StateStoreSaveExec StreamingDeduplicateExec StreamingGlobalLimitExec","title":" Creating StateStoreRDD (with storeUpdateFunction Aborting StateStore When Task Fails)"},{"location":"StateStoreRDD/","text":"StateStoreRDD \u00b6 StateStoreRDD is an RDD for < > with StateStore (and data from partitions of the < >). StateStoreRDD is < > for the following stateful physical operators (using StateStoreOps.mapPartitionsWithStateStore ): FlatMapGroupsWithStateExec StateStoreRestoreExec StateStoreSaveExec StreamingDeduplicateExec StreamingGlobalLimitExec StateStoreRDD uses StateStoreCoordinator for the < > for job scheduling. [[getPartitions]] getPartitions is exactly the partitions of the < >. Computing Partition \u00b6 compute ( partition : Partition , ctxt : TaskContext ): Iterator [ U ] compute is part of the RDD abstraction. compute computes < > passing the result on to < > (with a configured StateStore ). Internally, (and similarly to < >) compute creates a < > with StateStoreId (using < >, < > and the index of the input partition ) and < >. compute then requests StateStore for the store for the StateStoreProviderId . In the end, compute computes < > (using the input partition and ctxt ) followed by executing < > (with the store and the result). === [[getPreferredLocations]] Placement Preferences of Partition (Preferred Locations) -- getPreferredLocations Method [source, scala] \u00b6 getPreferredLocations(partition: Partition): Seq[String] \u00b6 NOTE: getPreferredLocations is a part of the RDD Contract to specify placement preferences (aka preferred task locations ), i.e. where tasks should be executed to be as close to the data as possible. getPreferredLocations creates a < > with StateStoreId (using < >, < > and the index of the input partition ) and < >. NOTE: < > and < > are shared across different partitions and so the only difference in < > is the partition index. In the end, getPreferredLocations requests < > for the location of the state store for the StateStoreProviderId . Creating Instance \u00b6 StateStoreRDD takes the following to be created: [[dataRDD]] Data RDD ( RDD[T] to update the aggregates in a state store) [[storeUpdateFunction]] Store update function ( (StateStore, Iterator[T]) => Iterator[U] where T is the type of rows in the < >) [[checkpointLocation]] Checkpoint directory [[queryRunId]] Run ID of the streaming query [[operatorId]] Operator ID [[storeVersion]] Version of the store [[keySchema]] Key schema - schema of the keys [[valueSchema]] Value schema - schema of the values [[indexOrdinal]] Index [[sessionState]] SessionState [[storeCoordinator]] Optional StateStoreCoordinatorRef === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | hadoopConfBroadcast | [[hadoopConfBroadcast]] | storeConf | [[storeConf]] Configuration parameters (as StateStoreConf ) using the current SQLConf (from SessionState ) |===","title":"StateStoreRDD"},{"location":"StateStoreRDD/#statestorerdd","text":"StateStoreRDD is an RDD for < > with StateStore (and data from partitions of the < >). StateStoreRDD is < > for the following stateful physical operators (using StateStoreOps.mapPartitionsWithStateStore ): FlatMapGroupsWithStateExec StateStoreRestoreExec StateStoreSaveExec StreamingDeduplicateExec StreamingGlobalLimitExec StateStoreRDD uses StateStoreCoordinator for the < > for job scheduling. [[getPartitions]] getPartitions is exactly the partitions of the < >.","title":"StateStoreRDD"},{"location":"StateStoreRDD/#computing-partition","text":"compute ( partition : Partition , ctxt : TaskContext ): Iterator [ U ] compute is part of the RDD abstraction. compute computes < > passing the result on to < > (with a configured StateStore ). Internally, (and similarly to < >) compute creates a < > with StateStoreId (using < >, < > and the index of the input partition ) and < >. compute then requests StateStore for the store for the StateStoreProviderId . In the end, compute computes < > (using the input partition and ctxt ) followed by executing < > (with the store and the result). === [[getPreferredLocations]] Placement Preferences of Partition (Preferred Locations) -- getPreferredLocations Method","title":" Computing Partition"},{"location":"StateStoreRDD/#source-scala","text":"","title":"[source, scala]"},{"location":"StateStoreRDD/#getpreferredlocationspartition-partition-seqstring","text":"NOTE: getPreferredLocations is a part of the RDD Contract to specify placement preferences (aka preferred task locations ), i.e. where tasks should be executed to be as close to the data as possible. getPreferredLocations creates a < > with StateStoreId (using < >, < > and the index of the input partition ) and < >. NOTE: < > and < > are shared across different partitions and so the only difference in < > is the partition index. In the end, getPreferredLocations requests < > for the location of the state store for the StateStoreProviderId .","title":"getPreferredLocations(partition: Partition): Seq[String]"},{"location":"StateStoreRDD/#creating-instance","text":"StateStoreRDD takes the following to be created: [[dataRDD]] Data RDD ( RDD[T] to update the aggregates in a state store) [[storeUpdateFunction]] Store update function ( (StateStore, Iterator[T]) => Iterator[U] where T is the type of rows in the < >) [[checkpointLocation]] Checkpoint directory [[queryRunId]] Run ID of the streaming query [[operatorId]] Operator ID [[storeVersion]] Version of the store [[keySchema]] Key schema - schema of the keys [[valueSchema]] Value schema - schema of the values [[indexOrdinal]] Index [[sessionState]] SessionState [[storeCoordinator]] Optional StateStoreCoordinatorRef === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | hadoopConfBroadcast | [[hadoopConfBroadcast]] | storeConf | [[storeConf]] Configuration parameters (as StateStoreConf ) using the current SQLConf (from SessionState ) |===","title":"Creating Instance"},{"location":"StatefulAggregationStrategy/","text":"StatefulAggregationStrategy Execution Planning Strategy \u00b6 StatefulAggregationStrategy is an execution planning strategy that is used to < > with the two logical operators: EventTimeWatermark logical operator ( Dataset.withWatermark operator) Aggregate logical operator (for Dataset.groupBy and Dataset.groupByKey operators, and GROUP BY SQL clause) TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy.html[Execution Planning Strategies] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. StatefulAggregationStrategy is used exclusively when IncrementalExecution is requested to plan a streaming query. StatefulAggregationStrategy is available using SessionState . spark . sessionState . planner . StatefulAggregationStrategy [[apply]] [[selection-requirements]] .StatefulAggregationStrategy's Logical to Physical Operator Conversions [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Logical Operator | Physical Operator | EventTimeWatermark a| [[EventTimeWatermark]] EventTimeWatermarkExec | Aggregate a| [[Aggregate]] In the order of preference: HashAggregateExec ObjectHashAggregateExec SortAggregateExec TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy-Aggregation.html[Aggregation Execution Planning Strategy for Aggregate Physical Operators] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. |=== [source, scala] \u00b6 val counts = spark. readStream. format(\"rate\"). load. groupBy(window($\"timestamp\", \"5 seconds\") as \"group\"). agg(count(\"value\") as \"count\"). orderBy(\"group\") scala> counts.explain == Physical Plan == *Sort [group#6 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(group#6 ASC NULLS FIRST, 200) +- *HashAggregate(keys=[window#13], functions=[count(value#1L)]) +- StateStoreSave [window#13], StatefulOperatorStateInfo( ,736d67c2-6daa-4c4c-9c4b-c12b15af20f4,0,0), Append, 0 +- *HashAggregate(keys=[window#13], functions=[merge_count(value#1L)]) +- StateStoreRestore [window#13], StatefulOperatorStateInfo( ,736d67c2-6daa-4c4c-9c4b-c12b15af20f4,0,0) +- *HashAggregate(keys=[window#13], functions=[merge_count(value#1L)]) +- Exchange hashpartitioning(window#13, 200) +- *HashAggregate(keys=[window#13], functions=[partial_count(value#1L)]) +- *Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#13, value#1L] +- *Filter isnotnull(timestamp#0) +- StreamingRelation rate, [timestamp#0, value#1L] import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val consoleOutput = counts. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). queryName(\"counts\"). outputMode(OutputMode.Complete). // \u2190 required for groupBy start // Eventually... consoleOutput.stop === [[planStreamingAggregation]][[AggUtils-planStreamingAggregation]] Selecting Aggregate Physical Operator Given Aggregate Expressions\u2009\u2014\u2009 AggUtils.planStreamingAggregation Internal Method [source, scala] \u00b6 planStreamingAggregation( groupingExpressions: Seq[NamedExpression], functionsWithoutDistinct: Seq[AggregateExpression], resultExpressions: Seq[NamedExpression], child: SparkPlan): Seq[SparkPlan] planStreamingAggregation takes the grouping attributes (from groupingExpressions ). NOTE: groupingExpressions corresponds to the grouping function in groupBy operator. [[partialAggregate]] planStreamingAggregation creates an aggregate physical operator (called partialAggregate ) with: requiredChildDistributionExpressions undefined (i.e. None ) initialInputBufferOffset as 0 functionsWithoutDistinct in Partial mode child operator as the input child [NOTE] \u00b6 planStreamingAggregation creates one of the following aggregate physical operators (in the order of preference): HashAggregateExec ObjectHashAggregateExec SortAggregateExec planStreamingAggregation uses AggUtils.createAggregate method to select an aggregate physical operator that you can read about in https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-SparkStrategy-Aggregation.html#AggUtils-createAggregate[Selecting Aggregate Physical Operator Given Aggregate Expressions -- AggUtils.createAggregate Internal Method] in Mastering Apache Spark 2 gitbook. \u00b6 [[partialMerged1]] planStreamingAggregation creates an aggregate physical operator (called partialMerged1 ) with: requiredChildDistributionExpressions based on the input groupingExpressions initialInputBufferOffset as the length of groupingExpressions functionsWithoutDistinct in PartialMerge mode child operator as < > aggregate physical operator created above [[restored]] planStreamingAggregation creates StateStoreRestoreExec physical operator with the grouping attributes, undefined StatefulOperatorStateInfo , and < > aggregate physical operator created above. [[partialMerged2]] planStreamingAggregation creates an aggregate physical operator (called partialMerged2 ) with: child operator as < > physical operator created above NOTE: The only difference between < > and < > steps is the child physical operator. [[saved]] planStreamingAggregation creates StateStoreSaveExec.md#creating-instance[StateStoreSaveExec] with: the grouping attributes based on the input groupingExpressions No stateInfo , outputMode and eventTimeWatermark child operator as < > aggregate physical operator created above [[finalAndCompleteAggregate]] In the end, planStreamingAggregation creates the final aggregate physical operator (called finalAndCompleteAggregate ) with: requiredChildDistributionExpressions based on the input groupingExpressions initialInputBufferOffset as the length of groupingExpressions functionsWithoutDistinct in Final mode child operator as < > physical operator created above","title":"StatefulAggregationStrategy"},{"location":"StatefulAggregationStrategy/#statefulaggregationstrategy-execution-planning-strategy","text":"StatefulAggregationStrategy is an execution planning strategy that is used to < > with the two logical operators: EventTimeWatermark logical operator ( Dataset.withWatermark operator) Aggregate logical operator (for Dataset.groupBy and Dataset.groupByKey operators, and GROUP BY SQL clause) TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy.html[Execution Planning Strategies] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. StatefulAggregationStrategy is used exclusively when IncrementalExecution is requested to plan a streaming query. StatefulAggregationStrategy is available using SessionState . spark . sessionState . planner . StatefulAggregationStrategy [[apply]] [[selection-requirements]] .StatefulAggregationStrategy's Logical to Physical Operator Conversions [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Logical Operator | Physical Operator | EventTimeWatermark a| [[EventTimeWatermark]] EventTimeWatermarkExec | Aggregate a| [[Aggregate]] In the order of preference: HashAggregateExec ObjectHashAggregateExec SortAggregateExec TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy-Aggregation.html[Aggregation Execution Planning Strategy for Aggregate Physical Operators] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. |===","title":"StatefulAggregationStrategy Execution Planning Strategy"},{"location":"StatefulAggregationStrategy/#source-scala","text":"val counts = spark. readStream. format(\"rate\"). load. groupBy(window($\"timestamp\", \"5 seconds\") as \"group\"). agg(count(\"value\") as \"count\"). orderBy(\"group\") scala> counts.explain == Physical Plan == *Sort [group#6 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(group#6 ASC NULLS FIRST, 200) +- *HashAggregate(keys=[window#13], functions=[count(value#1L)]) +- StateStoreSave [window#13], StatefulOperatorStateInfo( ,736d67c2-6daa-4c4c-9c4b-c12b15af20f4,0,0), Append, 0 +- *HashAggregate(keys=[window#13], functions=[merge_count(value#1L)]) +- StateStoreRestore [window#13], StatefulOperatorStateInfo( ,736d67c2-6daa-4c4c-9c4b-c12b15af20f4,0,0) +- *HashAggregate(keys=[window#13], functions=[merge_count(value#1L)]) +- Exchange hashpartitioning(window#13, 200) +- *HashAggregate(keys=[window#13], functions=[partial_count(value#1L)]) +- *Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#13, value#1L] +- *Filter isnotnull(timestamp#0) +- StreamingRelation rate, [timestamp#0, value#1L] import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val consoleOutput = counts. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). queryName(\"counts\"). outputMode(OutputMode.Complete). // \u2190 required for groupBy start // Eventually... consoleOutput.stop === [[planStreamingAggregation]][[AggUtils-planStreamingAggregation]] Selecting Aggregate Physical Operator Given Aggregate Expressions\u2009\u2014\u2009 AggUtils.planStreamingAggregation Internal Method","title":"[source, scala]"},{"location":"StatefulAggregationStrategy/#source-scala_1","text":"planStreamingAggregation( groupingExpressions: Seq[NamedExpression], functionsWithoutDistinct: Seq[AggregateExpression], resultExpressions: Seq[NamedExpression], child: SparkPlan): Seq[SparkPlan] planStreamingAggregation takes the grouping attributes (from groupingExpressions ). NOTE: groupingExpressions corresponds to the grouping function in groupBy operator. [[partialAggregate]] planStreamingAggregation creates an aggregate physical operator (called partialAggregate ) with: requiredChildDistributionExpressions undefined (i.e. None ) initialInputBufferOffset as 0 functionsWithoutDistinct in Partial mode child operator as the input child","title":"[source, scala]"},{"location":"StatefulAggregationStrategy/#note","text":"planStreamingAggregation creates one of the following aggregate physical operators (in the order of preference): HashAggregateExec ObjectHashAggregateExec SortAggregateExec","title":"[NOTE]"},{"location":"StatefulAggregationStrategy/#planstreamingaggregation-uses-aggutilscreateaggregate-method-to-select-an-aggregate-physical-operator-that-you-can-read-about-in-httpsjaceklaskowskigitbooksiomastering-apache-sparkspark-sql-sparkstrategy-aggregationhtmlaggutils-createaggregateselecting-aggregate-physical-operator-given-aggregate-expressions-aggutilscreateaggregate-internal-method-in-mastering-apache-spark-2-gitbook","text":"[[partialMerged1]] planStreamingAggregation creates an aggregate physical operator (called partialMerged1 ) with: requiredChildDistributionExpressions based on the input groupingExpressions initialInputBufferOffset as the length of groupingExpressions functionsWithoutDistinct in PartialMerge mode child operator as < > aggregate physical operator created above [[restored]] planStreamingAggregation creates StateStoreRestoreExec physical operator with the grouping attributes, undefined StatefulOperatorStateInfo , and < > aggregate physical operator created above. [[partialMerged2]] planStreamingAggregation creates an aggregate physical operator (called partialMerged2 ) with: child operator as < > physical operator created above NOTE: The only difference between < > and < > steps is the child physical operator. [[saved]] planStreamingAggregation creates StateStoreSaveExec.md#creating-instance[StateStoreSaveExec] with: the grouping attributes based on the input groupingExpressions No stateInfo , outputMode and eventTimeWatermark child operator as < > aggregate physical operator created above [[finalAndCompleteAggregate]] In the end, planStreamingAggregation creates the final aggregate physical operator (called finalAndCompleteAggregate ) with: requiredChildDistributionExpressions based on the input groupingExpressions initialInputBufferOffset as the length of groupingExpressions functionsWithoutDistinct in Final mode child operator as < > physical operator created above","title":"planStreamingAggregation uses AggUtils.createAggregate method to select an aggregate physical operator that you can read about in https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-SparkStrategy-Aggregation.html#AggUtils-createAggregate[Selecting Aggregate Physical Operator Given Aggregate Expressions -- AggUtils.createAggregate Internal Method] in Mastering Apache Spark 2 gitbook."},{"location":"StatefulOperatorStateInfo/","text":"StatefulOperatorStateInfo \u00b6 StatefulOperatorStateInfo identifies the state store for a given stateful physical operator: [[checkpointLocation]] Checkpoint directory ( checkpointLocation ) [[queryRunId]] < > of a streaming query ( queryRunId ) [[operatorId]] Stateful operator ID ( operatorId ) [[storeVersion]] < > ( storeVersion ) [[numPartitions]] Number of partitions StatefulOperatorStateInfo is < > exclusively when IncrementalExecution is requested for nextStatefulOperationStateInfo . [[toString]] When requested for a textual representation ( toString ), StatefulOperatorStateInfo returns the following: state info [ checkpoint = [checkpointLocation], runId = [queryRunId], opId = [operatorId], ver = [storeVersion], numPartitions = [numPartitions]] State Version and Batch ID \u00b6 When created (when IncrementalExecution is requested for the next StatefulOperatorStateInfo ), a StatefulOperatorStateInfo is given a state version . The state version is exactly the batch ID of the IncrementalExecution .","title":"StatefulOperatorStateInfo"},{"location":"StatefulOperatorStateInfo/#statefuloperatorstateinfo","text":"StatefulOperatorStateInfo identifies the state store for a given stateful physical operator: [[checkpointLocation]] Checkpoint directory ( checkpointLocation ) [[queryRunId]] < > of a streaming query ( queryRunId ) [[operatorId]] Stateful operator ID ( operatorId ) [[storeVersion]] < > ( storeVersion ) [[numPartitions]] Number of partitions StatefulOperatorStateInfo is < > exclusively when IncrementalExecution is requested for nextStatefulOperationStateInfo . [[toString]] When requested for a textual representation ( toString ), StatefulOperatorStateInfo returns the following: state info [ checkpoint = [checkpointLocation], runId = [queryRunId], opId = [operatorId], ver = [storeVersion], numPartitions = [numPartitions]]","title":"StatefulOperatorStateInfo"},{"location":"StatefulOperatorStateInfo/#state-version-and-batch-id","text":"When created (when IncrementalExecution is requested for the next StatefulOperatorStateInfo ), a StatefulOperatorStateInfo is given a state version . The state version is exactly the batch ID of the IncrementalExecution .","title":" State Version and Batch ID"},{"location":"StreamExecution/","text":"StreamExecution \u00b6 StreamExecution is an abstraction of stream execution engines ( streaming query processing engines ) that can run a structured query (on a stream execution thread ). Note Continuous query , streaming query , continuous Dataset , streaming Dataset are all considered high-level synonyms for an executable entity that stream execution engines run using the analyzed logical plan internally. Important StreamExecution does not support adaptive query execution and cost-based optimizer (and turns them off when requested to run stream processing ). StreamExecution is the execution environment of a streaming query that is executed every trigger and in the end adds the results to a sink . StreamExecution corresponds to a single streaming query with one or more streaming sources and exactly one streaming sink . import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val q = spark. readStream. format(\"rate\"). load. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.minutes)). start scala> :type q org.apache.spark.sql.streaming.StreamingQuery // Pull out StreamExecution off StreamingQueryWrapper import org.apache.spark.sql.execution.streaming.{StreamExecution, StreamingQueryWrapper} val se = q.asInstanceOf[StreamingQueryWrapper].streamingQuery scala> :type se org.apache.spark.sql.execution.streaming.StreamExecution Contract \u00b6 Logical Plan \u00b6 logicalPlan : LogicalPlan Analyzed logical plan of the streaming query to execute Used when StreamExecution is requested to run stream processing logicalPlan is part of the ProgressReporter abstraction. Running Activated Streaming Query \u00b6 runActivatedStream ( sparkSessionForStream : SparkSession ): Unit Executes ( runs ) the activated streaming query (that is described by the logical plan ) Used when StreamExecution is requested to run the streaming query (when transitioning from INITIALIZING to ACTIVE state) Implementations \u00b6 ContinuousExecution MicroBatchExecution Creating Instance \u00b6 StreamExecution takes the following to be created: SparkSession Name of the streaming query (can be null ) Path of the checkpoint directory ( metadata directory ) Streaming query (not used due to logicalPlan ) Table ( Spark SQL ) Trigger Clock OutputMode deleteCheckpointOnStop flag (whether to delete the checkpoint directory on stop) Abstract Class StreamExecution is an abstract class and cannot be created directly. It is created indirectly for the concrete StreamExecutions . Demo \u00b6 import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val se = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery scala> :type se org.apache.spark.sql.execution.streaming.StreamExecution Configuration Properties \u00b6 s.s.s.minBatchesToRetain \u00b6 StreamExecution uses the spark.sql.streaming.minBatchesToRetain configuration property to allow the StreamExecutions to discard old log entries (from the offset and commit logs). s.s.s.pollingDelay \u00b6 StreamExecution uses spark.sql.streaming.pollingDelay configuration property to control how long to delay polling for new data (when no data was available to process in a batch). ProgressReporter \u00b6 StreamExecution is a ProgressReporter and reports status of the streaming query (when it starts, progresses and terminates) by posting StreamingQueryListener events. import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .text(\"server-logs\") .writeStream .format(\"console\") .queryName(\"debug\") .trigger(Trigger.ProcessingTime(20.seconds)) .start // Enable the log level to see the INFO and DEBUG messages // log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=DEBUG 17/06/18 21:21:07 INFO StreamExecution: Starting new streaming query. 17/06/18 21:21:07 DEBUG StreamExecution: getOffset took 5 ms 17/06/18 21:21:07 DEBUG StreamExecution: Stream running from {} to {} 17/06/18 21:21:07 DEBUG StreamExecution: triggerExecution took 9 ms 17/06/18 21:21:07 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map()) 17/06/18 21:21:07 INFO StreamExecution: Streaming query made progress: { \"id\" : \"8b57b0bd-fc4a-42eb-81a3-777d7ba5e370\", \"runId\" : \"920b227e-6d02-4a03-a271-c62120258cea\", \"name\" : \"debug\", \"timestamp\" : \"2017-06-18T19:21:07.693Z\", \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0, \"durationMs\" : { \"getOffset\" : 5, \"triggerExecution\" : 9 }, \"stateOperators\" : [ ], \"sources\" : [ { \"description\" : \"FileStreamSource[file:/Users/jacek/dev/oss/spark/server-logs]\", \"startOffset\" : null, \"endOffset\" : null, \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0 } ], \"sink\" : { \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@2460208a\" } } 17/06/18 21:21:10 DEBUG StreamExecution: Starting Trigger Calculation 17/06/18 21:21:10 DEBUG StreamExecution: getOffset took 3 ms 17/06/18 21:21:10 DEBUG StreamExecution: triggerExecution took 3 ms 17/06/18 21:21:10 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map()) Unique Streaming Sources \u00b6 StreamExecution tracks unique streaming data sources in uniqueSources internal registry. Used when StreamExecution : Constructs the next streaming micro-batch (and gets new offsets for every streaming data source) Stops all streaming data sources Streaming Query Identifiers \u00b6 The name , id and runId are all unique across all active queries (in a StreamingQueryManager ). The difference is that: name is optional and user-defined id is a UUID that is auto-generated at the time StreamExecution is created and persisted to metadata checkpoint file runId is a UUID that is auto-generated every time StreamExecution is created Id \u00b6 StreamExecution is uniquely identified by an ID of the streaming query (which is the id of the StreamMetadata ). Since the StreamMetadata is persisted (to the metadata file in the checkpoint directory ), the streaming query ID \"survives\" query restarts as long as the checkpoint directory is preserved. Run Id \u00b6 StreamExecution is uniquely identified by a run ID of the streaming query . A run ID is a randomly-generated 128-bit universally unique identifier (UUID) that is assigned at the time StreamExecution is created. runId does not \"survive\" query restarts and will always be different yet unique (across all active queries). StreamMetadata \u00b6 StreamExecution uses a StreamMetadata that is persisted in the metadata file in the checkpoint directory . If the metadata file is available it is read and is the way to recover the id of a streaming query when resumed (i.e. restarted after a failure or a planned stop). Metadata Logs \u00b6 Write-Ahead Offset Log \u00b6 offsetLog : OffsetSeqLog offsetLog is a Hadoop DFS-based metadata storage (of OffsetSeq s) with offsets metadata directory . offsetLog is used as a Write-Ahead Log of Offsets to persist offsets of the data about to be processed in every trigger. Tip Monitor offsets and commits metadata logs to know the progress of a streaming query. The number of entries in the OffsetSeqLog is controlled using spark.sql.streaming.minBatchesToRetain configuration property. offsetLog is used when: ContinuousExecution stream execution engine is requested to commit an epoch , getStartOffsets , and addOffset MicroBatchExecution stream execution engine is requested to populate start offsets and construct (or skip) the next streaming micro-batch Offset Commit Log \u00b6 StreamExecution uses offset commit log ( CommitLog with commits metadata checkpoint directory ) for streaming batches successfully executed (with a single file per batch with a file name being the batch id) or committed epochs. Note Metadata log or metadata checkpoint are synonyms and are often used interchangeably. commitLog is used by the < > for the following: MicroBatchExecution is requested to < > (that in turn requests to < > at the very beginning of the streaming query execution and later regularly every < >) ContinuousExecution is requested to < > (that in turn requests to < > at the very beginning of the streaming query execution and later regularly every < >) State of Streaming Query \u00b6 state : AtomicReference [ State ] state indicates the internal state of execution of the streaming query (as java.util.concurrent.atomic.AtomicReference ). ACTIVE \u00b6 StreamExecution has been requested to < > (and is about to < >) INITIALIZING \u00b6 StreamExecution has been created . TERMINATED \u00b6 Indicates that: MicroBatchExecution has been requested to stop ContinuousExecution has been requested to stop StreamExecution has been requested to run stream processing (and has finished running the activated streaming query ) RECONFIGURING \u00b6 Used when ContinuousExecution is requested to run a streaming query in continuous mode (and the ContinuousReader indicated a need for reconfiguration ) Creating StreamingWrite \u00b6 createStreamingWrite ( table : SupportsWrite , options : Map [ String , String ], inputPlan : LogicalPlan ): StreamingWrite createStreamingWrite creates a LogicalWriteInfoImpl (with the query ID , the schema of the input LogicalPlan and the given options). createStreamingWrite requests the given SupportsWrite table for a WriteBuilder (for the LogicalWriteInfoImpl ). Tip Learn more about SupportsWrite and WriteBuilder in The Internals of Spark SQL online book. createStreamingWrite branches based on the OutputMode : For Append output mode, createStreamingWrite requests the WriteBuilder to build a StreamingWrite . For Complete output mode, createStreamingWrite assumes that the WriteBuilder is a SupportsTruncate and requests it to truncate followed by buildForStreaming For Update output mode, createStreamingWrite assumes that the WriteBuilder is a SupportsStreamingUpdate and requests it to update followed by buildForStreaming Tip Learn more about SupportsTruncate and SupportsStreamingUpdate in The Internals of Spark SQL online book. createStreamingWrite is used when MicroBatchExecution and ContinuousExecution stream execution engines are requested for analyzed logical plans. Available Offsets (StreamProgress) \u00b6 availableOffsets : StreamProgress availableOffsets is a registry of offsets per streaming source to track what data (by offset ) is available for processing for every streaming source in the streaming query (and have not yet been committed ). availableOffsets works in tandem with the committedOffsets internal registry. availableOffsets is empty when StreamExecution is created (i.e. no offsets are reported for any streaming source in the streaming query). availableOffsets is used when: MicroBatchExecution stream execution engine is requested to < >, < >, < > and < > ContinuousExecution stream execution engine is requested to commit an epoch StreamExecution is requested for the internal string representation Committed Offsets (StreamProgress) \u00b6 committedOffsets : StreamProgress committedOffsets is a registry of offsets per streaming source to track what data (by offset ) has already been processed and committed (to the sink or state stores) for every streaming source in the streaming query . committedOffsets works in tandem with the availableOffsets internal registry. committedOffsets is used when: MicroBatchExecution stream execution engine is requested for the < >, to < > and < > ContinuousExecution stream execution engine is requested for the < > and to < > StreamExecution is requested for the internal string representation Fully-Qualified (Resolved) Path to Checkpoint Root Directory \u00b6 resolvedCheckpointRoot : String resolvedCheckpointRoot is a fully-qualified path of the given checkpoint root directory . The given checkpoint root directory is defined using checkpointLocation option or the spark.sql.streaming.checkpointLocation configuration property with queryName option. checkpointLocation and queryName options are defined when StreamingQueryManager is requested to create a streaming query . resolvedCheckpointRoot is used when creating the path to the checkpoint directory and when StreamExecution finishes running streaming batches . resolvedCheckpointRoot is used for the logicalPlan (while transforming analyzedPlan and planning StreamingRelation logical operators to corresponding StreamingExecutionRelation physical operators with the streaming data sources created passing in the path to sources directory to store checkpointing metadata). resolvedCheckpointRoot is printed out immediately when resolved as a INFO message to the logs: Checkpoint root [checkpointRoot] resolved to [resolvedCheckpointRoot]. resolvedCheckpointRoot is printed out again as a INFO message to the logs when StreamExecution is started : Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint. StreamWriterCommitProgress \u00b6 sinkCommitProgress : Option [ StreamWriterCommitProgress ] sinkCommitProgress is part of the ProgressReporter abstraction. StreamExecution initializes sinkCommitProgress registry to be None when created . Last Query Execution Of Streaming Query (IncrementalExecution) \u00b6 lastExecution : IncrementalExecution lastExecution is part of the ProgressReporter abstraction. lastExecution is a IncrementalExecution (a QueryExecution of a streaming query) of the most recent ( last ) execution. lastExecution is created when the < > are requested for the following: MicroBatchExecution is requested to < > (when in < >) ContinuousExecution stream execution engine is requested to < > (when in < >) lastExecution is used when: StreamExecution is requested to < > (via < >) ProgressReporter is requested to extractStateOperatorMetrics , extractExecutionStats , and extractSourceToNumInputRows MicroBatchExecution stream execution engine is requested to < > (based on StateStoreWriters in a streaming query ), < > (when in < > and < >) ContinuousExecution stream execution engine is requested to < > (when in < >) For debugging query execution of streaming queries (using debugCodegen ) Explaining Streaming Query \u00b6 explain (): Unit // <1> explain ( extended : Boolean ): Unit <1> Turns the extended flag off ( false ) explain simply prints out < > to the standard output. Stopping Streaming Sources and Readers \u00b6 stopSources (): Unit stopSources requests every streaming source to stop. In case of an non-fatal exception, stopSources prints out the following WARN message to the logs: Failed to stop streaming source: [source]. Resources may have leaked. stopSources is used when: StreamExecution is requested to < > (and < > successfully or not) ContinuousExecution is requested to < > (and terminates) Running Stream Processing \u00b6 runStream (): Unit runStream simply prepares the environment to execute the activated streaming query . runStream is used when the stream execution thread is requested to start (when DataStreamWriter is requested to start an execution of the streaming query ). Internally, runStream sets the job group (to all the Spark jobs started by this thread) as follows: runId for the job group ID getBatchDescriptionString for the job group description (to display in web UI) interruptOnCancel flag on Note runStream uses the SparkSession to access SparkContext and assign the job group id. Learn more about SparkContext.setJobGroup method in The Internals of Apache Spark online book. runStream sets sql.streaming.queryId local property to id . runStream requests the MetricsSystem to register the MetricsReporter when spark.sql.streaming.metricsEnabled configuration property is enabled. runStream notifies StreamingQueryListeners that the streaming query has been started (by posting a new QueryStartedEvent event with id , runId , and name ). runStream unblocks the main starting thread (by decrementing the count of the startLatch that when 0 lets the starting thread continue). runStream updates the status message to be Initializing sources . runStream initializes the analyzed logical plan . Lazy Value The analyzed logical plan is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards. runStream disables adaptive query execution and cost-based join optimization (by turning spark.sql.adaptive.enabled and spark.sql.cbo.enabled configuration properties off, respectively). runStream creates a new \"zero\" OffsetSeqMetadata . (when in INITIALIZING state) runStream enters ACTIVE state: Decrements the count of initializationLatch Executes the activated streaming query Note runBatches does the main work only when first started (when in INITIALIZING state). runStream ...FIXME (describe the failed and stop states) Once TriggerExecutor has finished executing batches, runBatches updates the status message to Stopped . NOTE: TriggerExecutor finishes executing batches when the batch runner returns whether the streaming query is stopped or not (while active ). finally Block \u00b6 runStream releases the startLatch and initializationLatch latches. runStream stopSources . runStream enters TERMINATED state. runStream sets the StreamingQueryStatus with the isTriggerActive and isDataAvailable flags off ( false ). runStream removes the stream metrics reporter from the application's MetricsSystem . runStream requests the StreamingQueryManager to handle termination of a streaming query . runStream creates a new QueryTerminatedEvent (with the id and run id of the streaming query) and posts it . With the deleteCheckpointOnStop flag enabled and no StreamingQueryException , runStream deletes the checkpoint directory . In the end, runStream releases the terminationLatch latch. TriggerExecutor's Batch Runner \u00b6 Batch Runner ( batchRunner ) is an executable block executed by TriggerExecutor in runBatches . batchRunner starts trigger calculation . As long as the query is not stopped (i.e. state is not TERMINATED ), batchRunner executes the streaming batch for the trigger. In triggerExecution time-tracking section , runBatches branches off per currentBatchId : For currentBatchId < 0 : populateStartOffsets Setting Job Description as getBatchDescriptionString Stream running from [committedOffsets] to [availableOffsets] For currentBatchId >= 0 : Constructing the next streaming micro-batch If there is data available in the sources, batchRunner marks currentStatus with isDataAvailable enabled. Tip You can check out the status of a streaming query using status method. scala> spark.streams.active(0).status res1: org.apache.spark.sql.streaming.StreamingQueryStatus = { \"message\" : \"Waiting for next trigger\", \"isDataAvailable\" : false, \"isTriggerActive\" : false } batchRunner then updates the status message to Processing new data and runs the current streaming batch . After triggerExecution section has finished, batchRunner finishes the streaming batch for the trigger (and collects query execution statistics). When there was < > in the sources, batchRunner updates committed offsets (by adding the < > to BatchCommitLog and adding availableOffsets to committedOffsets ). batchRunner prints out the following DEBUG message to the logs: batch [currentBatchId] committed batchRunner increments the current batch id and sets the job description for all the following Spark jobs to include the new batch id . When no data was available in the sources to process, batchRunner does the following: Marks currentStatus with isDataAvailable disabled Updates the status message to Waiting for data to arrive Sleeps the current thread for pollingDelayMs milliseconds. batchRunner updates the status message to Waiting for next trigger and returns whether the query is currently active or not (so TriggerExecutor can decide whether to finish executing the batches or not) Starting Streaming Query (on Stream Execution Thread) \u00b6 start (): Unit start starts a stream execution thread that simply runs stream processing (and hence the streaming query). start prints out the following INFO message to the logs: Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint. start then starts the < > (as a daemon thread). NOTE: start uses Java's ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#start--++[java.lang.Thread.start ] to run the streaming query on a separate execution thread. NOTE: When started, a streaming query runs in its own execution thread on JVM. In the end, start pauses the main thread (using the < > until StreamExecution is requested to < > that in turn sends a QueryStartedEvent to all streaming listeners followed by decrementing the count of the < >). start is used when StreamingQueryManager is requested to start a streaming query (when DataStreamWriter is requested to start an execution of the streaming query ). Path to Checkpoint Directory \u00b6 checkpointFile ( name : String ): String checkpointFile gives the path of a directory with name in checkpoint directory . checkpointFile is used for streamMetadata , OffsetSeqLog , BatchCommitLog , and lastExecution (for runBatch ). Posting StreamingQueryListener Event \u00b6 postEvent ( event : StreamingQueryListener . Event ): Unit postEvent is a part of the ProgressReporter abstraction. postEvent simply requests the StreamingQueryManager to post the input event (to the StreamingQueryListenerBus in the current SparkSession ). Note postEvent uses SparkSession to access the current StreamingQueryManager . postEvent is used when: ProgressReporter is requested to report update progress (while finishing a trigger ) StreamExecution runs streaming batches (and announces starting a streaming query by posting a QueryStartedEvent and query termination by posting a QueryTerminatedEvent ) Waiting Until No New Data Available in Sources or Query Has Been Terminated \u00b6 processAllAvailable (): Unit processAllAvailable is a part of the StreamingQuery abstraction. processAllAvailable reports the < > if reported (and returns immediately). NOTE: < > is reported exclusively when StreamExecution is requested to < > (that terminated with an exception). processAllAvailable returns immediately when StreamExecution is no longer < > (in TERMINATED state). processAllAvailable acquires a lock on the < > and turns the < > internal flag off ( false ). processAllAvailable keeps polling with 10-second pauses (locked on < >) until < > flag is turned on ( true ) or StreamExecution is no longer < > (in TERMINATED state). NOTE: The 10-second pause is hardcoded and cannot be changed. In the end, processAllAvailable releases < > lock. processAllAvailable throws an IllegalStateException when executed on the < >: Cannot wait for a query state from the same thread that is running the query Stream Execution Thread \u00b6 queryExecutionThread : QueryExecutionThread queryExecutionThread is a Java thread of execution ( java.util.Thread ) that runs a streaming query . queryExecutionThread is started (as a daemon thread) when StreamExecution is requested to < >. At that time, start prints out the following INFO message to the logs (with the < > and the < >): Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint. When started, queryExecutionThread sets the < > and < >. queryExecutionThread uses the name stream execution thread for [id] (that uses < > for the id, i.e. queryName [id = [id], runId = [runId]] ). queryExecutionThread is a QueryExecutionThread that is a custom UninterruptibleThread from Apache Spark with runUninterruptibly method for running a block of code without being interrupted by Thread.interrupt() . Tip Use Java's jconsole or jstack to monitor stream execution threads. $ jstack <driver-pid> | grep -e \"stream execution thread\" \"stream execution thread for kafka-topic1 [id =... Current Batch Metadata (Event-Time Watermark and Timestamp) \u00b6 offsetSeqMetadata : OffsetSeqMetadata offsetSeqMetadata is a OffsetSeqMetadata . offsetSeqMetadata is used to create an IncrementalExecution in the queryPlanning phase of the MicroBatchExecution and ContinuousExecution execution engines. offsetSeqMetadata is initialized (with 0 for batchWatermarkMs and batchTimestampMs ) when StreamExecution is requested to < >. offsetSeqMetadata is then updated (with the current event-time watermark and timestamp) when MicroBatchExecution is requested to < >. NOTE: MicroBatchExecution uses the < > for the current event-time watermark and the < > for the current batch timestamp. offsetSeqMetadata is stored ( checkpointed ) in < > of MicroBatchExecution (and printed out as INFO message to the logs). offsetSeqMetadata is restored ( re-created ) from a checkpointed state when MicroBatchExecution is requested to < >. offsetSeqMetadata is part of the ProgressReporter abstraction. isActive \u00b6 isActive : Boolean isActive is part of the StreamingQuery abstraction. isActive is enabled ( true ) as long as the State is not TERMINATED . Human-Readable HTML Description of Spark Jobs (for web UI) \u00b6 getBatchDescriptionString : String getBatchDescriptionString is a human-readable description (in HTML format) that uses the optional name if defined, the < >, the < > and batchDescription that can be init (for the < > negative) or the current batch ID itself. getBatchDescriptionString is of the following format: [name] id = [id] runId = [runId] batch = [batchDescription] getBatchDescriptionString is used when: MicroBatchExecution stream execution engine is requested to < > (as the job description of any Spark jobs triggerred as part of query execution) StreamExecution is requested to < > (as the job group description of any Spark jobs triggerred as part of query execution) No New Data Available \u00b6 noNewData : Boolean noNewData is a flag that indicates that a batch has completed with no new data left and processAllAvailable could stop waiting till all streaming data is processed. Default: false Turned on ( true ) when: MicroBatchExecution stream execution engine is requested to < > (while < >) ContinuousExecution stream execution engine is requested to < > Turned off ( false ) when: MicroBatchExecution stream execution engine is requested to < > (right after the < > phase) StreamExecution is requested to < > Current Batch ID \u00b6 -1 when StreamExecution is created 0 when StreamExecution populates start offsets (and OffsetSeqLog is empty, i.e. no offset files in offsets directory in checkpoint) Incremented when StreamExecution runs streaming batches and finishes a trigger that had data available from sources (right after committing the batch ). newData Registry \u00b6 newData : Map [ BaseStreamingSource , LogicalPlan ] Registry of the streaming sources (in the logical query plan ) that have new data available in the current batch. The new data is a streaming DataFrame . newData is part of the ProgressReporter abstraction. Set when StreamExecution is requested to requests unprocessed data from streaming sources (while running a single streaming batch ) Used when StreamExecution is requested to transform the logical plan (of the streaming query) to include the Sources and the MicroBatchReaders with new data (while running a single streaming batch ) Streaming Metrics \u00b6 StreamExecution uses MetricsReporter for reporting streaming metrics. MetricsReporter is created with the following source name (with name if defined or id ): spark.streaming.[name or id] MetricsReporter is registered only when spark.sql.streaming.metricsEnabled configuration property is enabled (when StreamExecution is requested to runStream ). MetricsReporter is deactivated ( removed ) when a streaming query is stopped (when StreamExecution is requested to runStream ). Latches \u00b6 StreamExecution uses java.util.concurrent.CountDownLatch es (with count 1 ). initializationLatch \u00b6 Counted down when requested to runStream : Changes state from INITIALIZING to ACTIVE just before runActivatedStream In runStream's finally block Awaited for tests only (which seems to indicate that it is a test-only latch) startLatch \u00b6 Counted down when requested to runStream : Right after posting a QueryStartedEvent In runStream's finally block Awaited when requested to start (to pause the main thread until StreamExecution was requested to run the streaming query on a separate thread) terminationLatch \u00b6 Counted down at the end of runStream Awaited when requested to awaitTermination (that pauses the thread until the streaming query has finished successfully or not). Locks \u00b6 awaitProgressLock \u00b6 StreamExecution uses a fair reentrant mutual exclusion java.util.concurrent.locks.ReentrantLock (that favors granting access to the longest-waiting thread under contention) __is_continuous_processing Local Property \u00b6 StreamExecution uses __is_continuous_processing local property (default: false ) to differentiate between < > ( true ) and < > ( false ) which is used when StateStoreRDD is requested to compute a partition (and finds a StateStore for a given version). Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.execution.streaming.StreamExecution logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=ALL Refer to Logging .","title":"StreamExecution"},{"location":"StreamExecution/#streamexecution","text":"StreamExecution is an abstraction of stream execution engines ( streaming query processing engines ) that can run a structured query (on a stream execution thread ). Note Continuous query , streaming query , continuous Dataset , streaming Dataset are all considered high-level synonyms for an executable entity that stream execution engines run using the analyzed logical plan internally. Important StreamExecution does not support adaptive query execution and cost-based optimizer (and turns them off when requested to run stream processing ). StreamExecution is the execution environment of a streaming query that is executed every trigger and in the end adds the results to a sink . StreamExecution corresponds to a single streaming query with one or more streaming sources and exactly one streaming sink . import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val q = spark. readStream. format(\"rate\"). load. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.minutes)). start scala> :type q org.apache.spark.sql.streaming.StreamingQuery // Pull out StreamExecution off StreamingQueryWrapper import org.apache.spark.sql.execution.streaming.{StreamExecution, StreamingQueryWrapper} val se = q.asInstanceOf[StreamingQueryWrapper].streamingQuery scala> :type se org.apache.spark.sql.execution.streaming.StreamExecution","title":"StreamExecution"},{"location":"StreamExecution/#contract","text":"","title":"Contract"},{"location":"StreamExecution/#logical-plan","text":"logicalPlan : LogicalPlan Analyzed logical plan of the streaming query to execute Used when StreamExecution is requested to run stream processing logicalPlan is part of the ProgressReporter abstraction.","title":" Logical Plan"},{"location":"StreamExecution/#running-activated-streaming-query","text":"runActivatedStream ( sparkSessionForStream : SparkSession ): Unit Executes ( runs ) the activated streaming query (that is described by the logical plan ) Used when StreamExecution is requested to run the streaming query (when transitioning from INITIALIZING to ACTIVE state)","title":" Running Activated Streaming Query"},{"location":"StreamExecution/#implementations","text":"ContinuousExecution MicroBatchExecution","title":"Implementations"},{"location":"StreamExecution/#creating-instance","text":"StreamExecution takes the following to be created: SparkSession Name of the streaming query (can be null ) Path of the checkpoint directory ( metadata directory ) Streaming query (not used due to logicalPlan ) Table ( Spark SQL ) Trigger Clock OutputMode deleteCheckpointOnStop flag (whether to delete the checkpoint directory on stop) Abstract Class StreamExecution is an abstract class and cannot be created directly. It is created indirectly for the concrete StreamExecutions .","title":"Creating Instance"},{"location":"StreamExecution/#demo","text":"import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val se = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery scala> :type se org.apache.spark.sql.execution.streaming.StreamExecution","title":"Demo"},{"location":"StreamExecution/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"StreamExecution/#sssminbatchestoretain","text":"StreamExecution uses the spark.sql.streaming.minBatchesToRetain configuration property to allow the StreamExecutions to discard old log entries (from the offset and commit logs).","title":" s.s.s.minBatchesToRetain"},{"location":"StreamExecution/#ssspollingdelay","text":"StreamExecution uses spark.sql.streaming.pollingDelay configuration property to control how long to delay polling for new data (when no data was available to process in a batch).","title":" s.s.s.pollingDelay"},{"location":"StreamExecution/#progressreporter","text":"StreamExecution is a ProgressReporter and reports status of the streaming query (when it starts, progresses and terminates) by posting StreamingQueryListener events. import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .text(\"server-logs\") .writeStream .format(\"console\") .queryName(\"debug\") .trigger(Trigger.ProcessingTime(20.seconds)) .start // Enable the log level to see the INFO and DEBUG messages // log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=DEBUG 17/06/18 21:21:07 INFO StreamExecution: Starting new streaming query. 17/06/18 21:21:07 DEBUG StreamExecution: getOffset took 5 ms 17/06/18 21:21:07 DEBUG StreamExecution: Stream running from {} to {} 17/06/18 21:21:07 DEBUG StreamExecution: triggerExecution took 9 ms 17/06/18 21:21:07 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map()) 17/06/18 21:21:07 INFO StreamExecution: Streaming query made progress: { \"id\" : \"8b57b0bd-fc4a-42eb-81a3-777d7ba5e370\", \"runId\" : \"920b227e-6d02-4a03-a271-c62120258cea\", \"name\" : \"debug\", \"timestamp\" : \"2017-06-18T19:21:07.693Z\", \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0, \"durationMs\" : { \"getOffset\" : 5, \"triggerExecution\" : 9 }, \"stateOperators\" : [ ], \"sources\" : [ { \"description\" : \"FileStreamSource[file:/Users/jacek/dev/oss/spark/server-logs]\", \"startOffset\" : null, \"endOffset\" : null, \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0 } ], \"sink\" : { \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@2460208a\" } } 17/06/18 21:21:10 DEBUG StreamExecution: Starting Trigger Calculation 17/06/18 21:21:10 DEBUG StreamExecution: getOffset took 3 ms 17/06/18 21:21:10 DEBUG StreamExecution: triggerExecution took 3 ms 17/06/18 21:21:10 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map())","title":"ProgressReporter"},{"location":"StreamExecution/#unique-streaming-sources","text":"StreamExecution tracks unique streaming data sources in uniqueSources internal registry. Used when StreamExecution : Constructs the next streaming micro-batch (and gets new offsets for every streaming data source) Stops all streaming data sources","title":" Unique Streaming Sources"},{"location":"StreamExecution/#streaming-query-identifiers","text":"The name , id and runId are all unique across all active queries (in a StreamingQueryManager ). The difference is that: name is optional and user-defined id is a UUID that is auto-generated at the time StreamExecution is created and persisted to metadata checkpoint file runId is a UUID that is auto-generated every time StreamExecution is created","title":"Streaming Query Identifiers"},{"location":"StreamExecution/#id","text":"StreamExecution is uniquely identified by an ID of the streaming query (which is the id of the StreamMetadata ). Since the StreamMetadata is persisted (to the metadata file in the checkpoint directory ), the streaming query ID \"survives\" query restarts as long as the checkpoint directory is preserved.","title":" Id"},{"location":"StreamExecution/#run-id","text":"StreamExecution is uniquely identified by a run ID of the streaming query . A run ID is a randomly-generated 128-bit universally unique identifier (UUID) that is assigned at the time StreamExecution is created. runId does not \"survive\" query restarts and will always be different yet unique (across all active queries).","title":" Run Id"},{"location":"StreamExecution/#streammetadata","text":"StreamExecution uses a StreamMetadata that is persisted in the metadata file in the checkpoint directory . If the metadata file is available it is read and is the way to recover the id of a streaming query when resumed (i.e. restarted after a failure or a planned stop).","title":" StreamMetadata"},{"location":"StreamExecution/#metadata-logs","text":"","title":"Metadata Logs"},{"location":"StreamExecution/#write-ahead-offset-log","text":"offsetLog : OffsetSeqLog offsetLog is a Hadoop DFS-based metadata storage (of OffsetSeq s) with offsets metadata directory . offsetLog is used as a Write-Ahead Log of Offsets to persist offsets of the data about to be processed in every trigger. Tip Monitor offsets and commits metadata logs to know the progress of a streaming query. The number of entries in the OffsetSeqLog is controlled using spark.sql.streaming.minBatchesToRetain configuration property. offsetLog is used when: ContinuousExecution stream execution engine is requested to commit an epoch , getStartOffsets , and addOffset MicroBatchExecution stream execution engine is requested to populate start offsets and construct (or skip) the next streaming micro-batch","title":" Write-Ahead Offset Log"},{"location":"StreamExecution/#offset-commit-log","text":"StreamExecution uses offset commit log ( CommitLog with commits metadata checkpoint directory ) for streaming batches successfully executed (with a single file per batch with a file name being the batch id) or committed epochs. Note Metadata log or metadata checkpoint are synonyms and are often used interchangeably. commitLog is used by the < > for the following: MicroBatchExecution is requested to < > (that in turn requests to < > at the very beginning of the streaming query execution and later regularly every < >) ContinuousExecution is requested to < > (that in turn requests to < > at the very beginning of the streaming query execution and later regularly every < >)","title":" Offset Commit Log"},{"location":"StreamExecution/#state-of-streaming-query","text":"state : AtomicReference [ State ] state indicates the internal state of execution of the streaming query (as java.util.concurrent.atomic.AtomicReference ).","title":" State of Streaming Query"},{"location":"StreamExecution/#active","text":"StreamExecution has been requested to < > (and is about to < >)","title":" ACTIVE"},{"location":"StreamExecution/#initializing","text":"StreamExecution has been created .","title":" INITIALIZING"},{"location":"StreamExecution/#terminated","text":"Indicates that: MicroBatchExecution has been requested to stop ContinuousExecution has been requested to stop StreamExecution has been requested to run stream processing (and has finished running the activated streaming query )","title":" TERMINATED"},{"location":"StreamExecution/#reconfiguring","text":"Used when ContinuousExecution is requested to run a streaming query in continuous mode (and the ContinuousReader indicated a need for reconfiguration )","title":" RECONFIGURING"},{"location":"StreamExecution/#creating-streamingwrite","text":"createStreamingWrite ( table : SupportsWrite , options : Map [ String , String ], inputPlan : LogicalPlan ): StreamingWrite createStreamingWrite creates a LogicalWriteInfoImpl (with the query ID , the schema of the input LogicalPlan and the given options). createStreamingWrite requests the given SupportsWrite table for a WriteBuilder (for the LogicalWriteInfoImpl ). Tip Learn more about SupportsWrite and WriteBuilder in The Internals of Spark SQL online book. createStreamingWrite branches based on the OutputMode : For Append output mode, createStreamingWrite requests the WriteBuilder to build a StreamingWrite . For Complete output mode, createStreamingWrite assumes that the WriteBuilder is a SupportsTruncate and requests it to truncate followed by buildForStreaming For Update output mode, createStreamingWrite assumes that the WriteBuilder is a SupportsStreamingUpdate and requests it to update followed by buildForStreaming Tip Learn more about SupportsTruncate and SupportsStreamingUpdate in The Internals of Spark SQL online book. createStreamingWrite is used when MicroBatchExecution and ContinuousExecution stream execution engines are requested for analyzed logical plans.","title":" Creating StreamingWrite"},{"location":"StreamExecution/#available-offsets-streamprogress","text":"availableOffsets : StreamProgress availableOffsets is a registry of offsets per streaming source to track what data (by offset ) is available for processing for every streaming source in the streaming query (and have not yet been committed ). availableOffsets works in tandem with the committedOffsets internal registry. availableOffsets is empty when StreamExecution is created (i.e. no offsets are reported for any streaming source in the streaming query). availableOffsets is used when: MicroBatchExecution stream execution engine is requested to < >, < >, < > and < > ContinuousExecution stream execution engine is requested to commit an epoch StreamExecution is requested for the internal string representation","title":" Available Offsets (StreamProgress)"},{"location":"StreamExecution/#committed-offsets-streamprogress","text":"committedOffsets : StreamProgress committedOffsets is a registry of offsets per streaming source to track what data (by offset ) has already been processed and committed (to the sink or state stores) for every streaming source in the streaming query . committedOffsets works in tandem with the availableOffsets internal registry. committedOffsets is used when: MicroBatchExecution stream execution engine is requested for the < >, to < > and < > ContinuousExecution stream execution engine is requested for the < > and to < > StreamExecution is requested for the internal string representation","title":" Committed Offsets (StreamProgress)"},{"location":"StreamExecution/#fully-qualified-resolved-path-to-checkpoint-root-directory","text":"resolvedCheckpointRoot : String resolvedCheckpointRoot is a fully-qualified path of the given checkpoint root directory . The given checkpoint root directory is defined using checkpointLocation option or the spark.sql.streaming.checkpointLocation configuration property with queryName option. checkpointLocation and queryName options are defined when StreamingQueryManager is requested to create a streaming query . resolvedCheckpointRoot is used when creating the path to the checkpoint directory and when StreamExecution finishes running streaming batches . resolvedCheckpointRoot is used for the logicalPlan (while transforming analyzedPlan and planning StreamingRelation logical operators to corresponding StreamingExecutionRelation physical operators with the streaming data sources created passing in the path to sources directory to store checkpointing metadata). resolvedCheckpointRoot is printed out immediately when resolved as a INFO message to the logs: Checkpoint root [checkpointRoot] resolved to [resolvedCheckpointRoot]. resolvedCheckpointRoot is printed out again as a INFO message to the logs when StreamExecution is started : Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint.","title":" Fully-Qualified (Resolved) Path to Checkpoint Root Directory"},{"location":"StreamExecution/#streamwritercommitprogress","text":"sinkCommitProgress : Option [ StreamWriterCommitProgress ] sinkCommitProgress is part of the ProgressReporter abstraction. StreamExecution initializes sinkCommitProgress registry to be None when created .","title":" StreamWriterCommitProgress"},{"location":"StreamExecution/#last-query-execution-of-streaming-query-incrementalexecution","text":"lastExecution : IncrementalExecution lastExecution is part of the ProgressReporter abstraction. lastExecution is a IncrementalExecution (a QueryExecution of a streaming query) of the most recent ( last ) execution. lastExecution is created when the < > are requested for the following: MicroBatchExecution is requested to < > (when in < >) ContinuousExecution stream execution engine is requested to < > (when in < >) lastExecution is used when: StreamExecution is requested to < > (via < >) ProgressReporter is requested to extractStateOperatorMetrics , extractExecutionStats , and extractSourceToNumInputRows MicroBatchExecution stream execution engine is requested to < > (based on StateStoreWriters in a streaming query ), < > (when in < > and < >) ContinuousExecution stream execution engine is requested to < > (when in < >) For debugging query execution of streaming queries (using debugCodegen )","title":" Last Query Execution Of Streaming Query (IncrementalExecution)"},{"location":"StreamExecution/#explaining-streaming-query","text":"explain (): Unit // <1> explain ( extended : Boolean ): Unit <1> Turns the extended flag off ( false ) explain simply prints out < > to the standard output.","title":" Explaining Streaming Query"},{"location":"StreamExecution/#stopping-streaming-sources-and-readers","text":"stopSources (): Unit stopSources requests every streaming source to stop. In case of an non-fatal exception, stopSources prints out the following WARN message to the logs: Failed to stop streaming source: [source]. Resources may have leaked. stopSources is used when: StreamExecution is requested to < > (and < > successfully or not) ContinuousExecution is requested to < > (and terminates)","title":" Stopping Streaming Sources and Readers"},{"location":"StreamExecution/#running-stream-processing","text":"runStream (): Unit runStream simply prepares the environment to execute the activated streaming query . runStream is used when the stream execution thread is requested to start (when DataStreamWriter is requested to start an execution of the streaming query ). Internally, runStream sets the job group (to all the Spark jobs started by this thread) as follows: runId for the job group ID getBatchDescriptionString for the job group description (to display in web UI) interruptOnCancel flag on Note runStream uses the SparkSession to access SparkContext and assign the job group id. Learn more about SparkContext.setJobGroup method in The Internals of Apache Spark online book. runStream sets sql.streaming.queryId local property to id . runStream requests the MetricsSystem to register the MetricsReporter when spark.sql.streaming.metricsEnabled configuration property is enabled. runStream notifies StreamingQueryListeners that the streaming query has been started (by posting a new QueryStartedEvent event with id , runId , and name ). runStream unblocks the main starting thread (by decrementing the count of the startLatch that when 0 lets the starting thread continue). runStream updates the status message to be Initializing sources . runStream initializes the analyzed logical plan . Lazy Value The analyzed logical plan is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards. runStream disables adaptive query execution and cost-based join optimization (by turning spark.sql.adaptive.enabled and spark.sql.cbo.enabled configuration properties off, respectively). runStream creates a new \"zero\" OffsetSeqMetadata . (when in INITIALIZING state) runStream enters ACTIVE state: Decrements the count of initializationLatch Executes the activated streaming query Note runBatches does the main work only when first started (when in INITIALIZING state). runStream ...FIXME (describe the failed and stop states) Once TriggerExecutor has finished executing batches, runBatches updates the status message to Stopped . NOTE: TriggerExecutor finishes executing batches when the batch runner returns whether the streaming query is stopped or not (while active ).","title":" Running Stream Processing"},{"location":"StreamExecution/#finally-block","text":"runStream releases the startLatch and initializationLatch latches. runStream stopSources . runStream enters TERMINATED state. runStream sets the StreamingQueryStatus with the isTriggerActive and isDataAvailable flags off ( false ). runStream removes the stream metrics reporter from the application's MetricsSystem . runStream requests the StreamingQueryManager to handle termination of a streaming query . runStream creates a new QueryTerminatedEvent (with the id and run id of the streaming query) and posts it . With the deleteCheckpointOnStop flag enabled and no StreamingQueryException , runStream deletes the checkpoint directory . In the end, runStream releases the terminationLatch latch.","title":" finally Block"},{"location":"StreamExecution/#triggerexecutors-batch-runner","text":"Batch Runner ( batchRunner ) is an executable block executed by TriggerExecutor in runBatches . batchRunner starts trigger calculation . As long as the query is not stopped (i.e. state is not TERMINATED ), batchRunner executes the streaming batch for the trigger. In triggerExecution time-tracking section , runBatches branches off per currentBatchId : For currentBatchId < 0 : populateStartOffsets Setting Job Description as getBatchDescriptionString Stream running from [committedOffsets] to [availableOffsets] For currentBatchId >= 0 : Constructing the next streaming micro-batch If there is data available in the sources, batchRunner marks currentStatus with isDataAvailable enabled. Tip You can check out the status of a streaming query using status method. scala> spark.streams.active(0).status res1: org.apache.spark.sql.streaming.StreamingQueryStatus = { \"message\" : \"Waiting for next trigger\", \"isDataAvailable\" : false, \"isTriggerActive\" : false } batchRunner then updates the status message to Processing new data and runs the current streaming batch . After triggerExecution section has finished, batchRunner finishes the streaming batch for the trigger (and collects query execution statistics). When there was < > in the sources, batchRunner updates committed offsets (by adding the < > to BatchCommitLog and adding availableOffsets to committedOffsets ). batchRunner prints out the following DEBUG message to the logs: batch [currentBatchId] committed batchRunner increments the current batch id and sets the job description for all the following Spark jobs to include the new batch id . When no data was available in the sources to process, batchRunner does the following: Marks currentStatus with isDataAvailable disabled Updates the status message to Waiting for data to arrive Sleeps the current thread for pollingDelayMs milliseconds. batchRunner updates the status message to Waiting for next trigger and returns whether the query is currently active or not (so TriggerExecutor can decide whether to finish executing the batches or not)","title":" TriggerExecutor's Batch Runner"},{"location":"StreamExecution/#starting-streaming-query-on-stream-execution-thread","text":"start (): Unit start starts a stream execution thread that simply runs stream processing (and hence the streaming query). start prints out the following INFO message to the logs: Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint. start then starts the < > (as a daemon thread). NOTE: start uses Java's ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#start--++[java.lang.Thread.start ] to run the streaming query on a separate execution thread. NOTE: When started, a streaming query runs in its own execution thread on JVM. In the end, start pauses the main thread (using the < > until StreamExecution is requested to < > that in turn sends a QueryStartedEvent to all streaming listeners followed by decrementing the count of the < >). start is used when StreamingQueryManager is requested to start a streaming query (when DataStreamWriter is requested to start an execution of the streaming query ).","title":" Starting Streaming Query (on Stream Execution Thread)"},{"location":"StreamExecution/#path-to-checkpoint-directory","text":"checkpointFile ( name : String ): String checkpointFile gives the path of a directory with name in checkpoint directory . checkpointFile is used for streamMetadata , OffsetSeqLog , BatchCommitLog , and lastExecution (for runBatch ).","title":" Path to Checkpoint Directory"},{"location":"StreamExecution/#posting-streamingquerylistener-event","text":"postEvent ( event : StreamingQueryListener . Event ): Unit postEvent is a part of the ProgressReporter abstraction. postEvent simply requests the StreamingQueryManager to post the input event (to the StreamingQueryListenerBus in the current SparkSession ). Note postEvent uses SparkSession to access the current StreamingQueryManager . postEvent is used when: ProgressReporter is requested to report update progress (while finishing a trigger ) StreamExecution runs streaming batches (and announces starting a streaming query by posting a QueryStartedEvent and query termination by posting a QueryTerminatedEvent )","title":" Posting StreamingQueryListener Event"},{"location":"StreamExecution/#waiting-until-no-new-data-available-in-sources-or-query-has-been-terminated","text":"processAllAvailable (): Unit processAllAvailable is a part of the StreamingQuery abstraction. processAllAvailable reports the < > if reported (and returns immediately). NOTE: < > is reported exclusively when StreamExecution is requested to < > (that terminated with an exception). processAllAvailable returns immediately when StreamExecution is no longer < > (in TERMINATED state). processAllAvailable acquires a lock on the < > and turns the < > internal flag off ( false ). processAllAvailable keeps polling with 10-second pauses (locked on < >) until < > flag is turned on ( true ) or StreamExecution is no longer < > (in TERMINATED state). NOTE: The 10-second pause is hardcoded and cannot be changed. In the end, processAllAvailable releases < > lock. processAllAvailable throws an IllegalStateException when executed on the < >: Cannot wait for a query state from the same thread that is running the query","title":" Waiting Until No New Data Available in Sources or Query Has Been Terminated"},{"location":"StreamExecution/#stream-execution-thread","text":"queryExecutionThread : QueryExecutionThread queryExecutionThread is a Java thread of execution ( java.util.Thread ) that runs a streaming query . queryExecutionThread is started (as a daemon thread) when StreamExecution is requested to < >. At that time, start prints out the following INFO message to the logs (with the < > and the < >): Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint. When started, queryExecutionThread sets the < > and < >. queryExecutionThread uses the name stream execution thread for [id] (that uses < > for the id, i.e. queryName [id = [id], runId = [runId]] ). queryExecutionThread is a QueryExecutionThread that is a custom UninterruptibleThread from Apache Spark with runUninterruptibly method for running a block of code without being interrupted by Thread.interrupt() . Tip Use Java's jconsole or jstack to monitor stream execution threads. $ jstack <driver-pid> | grep -e \"stream execution thread\" \"stream execution thread for kafka-topic1 [id =...","title":" Stream Execution Thread"},{"location":"StreamExecution/#current-batch-metadata-event-time-watermark-and-timestamp","text":"offsetSeqMetadata : OffsetSeqMetadata offsetSeqMetadata is a OffsetSeqMetadata . offsetSeqMetadata is used to create an IncrementalExecution in the queryPlanning phase of the MicroBatchExecution and ContinuousExecution execution engines. offsetSeqMetadata is initialized (with 0 for batchWatermarkMs and batchTimestampMs ) when StreamExecution is requested to < >. offsetSeqMetadata is then updated (with the current event-time watermark and timestamp) when MicroBatchExecution is requested to < >. NOTE: MicroBatchExecution uses the < > for the current event-time watermark and the < > for the current batch timestamp. offsetSeqMetadata is stored ( checkpointed ) in < > of MicroBatchExecution (and printed out as INFO message to the logs). offsetSeqMetadata is restored ( re-created ) from a checkpointed state when MicroBatchExecution is requested to < >. offsetSeqMetadata is part of the ProgressReporter abstraction.","title":" Current Batch Metadata (Event-Time Watermark and Timestamp)"},{"location":"StreamExecution/#isactive","text":"isActive : Boolean isActive is part of the StreamingQuery abstraction. isActive is enabled ( true ) as long as the State is not TERMINATED .","title":" isActive"},{"location":"StreamExecution/#human-readable-html-description-of-spark-jobs-for-web-ui","text":"getBatchDescriptionString : String getBatchDescriptionString is a human-readable description (in HTML format) that uses the optional name if defined, the < >, the < > and batchDescription that can be init (for the < > negative) or the current batch ID itself. getBatchDescriptionString is of the following format: [name] id = [id] runId = [runId] batch = [batchDescription] getBatchDescriptionString is used when: MicroBatchExecution stream execution engine is requested to < > (as the job description of any Spark jobs triggerred as part of query execution) StreamExecution is requested to < > (as the job group description of any Spark jobs triggerred as part of query execution)","title":" Human-Readable HTML Description of Spark Jobs (for web UI)"},{"location":"StreamExecution/#no-new-data-available","text":"noNewData : Boolean noNewData is a flag that indicates that a batch has completed with no new data left and processAllAvailable could stop waiting till all streaming data is processed. Default: false Turned on ( true ) when: MicroBatchExecution stream execution engine is requested to < > (while < >) ContinuousExecution stream execution engine is requested to < > Turned off ( false ) when: MicroBatchExecution stream execution engine is requested to < > (right after the < > phase) StreamExecution is requested to < >","title":" No New Data Available"},{"location":"StreamExecution/#current-batch-id","text":"-1 when StreamExecution is created 0 when StreamExecution populates start offsets (and OffsetSeqLog is empty, i.e. no offset files in offsets directory in checkpoint) Incremented when StreamExecution runs streaming batches and finishes a trigger that had data available from sources (right after committing the batch ).","title":" Current Batch ID"},{"location":"StreamExecution/#newdata-registry","text":"newData : Map [ BaseStreamingSource , LogicalPlan ] Registry of the streaming sources (in the logical query plan ) that have new data available in the current batch. The new data is a streaming DataFrame . newData is part of the ProgressReporter abstraction. Set when StreamExecution is requested to requests unprocessed data from streaming sources (while running a single streaming batch ) Used when StreamExecution is requested to transform the logical plan (of the streaming query) to include the Sources and the MicroBatchReaders with new data (while running a single streaming batch )","title":" newData Registry"},{"location":"StreamExecution/#streaming-metrics","text":"StreamExecution uses MetricsReporter for reporting streaming metrics. MetricsReporter is created with the following source name (with name if defined or id ): spark.streaming.[name or id] MetricsReporter is registered only when spark.sql.streaming.metricsEnabled configuration property is enabled (when StreamExecution is requested to runStream ). MetricsReporter is deactivated ( removed ) when a streaming query is stopped (when StreamExecution is requested to runStream ).","title":" Streaming Metrics"},{"location":"StreamExecution/#latches","text":"StreamExecution uses java.util.concurrent.CountDownLatch es (with count 1 ).","title":"Latches"},{"location":"StreamExecution/#initializationlatch","text":"Counted down when requested to runStream : Changes state from INITIALIZING to ACTIVE just before runActivatedStream In runStream's finally block Awaited for tests only (which seems to indicate that it is a test-only latch)","title":" initializationLatch"},{"location":"StreamExecution/#startlatch","text":"Counted down when requested to runStream : Right after posting a QueryStartedEvent In runStream's finally block Awaited when requested to start (to pause the main thread until StreamExecution was requested to run the streaming query on a separate thread)","title":" startLatch"},{"location":"StreamExecution/#terminationlatch","text":"Counted down at the end of runStream Awaited when requested to awaitTermination (that pauses the thread until the streaming query has finished successfully or not).","title":" terminationLatch"},{"location":"StreamExecution/#locks","text":"","title":"Locks"},{"location":"StreamExecution/#awaitprogresslock","text":"StreamExecution uses a fair reentrant mutual exclusion java.util.concurrent.locks.ReentrantLock (that favors granting access to the longest-waiting thread under contention)","title":" awaitProgressLock"},{"location":"StreamExecution/#__is_continuous_processing-local-property","text":"StreamExecution uses __is_continuous_processing local property (default: false ) to differentiate between < > ( true ) and < > ( false ) which is used when StateStoreRDD is requested to compute a partition (and finds a StateStore for a given version).","title":" __is_continuous_processing Local Property"},{"location":"StreamExecution/#logging","text":"Enable ALL logging level for org.apache.spark.sql.execution.streaming.StreamExecution logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=ALL Refer to Logging .","title":"Logging"},{"location":"StreamMetadata/","text":"StreamMetadata \u00b6 StreamMetadata is a metadata associated with a < > (indirectly through StreamExecution ). [[creating-instance]] [[id]] StreamMetadata takes an ID to be created. StreamMetadata is < > exclusively when StreamExecution is created (with a randomly-generated 128-bit universally unique identifier (UUID)). StreamMetadata can be < > to and < > from a JSON file. StreamMetadata uses http://json4s.org/[json4s-jackson ] library for JSON persistence. import org.apache.spark.sql.execution.streaming.StreamMetadata import org.apache.hadoop.fs.Path val metadataPath = new Path(\"metadata\") scala> :type spark org.apache.spark.sql.SparkSession val hadoopConf = spark.sessionState.newHadoopConf() val sm = StreamMetadata.read(metadataPath, hadoopConf) scala> :type sm Option[org.apache.spark.sql.execution.streaming.StreamMetadata] === [[read]] Unpersisting StreamMetadata (from JSON File) -- read Object Method [source, scala] \u00b6 read( metadataFile: Path, hadoopConf: Configuration): Option[StreamMetadata] read unpersists StreamMetadata from the given metadataFile file if available. read returns a StreamMetadata if the metadata file was available and the content could be read in JSON format. Otherwise, read returns None . NOTE: read uses org.json4s.jackson.Serialization.read for JSON deserialization. NOTE: read is used exclusively when StreamExecution is created> (and tries to read the metadata checkpoint file). === [[write]] Persisting Metadata -- write Object Method [source, scala] \u00b6 write( metadata: StreamMetadata, metadataFile: Path, hadoopConf: Configuration): Unit write persists the given StreamMetadata to the given metadataFile file in JSON format. NOTE: write uses org.json4s.jackson.Serialization.write for JSON serialization. write is used when StreamExecution is created (and the metadata checkpoint file is not available).","title":"StreamMetadata"},{"location":"StreamMetadata/#streammetadata","text":"StreamMetadata is a metadata associated with a < > (indirectly through StreamExecution ). [[creating-instance]] [[id]] StreamMetadata takes an ID to be created. StreamMetadata is < > exclusively when StreamExecution is created (with a randomly-generated 128-bit universally unique identifier (UUID)). StreamMetadata can be < > to and < > from a JSON file. StreamMetadata uses http://json4s.org/[json4s-jackson ] library for JSON persistence. import org.apache.spark.sql.execution.streaming.StreamMetadata import org.apache.hadoop.fs.Path val metadataPath = new Path(\"metadata\") scala> :type spark org.apache.spark.sql.SparkSession val hadoopConf = spark.sessionState.newHadoopConf() val sm = StreamMetadata.read(metadataPath, hadoopConf) scala> :type sm Option[org.apache.spark.sql.execution.streaming.StreamMetadata] === [[read]] Unpersisting StreamMetadata (from JSON File) -- read Object Method","title":"StreamMetadata"},{"location":"StreamMetadata/#source-scala","text":"read( metadataFile: Path, hadoopConf: Configuration): Option[StreamMetadata] read unpersists StreamMetadata from the given metadataFile file if available. read returns a StreamMetadata if the metadata file was available and the content could be read in JSON format. Otherwise, read returns None . NOTE: read uses org.json4s.jackson.Serialization.read for JSON deserialization. NOTE: read is used exclusively when StreamExecution is created> (and tries to read the metadata checkpoint file). === [[write]] Persisting Metadata -- write Object Method","title":"[source, scala]"},{"location":"StreamMetadata/#source-scala_1","text":"write( metadata: StreamMetadata, metadataFile: Path, hadoopConf: Configuration): Unit write persists the given StreamMetadata to the given metadataFile file in JSON format. NOTE: write uses org.json4s.jackson.Serialization.write for JSON serialization. write is used when StreamExecution is created (and the metadata checkpoint file is not available).","title":"[source, scala]"},{"location":"StreamProgress/","text":"StreamProgress \u2014 Collection of Offsets per Streaming Source \u00b6 StreamProgress is a collection of Offset s per streaming source. StreamProgress is < > when: StreamExecution is created (and creates committed and available offsets) OffsetSeq is requested to convert to StreamProgress StreamProgress is an extension of Scala's scala.collection.immutable.Map with streaming sources as keys and their Offset s as values. Creating Instance \u00b6 StreamProgress takes the following to be created: [[baseMap]] Offset s per streaming source ( Map[BaseStreamingSource, Offset] ) (default: empty) === [[get]] Looking Up Offset by Streaming Source -- get Method [source, scala] \u00b6 get(key: BaseStreamingSource): Option[Offset] \u00b6 NOTE: get is part of the Scala's scala.collection.MapLike to...FIXME. get simply looks up an Offset s for the given streaming source in the < >. === [[plusplus]] ++ Method [source, scala] \u00b6 ++( updates: GenTraversableOnce[(BaseStreamingSource, Offset)]): StreamProgress ++ simply creates a new < > with the < > and the given updates. ++ is used exclusively when OffsetSeq is requested to convert to StreamProgress . === [[toOffsetSeq]] Converting to OffsetSeq -- toOffsetSeq Method [source, scala] \u00b6 toOffsetSeq( sources: Seq[BaseStreamingSource], metadata: OffsetSeqMetadata): OffsetSeq toOffsetSeq creates a OffsetSeq with offsets that are < > for every streaming source. toOffsetSeq is used when: MicroBatchExecution stream execution engine is requested to construct the next streaming micro-batch (to commit available offsets for a batch to the write-ahead log ) StreamExecution is requested to run stream processing (that failed with a Throwable )","title":"StreamProgress"},{"location":"StreamProgress/#streamprogress-collection-of-offsets-per-streaming-source","text":"StreamProgress is a collection of Offset s per streaming source. StreamProgress is < > when: StreamExecution is created (and creates committed and available offsets) OffsetSeq is requested to convert to StreamProgress StreamProgress is an extension of Scala's scala.collection.immutable.Map with streaming sources as keys and their Offset s as values.","title":"StreamProgress &mdash; Collection of Offsets per Streaming Source"},{"location":"StreamProgress/#creating-instance","text":"StreamProgress takes the following to be created: [[baseMap]] Offset s per streaming source ( Map[BaseStreamingSource, Offset] ) (default: empty) === [[get]] Looking Up Offset by Streaming Source -- get Method","title":"Creating Instance"},{"location":"StreamProgress/#source-scala","text":"","title":"[source, scala]"},{"location":"StreamProgress/#getkey-basestreamingsource-optionoffset","text":"NOTE: get is part of the Scala's scala.collection.MapLike to...FIXME. get simply looks up an Offset s for the given streaming source in the < >. === [[plusplus]] ++ Method","title":"get(key: BaseStreamingSource): Option[Offset]"},{"location":"StreamProgress/#source-scala_1","text":"++( updates: GenTraversableOnce[(BaseStreamingSource, Offset)]): StreamProgress ++ simply creates a new < > with the < > and the given updates. ++ is used exclusively when OffsetSeq is requested to convert to StreamProgress . === [[toOffsetSeq]] Converting to OffsetSeq -- toOffsetSeq Method","title":"[source, scala]"},{"location":"StreamProgress/#source-scala_2","text":"toOffsetSeq( sources: Seq[BaseStreamingSource], metadata: OffsetSeqMetadata): OffsetSeq toOffsetSeq creates a OffsetSeq with offsets that are < > for every streaming source. toOffsetSeq is used when: MicroBatchExecution stream execution engine is requested to construct the next streaming micro-batch (to commit available offsets for a batch to the write-ahead log ) StreamExecution is requested to run stream processing (that failed with a Throwable )","title":"[source, scala]"},{"location":"StreamSinkProvider/","text":"StreamSinkProvider \u00b6 StreamSinkProvider is the < > of < > that can < > for a file format (e.g. parquet ) or system (e.g. kafka ). [[contract]] .StreamSinkProvider Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | createSink a| [[createSink]] [source, scala] \u00b6 createSink( sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode): Sink Creates a streaming sink Used when DataSource is requested for a streaming sink (when DataStreamWriter is requested to start a streaming query ) |=== [[implementations]] NOTE: KafkaSourceProvider is the only known StreamSinkProvider in Spark Structured Streaming.","title":"StreamSinkProvider"},{"location":"StreamSinkProvider/#streamsinkprovider","text":"StreamSinkProvider is the < > of < > that can < > for a file format (e.g. parquet ) or system (e.g. kafka ). [[contract]] .StreamSinkProvider Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | createSink a| [[createSink]]","title":"StreamSinkProvider"},{"location":"StreamSinkProvider/#source-scala","text":"createSink( sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode): Sink Creates a streaming sink Used when DataSource is requested for a streaming sink (when DataStreamWriter is requested to start a streaming query ) |=== [[implementations]] NOTE: KafkaSourceProvider is the only known StreamSinkProvider in Spark Structured Streaming.","title":"[source, scala]"},{"location":"StreamSourceProvider/","text":"StreamSourceProvider \u00b6 StreamSourceProvider is an abstraction of data source providers that can create a streaming source for a data format or system. StreamSourceProvider is part of Data Source API V1 for Micro-Batch Stream Processing . Contract \u00b6 Creating Streaming Source \u00b6 createSource ( sqlContext : SQLContext , metadataPath : String , schema : Option [ StructType ], providerName : String , parameters : Map [ String , String ]): Source Creates a Streaming Source metadataPath is the value of the optional user-specified checkpointLocation option or resolved by StreamingQueryManager . Used when: DataSource is requested to create a streaming source (when MicroBatchExecution is requested to initialize the analyzed logical plan ) Source Schema \u00b6 sourceSchema ( sqlContext : SQLContext , schema : Option [ StructType ], providerName : String , parameters : Map [ String , String ]): ( String , StructType ) Name and schema of the Streaming Source Used when: DataSource is requested for metadata of a streaming source (when MicroBatchExecution is requested to initialize the analyzed logical plan ) Implementations \u00b6 KafkaSourceProvider","title":"StreamSourceProvider"},{"location":"StreamSourceProvider/#streamsourceprovider","text":"StreamSourceProvider is an abstraction of data source providers that can create a streaming source for a data format or system. StreamSourceProvider is part of Data Source API V1 for Micro-Batch Stream Processing .","title":"StreamSourceProvider"},{"location":"StreamSourceProvider/#contract","text":"","title":"Contract"},{"location":"StreamSourceProvider/#creating-streaming-source","text":"createSource ( sqlContext : SQLContext , metadataPath : String , schema : Option [ StructType ], providerName : String , parameters : Map [ String , String ]): Source Creates a Streaming Source metadataPath is the value of the optional user-specified checkpointLocation option or resolved by StreamingQueryManager . Used when: DataSource is requested to create a streaming source (when MicroBatchExecution is requested to initialize the analyzed logical plan )","title":" Creating Streaming Source"},{"location":"StreamSourceProvider/#source-schema","text":"sourceSchema ( sqlContext : SQLContext , schema : Option [ StructType ], providerName : String , parameters : Map [ String , String ]): ( String , StructType ) Name and schema of the Streaming Source Used when: DataSource is requested for metadata of a streaming source (when MicroBatchExecution is requested to initialize the analyzed logical plan )","title":" Source Schema"},{"location":"StreamSourceProvider/#implementations","text":"KafkaSourceProvider","title":"Implementations"},{"location":"StreamingAggregationStateManager/","text":"StreamingAggregationStateManager \u00b6 StreamingAggregationStateManager is the < > of < > that act as middlemen between state stores and the physical operators used in Streaming Aggregation (e.g. StateStoreSaveExec and StateStoreRestoreExec ). [[contract]] .StreamingAggregationStateManager Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | commit a| [[commit]] [source, scala] \u00b6 commit( store: StateStore): Long Commits all updates ( changes ) to the given StateStore and returns the new version Used when StateStoreSaveExec physical operator is executed. | get a| [[get]] [source, scala] \u00b6 get(store: StateStore, key: UnsafeRow): UnsafeRow \u00b6 Looks up the value of the key from the StateStore (the key is non- null ) Used exclusively when StateStoreRestoreExec physical operator is executed. | getKey a| [[getKey]] [source, scala] \u00b6 getKey(row: UnsafeRow): UnsafeRow \u00b6 Extracts the columns for the key from the input row Used when: StateStoreRestoreExec physical operator is executed StreamingAggregationStateManagerImplV1 legacy state manager is requested to put a row to a state store | getStateValueSchema a| [[getStateValueSchema]] [source, scala] \u00b6 getStateValueSchema: StructType \u00b6 Gets the schema of the values in StateStore s Used when StateStoreRestoreExec and StateStoreSaveExec physical operators are executed | iterator a| [[iterator]] [source, scala] \u00b6 iterator( store: StateStore): Iterator[UnsafeRowPair] Returns all UnsafeRow key-value pairs in the given StateStore Used exclusively when StateStoreSaveExec physical operator is executed. | keys a| [[keys]] [source, scala] \u00b6 keys(store: StateStore): Iterator[UnsafeRow] \u00b6 Returns all the keys in the given StateStore Used exclusively when physical operators with WatermarkSupport are requested to removeKeysOlderThanWatermark (when StateStoreSaveExec physical operator is executed). | put a| [[put]] [source, scala] \u00b6 put( store: StateStore, row: UnsafeRow): Unit Stores ( puts ) the given row in the given StateStore Used exclusively when StateStoreSaveExec physical operator is executed. | remove a| [[remove]] [source, scala] \u00b6 remove( store: StateStore, key: UnsafeRow): Unit Removes the key-value pair from the given StateStore per key Used exclusively when StateStoreSaveExec physical operator is executed (directly or indirectly as a WatermarkSupport ) | values a| [[values]] [source, scala] \u00b6 values( store: StateStore): Iterator[UnsafeRow] All values in the given StateStore Used exclusively when StateStoreSaveExec physical operator is executed. |=== [[supportedVersions]] StreamingAggregationStateManager supports < > (per the spark.sql.streaming.aggregation.stateFormatVersion internal configuration property): [[legacyVersion]] 1 (for the legacy StreamingAggregationStateManagerImplV1 ) [[default]] 2 (for the default StreamingAggregationStateManagerImplV2 ) [[implementations]] NOTE: StreamingAggregationStateManagerBaseImpl is the one and only known direct implementation of the < > in Spark Structured Streaming. NOTE: StreamingAggregationStateManager is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). === [[createStateManager]] Creating StreamingAggregationStateManager Instance -- createStateManager Factory Method [source, scala] \u00b6 createStateManager( keyExpressions: Seq[Attribute], inputRowAttributes: Seq[Attribute], stateFormatVersion: Int): StreamingAggregationStateManager createStateManager creates a new StreamingAggregationStateManager for a given stateFormatVersion : StreamingAggregationStateManagerImplV1 for stateFormatVersion being 1 StreamingAggregationStateManagerImplV2 for stateFormatVersion being 2 createStateManager throws a IllegalArgumentException for any other stateFormatVersion : Version [stateFormatVersion] is invalid createStateManager is used when StateStoreRestoreExec and StateStoreSaveExec physical operators are created.","title":"StreamingAggregationStateManager"},{"location":"StreamingAggregationStateManager/#streamingaggregationstatemanager","text":"StreamingAggregationStateManager is the < > of < > that act as middlemen between state stores and the physical operators used in Streaming Aggregation (e.g. StateStoreSaveExec and StateStoreRestoreExec ). [[contract]] .StreamingAggregationStateManager Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | commit a| [[commit]]","title":"StreamingAggregationStateManager"},{"location":"StreamingAggregationStateManager/#source-scala","text":"commit( store: StateStore): Long Commits all updates ( changes ) to the given StateStore and returns the new version Used when StateStoreSaveExec physical operator is executed. | get a| [[get]]","title":"[source, scala]"},{"location":"StreamingAggregationStateManager/#source-scala_1","text":"","title":"[source, scala]"},{"location":"StreamingAggregationStateManager/#getstore-statestore-key-unsaferow-unsaferow","text":"Looks up the value of the key from the StateStore (the key is non- null ) Used exclusively when StateStoreRestoreExec physical operator is executed. | getKey a| [[getKey]]","title":"get(store: StateStore, key: UnsafeRow): UnsafeRow"},{"location":"StreamingAggregationStateManager/#source-scala_2","text":"","title":"[source, scala]"},{"location":"StreamingAggregationStateManager/#getkeyrow-unsaferow-unsaferow","text":"Extracts the columns for the key from the input row Used when: StateStoreRestoreExec physical operator is executed StreamingAggregationStateManagerImplV1 legacy state manager is requested to put a row to a state store | getStateValueSchema a| [[getStateValueSchema]]","title":"getKey(row: UnsafeRow): UnsafeRow"},{"location":"StreamingAggregationStateManager/#source-scala_3","text":"","title":"[source, scala]"},{"location":"StreamingAggregationStateManager/#getstatevalueschema-structtype","text":"Gets the schema of the values in StateStore s Used when StateStoreRestoreExec and StateStoreSaveExec physical operators are executed | iterator a| [[iterator]]","title":"getStateValueSchema: StructType"},{"location":"StreamingAggregationStateManager/#source-scala_4","text":"iterator( store: StateStore): Iterator[UnsafeRowPair] Returns all UnsafeRow key-value pairs in the given StateStore Used exclusively when StateStoreSaveExec physical operator is executed. | keys a| [[keys]]","title":"[source, scala]"},{"location":"StreamingAggregationStateManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"StreamingAggregationStateManager/#keysstore-statestore-iteratorunsaferow","text":"Returns all the keys in the given StateStore Used exclusively when physical operators with WatermarkSupport are requested to removeKeysOlderThanWatermark (when StateStoreSaveExec physical operator is executed). | put a| [[put]]","title":"keys(store: StateStore): Iterator[UnsafeRow]"},{"location":"StreamingAggregationStateManager/#source-scala_6","text":"put( store: StateStore, row: UnsafeRow): Unit Stores ( puts ) the given row in the given StateStore Used exclusively when StateStoreSaveExec physical operator is executed. | remove a| [[remove]]","title":"[source, scala]"},{"location":"StreamingAggregationStateManager/#source-scala_7","text":"remove( store: StateStore, key: UnsafeRow): Unit Removes the key-value pair from the given StateStore per key Used exclusively when StateStoreSaveExec physical operator is executed (directly or indirectly as a WatermarkSupport ) | values a| [[values]]","title":"[source, scala]"},{"location":"StreamingAggregationStateManager/#source-scala_8","text":"values( store: StateStore): Iterator[UnsafeRow] All values in the given StateStore Used exclusively when StateStoreSaveExec physical operator is executed. |=== [[supportedVersions]] StreamingAggregationStateManager supports < > (per the spark.sql.streaming.aggregation.stateFormatVersion internal configuration property): [[legacyVersion]] 1 (for the legacy StreamingAggregationStateManagerImplV1 ) [[default]] 2 (for the default StreamingAggregationStateManagerImplV2 ) [[implementations]] NOTE: StreamingAggregationStateManagerBaseImpl is the one and only known direct implementation of the < > in Spark Structured Streaming. NOTE: StreamingAggregationStateManager is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). === [[createStateManager]] Creating StreamingAggregationStateManager Instance -- createStateManager Factory Method","title":"[source, scala]"},{"location":"StreamingAggregationStateManager/#source-scala_9","text":"createStateManager( keyExpressions: Seq[Attribute], inputRowAttributes: Seq[Attribute], stateFormatVersion: Int): StreamingAggregationStateManager createStateManager creates a new StreamingAggregationStateManager for a given stateFormatVersion : StreamingAggregationStateManagerImplV1 for stateFormatVersion being 1 StreamingAggregationStateManagerImplV2 for stateFormatVersion being 2 createStateManager throws a IllegalArgumentException for any other stateFormatVersion : Version [stateFormatVersion] is invalid createStateManager is used when StateStoreRestoreExec and StateStoreSaveExec physical operators are created.","title":"[source, scala]"},{"location":"StreamingAggregationStateManagerBaseImpl/","text":"StreamingAggregationStateManagerBaseImpl \u00b6 StreamingAggregationStateManagerBaseImpl is the base implementation of the StreamingAggregationStateManager contract for state managers for streaming aggregations . [[keyProjector]] StreamingAggregationStateManagerBaseImpl uses UnsafeProjection to < >. [[implementations]] .StreamingAggregationStateManagerBaseImpls [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | StreamingAggregationStateManagerBaseImpl | Description | StreamingAggregationStateManagerImplV1 | [[StreamingAggregationStateManagerImplV1]] Legacy StreamingAggregationStateManager (used when spark.sql.streaming.aggregation.stateFormatVersion configuration property is 1 ) | StreamingAggregationStateManagerImplV2 | [[StreamingAggregationStateManagerImplV2]] Default StreamingAggregationStateManager (used when spark.sql.streaming.aggregation.stateFormatVersion configuration property is 2 ) |=== [[creating-instance]] StreamingAggregationStateManagerBaseImpl takes the following to be created: [[keyExpressions]] Catalyst expressions for the keys ( Seq[Attribute] ) [[inputRowAttributes]] Catalyst expressions for the input rows ( Seq[Attribute] ) NOTE: StreamingAggregationStateManagerBaseImpl is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. Committing (Changes to) State Store \u00b6 commit ( store : StateStore ): Long commit is part of the StreamingAggregationStateManager abstraction. commit simply requests the state store to commit state changes . Removing Key From State Store \u00b6 remove ( store : StateStore , key : UnsafeRow ): Unit remove is part of the StreamingAggregationStateManager abstraction. remove ...FIXME getKey \u00b6 getKey ( row : UnsafeRow ): UnsafeRow getKey is part of the StreamingAggregationStateManager abstraction. getKey ...FIXME Getting All Keys in State Store \u00b6 keys ( store : StateStore ): Iterator [ UnsafeRow ] keys is part of the StreamingAggregationStateManager abstraction. keys ...FIXME","title":"StreamingAggregationStateManagerBaseImpl"},{"location":"StreamingAggregationStateManagerBaseImpl/#streamingaggregationstatemanagerbaseimpl","text":"StreamingAggregationStateManagerBaseImpl is the base implementation of the StreamingAggregationStateManager contract for state managers for streaming aggregations . [[keyProjector]] StreamingAggregationStateManagerBaseImpl uses UnsafeProjection to < >. [[implementations]] .StreamingAggregationStateManagerBaseImpls [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | StreamingAggregationStateManagerBaseImpl | Description | StreamingAggregationStateManagerImplV1 | [[StreamingAggregationStateManagerImplV1]] Legacy StreamingAggregationStateManager (used when spark.sql.streaming.aggregation.stateFormatVersion configuration property is 1 ) | StreamingAggregationStateManagerImplV2 | [[StreamingAggregationStateManagerImplV2]] Default StreamingAggregationStateManager (used when spark.sql.streaming.aggregation.stateFormatVersion configuration property is 2 ) |=== [[creating-instance]] StreamingAggregationStateManagerBaseImpl takes the following to be created: [[keyExpressions]] Catalyst expressions for the keys ( Seq[Attribute] ) [[inputRowAttributes]] Catalyst expressions for the input rows ( Seq[Attribute] ) NOTE: StreamingAggregationStateManagerBaseImpl is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >.","title":"StreamingAggregationStateManagerBaseImpl"},{"location":"StreamingAggregationStateManagerBaseImpl/#committing-changes-to-state-store","text":"commit ( store : StateStore ): Long commit is part of the StreamingAggregationStateManager abstraction. commit simply requests the state store to commit state changes .","title":" Committing (Changes to) State Store"},{"location":"StreamingAggregationStateManagerBaseImpl/#removing-key-from-state-store","text":"remove ( store : StateStore , key : UnsafeRow ): Unit remove is part of the StreamingAggregationStateManager abstraction. remove ...FIXME","title":" Removing Key From State Store"},{"location":"StreamingAggregationStateManagerBaseImpl/#getkey","text":"getKey ( row : UnsafeRow ): UnsafeRow getKey is part of the StreamingAggregationStateManager abstraction. getKey ...FIXME","title":" getKey"},{"location":"StreamingAggregationStateManagerBaseImpl/#getting-all-keys-in-state-store","text":"keys ( store : StateStore ): Iterator [ UnsafeRow ] keys is part of the StreamingAggregationStateManager abstraction. keys ...FIXME","title":" Getting All Keys in State Store"},{"location":"StreamingAggregationStateManagerImplV1/","text":"StreamingAggregationStateManagerImplV1 \u00b6 StreamingAggregationStateManagerImplV1 is the legacy state manager for streaming aggregations . Note The version of a state manager is controlled using spark.sql.streaming.aggregation.stateFormatVersion internal configuration property. Storing Row in State Store \u00b6 put ( store : StateStore , row : UnsafeRow ): Unit put is part of the StreamingAggregationStateManager abstraction. put ...FIXME Creating Instance \u00b6 StreamingAggregationStateManagerImplV1 takes the following when created: [[keyExpressions]] Attribute expressions for keys ( Seq[Attribute] ) [[inputRowAttributes]] Attribute expressions of input rows ( Seq[Attribute] ) StreamingAggregationStateManagerImplV1 is created when StreamingAggregationStateManager is requested for a new StreamingAggregationStateManager .","title":"StreamingAggregationStateManagerImplV1"},{"location":"StreamingAggregationStateManagerImplV1/#streamingaggregationstatemanagerimplv1","text":"StreamingAggregationStateManagerImplV1 is the legacy state manager for streaming aggregations . Note The version of a state manager is controlled using spark.sql.streaming.aggregation.stateFormatVersion internal configuration property.","title":"StreamingAggregationStateManagerImplV1"},{"location":"StreamingAggregationStateManagerImplV1/#storing-row-in-state-store","text":"put ( store : StateStore , row : UnsafeRow ): Unit put is part of the StreamingAggregationStateManager abstraction. put ...FIXME","title":" Storing Row in State Store"},{"location":"StreamingAggregationStateManagerImplV1/#creating-instance","text":"StreamingAggregationStateManagerImplV1 takes the following when created: [[keyExpressions]] Attribute expressions for keys ( Seq[Attribute] ) [[inputRowAttributes]] Attribute expressions of input rows ( Seq[Attribute] ) StreamingAggregationStateManagerImplV1 is created when StreamingAggregationStateManager is requested for a new StreamingAggregationStateManager .","title":"Creating Instance"},{"location":"StreamingAggregationStateManagerImplV2/","text":"StreamingAggregationStateManagerImplV2 \u00b6 StreamingAggregationStateManagerImplV2 is the default state manager for streaming aggregations . Note The version of a state manager is controlled using spark.sql.streaming.aggregation.stateFormatVersion internal configuration property. Creating Instance \u00b6 StreamingAggregationStateManagerImplV2 (like the parent StreamingAggregationStateManagerBaseImpl ) takes the following to be created: [[keyExpressions]] Catalyst expressions for the keys ( Seq[Attribute] ) [[inputRowAttributes]] Catalyst expressions for the input rows ( Seq[Attribute] ) StreamingAggregationStateManagerImplV2 is created when StreamingAggregationStateManager is requested for a new StreamingAggregationStateManager . Storing Row in State Store \u00b6 put ( store : StateStore , row : UnsafeRow ): Unit put is part of the StreamingAggregationStateManager abstraction. put ...FIXME Getting Saved State for Non-Null Key from State Store \u00b6 get ( store : StateStore , key : UnsafeRow ): UnsafeRow get is part of the StreamingAggregationStateManager abstraction. get requests the given StateStore for the current state value for the given key. get returns null if the key could not be found in the state store. Otherwise, get restoreOriginalRow (for the key and the saved state). === [[restoreOriginalRow]] restoreOriginalRow Internal Method [source, scala] \u00b6 restoreOriginalRow(key: UnsafeRow, value: UnsafeRow): UnsafeRow restoreOriginalRow(rowPair: UnsafeRowPair): UnsafeRow restoreOriginalRow ...FIXME NOTE: restoreOriginalRow is used when StreamingAggregationStateManagerImplV2 is requested to < >, < > and < >. getStateValueSchema \u00b6 getStateValueSchema : StructType getStateValueSchema is part of the StreamingAggregationStateManager abstraction. getStateValueSchema simply requests the valueExpressions for the schema. iterator \u00b6 iterator ( store : StateStore ): Iterator [ UnsafeRowPair ] iterator is part of the StreamingAggregationStateManager abstraction. iterator simply requests the input state store for the iterator that is mapped to an iterator of UnsafeRowPairs with the key (of the input UnsafeRowPair ) and the value as a restored original row . Note scala.collection.Iterator is a data structure that allows to iterate over a sequence of elements that are usually fetched lazily (i.e. no elements are fetched from the underlying store until processed). values \u00b6 values ( store : StateStore ): Iterator [ UnsafeRow ] values is part of the StreamingAggregationStateManager abstraction. values ...FIXME Internal Properties \u00b6 [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | joiner | [[joiner]] | keyValueJoinedExpressions a| [[keyValueJoinedExpressions]] | needToProjectToRestoreValue a| [[needToProjectToRestoreValue]] | restoreValueProjector a| [[restoreValueProjector]] | valueExpressions a| [[valueExpressions]] | valueProjector a| [[valueProjector]] |===","title":"StreamingAggregationStateManagerImplV2"},{"location":"StreamingAggregationStateManagerImplV2/#streamingaggregationstatemanagerimplv2","text":"StreamingAggregationStateManagerImplV2 is the default state manager for streaming aggregations . Note The version of a state manager is controlled using spark.sql.streaming.aggregation.stateFormatVersion internal configuration property.","title":"StreamingAggregationStateManagerImplV2"},{"location":"StreamingAggregationStateManagerImplV2/#creating-instance","text":"StreamingAggregationStateManagerImplV2 (like the parent StreamingAggregationStateManagerBaseImpl ) takes the following to be created: [[keyExpressions]] Catalyst expressions for the keys ( Seq[Attribute] ) [[inputRowAttributes]] Catalyst expressions for the input rows ( Seq[Attribute] ) StreamingAggregationStateManagerImplV2 is created when StreamingAggregationStateManager is requested for a new StreamingAggregationStateManager .","title":"Creating Instance"},{"location":"StreamingAggregationStateManagerImplV2/#storing-row-in-state-store","text":"put ( store : StateStore , row : UnsafeRow ): Unit put is part of the StreamingAggregationStateManager abstraction. put ...FIXME","title":" Storing Row in State Store"},{"location":"StreamingAggregationStateManagerImplV2/#getting-saved-state-for-non-null-key-from-state-store","text":"get ( store : StateStore , key : UnsafeRow ): UnsafeRow get is part of the StreamingAggregationStateManager abstraction. get requests the given StateStore for the current state value for the given key. get returns null if the key could not be found in the state store. Otherwise, get restoreOriginalRow (for the key and the saved state). === [[restoreOriginalRow]] restoreOriginalRow Internal Method","title":" Getting Saved State for Non-Null Key from State Store"},{"location":"StreamingAggregationStateManagerImplV2/#source-scala","text":"restoreOriginalRow(key: UnsafeRow, value: UnsafeRow): UnsafeRow restoreOriginalRow(rowPair: UnsafeRowPair): UnsafeRow restoreOriginalRow ...FIXME NOTE: restoreOriginalRow is used when StreamingAggregationStateManagerImplV2 is requested to < >, < > and < >.","title":"[source, scala]"},{"location":"StreamingAggregationStateManagerImplV2/#getstatevalueschema","text":"getStateValueSchema : StructType getStateValueSchema is part of the StreamingAggregationStateManager abstraction. getStateValueSchema simply requests the valueExpressions for the schema.","title":" getStateValueSchema"},{"location":"StreamingAggregationStateManagerImplV2/#iterator","text":"iterator ( store : StateStore ): Iterator [ UnsafeRowPair ] iterator is part of the StreamingAggregationStateManager abstraction. iterator simply requests the input state store for the iterator that is mapped to an iterator of UnsafeRowPairs with the key (of the input UnsafeRowPair ) and the value as a restored original row . Note scala.collection.Iterator is a data structure that allows to iterate over a sequence of elements that are usually fetched lazily (i.e. no elements are fetched from the underlying store until processed).","title":" iterator"},{"location":"StreamingAggregationStateManagerImplV2/#values","text":"values ( store : StateStore ): Iterator [ UnsafeRow ] values is part of the StreamingAggregationStateManager abstraction. values ...FIXME","title":" values"},{"location":"StreamingAggregationStateManagerImplV2/#internal-properties","text":"[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | joiner | [[joiner]] | keyValueJoinedExpressions a| [[keyValueJoinedExpressions]] | needToProjectToRestoreValue a| [[needToProjectToRestoreValue]] | restoreValueProjector a| [[restoreValueProjector]] | valueExpressions a| [[valueExpressions]] | valueProjector a| [[valueProjector]] |===","title":"Internal Properties"},{"location":"StreamingDeduplicationStrategy/","text":"StreamingDeduplicationStrategy Execution Planning Strategy \u00b6 StreamingDeduplicationStrategy is an execution planning strategy that can plan streaming queries with Deduplicate logical operators (over streaming queries) to StreamingDeduplicateExec physical operators. Tip Learn more about Execution Planning Strategies in The Internals of Spark SQL online book. StreamingDeduplicationStrategy is available using SessionState . spark . sessionState . planner . StreamingDeduplicationStrategy","title":"StreamingDeduplicationStrategy"},{"location":"StreamingDeduplicationStrategy/#streamingdeduplicationstrategy-execution-planning-strategy","text":"StreamingDeduplicationStrategy is an execution planning strategy that can plan streaming queries with Deduplicate logical operators (over streaming queries) to StreamingDeduplicateExec physical operators. Tip Learn more about Execution Planning Strategies in The Internals of Spark SQL online book. StreamingDeduplicationStrategy is available using SessionState . spark . sessionState . planner . StreamingDeduplicationStrategy","title":"StreamingDeduplicationStrategy Execution Planning Strategy"},{"location":"StreamingGlobalLimitStrategy/","text":"StreamingGlobalLimitStrategy Execution Planning Strategy \u00b6 StreamingGlobalLimitStrategy is an execution planning strategy that can plan streaming queries with ReturnAnswer and Limit logical operators (over streaming queries) with the Append output mode to StreamingGlobalLimitExec physical operator. StreamingGlobalLimitStrategy is used (and created) when IncrementalExecution is requested to plan a streaming query. Creating Instance \u00b6 StreamingGlobalLimitStrategy takes a single OutputMode to be created (which is the OutputMode of the IncrementalExecution ).","title":"StreamingGlobalLimitStrategy"},{"location":"StreamingGlobalLimitStrategy/#streaminggloballimitstrategy-execution-planning-strategy","text":"StreamingGlobalLimitStrategy is an execution planning strategy that can plan streaming queries with ReturnAnswer and Limit logical operators (over streaming queries) with the Append output mode to StreamingGlobalLimitExec physical operator. StreamingGlobalLimitStrategy is used (and created) when IncrementalExecution is requested to plan a streaming query.","title":"StreamingGlobalLimitStrategy Execution Planning Strategy"},{"location":"StreamingGlobalLimitStrategy/#creating-instance","text":"StreamingGlobalLimitStrategy takes a single OutputMode to be created (which is the OutputMode of the IncrementalExecution ).","title":"Creating Instance"},{"location":"StreamingJoinHelper/","text":"StreamingJoinHelper Utility \u00b6 StreamingJoinHelper is a Scala object with the following utility methods: < > [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.catalyst.analysis.StreamingJoinHelper to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.catalyst.analysis.StreamingJoinHelper=ALL Refer to < >. \u00b6 State Value Watermark \u00b6 getStateValueWatermark ( attributesToFindStateWatermarkFor : AttributeSet , attributesWithEventWatermark : AttributeSet , joinCondition : Option [ Expression ], eventWatermark : Option [ Long ]): Option [ Long ] getStateValueWatermark ...FIXME getStateValueWatermark is used when: UnsupportedOperationChecker utility is used to checkForStreaming StreamingSymmetricHashJoinHelper utility is used to create a JoinStateWatermarkPredicates","title":"StreamingJoinHelper"},{"location":"StreamingJoinHelper/#streamingjoinhelper-utility","text":"StreamingJoinHelper is a Scala object with the following utility methods: < > [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.catalyst.analysis.StreamingJoinHelper to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.catalyst.analysis.StreamingJoinHelper=ALL","title":"StreamingJoinHelper Utility"},{"location":"StreamingJoinHelper/#refer-to","text":"","title":"Refer to &lt;&gt;."},{"location":"StreamingJoinHelper/#state-value-watermark","text":"getStateValueWatermark ( attributesToFindStateWatermarkFor : AttributeSet , attributesWithEventWatermark : AttributeSet , joinCondition : Option [ Expression ], eventWatermark : Option [ Long ]): Option [ Long ] getStateValueWatermark ...FIXME getStateValueWatermark is used when: UnsupportedOperationChecker utility is used to checkForStreaming StreamingSymmetricHashJoinHelper utility is used to create a JoinStateWatermarkPredicates","title":" State Value Watermark"},{"location":"StreamingJoinStrategy/","text":"StreamingJoinStrategy Execution Planning Strategy \u2014 Stream-Stream Equi-Joins \u00b6 [[apply]] StreamingJoinStrategy is an execution planning strategy that can plan streaming queries with Join logical operators of two streaming queries to a < > physical operator. StreamingJoinStrategy throws an AnalysisException when applied to a Join logical operator with no equality predicate: Stream-stream join without equality predicate is not supported StreamingJoinStrategy is used when IncrementalExecution is requested to plan a streaming query. [[logging]] [TIP] ==== StreamingJoinStrategy does not print out any messages to the logs. StreamingJoinStrategy however uses ExtractEquiJoinKeys Scala extractor for destructuring Join logical operators that does print out DEBUG messages to the logs. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-ExtractEquiJoinKeys.html[ExtractEquiJoinKeys ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. Enable ALL logging level for org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys=ALL Refer to < >. \u00b6","title":"StreamingJoinStrategy"},{"location":"StreamingJoinStrategy/#streamingjoinstrategy-execution-planning-strategy-stream-stream-equi-joins","text":"[[apply]] StreamingJoinStrategy is an execution planning strategy that can plan streaming queries with Join logical operators of two streaming queries to a < > physical operator. StreamingJoinStrategy throws an AnalysisException when applied to a Join logical operator with no equality predicate: Stream-stream join without equality predicate is not supported StreamingJoinStrategy is used when IncrementalExecution is requested to plan a streaming query. [[logging]] [TIP] ==== StreamingJoinStrategy does not print out any messages to the logs. StreamingJoinStrategy however uses ExtractEquiJoinKeys Scala extractor for destructuring Join logical operators that does print out DEBUG messages to the logs. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-ExtractEquiJoinKeys.html[ExtractEquiJoinKeys ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. Enable ALL logging level for org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys=ALL","title":"StreamingJoinStrategy Execution Planning Strategy &mdash; Stream-Stream Equi-Joins"},{"location":"StreamingJoinStrategy/#refer-to","text":"","title":"Refer to &lt;&gt;."},{"location":"StreamingQuery/","text":"StreamingQuery \u00b6 StreamingQuery is an abstraction of handles to streaming queries (that are executed continuously and concurrently on a separate thread ). Creating StreamingQuery \u00b6 StreamingQuery is created when a streaming query is started using DataStreamWriter.start operator. Demo: Deep Dive into FileStreamSink Learn more in Demo: Deep Dive into FileStreamSink . Managing Active StreamingQueries \u00b6 StreamingQueryManager manages active StreamingQuery instances and allows to access one (by id ) or all active queries (using StreamingQueryManager.get or StreamingQueryManager.active operators, respectively). States \u00b6 StreamingQuery can be in two states : Active (started) Inactive (stopped) If inactive, StreamingQuery may have stopped due to an StreamingQueryException . Implementations \u00b6 StreamExecution StreamingQueryWrapper Contract \u00b6 awaitTermination \u00b6 awaitTermination (): Unit awaitTermination ( timeoutMs : Long ): Boolean Used when...FIXME StreamingQueryException \u00b6 exception : Option [ StreamingQueryException ] StreamingQueryException if the streaming query has finished due to an exception Used when...FIXME Explaining Streaming Query \u00b6 explain (): Unit explain ( extended : Boolean ): Unit Used when...FIXME Id \u00b6 id : UUID Unique identifier of the streaming query (that does not change across restarts unlike runId ) Used when...FIXME isActive \u00b6 isActive : Boolean Indicates whether the streaming query is active ( true ) or not ( false ) Used when...FIXME StreamingQueryProgress \u00b6 lastProgress : StreamingQueryProgress The latest StreamingQueryProgress of the streaming query Used when...FIXME Query Name \u00b6 name : String Name of the streaming query (unique across all active queries in SparkSession ) Used when...FIXME Processing All Available Data \u00b6 processAllAvailable (): Unit Pauses ( blocks ) the current thread until the streaming query has no more data to be processed or has been stopped . Intended for testing Used when...FIXME Recent StreamingQueryProgresses \u00b6 recentProgress : Array [ StreamingQueryProgress ] Recent StreamingQueryProgress updates. Used when...FIXME Run Id \u00b6 runId : UUID Unique identifier of the current execution of the streaming query (that is different every restart unlike id ) Used when...FIXME SparkSession \u00b6 sparkSession : SparkSession Used when...FIXME StreamingQueryStatus \u00b6 status : StreamingQueryStatus StreamingQueryStatus of the streaming query (as StreamExecution has accumulated being a ProgressReporter while running the streaming query) Used when...FIXME Stopping Streaming Query \u00b6 stop (): Unit Stops the streaming query Used when...FIXME","title":"StreamingQuery"},{"location":"StreamingQuery/#streamingquery","text":"StreamingQuery is an abstraction of handles to streaming queries (that are executed continuously and concurrently on a separate thread ).","title":"StreamingQuery"},{"location":"StreamingQuery/#creating-streamingquery","text":"StreamingQuery is created when a streaming query is started using DataStreamWriter.start operator. Demo: Deep Dive into FileStreamSink Learn more in Demo: Deep Dive into FileStreamSink .","title":"Creating StreamingQuery"},{"location":"StreamingQuery/#managing-active-streamingqueries","text":"StreamingQueryManager manages active StreamingQuery instances and allows to access one (by id ) or all active queries (using StreamingQueryManager.get or StreamingQueryManager.active operators, respectively).","title":"Managing Active StreamingQueries"},{"location":"StreamingQuery/#states","text":"StreamingQuery can be in two states : Active (started) Inactive (stopped) If inactive, StreamingQuery may have stopped due to an StreamingQueryException .","title":"States"},{"location":"StreamingQuery/#implementations","text":"StreamExecution StreamingQueryWrapper","title":"Implementations"},{"location":"StreamingQuery/#contract","text":"","title":"Contract"},{"location":"StreamingQuery/#awaittermination","text":"awaitTermination (): Unit awaitTermination ( timeoutMs : Long ): Boolean Used when...FIXME","title":" awaitTermination"},{"location":"StreamingQuery/#streamingqueryexception","text":"exception : Option [ StreamingQueryException ] StreamingQueryException if the streaming query has finished due to an exception Used when...FIXME","title":" StreamingQueryException"},{"location":"StreamingQuery/#explaining-streaming-query","text":"explain (): Unit explain ( extended : Boolean ): Unit Used when...FIXME","title":" Explaining Streaming Query"},{"location":"StreamingQuery/#id","text":"id : UUID Unique identifier of the streaming query (that does not change across restarts unlike runId ) Used when...FIXME","title":" Id"},{"location":"StreamingQuery/#isactive","text":"isActive : Boolean Indicates whether the streaming query is active ( true ) or not ( false ) Used when...FIXME","title":" isActive"},{"location":"StreamingQuery/#streamingqueryprogress","text":"lastProgress : StreamingQueryProgress The latest StreamingQueryProgress of the streaming query Used when...FIXME","title":" StreamingQueryProgress"},{"location":"StreamingQuery/#query-name","text":"name : String Name of the streaming query (unique across all active queries in SparkSession ) Used when...FIXME","title":" Query Name"},{"location":"StreamingQuery/#processing-all-available-data","text":"processAllAvailable (): Unit Pauses ( blocks ) the current thread until the streaming query has no more data to be processed or has been stopped . Intended for testing Used when...FIXME","title":" Processing All Available Data"},{"location":"StreamingQuery/#recent-streamingqueryprogresses","text":"recentProgress : Array [ StreamingQueryProgress ] Recent StreamingQueryProgress updates. Used when...FIXME","title":" Recent StreamingQueryProgresses"},{"location":"StreamingQuery/#run-id","text":"runId : UUID Unique identifier of the current execution of the streaming query (that is different every restart unlike id ) Used when...FIXME","title":" Run Id"},{"location":"StreamingQuery/#sparksession","text":"sparkSession : SparkSession Used when...FIXME","title":" SparkSession"},{"location":"StreamingQuery/#streamingquerystatus","text":"status : StreamingQueryStatus StreamingQueryStatus of the streaming query (as StreamExecution has accumulated being a ProgressReporter while running the streaming query) Used when...FIXME","title":" StreamingQueryStatus"},{"location":"StreamingQuery/#stopping-streaming-query","text":"stop (): Unit Stops the streaming query Used when...FIXME","title":" Stopping Streaming Query"},{"location":"StreamingQueryListenerBus/","text":"StreamingQueryListenerBus \u00b6 StreamingQueryListenerBus is an event bus for dispatching streaming events (of active streaming queries ) to StreamingQueryListener s. Tip Learn more about event buses in The Internals of Apache Spark online book. Creating Instance \u00b6 StreamingQueryListenerBus takes the following to be created: LiveListenerBus ( Spark Core ) When created, StreamingQueryListenerBus registers itself with the LiveListenerBus to streams event queue. StreamingQueryListenerBus is created for StreamingQueryManager (once per SparkSession ). SparkListener \u00b6 StreamingQueryListenerBus is an event listener ( SparkListener ) and registers itself with the LiveListenerBus to intercept QueryStartedEvents . Tip Learn more about SparkListener in The Internals of Apache Spark online book. Run IDs of Active Streaming Queries \u00b6 activeQueryRunIds : Set [ UUID ] activeQueryRunIds is an internal registry of run IDs of active streaming queries in the SparkSession . A runId is added when StreamingQueryListenerBus is requested to post a QueryStartedEvent A runId is removed when StreamingQueryListenerBus is requested to post a QueryTerminatedEvent activeQueryRunIds is used internally to dispatch a streaming event to a StreamingQueryListener (so the events gets sent out to streaming queries in the SparkSession ). Posting Streaming Event to LiveListenerBus \u00b6 post ( event : StreamingQueryListener . Event ): Unit post simply posts the input event directly to the LiveListenerBus unless it is a QueryStartedEvent . For a QueryStartedEvent , post adds the runId (of the streaming query that has been started) to the activeQueryRunIds internal registry first, posts the event to the LiveListenerBus and then postToAll . post is used when StreamingQueryManager is requested to post a streaming event . Notifying Listener about Event \u00b6 doPostEvent ( listener : StreamingQueryListener , event : StreamingQueryListener . Event ): Unit doPostEvent is part of the ListenerBus ( Spark Core ) abstraction. doPostEvent branches per the type of StreamingQueryListener.Event : For a QueryStartedEvent , requests the StreamingQueryListener to onQueryStarted For a QueryProgressEvent , requests the StreamingQueryListener to onQueryProgress For a QueryTerminatedEvent , requests the StreamingQueryListener to onQueryTerminated For any other event, doPostEvent simply does nothing ( swallows it ). Posting Event To All Listeners \u00b6 postToAll ( event : Event ): Unit postToAll is part of the ListenerBus ( Spark Core ) abstraction. postToAll first requests the parent ListenerBus to post the event to all registered listeners. For a QueryTerminatedEvent , postToAll simply removes the runId (of the streaming query that has been terminated) from the activeQueryRunIds internal registry.","title":"StreamingQueryListenerBus"},{"location":"StreamingQueryListenerBus/#streamingquerylistenerbus","text":"StreamingQueryListenerBus is an event bus for dispatching streaming events (of active streaming queries ) to StreamingQueryListener s. Tip Learn more about event buses in The Internals of Apache Spark online book.","title":"StreamingQueryListenerBus"},{"location":"StreamingQueryListenerBus/#creating-instance","text":"StreamingQueryListenerBus takes the following to be created: LiveListenerBus ( Spark Core ) When created, StreamingQueryListenerBus registers itself with the LiveListenerBus to streams event queue. StreamingQueryListenerBus is created for StreamingQueryManager (once per SparkSession ).","title":"Creating Instance"},{"location":"StreamingQueryListenerBus/#sparklistener","text":"StreamingQueryListenerBus is an event listener ( SparkListener ) and registers itself with the LiveListenerBus to intercept QueryStartedEvents . Tip Learn more about SparkListener in The Internals of Apache Spark online book.","title":" SparkListener"},{"location":"StreamingQueryListenerBus/#run-ids-of-active-streaming-queries","text":"activeQueryRunIds : Set [ UUID ] activeQueryRunIds is an internal registry of run IDs of active streaming queries in the SparkSession . A runId is added when StreamingQueryListenerBus is requested to post a QueryStartedEvent A runId is removed when StreamingQueryListenerBus is requested to post a QueryTerminatedEvent activeQueryRunIds is used internally to dispatch a streaming event to a StreamingQueryListener (so the events gets sent out to streaming queries in the SparkSession ).","title":" Run IDs of Active Streaming Queries"},{"location":"StreamingQueryListenerBus/#posting-streaming-event-to-livelistenerbus","text":"post ( event : StreamingQueryListener . Event ): Unit post simply posts the input event directly to the LiveListenerBus unless it is a QueryStartedEvent . For a QueryStartedEvent , post adds the runId (of the streaming query that has been started) to the activeQueryRunIds internal registry first, posts the event to the LiveListenerBus and then postToAll . post is used when StreamingQueryManager is requested to post a streaming event .","title":" Posting Streaming Event to LiveListenerBus"},{"location":"StreamingQueryListenerBus/#notifying-listener-about-event","text":"doPostEvent ( listener : StreamingQueryListener , event : StreamingQueryListener . Event ): Unit doPostEvent is part of the ListenerBus ( Spark Core ) abstraction. doPostEvent branches per the type of StreamingQueryListener.Event : For a QueryStartedEvent , requests the StreamingQueryListener to onQueryStarted For a QueryProgressEvent , requests the StreamingQueryListener to onQueryProgress For a QueryTerminatedEvent , requests the StreamingQueryListener to onQueryTerminated For any other event, doPostEvent simply does nothing ( swallows it ).","title":" Notifying Listener about Event"},{"location":"StreamingQueryListenerBus/#posting-event-to-all-listeners","text":"postToAll ( event : Event ): Unit postToAll is part of the ListenerBus ( Spark Core ) abstraction. postToAll first requests the parent ListenerBus to post the event to all registered listeners. For a QueryTerminatedEvent , postToAll simply removes the runId (of the streaming query that has been terminated) from the activeQueryRunIds internal registry.","title":" Posting Event To All Listeners"},{"location":"StreamingQueryManager/","text":"StreamingQueryManager \u00b6 StreamingQueryManager is the management interface for active streaming queries of a SparkSession . StreamingQueryManager is used (internally) to create a StreamingQuery (and its StreamExecution) . Creating Instance \u00b6 StreamingQueryManager takes the following to be created: SparkSession StreamingQueryManager is created when SessionState is requested for one. Tip Learn more about SessionState in The Internals of Spark SQL online book. All Active Streaming Queries \u00b6 active : Array [ StreamingQuery ] Active streaming queries Registering StreamingQueryListener \u00b6 addListener ( listener : StreamingQueryListener ): Unit Registers ( adds ) a StreamingQueryListener Awaiting Any Termination \u00b6 awaitAnyTermination (): Unit awaitAnyTermination ( timeoutMs : Long ): Boolean Waits until any streaming query terminates or timeoutMs elapses Getting Active StreamingQuery by ID \u00b6 get ( id : String ): StreamingQuery get ( id : UUID ): StreamingQuery Gets the StreamingQuery by id Deregistering StreamingQueryListener \u00b6 removeListener ( listener : StreamingQueryListener ): Unit De-registers ( removes ) the StreamingQueryListener Resetting Terminated Queries \u00b6 resetTerminated (): Unit Resets the internal registry of the terminated streaming queries (that lets awaitAnyTermination to be used again) Accessing StreamingQueryManager \u00b6 StreamingQueryManager is available using SparkSession.streams property. scala> :type spark org.apache.spark.sql.SparkSession scala> :type spark.streams org.apache.spark.sql.streaming.StreamingQueryManager StreamingQueryListenerBus \u00b6 listenerBus : StreamingQueryListenerBus listenerBus is a StreamingQueryListenerBus (for the current SparkSession ) that is created immediately when StreamingQueryManager is created . listenerBus is used for the following: Register or de-register a given StreamingQueryListener Post a streaming event (and notify registered StreamingQueryListeners about the event ) Registering StreamingQueryListener \u00b6 addListener ( listener : StreamingQueryListener ): Unit addListener requests the StreamingQueryListenerBus to add the input StreamingQueryListener . De-Registering StreamingQueryListener \u00b6 removeListener ( listener : StreamingQueryListener ): Unit removeListener requests StreamingQueryListenerBus to remove the input StreamingQueryListener . Creating Streaming Query \u00b6 createQuery ( userSpecifiedName : Option [ String ], userSpecifiedCheckpointLocation : Option [ String ], df : DataFrame , extraOptions : Map [ String , String ], sink : BaseStreamingSink , outputMode : OutputMode , useTempCheckpointLocation : Boolean , recoverFromCheckpointLocation : Boolean , trigger : Trigger , triggerClock : Clock ): StreamingQueryWrapper createQuery creates a StreamingQueryWrapper (for a StreamExecution per the input user-defined properties). Internally, createQuery first finds the name of the checkpoint directory of a query (aka checkpoint location ) in the following order: Exactly the input userSpecifiedCheckpointLocation if defined spark.sql.streaming.checkpointLocation Spark property if defined for the parent directory with a subdirectory per the optional userSpecifiedName (or a randomly-generated UUID) (only when useTempCheckpointLocation is enabled) A temporary directory (as specified by java.io.tmpdir JVM property) with a subdirectory with temporary prefix. Note userSpecifiedCheckpointLocation can be any path that is acceptable by Hadoop's Path . If the directory name for the checkpoint location could not be found, createQuery reports a AnalysisException . checkpointLocation must be specified either through option(\"checkpointLocation\", ...) or SparkSession.conf.set(\"spark.sql.streaming.checkpointLocation\", ...) createQuery reports a AnalysisException when the input recoverFromCheckpointLocation flag is turned off but there is offsets directory in the checkpoint location. createQuery makes sure that the logical plan of the structured query is analyzed (i.e. no logical errors have been found). Unless spark.sql.streaming.unsupportedOperationCheck configuration property is enabled, createQuery checks the logical plan of the streaming query for unsupported operations . (only when spark.sql.adaptive.enabled Spark property is turned on) createQuery prints out a WARN message to the logs: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled. In the end, createQuery creates a StreamingQueryWrapper with a new MicroBatchExecution . createQuery is used when StreamingQueryManager is requested to start a streaming query (when DataStreamWriter is requested to start an execution of a streaming query ). recoverFromCheckpointLocation \u00b6 recoverFromCheckpointLocation flag corresponds to recoverFromCheckpointLocation flag that StreamingQueryManager uses to start a streaming query and which is enabled by default (and is in fact the only place where createQuery is used). memory sink has the flag enabled for Complete output mode only foreach sink has the flag always enabled console sink has the flag always disabled all other sinks have the flag always enabled userSpecifiedName \u00b6 userSpecifiedName corresponds to queryName option (that can be defined using DataStreamWriter 's queryName method) while userSpecifiedCheckpointLocation is checkpointLocation option. Starting Streaming Query Execution \u00b6 startQuery ( userSpecifiedName : Option [ String ], userSpecifiedCheckpointLocation : Option [ String ], df : DataFrame , extraOptions : Map [ String , String ], sink : BaseStreamingSink , outputMode : OutputMode , useTempCheckpointLocation : Boolean = false , recoverFromCheckpointLocation : Boolean = true , trigger : Trigger = ProcessingTime ( 0 ), triggerClock : Clock = new SystemClock ()): StreamingQuery startQuery starts a streaming query and returns a handle to it. Internally, startQuery first creates a StreamingQueryWrapper , registers it in activeQueries internal registry (by the id ), requests it for the underlying StreamExecution and starts it . In the end, startQuery returns the StreamingQueryWrapper (as part of the fluent API so you can chain operators) or throws the exception that was reported when attempting to start the query. startQuery throws an IllegalArgumentException when there is another query registered under name . startQuery looks it up in the activeQueries internal registry. Cannot start query with name [name] as a query with that name is already active startQuery throws an IllegalStateException when a query is started again from checkpoint. startQuery looks it up in activeQueries internal registry. Cannot start query with id [id] as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active. startQuery is used when DataStreamWriter is requested to start an execution of the streaming query . Posting StreamingQueryListener Event to StreamingQueryListenerBus \u00b6 postListenerEvent ( event : StreamingQueryListener . Event ): Unit postListenerEvent simply posts the input event to the internal event bus for streaming events (StreamingQueryListenerBus) . postListenerEvent is used when StreamExecution is requested to post a streaming event . Handling Termination of Streaming Query (and Deactivating Query in StateStoreCoordinator) \u00b6 notifyQueryTermination ( terminatedQuery : StreamingQuery ): Unit notifyQueryTermination removes the terminatedQuery from activeQueries internal registry (by the query id ). notifyQueryTermination records the terminatedQuery in lastTerminatedQuery internal registry (when no earlier streaming query was recorded or the terminatedQuery terminated due to an exception). notifyQueryTermination notifies others that are blocked on awaitTerminationLock . In the end, notifyQueryTermination requests StateStoreCoordinator to deactivate all active runs of the streaming query . notifyQueryTermination is used when StreamExecution is requested to run a streaming query and the query has finished (running streaming batches) (with or without an exception). Active Streaming Queries by ID \u00b6 Registry of StreamingQuery s per UUID Used when StreamingQueryManager is requested for active streaming queries , get a streaming query by id , starts a streaming query and is notified that a streaming query has terminated . Last-Terminated Streaming Query \u00b6 StreamingQuery that has recently been terminated (i.e. stopped or due to an exception ). null when no streaming query has terminated yet or resetTerminated . Used in awaitAnyTermination to know when a streaming query has terminated Set when StreamingQueryManager is notified that a streaming query has terminated StateStoreCoordinatorRef \u00b6 StateStoreCoordinatorRef to the StateStoreCoordinator RPC Endpoint Created when StreamingQueryManager is created Used when: StreamingQueryManager is notified that a streaming query has terminated Stateful operators are executed ( FlatMapGroupsWithStateExec , StateStoreRestoreExec , StateStoreSaveExec , StreamingDeduplicateExec and StreamingSymmetricHashJoinExec ) Creating StateStoreRDD (with storeUpdateFunction aborting StateStore when a task fails)","title":"StreamingQueryManager"},{"location":"StreamingQueryManager/#streamingquerymanager","text":"StreamingQueryManager is the management interface for active streaming queries of a SparkSession . StreamingQueryManager is used (internally) to create a StreamingQuery (and its StreamExecution) .","title":"StreamingQueryManager"},{"location":"StreamingQueryManager/#creating-instance","text":"StreamingQueryManager takes the following to be created: SparkSession StreamingQueryManager is created when SessionState is requested for one. Tip Learn more about SessionState in The Internals of Spark SQL online book.","title":"Creating Instance"},{"location":"StreamingQueryManager/#all-active-streaming-queries","text":"active : Array [ StreamingQuery ] Active streaming queries","title":" All Active Streaming Queries"},{"location":"StreamingQueryManager/#registering-streamingquerylistener","text":"addListener ( listener : StreamingQueryListener ): Unit Registers ( adds ) a StreamingQueryListener","title":" Registering StreamingQueryListener"},{"location":"StreamingQueryManager/#awaiting-any-termination","text":"awaitAnyTermination (): Unit awaitAnyTermination ( timeoutMs : Long ): Boolean Waits until any streaming query terminates or timeoutMs elapses","title":" Awaiting Any Termination"},{"location":"StreamingQueryManager/#getting-active-streamingquery-by-id","text":"get ( id : String ): StreamingQuery get ( id : UUID ): StreamingQuery Gets the StreamingQuery by id","title":" Getting Active StreamingQuery by ID"},{"location":"StreamingQueryManager/#deregistering-streamingquerylistener","text":"removeListener ( listener : StreamingQueryListener ): Unit De-registers ( removes ) the StreamingQueryListener","title":" Deregistering StreamingQueryListener"},{"location":"StreamingQueryManager/#resetting-terminated-queries","text":"resetTerminated (): Unit Resets the internal registry of the terminated streaming queries (that lets awaitAnyTermination to be used again)","title":" Resetting Terminated Queries"},{"location":"StreamingQueryManager/#accessing-streamingquerymanager","text":"StreamingQueryManager is available using SparkSession.streams property. scala> :type spark org.apache.spark.sql.SparkSession scala> :type spark.streams org.apache.spark.sql.streaming.StreamingQueryManager","title":"Accessing StreamingQueryManager"},{"location":"StreamingQueryManager/#streamingquerylistenerbus","text":"listenerBus : StreamingQueryListenerBus listenerBus is a StreamingQueryListenerBus (for the current SparkSession ) that is created immediately when StreamingQueryManager is created . listenerBus is used for the following: Register or de-register a given StreamingQueryListener Post a streaming event (and notify registered StreamingQueryListeners about the event )","title":" StreamingQueryListenerBus"},{"location":"StreamingQueryManager/#registering-streamingquerylistener_1","text":"addListener ( listener : StreamingQueryListener ): Unit addListener requests the StreamingQueryListenerBus to add the input StreamingQueryListener .","title":" Registering StreamingQueryListener"},{"location":"StreamingQueryManager/#de-registering-streamingquerylistener","text":"removeListener ( listener : StreamingQueryListener ): Unit removeListener requests StreamingQueryListenerBus to remove the input StreamingQueryListener .","title":" De-Registering StreamingQueryListener"},{"location":"StreamingQueryManager/#creating-streaming-query","text":"createQuery ( userSpecifiedName : Option [ String ], userSpecifiedCheckpointLocation : Option [ String ], df : DataFrame , extraOptions : Map [ String , String ], sink : BaseStreamingSink , outputMode : OutputMode , useTempCheckpointLocation : Boolean , recoverFromCheckpointLocation : Boolean , trigger : Trigger , triggerClock : Clock ): StreamingQueryWrapper createQuery creates a StreamingQueryWrapper (for a StreamExecution per the input user-defined properties). Internally, createQuery first finds the name of the checkpoint directory of a query (aka checkpoint location ) in the following order: Exactly the input userSpecifiedCheckpointLocation if defined spark.sql.streaming.checkpointLocation Spark property if defined for the parent directory with a subdirectory per the optional userSpecifiedName (or a randomly-generated UUID) (only when useTempCheckpointLocation is enabled) A temporary directory (as specified by java.io.tmpdir JVM property) with a subdirectory with temporary prefix. Note userSpecifiedCheckpointLocation can be any path that is acceptable by Hadoop's Path . If the directory name for the checkpoint location could not be found, createQuery reports a AnalysisException . checkpointLocation must be specified either through option(\"checkpointLocation\", ...) or SparkSession.conf.set(\"spark.sql.streaming.checkpointLocation\", ...) createQuery reports a AnalysisException when the input recoverFromCheckpointLocation flag is turned off but there is offsets directory in the checkpoint location. createQuery makes sure that the logical plan of the structured query is analyzed (i.e. no logical errors have been found). Unless spark.sql.streaming.unsupportedOperationCheck configuration property is enabled, createQuery checks the logical plan of the streaming query for unsupported operations . (only when spark.sql.adaptive.enabled Spark property is turned on) createQuery prints out a WARN message to the logs: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled. In the end, createQuery creates a StreamingQueryWrapper with a new MicroBatchExecution . createQuery is used when StreamingQueryManager is requested to start a streaming query (when DataStreamWriter is requested to start an execution of a streaming query ).","title":" Creating Streaming Query"},{"location":"StreamingQueryManager/#recoverfromcheckpointlocation","text":"recoverFromCheckpointLocation flag corresponds to recoverFromCheckpointLocation flag that StreamingQueryManager uses to start a streaming query and which is enabled by default (and is in fact the only place where createQuery is used). memory sink has the flag enabled for Complete output mode only foreach sink has the flag always enabled console sink has the flag always disabled all other sinks have the flag always enabled","title":" recoverFromCheckpointLocation"},{"location":"StreamingQueryManager/#userspecifiedname","text":"userSpecifiedName corresponds to queryName option (that can be defined using DataStreamWriter 's queryName method) while userSpecifiedCheckpointLocation is checkpointLocation option.","title":" userSpecifiedName"},{"location":"StreamingQueryManager/#starting-streaming-query-execution","text":"startQuery ( userSpecifiedName : Option [ String ], userSpecifiedCheckpointLocation : Option [ String ], df : DataFrame , extraOptions : Map [ String , String ], sink : BaseStreamingSink , outputMode : OutputMode , useTempCheckpointLocation : Boolean = false , recoverFromCheckpointLocation : Boolean = true , trigger : Trigger = ProcessingTime ( 0 ), triggerClock : Clock = new SystemClock ()): StreamingQuery startQuery starts a streaming query and returns a handle to it. Internally, startQuery first creates a StreamingQueryWrapper , registers it in activeQueries internal registry (by the id ), requests it for the underlying StreamExecution and starts it . In the end, startQuery returns the StreamingQueryWrapper (as part of the fluent API so you can chain operators) or throws the exception that was reported when attempting to start the query. startQuery throws an IllegalArgumentException when there is another query registered under name . startQuery looks it up in the activeQueries internal registry. Cannot start query with name [name] as a query with that name is already active startQuery throws an IllegalStateException when a query is started again from checkpoint. startQuery looks it up in activeQueries internal registry. Cannot start query with id [id] as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active. startQuery is used when DataStreamWriter is requested to start an execution of the streaming query .","title":" Starting Streaming Query Execution"},{"location":"StreamingQueryManager/#posting-streamingquerylistener-event-to-streamingquerylistenerbus","text":"postListenerEvent ( event : StreamingQueryListener . Event ): Unit postListenerEvent simply posts the input event to the internal event bus for streaming events (StreamingQueryListenerBus) . postListenerEvent is used when StreamExecution is requested to post a streaming event .","title":" Posting StreamingQueryListener Event to StreamingQueryListenerBus"},{"location":"StreamingQueryManager/#handling-termination-of-streaming-query-and-deactivating-query-in-statestorecoordinator","text":"notifyQueryTermination ( terminatedQuery : StreamingQuery ): Unit notifyQueryTermination removes the terminatedQuery from activeQueries internal registry (by the query id ). notifyQueryTermination records the terminatedQuery in lastTerminatedQuery internal registry (when no earlier streaming query was recorded or the terminatedQuery terminated due to an exception). notifyQueryTermination notifies others that are blocked on awaitTerminationLock . In the end, notifyQueryTermination requests StateStoreCoordinator to deactivate all active runs of the streaming query . notifyQueryTermination is used when StreamExecution is requested to run a streaming query and the query has finished (running streaming batches) (with or without an exception).","title":" Handling Termination of Streaming Query (and Deactivating Query in StateStoreCoordinator)"},{"location":"StreamingQueryManager/#active-streaming-queries-by-id","text":"Registry of StreamingQuery s per UUID Used when StreamingQueryManager is requested for active streaming queries , get a streaming query by id , starts a streaming query and is notified that a streaming query has terminated .","title":" Active Streaming Queries by ID"},{"location":"StreamingQueryManager/#last-terminated-streaming-query","text":"StreamingQuery that has recently been terminated (i.e. stopped or due to an exception ). null when no streaming query has terminated yet or resetTerminated . Used in awaitAnyTermination to know when a streaming query has terminated Set when StreamingQueryManager is notified that a streaming query has terminated","title":" Last-Terminated Streaming Query"},{"location":"StreamingQueryManager/#statestorecoordinatorref","text":"StateStoreCoordinatorRef to the StateStoreCoordinator RPC Endpoint Created when StreamingQueryManager is created Used when: StreamingQueryManager is notified that a streaming query has terminated Stateful operators are executed ( FlatMapGroupsWithStateExec , StateStoreRestoreExec , StateStoreSaveExec , StreamingDeduplicateExec and StreamingSymmetricHashJoinExec ) Creating StateStoreRDD (with storeUpdateFunction aborting StateStore when a task fails)","title":" StateStoreCoordinatorRef"},{"location":"StreamingQueryWrapper/","text":"StreamingQueryWrapper \u2014 Serializable StreamExecution \u00b6 StreamingQueryWrapper is a serializable interface of a StreamExecution . StreamingQueryWrapper has the same StreamExecution API and simply passes all the method calls along to the underlying StreamExecution . StreamingQueryWrapper is created when StreamingQueryManager is requested to create a streaming query (when DataStreamWriter is requested to start an execution of the streaming query ). Demo: Any Streaming Query is StreamingQueryWrapper \u00b6 import org . apache . spark . sql . execution . streaming . StreamingQueryWrapper val query = spark . readStream . format ( \"rate\" ) . load . writeStream . format ( \"memory\" ) . queryName ( \"rate2memory\" ) . start assert ( query . isInstanceOf [ StreamingQueryWrapper ])","title":"StreamingQueryWrapper"},{"location":"StreamingQueryWrapper/#streamingquerywrapper-serializable-streamexecution","text":"StreamingQueryWrapper is a serializable interface of a StreamExecution . StreamingQueryWrapper has the same StreamExecution API and simply passes all the method calls along to the underlying StreamExecution . StreamingQueryWrapper is created when StreamingQueryManager is requested to create a streaming query (when DataStreamWriter is requested to start an execution of the streaming query ).","title":"StreamingQueryWrapper &mdash; Serializable StreamExecution"},{"location":"StreamingQueryWrapper/#demo-any-streaming-query-is-streamingquerywrapper","text":"import org . apache . spark . sql . execution . streaming . StreamingQueryWrapper val query = spark . readStream . format ( \"rate\" ) . load . writeStream . format ( \"memory\" ) . queryName ( \"rate2memory\" ) . start assert ( query . isInstanceOf [ StreamingQueryWrapper ])","title":"Demo: Any Streaming Query is StreamingQueryWrapper"},{"location":"StreamingRelationStrategy/","text":"StreamingRelationStrategy Execution Planning Strategy \u00b6 [[apply]] StreamingRelationStrategy is an execution planning strategy that can plan streaming queries with StreamingRelation , StreamingExecutionRelation , and StreamingRelationV2 logical operators to StreamingRelationExec physical operators. StreamingRelationStrategy is used when IncrementalExecution is requested to plan a streaming query. StreamingRelationStrategy is available using SessionState (of a SparkSession ). spark . sessionState . planner . StreamingRelationStrategy Demo \u00b6 val rates = spark. readStream. format(\"rate\"). load // <-- gives a streaming Dataset with a logical plan with StreamingRelation logical operator // StreamingRelation logical operator for the rate streaming source scala> println(rates.queryExecution.logical.numberedTreeString) 00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@31ba0af0,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] // StreamingRelationExec physical operator (shown without \"Exec\" suffix) scala> rates.explain == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L] // Let's do the planning manually import spark.sessionState.planner.StreamingRelationStrategy val physicalPlan = StreamingRelationStrategy.apply(rates.queryExecution.logical).head scala> println(physicalPlan.numberedTreeString) 00 StreamingRelation rate, [timestamp#0, value#1L]","title":"StreamingRelationStrategy"},{"location":"StreamingRelationStrategy/#streamingrelationstrategy-execution-planning-strategy","text":"[[apply]] StreamingRelationStrategy is an execution planning strategy that can plan streaming queries with StreamingRelation , StreamingExecutionRelation , and StreamingRelationV2 logical operators to StreamingRelationExec physical operators. StreamingRelationStrategy is used when IncrementalExecution is requested to plan a streaming query. StreamingRelationStrategy is available using SessionState (of a SparkSession ). spark . sessionState . planner . StreamingRelationStrategy","title":"StreamingRelationStrategy Execution Planning Strategy"},{"location":"StreamingRelationStrategy/#demo","text":"val rates = spark. readStream. format(\"rate\"). load // <-- gives a streaming Dataset with a logical plan with StreamingRelation logical operator // StreamingRelation logical operator for the rate streaming source scala> println(rates.queryExecution.logical.numberedTreeString) 00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@31ba0af0,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] // StreamingRelationExec physical operator (shown without \"Exec\" suffix) scala> rates.explain == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L] // Let's do the planning manually import spark.sessionState.planner.StreamingRelationStrategy val physicalPlan = StreamingRelationStrategy.apply(rates.queryExecution.logical).head scala> println(physicalPlan.numberedTreeString) 00 StreamingRelation rate, [timestamp#0, value#1L]","title":"Demo"},{"location":"StreamingSymmetricHashJoinHelper/","text":"StreamingSymmetricHashJoinHelper Utility \u00b6 StreamingSymmetricHashJoinHelper is a Scala object with the following utility methods: getStateWatermarkPredicates === [[getStateWatermarkPredicates]] Creating JoinStateWatermarkPredicates -- getStateWatermarkPredicates Object Method [source, scala] \u00b6 getStateWatermarkPredicates( leftAttributes: Seq[Attribute], rightAttributes: Seq[Attribute], leftKeys: Seq[Expression], rightKeys: Seq[Expression], condition: Option[Expression], eventTimeWatermark: Option[Long]): JoinStateWatermarkPredicates [[getStateWatermarkPredicates-joinKeyOrdinalForWatermark]] getStateWatermarkPredicates tries to find the index of the watermark attribute among the left keys first, and if not found, the right keys. getStateWatermarkPredicates < > for the left side of a join (for the given leftAttributes , the leftKeys and the rightAttributes ). getStateWatermarkPredicates < > for the right side of a join (for the given rightAttributes , the rightKeys and the leftAttributes ). In the end, getStateWatermarkPredicates creates a JoinStateWatermarkPredicates with the left- and right-side state watermark predicates. NOTE: getStateWatermarkPredicates is used exclusively when IncrementalExecution is requested to apply the state preparation rule for batch-specific configuration (while optimizing query plans with StreamingSymmetricHashJoinExec physical operators). ==== [[getOneSideStateWatermarkPredicate]] Join State Watermark Predicate (for One Side of Join) -- getOneSideStateWatermarkPredicate Internal Method [source, scala] \u00b6 getOneSideStateWatermarkPredicate( oneSideInputAttributes: Seq[Attribute], oneSideJoinKeys: Seq[Expression], otherSideInputAttributes: Seq[Attribute]): Option[JoinStateWatermarkPredicate] getOneSideStateWatermarkPredicate finds what attributes were used to define the watermark attribute (the oneSideInputAttributes attributes, the < >) and creates a < > as follows: < > if the watermark was defined on a join key (with the watermark expression for the index of the join key expression) < > if the watermark was defined among the oneSideInputAttributes (with the state value watermark based on the given oneSideInputAttributes and otherSideInputAttributes ) NOTE: getOneSideStateWatermarkPredicate creates no < > ( None ) for no watermark found. NOTE: getStateWatermarkPredicates is used exclusively to < >.","title":"StreamingSymmetricHashJoinHelper"},{"location":"StreamingSymmetricHashJoinHelper/#streamingsymmetrichashjoinhelper-utility","text":"StreamingSymmetricHashJoinHelper is a Scala object with the following utility methods: getStateWatermarkPredicates === [[getStateWatermarkPredicates]] Creating JoinStateWatermarkPredicates -- getStateWatermarkPredicates Object Method","title":"StreamingSymmetricHashJoinHelper Utility"},{"location":"StreamingSymmetricHashJoinHelper/#source-scala","text":"getStateWatermarkPredicates( leftAttributes: Seq[Attribute], rightAttributes: Seq[Attribute], leftKeys: Seq[Expression], rightKeys: Seq[Expression], condition: Option[Expression], eventTimeWatermark: Option[Long]): JoinStateWatermarkPredicates [[getStateWatermarkPredicates-joinKeyOrdinalForWatermark]] getStateWatermarkPredicates tries to find the index of the watermark attribute among the left keys first, and if not found, the right keys. getStateWatermarkPredicates < > for the left side of a join (for the given leftAttributes , the leftKeys and the rightAttributes ). getStateWatermarkPredicates < > for the right side of a join (for the given rightAttributes , the rightKeys and the leftAttributes ). In the end, getStateWatermarkPredicates creates a JoinStateWatermarkPredicates with the left- and right-side state watermark predicates. NOTE: getStateWatermarkPredicates is used exclusively when IncrementalExecution is requested to apply the state preparation rule for batch-specific configuration (while optimizing query plans with StreamingSymmetricHashJoinExec physical operators). ==== [[getOneSideStateWatermarkPredicate]] Join State Watermark Predicate (for One Side of Join) -- getOneSideStateWatermarkPredicate Internal Method","title":"[source, scala]"},{"location":"StreamingSymmetricHashJoinHelper/#source-scala_1","text":"getOneSideStateWatermarkPredicate( oneSideInputAttributes: Seq[Attribute], oneSideJoinKeys: Seq[Expression], otherSideInputAttributes: Seq[Attribute]): Option[JoinStateWatermarkPredicate] getOneSideStateWatermarkPredicate finds what attributes were used to define the watermark attribute (the oneSideInputAttributes attributes, the < >) and creates a < > as follows: < > if the watermark was defined on a join key (with the watermark expression for the index of the join key expression) < > if the watermark was defined among the oneSideInputAttributes (with the state value watermark based on the given oneSideInputAttributes and otherSideInputAttributes ) NOTE: getOneSideStateWatermarkPredicate creates no < > ( None ) for no watermark found. NOTE: getStateWatermarkPredicates is used exclusively to < >.","title":"[source, scala]"},{"location":"SupportsAdmissionControl/","text":"SupportsAdmissionControl \u00b6 SupportsAdmissionControl is an extension of the SparkDataStream abstraction for streaming sources that want to control the rate of data ingested in Micro-Batch Stream Processing . Contract \u00b6 Default ReadLimit \u00b6 ReadLimit getDefaultReadLimit () Default: ReadLimit.allAvailable Used when MicroBatchExecution stream execution engine is requested for the analyzed logical plan (of the streaming query) Latest Offset \u00b6 Offset latestOffset ( Offset startOffset , ReadLimit limit ) Used when MicroBatchExecution stream execution engine is requested for the next micro-batch Implementations \u00b6 FileStreamSource KafkaMicroBatchStream KafkaSource","title":"SupportsAdmissionControl"},{"location":"SupportsAdmissionControl/#supportsadmissioncontrol","text":"SupportsAdmissionControl is an extension of the SparkDataStream abstraction for streaming sources that want to control the rate of data ingested in Micro-Batch Stream Processing .","title":"SupportsAdmissionControl"},{"location":"SupportsAdmissionControl/#contract","text":"","title":"Contract"},{"location":"SupportsAdmissionControl/#default-readlimit","text":"ReadLimit getDefaultReadLimit () Default: ReadLimit.allAvailable Used when MicroBatchExecution stream execution engine is requested for the analyzed logical plan (of the streaming query)","title":" Default ReadLimit"},{"location":"SupportsAdmissionControl/#latest-offset","text":"Offset latestOffset ( Offset startOffset , ReadLimit limit ) Used when MicroBatchExecution stream execution engine is requested for the next micro-batch","title":" Latest Offset"},{"location":"SupportsAdmissionControl/#implementations","text":"FileStreamSource KafkaMicroBatchStream KafkaSource","title":"Implementations"},{"location":"SymmetricHashJoinStateManager/","text":"SymmetricHashJoinStateManager \u00b6 SymmetricHashJoinStateManager is < > for the left and right OneSideHashJoiners of a StreamingSymmetricHashJoinExec physical operator (one for each side when StreamingSymmetricHashJoinExec is requested to process partitions of the left and right sides of a stream-stream join ). SymmetricHashJoinStateManager manages join state using the < > and the < > state store handlers (and simply acts like their facade). Creating Instance \u00b6 SymmetricHashJoinStateManager takes the following to be created: [[joinSide]] JoinSide [[inputValueAttributes]] Attributes of input values [[joinKeys]] Join keys ( Seq[Expression] ) [[stateInfo]] StatefulOperatorStateInfo [[storeConf]] StateStoreConf [[hadoopConf]] Hadoop Configuration === [[keyToNumValues]][[keyWithIndexToValue]] KeyToNumValuesStore and KeyWithIndexToValueStore State Store Handlers -- keyToNumValues and keyWithIndexToValue Internal Properties SymmetricHashJoinStateManager uses a < > ( keyToNumValues ) and a < > ( keyWithIndexToValue ) internally that are created immediately when SymmetricHashJoinStateManager is < > (for a OneSideHashJoiner ). keyToNumValues and keyWithIndexToValue are used when SymmetricHashJoinStateManager is requested for the following: < > < > < > < > < > < > < > === [[joinSide-internals]] Join Side Marker -- JoinSide Internal Enum JoinSide can be one of the two possible values: [[LeftSide]][[left]] LeftSide (alias: left ) [[RightSide]][[right]] RightSide (alias: right ) They are both used exclusively when StreamingSymmetricHashJoinExec binary physical operator is requested to < > (and < > with an OneSideHashJoiner ). === [[metrics]] Performance Metrics -- metrics Method [source, scala] \u00b6 metrics: StateStoreMetrics \u00b6 metrics returns the combined < > of the < > and the < > state store handlers. metrics is used when OneSideHashJoiner is requested to commitStateAndGetMetrics . === [[removeByKeyCondition]] removeByKeyCondition Method [source, scala] \u00b6 removeByKeyCondition( removalCondition: UnsafeRow => Boolean): Iterator[UnsafeRowPair] removeByKeyCondition creates an Iterator of UnsafeRowPairs that < > for which the given removalCondition predicate holds. [[removeByKeyCondition-allKeyToNumValues]] removeByKeyCondition uses the < > for < >. removeByKeyCondition is used when OneSideHashJoiner is requested to remove an old state (for JoinStateKeyWatermarkPredicate ). ==== [[removeByKeyCondition-getNext]] getNext Internal Method (of removeByKeyCondition Method) [source, scala] \u00b6 getNext(): UnsafeRowPair \u00b6 getNext goes over the keys and values in the < > sequence and < > (from the < >) and the < > (from the < >) for which the given removalCondition predicate holds. === [[removeByValueCondition]] removeByValueCondition Method [source, scala] \u00b6 removeByValueCondition( removalCondition: UnsafeRow => Boolean): Iterator[UnsafeRowPair] removeByValueCondition creates an Iterator of UnsafeRowPairs that < > for which the given removalCondition predicate holds. removeByValueCondition is used when OneSideHashJoiner is requested to remove an old state (when JoinStateValueWatermarkPredicate is used). ==== [[removeByValueCondition-getNext]] getNext Internal Method (of removeByValueCondition Method) [source, scala] \u00b6 getNext(): UnsafeRowPair \u00b6 getNext ...FIXME === [[append]] Appending New Value Row to Key -- append Method [source, scala] \u00b6 append( key: UnsafeRow, value: UnsafeRow): Unit append requests the < > for the < >. In the end, append requests the stores for the following: < > to < > < > to < >. append is used when OneSideHashJoiner is requested to storeAndJoinWithOtherSide . === [[get]] Retrieving Value Rows By Key -- get Method [source, scala] \u00b6 get(key: UnsafeRow): Iterator[UnsafeRow] \u00b6 get requests the < > for the < >. In the end, get requests the < > to < > and leaves value rows only. get is used when OneSideHashJoiner is requested to storeAndJoinWithOtherSide and retrieving value rows for a key . === [[commit]] Committing State (Changes) -- commit Method [source, scala] \u00b6 commit(): Unit \u00b6 commit simply requests the < > and < > state store handlers to < >. commit is used when OneSideHashJoiner is requested to commit state changes and get performance metrics . === [[abortIfNeeded]] Aborting State (Changes) -- abortIfNeeded Method [source, scala] \u00b6 abortIfNeeded(): Unit \u00b6 abortIfNeeded ...FIXME NOTE: abortIfNeeded is used when...FIXME === [[allStateStoreNames]] allStateStoreNames Object Method [source, scala] \u00b6 allStateStoreNames(joinSides: JoinSide*): Seq[String] \u00b6 allStateStoreNames simply returns the < > for all possible combinations of the given JoinSides and the two possible store types (e.g. < > and < >). NOTE: allStateStoreNames is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < > (as a RDD[InternalRow] ). === [[getStateStoreName]] getStateStoreName Object Method [source, scala] \u00b6 getStateStoreName( joinSide: JoinSide, storeType: StateStoreType): String getStateStoreName simply returns a string of the following format: [joinSide]-[storeType] [NOTE] \u00b6 getStateStoreName is used when: StateStoreHandler is requested to < > * SymmetricHashJoinStateManager utility is requested for < > (for StreamingSymmetricHashJoinExec physical operator to < >) \u00b6 === [[updateNumValueForCurrentKey]] updateNumValueForCurrentKey Internal Method [source, scala] \u00b6 updateNumValueForCurrentKey(): Unit \u00b6 updateNumValueForCurrentKey ...FIXME NOTE: updateNumValueForCurrentKey is used exclusively when SymmetricHashJoinStateManager is requested to < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | keyAttributes a| [[keyAttributes]] Key attributes, i.e. AttributeReferences of the < > Used exclusively in KeyWithIndexToValueStore when requested for the < >, < >, < > and < > | keySchema a| [[keySchema]] Key schema ( StructType ) based on the < > with the names in the format of field and their ordinals (index) Used when: SymmetricHashJoinStateManager is requested for the < > (for < >) KeyToNumValuesStore is requested for the < > KeyWithIndexToValueStore is requested for the < > (for the internal < >) |===","title":"SymmetricHashJoinStateManager"},{"location":"SymmetricHashJoinStateManager/#symmetrichashjoinstatemanager","text":"SymmetricHashJoinStateManager is < > for the left and right OneSideHashJoiners of a StreamingSymmetricHashJoinExec physical operator (one for each side when StreamingSymmetricHashJoinExec is requested to process partitions of the left and right sides of a stream-stream join ). SymmetricHashJoinStateManager manages join state using the < > and the < > state store handlers (and simply acts like their facade).","title":"SymmetricHashJoinStateManager"},{"location":"SymmetricHashJoinStateManager/#creating-instance","text":"SymmetricHashJoinStateManager takes the following to be created: [[joinSide]] JoinSide [[inputValueAttributes]] Attributes of input values [[joinKeys]] Join keys ( Seq[Expression] ) [[stateInfo]] StatefulOperatorStateInfo [[storeConf]] StateStoreConf [[hadoopConf]] Hadoop Configuration === [[keyToNumValues]][[keyWithIndexToValue]] KeyToNumValuesStore and KeyWithIndexToValueStore State Store Handlers -- keyToNumValues and keyWithIndexToValue Internal Properties SymmetricHashJoinStateManager uses a < > ( keyToNumValues ) and a < > ( keyWithIndexToValue ) internally that are created immediately when SymmetricHashJoinStateManager is < > (for a OneSideHashJoiner ). keyToNumValues and keyWithIndexToValue are used when SymmetricHashJoinStateManager is requested for the following: < > < > < > < > < > < > < > === [[joinSide-internals]] Join Side Marker -- JoinSide Internal Enum JoinSide can be one of the two possible values: [[LeftSide]][[left]] LeftSide (alias: left ) [[RightSide]][[right]] RightSide (alias: right ) They are both used exclusively when StreamingSymmetricHashJoinExec binary physical operator is requested to < > (and < > with an OneSideHashJoiner ). === [[metrics]] Performance Metrics -- metrics Method","title":"Creating Instance"},{"location":"SymmetricHashJoinStateManager/#source-scala","text":"","title":"[source, scala]"},{"location":"SymmetricHashJoinStateManager/#metrics-statestoremetrics","text":"metrics returns the combined < > of the < > and the < > state store handlers. metrics is used when OneSideHashJoiner is requested to commitStateAndGetMetrics . === [[removeByKeyCondition]] removeByKeyCondition Method","title":"metrics: StateStoreMetrics"},{"location":"SymmetricHashJoinStateManager/#source-scala_1","text":"removeByKeyCondition( removalCondition: UnsafeRow => Boolean): Iterator[UnsafeRowPair] removeByKeyCondition creates an Iterator of UnsafeRowPairs that < > for which the given removalCondition predicate holds. [[removeByKeyCondition-allKeyToNumValues]] removeByKeyCondition uses the < > for < >. removeByKeyCondition is used when OneSideHashJoiner is requested to remove an old state (for JoinStateKeyWatermarkPredicate ). ==== [[removeByKeyCondition-getNext]] getNext Internal Method (of removeByKeyCondition Method)","title":"[source, scala]"},{"location":"SymmetricHashJoinStateManager/#source-scala_2","text":"","title":"[source, scala]"},{"location":"SymmetricHashJoinStateManager/#getnext-unsaferowpair","text":"getNext goes over the keys and values in the < > sequence and < > (from the < >) and the < > (from the < >) for which the given removalCondition predicate holds. === [[removeByValueCondition]] removeByValueCondition Method","title":"getNext(): UnsafeRowPair"},{"location":"SymmetricHashJoinStateManager/#source-scala_3","text":"removeByValueCondition( removalCondition: UnsafeRow => Boolean): Iterator[UnsafeRowPair] removeByValueCondition creates an Iterator of UnsafeRowPairs that < > for which the given removalCondition predicate holds. removeByValueCondition is used when OneSideHashJoiner is requested to remove an old state (when JoinStateValueWatermarkPredicate is used). ==== [[removeByValueCondition-getNext]] getNext Internal Method (of removeByValueCondition Method)","title":"[source, scala]"},{"location":"SymmetricHashJoinStateManager/#source-scala_4","text":"","title":"[source, scala]"},{"location":"SymmetricHashJoinStateManager/#getnext-unsaferowpair_1","text":"getNext ...FIXME === [[append]] Appending New Value Row to Key -- append Method","title":"getNext(): UnsafeRowPair"},{"location":"SymmetricHashJoinStateManager/#source-scala_5","text":"append( key: UnsafeRow, value: UnsafeRow): Unit append requests the < > for the < >. In the end, append requests the stores for the following: < > to < > < > to < >. append is used when OneSideHashJoiner is requested to storeAndJoinWithOtherSide . === [[get]] Retrieving Value Rows By Key -- get Method","title":"[source, scala]"},{"location":"SymmetricHashJoinStateManager/#source-scala_6","text":"","title":"[source, scala]"},{"location":"SymmetricHashJoinStateManager/#getkey-unsaferow-iteratorunsaferow","text":"get requests the < > for the < >. In the end, get requests the < > to < > and leaves value rows only. get is used when OneSideHashJoiner is requested to storeAndJoinWithOtherSide and retrieving value rows for a key . === [[commit]] Committing State (Changes) -- commit Method","title":"get(key: UnsafeRow): Iterator[UnsafeRow]"},{"location":"SymmetricHashJoinStateManager/#source-scala_7","text":"","title":"[source, scala]"},{"location":"SymmetricHashJoinStateManager/#commit-unit","text":"commit simply requests the < > and < > state store handlers to < >. commit is used when OneSideHashJoiner is requested to commit state changes and get performance metrics . === [[abortIfNeeded]] Aborting State (Changes) -- abortIfNeeded Method","title":"commit(): Unit"},{"location":"SymmetricHashJoinStateManager/#source-scala_8","text":"","title":"[source, scala]"},{"location":"SymmetricHashJoinStateManager/#abortifneeded-unit","text":"abortIfNeeded ...FIXME NOTE: abortIfNeeded is used when...FIXME === [[allStateStoreNames]] allStateStoreNames Object Method","title":"abortIfNeeded(): Unit"},{"location":"SymmetricHashJoinStateManager/#source-scala_9","text":"","title":"[source, scala]"},{"location":"SymmetricHashJoinStateManager/#allstatestorenamesjoinsides-joinside-seqstring","text":"allStateStoreNames simply returns the < > for all possible combinations of the given JoinSides and the two possible store types (e.g. < > and < >). NOTE: allStateStoreNames is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < > (as a RDD[InternalRow] ). === [[getStateStoreName]] getStateStoreName Object Method","title":"allStateStoreNames(joinSides: JoinSide*): Seq[String]"},{"location":"SymmetricHashJoinStateManager/#source-scala_10","text":"getStateStoreName( joinSide: JoinSide, storeType: StateStoreType): String getStateStoreName simply returns a string of the following format: [joinSide]-[storeType]","title":"[source, scala]"},{"location":"SymmetricHashJoinStateManager/#note","text":"getStateStoreName is used when: StateStoreHandler is requested to < >","title":"[NOTE]"},{"location":"SymmetricHashJoinStateManager/#symmetrichashjoinstatemanager-utility-is-requested-for-for-streamingsymmetrichashjoinexec-physical-operator-to","text":"=== [[updateNumValueForCurrentKey]] updateNumValueForCurrentKey Internal Method","title":"* SymmetricHashJoinStateManager utility is requested for &lt;&gt; (for StreamingSymmetricHashJoinExec physical operator to &lt;&gt;)"},{"location":"SymmetricHashJoinStateManager/#source-scala_11","text":"","title":"[source, scala]"},{"location":"SymmetricHashJoinStateManager/#updatenumvalueforcurrentkey-unit","text":"updateNumValueForCurrentKey ...FIXME NOTE: updateNumValueForCurrentKey is used exclusively when SymmetricHashJoinStateManager is requested to < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | keyAttributes a| [[keyAttributes]] Key attributes, i.e. AttributeReferences of the < > Used exclusively in KeyWithIndexToValueStore when requested for the < >, < >, < > and < > | keySchema a| [[keySchema]] Key schema ( StructType ) based on the < > with the names in the format of field and their ordinals (index) Used when: SymmetricHashJoinStateManager is requested for the < > (for < >) KeyToNumValuesStore is requested for the < > KeyWithIndexToValueStore is requested for the < > (for the internal < >) |===","title":"updateNumValueForCurrentKey(): Unit"},{"location":"Trigger/","text":"Trigger \u00b6 Trigger defines how often a streaming query should be executed ( triggered ) and emit a new data (which StreamExecution uses to resolve a TriggerExecutor ). [[available-implementations]] [[available-triggers]] [[triggers]] .Trigger's Factory Methods [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Trigger | Creating Instance | ContinuousTrigger a| [[ContinuousTrigger]][[Continuous]] [source, java] \u00b6 Trigger Continuous(long intervalMs) Trigger Continuous(long interval, TimeUnit timeUnit) Trigger Continuous(Duration interval) Trigger Continuous(String interval) | OneTimeTrigger a| [[OneTimeTrigger]][[Once]] [source, java] \u00b6 Trigger Once() \u00b6 | ProcessingTime a| [[ProcessingTime]] [source, java] \u00b6 Trigger ProcessingTime(Duration interval) Trigger ProcessingTime(long intervalMs) Trigger ProcessingTime(long interval, TimeUnit timeUnit) Trigger ProcessingTime(String interval) < > |=== NOTE: You specify the trigger for a streaming query using DataStreamWriter 's trigger method. [source, scala] \u00b6 import org.apache.spark.sql.streaming.Trigger val query = spark. readStream. format(\"rate\"). load. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.Once). // \u2190 execute once and stop queryName(\"rate-once\"). start assert(query.isActive == false) scala> println(query.lastProgress) { \"id\" : \"2ae4b0a4-434f-4ca7-a523-4e859c07175b\", \"runId\" : \"24039ce5-906c-4f90-b6e7-bbb3ec38a1f5\", \"name\" : \"rate-once\", \"timestamp\" : \"2017-07-04T18:39:35.998Z\", \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0, \"durationMs\" : { \"addBatch\" : 1365, \"getBatch\" : 29, \"getOffset\" : 0, \"queryPlanning\" : 285, \"triggerExecution\" : 1742, \"walCommit\" : 40 }, \"stateOperators\" : [ ], \"sources\" : [ { \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\", \"startOffset\" : null, \"endOffset\" : 0, \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0 } ], \"sink\" : { \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@7dbf277\" } } NOTE: Although Trigger allows for custom implementations, StreamExecution StreamExecution.md#triggerExecutor[refuses such attempts] and reports an IllegalStateException . [source, scala] \u00b6 import org.apache.spark.sql.streaming.Trigger case object MyTrigger extends Trigger scala> val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .trigger(MyTrigger) // \u2190 use custom trigger .queryName(\"rate-custom-trigger\") .start java.lang.IllegalStateException: Unknown type of trigger: MyTrigger at org.apache.spark.sql.execution.streaming.MicroBatchExecution. (MicroBatchExecution.scala:60) at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:275) at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:316) at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325) ... 57 elided NOTE: Trigger was introduced in https://github.com/apache/spark/commit/855ed44ed31210d2001d7ce67c8fa99f8416edd3[the commit for [SPARK-14176][SQL] Add DataFrameWriter.trigger to set the stream batch period]. === [[ProcessingTime-examples]] Examples of ProcessingTime ProcessingTime is a Trigger that assumes that milliseconds is the minimum time unit. You can create an instance of ProcessingTime using the following constructors: ProcessingTime(Long) that accepts non-negative values that represent milliseconds. + ProcessingTime(10) ProcessingTime(interval: String) or ProcessingTime.create(interval: String) that accept CalendarInterval instances with or without leading interval string. + ProcessingTime(\"10 milliseconds\") ProcessingTime(\"interval 10 milliseconds\") ProcessingTime(Duration) that accepts scala.concurrent.duration.Duration instances. + ProcessingTime(10.seconds) ProcessingTime.create(interval: Long, unit: TimeUnit) for Long and java.util.concurrent.TimeUnit instances. + ProcessingTime.create(10, TimeUnit.SECONDS)","title":"Trigger"},{"location":"Trigger/#trigger","text":"Trigger defines how often a streaming query should be executed ( triggered ) and emit a new data (which StreamExecution uses to resolve a TriggerExecutor ). [[available-implementations]] [[available-triggers]] [[triggers]] .Trigger's Factory Methods [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Trigger | Creating Instance | ContinuousTrigger a| [[ContinuousTrigger]][[Continuous]]","title":"Trigger"},{"location":"Trigger/#source-java","text":"Trigger Continuous(long intervalMs) Trigger Continuous(long interval, TimeUnit timeUnit) Trigger Continuous(Duration interval) Trigger Continuous(String interval) | OneTimeTrigger a| [[OneTimeTrigger]][[Once]]","title":"[source, java]"},{"location":"Trigger/#source-java_1","text":"","title":"[source, java]"},{"location":"Trigger/#trigger-once","text":"| ProcessingTime a| [[ProcessingTime]]","title":"Trigger Once()"},{"location":"Trigger/#source-java_2","text":"Trigger ProcessingTime(Duration interval) Trigger ProcessingTime(long intervalMs) Trigger ProcessingTime(long interval, TimeUnit timeUnit) Trigger ProcessingTime(String interval) < > |=== NOTE: You specify the trigger for a streaming query using DataStreamWriter 's trigger method.","title":"[source, java]"},{"location":"Trigger/#source-scala","text":"import org.apache.spark.sql.streaming.Trigger val query = spark. readStream. format(\"rate\"). load. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.Once). // \u2190 execute once and stop queryName(\"rate-once\"). start assert(query.isActive == false) scala> println(query.lastProgress) { \"id\" : \"2ae4b0a4-434f-4ca7-a523-4e859c07175b\", \"runId\" : \"24039ce5-906c-4f90-b6e7-bbb3ec38a1f5\", \"name\" : \"rate-once\", \"timestamp\" : \"2017-07-04T18:39:35.998Z\", \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0, \"durationMs\" : { \"addBatch\" : 1365, \"getBatch\" : 29, \"getOffset\" : 0, \"queryPlanning\" : 285, \"triggerExecution\" : 1742, \"walCommit\" : 40 }, \"stateOperators\" : [ ], \"sources\" : [ { \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\", \"startOffset\" : null, \"endOffset\" : 0, \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0 } ], \"sink\" : { \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@7dbf277\" } } NOTE: Although Trigger allows for custom implementations, StreamExecution StreamExecution.md#triggerExecutor[refuses such attempts] and reports an IllegalStateException .","title":"[source, scala]"},{"location":"Trigger/#source-scala_1","text":"import org.apache.spark.sql.streaming.Trigger case object MyTrigger extends Trigger scala> val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .trigger(MyTrigger) // \u2190 use custom trigger .queryName(\"rate-custom-trigger\") .start java.lang.IllegalStateException: Unknown type of trigger: MyTrigger at org.apache.spark.sql.execution.streaming.MicroBatchExecution. (MicroBatchExecution.scala:60) at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:275) at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:316) at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325) ... 57 elided NOTE: Trigger was introduced in https://github.com/apache/spark/commit/855ed44ed31210d2001d7ce67c8fa99f8416edd3[the commit for [SPARK-14176][SQL] Add DataFrameWriter.trigger to set the stream batch period]. === [[ProcessingTime-examples]] Examples of ProcessingTime ProcessingTime is a Trigger that assumes that milliseconds is the minimum time unit. You can create an instance of ProcessingTime using the following constructors: ProcessingTime(Long) that accepts non-negative values that represent milliseconds. + ProcessingTime(10) ProcessingTime(interval: String) or ProcessingTime.create(interval: String) that accept CalendarInterval instances with or without leading interval string. + ProcessingTime(\"10 milliseconds\") ProcessingTime(\"interval 10 milliseconds\") ProcessingTime(Duration) that accepts scala.concurrent.duration.Duration instances. + ProcessingTime(10.seconds) ProcessingTime.create(interval: Long, unit: TimeUnit) for Long and java.util.concurrent.TimeUnit instances. + ProcessingTime.create(10, TimeUnit.SECONDS)","title":"[source, scala]"},{"location":"TriggerExecutor/","text":"== [[TriggerExecutor]] TriggerExecutor TriggerExecutor is the < > for trigger executors that StreamExecution StreamExecution.md#runStream[uses to execute a batch runner]. [[batchRunner]] NOTE: Batch runner is an executable code that is executed at regular intervals. It is also called a trigger handler . [[contract]] [[execute]] [source, scala] package org.apache.spark.sql.execution.streaming trait TriggerExecutor { def execute(batchRunner: () => Boolean): Unit } NOTE: StreamExecution reports a IllegalStateException when StreamExecution.md#triggerExecutor[TriggerExecutor] is different from the < >: OneTimeExecutor or ProcessingTimeExecutor . [[available-implementations]] .TriggerExecutor's Available Implementations [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | TriggerExecutor | Description | [[OneTimeExecutor]] OneTimeExecutor | Executes batchRunner exactly once. | [[ProcessingTimeExecutor]] ProcessingTimeExecutor a| Executes batchRunner at regular intervals (as defined using ProcessingTime and DataStreamWriter.trigger method). [source, scala] \u00b6 ProcessingTimeExecutor( processingTime: ProcessingTime, clock: Clock = new SystemClock()) NOTE: Processing terminates when batchRunner returns false . |=== === [[notifyBatchFallingBehind]] notifyBatchFallingBehind Method CAUTION: FIXME","title":"TriggerExecutor"},{"location":"TriggerExecutor/#source-scala","text":"ProcessingTimeExecutor( processingTime: ProcessingTime, clock: Clock = new SystemClock()) NOTE: Processing terminates when batchRunner returns false . |=== === [[notifyBatchFallingBehind]] notifyBatchFallingBehind Method CAUTION: FIXME","title":"[source, scala]"},{"location":"UnsupportedOperationChecker/","text":"UnsupportedOperationChecker \u00b6 UnsupportedOperationChecker checks whether the logical plan of a streaming query uses supported operations only . UnsupportedOperationChecker is used when the internal spark.sql.streaming.unsupportedOperationCheck Spark property is enabled. Note UnsupportedOperationChecker comes actually with two methods, i.e. checkForBatch and < >, whose names reveal the different flavours of Spark SQL (as of 2.0), i.e. batch and streaming, respectively. The Spark Structured Streaming gitbook is solely focused on < > method. checkForStreaming Method \u00b6 checkForStreaming ( plan : LogicalPlan , outputMode : OutputMode ): Unit checkForStreaming asserts that the following requirements hold: < > < > (on the grouping expressions) < > checkForStreaming ...FIXME checkForStreaming finds all streaming aggregates (i.e. Aggregate logical operators with streaming sources). Note Aggregate logical operator represents Dataset.groupBy and Dataset.groupByKey operators (and SQL's GROUP BY clause) in a logical query plan. [[only-one-streaming-aggregation-allowed]] checkForStreaming asserts that there is exactly one streaming aggregation in a streaming query. Otherwise, checkForStreaming reports a AnalysisException : Multiple streaming aggregations are not supported with streaming DataFrames/Datasets [[streaming-aggregation-append-mode-requires-watermark]] checkForStreaming asserts that watermark was defined for a streaming aggregation with Append output mode (on at least one of the grouping expressions). Otherwise, checkForStreaming reports a AnalysisException : Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark CAUTION: FIXME checkForStreaming counts all FlatMapGroupsWithState logical operators (on streaming Datasets with isMapGroupsWithState flag disabled). Note FlatMapGroupsWithState.isMapGroupsWithState flag is disabled when...FIXME [[multiple-flatMapGroupsWithState]] checkForStreaming asserts that multiple FlatMapGroupsWithState logical operators are only used when: outputMode is Append output mode outputMode of the FlatMapGroupsWithState logical operators is also Append output mode CAUTION: FIXME Reference to an example in flatMapGroupsWithState Otherwise, checkForStreaming reports a AnalysisException : Multiple flatMapGroupsWithStates are not supported when they are not all in append mode or the output mode is not append on a streaming DataFrames/Datasets CAUTION: FIXME checkForStreaming is used when StreamingQueryManager is requested to create a StreamingQueryWrapper (for starting a streaming query), but only when the internal spark.sql.streaming.unsupportedOperationCheck configuration property is enabled.","title":"UnsupportedOperationChecker"},{"location":"UnsupportedOperationChecker/#unsupportedoperationchecker","text":"UnsupportedOperationChecker checks whether the logical plan of a streaming query uses supported operations only . UnsupportedOperationChecker is used when the internal spark.sql.streaming.unsupportedOperationCheck Spark property is enabled. Note UnsupportedOperationChecker comes actually with two methods, i.e. checkForBatch and < >, whose names reveal the different flavours of Spark SQL (as of 2.0), i.e. batch and streaming, respectively. The Spark Structured Streaming gitbook is solely focused on < > method.","title":"UnsupportedOperationChecker"},{"location":"UnsupportedOperationChecker/#checkforstreaming-method","text":"checkForStreaming ( plan : LogicalPlan , outputMode : OutputMode ): Unit checkForStreaming asserts that the following requirements hold: < > < > (on the grouping expressions) < > checkForStreaming ...FIXME checkForStreaming finds all streaming aggregates (i.e. Aggregate logical operators with streaming sources). Note Aggregate logical operator represents Dataset.groupBy and Dataset.groupByKey operators (and SQL's GROUP BY clause) in a logical query plan. [[only-one-streaming-aggregation-allowed]] checkForStreaming asserts that there is exactly one streaming aggregation in a streaming query. Otherwise, checkForStreaming reports a AnalysisException : Multiple streaming aggregations are not supported with streaming DataFrames/Datasets [[streaming-aggregation-append-mode-requires-watermark]] checkForStreaming asserts that watermark was defined for a streaming aggregation with Append output mode (on at least one of the grouping expressions). Otherwise, checkForStreaming reports a AnalysisException : Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark CAUTION: FIXME checkForStreaming counts all FlatMapGroupsWithState logical operators (on streaming Datasets with isMapGroupsWithState flag disabled). Note FlatMapGroupsWithState.isMapGroupsWithState flag is disabled when...FIXME [[multiple-flatMapGroupsWithState]] checkForStreaming asserts that multiple FlatMapGroupsWithState logical operators are only used when: outputMode is Append output mode outputMode of the FlatMapGroupsWithState logical operators is also Append output mode CAUTION: FIXME Reference to an example in flatMapGroupsWithState Otherwise, checkForStreaming reports a AnalysisException : Multiple flatMapGroupsWithStates are not supported when they are not all in append mode or the output mode is not append on a streaming DataFrames/Datasets CAUTION: FIXME checkForStreaming is used when StreamingQueryManager is requested to create a StreamingQueryWrapper (for starting a streaming query), but only when the internal spark.sql.streaming.unsupportedOperationCheck configuration property is enabled.","title":" checkForStreaming Method"},{"location":"WatermarkSupport/","text":"WatermarkSupport Unary Physical Operators \u00b6 WatermarkSupport is the < > of unary physical operators ( UnaryExecNode ) with support for streaming event-time watermark. [NOTE] \u00b6 Watermark (aka \"allowed lateness\" ) is a moving threshold of event time and specifies what data to consider for aggregations, i.e. the threshold of late data so the engine can automatically drop incoming late data given event time and clean up old state accordingly. Read the official documentation of Spark in http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking[Handling Late Data and Watermarking]. \u00b6 [[properties]] .WatermarkSupport's (Lazily-Initialized) Properties [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Property | Description | [[watermarkExpression]] watermarkExpression a| Optional Catalyst expression that matches rows older than the event time watermark. Note Use withWatermark operator to specify streaming watermark. When initialized, watermarkExpression finds spark.watermarkDelayMs watermark attribute in the child output's metadata. If found, watermarkExpression creates evictionExpression with the watermark attribute that is less than or equal < >. The watermark attribute may be of type StructType . If it is, watermarkExpression uses the first field as the watermark. watermarkExpression prints out the following INFO message to the logs when spark.watermarkDelayMs watermark attribute is found. INFO [physicalOperator]Exec: Filtering state store on: [evictionExpression] NOTE: physicalOperator can be FlatMapGroupsWithStateExec , StateStoreSaveExec.md[StateStoreSaveExec] or physical-operators/StreamingDeduplicateExec.md[StreamingDeduplicateExec]. TIP: Enable INFO logging level for one of the stateful physical operators to see the INFO message in the logs. | [[watermarkPredicateForData]] watermarkPredicateForData | Optional Predicate that uses < > and the child output to match rows older than the event-time watermark | [[watermarkPredicateForKeys]] watermarkPredicateForKeys | Optional Predicate that uses < > to match rows older than the event time watermark. |=== === [[contract]] WatermarkSupport Contract [source, scala] \u00b6 package org.apache.spark.sql.execution.streaming trait WatermarkSupport extends UnaryExecNode { // only required methods that have no implementation def eventTimeWatermark: Option[Long] def keyExpressions: Seq[Attribute] } .WatermarkSupport Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | [[eventTimeWatermark]] eventTimeWatermark | Used mainly in < > to create a LessThanOrEqual Catalyst binary expression that matches rows older than the watermark. | [[keyExpressions]] keyExpressions | Grouping keys (in FlatMapGroupsWithStateExec ), duplicate keys (in physical-operators/StreamingDeduplicateExec.md#keyExpressions[StreamingDeduplicateExec]) or key attributes (in StateStoreSaveExec.md#keyExpressions[StateStoreSaveExec]) with at most one that may have spark.watermarkDelayMs watermark attribute in metadata Used in < > to create a Predicate to match rows older than the event time watermark. Used also when StateStoreSaveExec.md#doExecute[StateStoreSaveExec] and physical-operators/StreamingDeduplicateExec.md#doExecute[StreamingDeduplicateExec] physical operators are executed. |=== === [[removeKeysOlderThanWatermark]][[removeKeysOlderThanWatermark-StateStore]] Removing Keys From StateStore Older Than Watermark -- removeKeysOlderThanWatermark Method [source, scala] \u00b6 removeKeysOlderThanWatermark(store: StateStore): Unit \u00b6 removeKeysOlderThanWatermark requests the input store for all rows . removeKeysOlderThanWatermark then uses watermarkPredicateForKeys to remove matching rows from the store . removeKeysOlderThanWatermark is used when StreamingDeduplicateExec physical operator is requested to execute. === [[removeKeysOlderThanWatermark-StreamingAggregationStateManager-store]] removeKeysOlderThanWatermark Method [source, scala] \u00b6 removeKeysOlderThanWatermark( storeManager: StreamingAggregationStateManager, store: StateStore): Unit removeKeysOlderThanWatermark ...FIXME NOTE: removeKeysOlderThanWatermark is used exclusively when StateStoreSaveExec physical operator is requested to < >.","title":"WatermarkSupport"},{"location":"WatermarkSupport/#watermarksupport-unary-physical-operators","text":"WatermarkSupport is the < > of unary physical operators ( UnaryExecNode ) with support for streaming event-time watermark.","title":"WatermarkSupport Unary Physical Operators"},{"location":"WatermarkSupport/#note","text":"Watermark (aka \"allowed lateness\" ) is a moving threshold of event time and specifies what data to consider for aggregations, i.e. the threshold of late data so the engine can automatically drop incoming late data given event time and clean up old state accordingly.","title":"[NOTE]"},{"location":"WatermarkSupport/#read-the-official-documentation-of-spark-in-httpsparkapacheorgdocslateststructured-streaming-programming-guidehtmlhandling-late-data-and-watermarkinghandling-late-data-and-watermarking","text":"[[properties]] .WatermarkSupport's (Lazily-Initialized) Properties [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Property | Description | [[watermarkExpression]] watermarkExpression a| Optional Catalyst expression that matches rows older than the event time watermark. Note Use withWatermark operator to specify streaming watermark. When initialized, watermarkExpression finds spark.watermarkDelayMs watermark attribute in the child output's metadata. If found, watermarkExpression creates evictionExpression with the watermark attribute that is less than or equal < >. The watermark attribute may be of type StructType . If it is, watermarkExpression uses the first field as the watermark. watermarkExpression prints out the following INFO message to the logs when spark.watermarkDelayMs watermark attribute is found. INFO [physicalOperator]Exec: Filtering state store on: [evictionExpression] NOTE: physicalOperator can be FlatMapGroupsWithStateExec , StateStoreSaveExec.md[StateStoreSaveExec] or physical-operators/StreamingDeduplicateExec.md[StreamingDeduplicateExec]. TIP: Enable INFO logging level for one of the stateful physical operators to see the INFO message in the logs. | [[watermarkPredicateForData]] watermarkPredicateForData | Optional Predicate that uses < > and the child output to match rows older than the event-time watermark | [[watermarkPredicateForKeys]] watermarkPredicateForKeys | Optional Predicate that uses < > to match rows older than the event time watermark. |=== === [[contract]] WatermarkSupport Contract","title":"Read the official documentation of Spark in http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking[Handling Late Data and Watermarking]."},{"location":"WatermarkSupport/#source-scala","text":"package org.apache.spark.sql.execution.streaming trait WatermarkSupport extends UnaryExecNode { // only required methods that have no implementation def eventTimeWatermark: Option[Long] def keyExpressions: Seq[Attribute] } .WatermarkSupport Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | [[eventTimeWatermark]] eventTimeWatermark | Used mainly in < > to create a LessThanOrEqual Catalyst binary expression that matches rows older than the watermark. | [[keyExpressions]] keyExpressions | Grouping keys (in FlatMapGroupsWithStateExec ), duplicate keys (in physical-operators/StreamingDeduplicateExec.md#keyExpressions[StreamingDeduplicateExec]) or key attributes (in StateStoreSaveExec.md#keyExpressions[StateStoreSaveExec]) with at most one that may have spark.watermarkDelayMs watermark attribute in metadata Used in < > to create a Predicate to match rows older than the event time watermark. Used also when StateStoreSaveExec.md#doExecute[StateStoreSaveExec] and physical-operators/StreamingDeduplicateExec.md#doExecute[StreamingDeduplicateExec] physical operators are executed. |=== === [[removeKeysOlderThanWatermark]][[removeKeysOlderThanWatermark-StateStore]] Removing Keys From StateStore Older Than Watermark -- removeKeysOlderThanWatermark Method","title":"[source, scala]"},{"location":"WatermarkSupport/#source-scala_1","text":"","title":"[source, scala]"},{"location":"WatermarkSupport/#removekeysolderthanwatermarkstore-statestore-unit","text":"removeKeysOlderThanWatermark requests the input store for all rows . removeKeysOlderThanWatermark then uses watermarkPredicateForKeys to remove matching rows from the store . removeKeysOlderThanWatermark is used when StreamingDeduplicateExec physical operator is requested to execute. === [[removeKeysOlderThanWatermark-StreamingAggregationStateManager-store]] removeKeysOlderThanWatermark Method","title":"removeKeysOlderThanWatermark(store: StateStore): Unit"},{"location":"WatermarkSupport/#source-scala_2","text":"removeKeysOlderThanWatermark( storeManager: StreamingAggregationStateManager, store: StateStore): Unit removeKeysOlderThanWatermark ...FIXME NOTE: removeKeysOlderThanWatermark is used exclusively when StateStoreSaveExec physical operator is requested to < >.","title":"[source, scala]"},{"location":"WatermarkTracker/","text":"WatermarkTracker \u00b6 WatermarkTracker tracks the event-time watermark of a streaming query (across EventTimeWatermarkExec operators in a physical query plan) based on a given MultipleWatermarkPolicy . WatermarkTracker is used in MicroBatchExecution . Creating Instance \u00b6 WatermarkTracker takes the following to be created: MultipleWatermarkPolicy WatermarkTracker is created (using apply ) when MicroBatchExecution is requested to populate start offsets at start or restart (from a checkpoint). MultipleWatermarkPolicy \u00b6 WatermarkTracker is given a MultipleWatermarkPolicy when created that can be one of the following: MaxWatermark (alias: min ) MinWatermark (alias: max ) Creating WatermarkTracker \u00b6 apply ( conf : RuntimeConfig ): WatermarkTracker apply uses the spark.sql.streaming.multipleWatermarkPolicy configuration property for the global watermark policy (default: min ) and creates a WatermarkTracker . apply is used when MicroBatchExecution is requested to populate start offsets at start or restart (from a checkpoint). Global Event-Time Watermark \u00b6 globalWatermarkMs : Long WatermarkTracker uses globalWatermarkMs internal registry to keep track of global event-time watermark (based on MultipleWatermarkPolicy across all EventTimeWatermarkExec operators in a physical query plan). Default: 0 globalWatermarkMs is used when WatermarkTracker is requested to updateWatermark . The event-time watermark can be updated in setWatermark and updateWatermark . The event-time watermark is used (as currentWatermark method) when MicroBatchExecution stream execution engine is requested to populateStartOffsets and constructNextBatch and runBatch . Updating Watermark (at Startup and Restart) \u00b6 setWatermark ( newWatermarkMs : Long ): Unit setWatermark sets the global event-time watermark to the given newWatermarkMs value. setWatermark is used when MicroBatchExecution is requested to populate start offsets at start or restart (from a checkpoint). Updating Watermark (at Execution) \u00b6 updateWatermark ( executedPlan : SparkPlan ): Unit updateWatermark requests the given SparkPlan physical operator to collect all EventTimeWatermarkExec unary physical operators. updateWatermark simply exits when no EventTimeWatermarkExec was found. updateWatermark ...FIXME updateWatermark is used when MicroBatchExecution is requested to run a single streaming batch (when requested to run an activated streaming query ). Watermarks by EventTimeWatermarkExec Operator Registry \u00b6 operatorToWatermarkMap : Map [ Int , Long ] WatermarkTracker uses operatorToWatermarkMap internal registry to keep track of event-time watermarks of every EventTimeWatermarkExec physical operator in a streaming query plan. operatorToWatermarkMap is used when WatermarkTracker is requested to updateWatermark . Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.execution.streaming.WatermarkTracker logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.WatermarkTracker=ALL Refer to Logging .","title":"WatermarkTracker"},{"location":"WatermarkTracker/#watermarktracker","text":"WatermarkTracker tracks the event-time watermark of a streaming query (across EventTimeWatermarkExec operators in a physical query plan) based on a given MultipleWatermarkPolicy . WatermarkTracker is used in MicroBatchExecution .","title":"WatermarkTracker"},{"location":"WatermarkTracker/#creating-instance","text":"WatermarkTracker takes the following to be created: MultipleWatermarkPolicy WatermarkTracker is created (using apply ) when MicroBatchExecution is requested to populate start offsets at start or restart (from a checkpoint).","title":"Creating Instance"},{"location":"WatermarkTracker/#multiplewatermarkpolicy","text":"WatermarkTracker is given a MultipleWatermarkPolicy when created that can be one of the following: MaxWatermark (alias: min ) MinWatermark (alias: max )","title":" MultipleWatermarkPolicy"},{"location":"WatermarkTracker/#creating-watermarktracker","text":"apply ( conf : RuntimeConfig ): WatermarkTracker apply uses the spark.sql.streaming.multipleWatermarkPolicy configuration property for the global watermark policy (default: min ) and creates a WatermarkTracker . apply is used when MicroBatchExecution is requested to populate start offsets at start or restart (from a checkpoint).","title":" Creating WatermarkTracker"},{"location":"WatermarkTracker/#global-event-time-watermark","text":"globalWatermarkMs : Long WatermarkTracker uses globalWatermarkMs internal registry to keep track of global event-time watermark (based on MultipleWatermarkPolicy across all EventTimeWatermarkExec operators in a physical query plan). Default: 0 globalWatermarkMs is used when WatermarkTracker is requested to updateWatermark . The event-time watermark can be updated in setWatermark and updateWatermark . The event-time watermark is used (as currentWatermark method) when MicroBatchExecution stream execution engine is requested to populateStartOffsets and constructNextBatch and runBatch .","title":" Global Event-Time Watermark"},{"location":"WatermarkTracker/#updating-watermark-at-startup-and-restart","text":"setWatermark ( newWatermarkMs : Long ): Unit setWatermark sets the global event-time watermark to the given newWatermarkMs value. setWatermark is used when MicroBatchExecution is requested to populate start offsets at start or restart (from a checkpoint).","title":" Updating Watermark (at Startup and Restart)"},{"location":"WatermarkTracker/#updating-watermark-at-execution","text":"updateWatermark ( executedPlan : SparkPlan ): Unit updateWatermark requests the given SparkPlan physical operator to collect all EventTimeWatermarkExec unary physical operators. updateWatermark simply exits when no EventTimeWatermarkExec was found. updateWatermark ...FIXME updateWatermark is used when MicroBatchExecution is requested to run a single streaming batch (when requested to run an activated streaming query ).","title":" Updating Watermark (at Execution)"},{"location":"WatermarkTracker/#watermarks-by-eventtimewatermarkexec-operator-registry","text":"operatorToWatermarkMap : Map [ Int , Long ] WatermarkTracker uses operatorToWatermarkMap internal registry to keep track of event-time watermarks of every EventTimeWatermarkExec physical operator in a streaming query plan. operatorToWatermarkMap is used when WatermarkTracker is requested to updateWatermark .","title":" Watermarks by EventTimeWatermarkExec Operator Registry"},{"location":"WatermarkTracker/#logging","text":"Enable ALL logging level for org.apache.spark.sql.execution.streaming.WatermarkTracker logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.WatermarkTracker=ALL Refer to Logging .","title":"Logging"},{"location":"arbitrary-stateful-streaming-aggregation/","text":"Arbitrary Stateful Streaming Aggregation \u00b6 Arbitrary Stateful Streaming Aggregation is a streaming aggregation query that uses the following KeyValueGroupedDataset operators: mapGroupsWithState for implicit state logic flatMapGroupsWithState for explicit state logic KeyValueGroupedDataset represents a grouped dataset as a result of Dataset.groupByKey operator. mapGroupsWithState and flatMapGroupsWithState operators use GroupState as group streaming aggregation state that is created separately for every aggregation key with an aggregation state value (of a user-defined type). mapGroupsWithState and flatMapGroupsWithState operators use GroupStateTimeout as an aggregation state timeout that defines when a GroupState is considered timed-out ( expired ). Demos \u00b6 Use the following demos and complete applications to learn more: Demo: Internals of FlatMapGroupsWithStateExec Physical Operator Demo: Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator groupByKey Streaming Aggregation in Update Mode FlatMapGroupsWithStateApp Performance Metrics \u00b6 Arbitrary Stateful Streaming Aggregation uses performance metrics (of the StateStoreWriter through FlatMapGroupsWithStateExec physical operator). Internals \u00b6 One of the most important internal execution components of Arbitrary Stateful Streaming Aggregation is FlatMapGroupsWithStateExec physical operator. When executed, FlatMapGroupsWithStateExec first validates a selected GroupStateTimeout : For ProcessingTimeTimeout , batch timeout threshold has to be defined For EventTimeTimeout , event-time watermark has to be defined and the input schema has the watermark attribute Note FIXME When are the above requirements met? FlatMapGroupsWithStateExec physical operator then mapPartitionsWithStateStore with a custom storeUpdateFunction of the following signature: ( StateStore , Iterator [ T ]) => Iterator [ U ] While generating the recipe, FlatMapGroupsWithStateExec uses StateStoreOps extension method object to register a listener that is executed on a task completion. The listener makes sure that a given StateStore has all state changes either committed or aborted . In the end, FlatMapGroupsWithStateExec creates a new StateStoreRDD and adds it to the RDD lineage. StateStoreRDD is used to properly distribute tasks across executors (per preferred locations ) with help of StateStoreCoordinator (that runs on the driver). StateStoreRDD uses StateStore helper to look up a StateStore by StateStoreProviderId and store version. FlatMapGroupsWithStateExec physical operator uses state managers that are different than state managers for Streaming Aggregation . StateStore abstraction is the same as in Streaming Aggregation . One of the important execution steps is when InputProcessor (of FlatMapGroupsWithStateExec physical operator) is requested to callFunctionAndUpdateState . That executes the user-defined state function on a per-group state key object, value objects, and a GroupStateImpl .","title":"Arbitrary Stateful Streaming Aggregation"},{"location":"arbitrary-stateful-streaming-aggregation/#arbitrary-stateful-streaming-aggregation","text":"Arbitrary Stateful Streaming Aggregation is a streaming aggregation query that uses the following KeyValueGroupedDataset operators: mapGroupsWithState for implicit state logic flatMapGroupsWithState for explicit state logic KeyValueGroupedDataset represents a grouped dataset as a result of Dataset.groupByKey operator. mapGroupsWithState and flatMapGroupsWithState operators use GroupState as group streaming aggregation state that is created separately for every aggregation key with an aggregation state value (of a user-defined type). mapGroupsWithState and flatMapGroupsWithState operators use GroupStateTimeout as an aggregation state timeout that defines when a GroupState is considered timed-out ( expired ).","title":"Arbitrary Stateful Streaming Aggregation"},{"location":"arbitrary-stateful-streaming-aggregation/#demos","text":"Use the following demos and complete applications to learn more: Demo: Internals of FlatMapGroupsWithStateExec Physical Operator Demo: Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator groupByKey Streaming Aggregation in Update Mode FlatMapGroupsWithStateApp","title":"Demos"},{"location":"arbitrary-stateful-streaming-aggregation/#performance-metrics","text":"Arbitrary Stateful Streaming Aggregation uses performance metrics (of the StateStoreWriter through FlatMapGroupsWithStateExec physical operator).","title":" Performance Metrics"},{"location":"arbitrary-stateful-streaming-aggregation/#internals","text":"One of the most important internal execution components of Arbitrary Stateful Streaming Aggregation is FlatMapGroupsWithStateExec physical operator. When executed, FlatMapGroupsWithStateExec first validates a selected GroupStateTimeout : For ProcessingTimeTimeout , batch timeout threshold has to be defined For EventTimeTimeout , event-time watermark has to be defined and the input schema has the watermark attribute Note FIXME When are the above requirements met? FlatMapGroupsWithStateExec physical operator then mapPartitionsWithStateStore with a custom storeUpdateFunction of the following signature: ( StateStore , Iterator [ T ]) => Iterator [ U ] While generating the recipe, FlatMapGroupsWithStateExec uses StateStoreOps extension method object to register a listener that is executed on a task completion. The listener makes sure that a given StateStore has all state changes either committed or aborted . In the end, FlatMapGroupsWithStateExec creates a new StateStoreRDD and adds it to the RDD lineage. StateStoreRDD is used to properly distribute tasks across executors (per preferred locations ) with help of StateStoreCoordinator (that runs on the driver). StateStoreRDD uses StateStore helper to look up a StateStore by StateStoreProviderId and store version. FlatMapGroupsWithStateExec physical operator uses state managers that are different than state managers for Streaming Aggregation . StateStore abstraction is the same as in Streaming Aggregation . One of the important execution steps is when InputProcessor (of FlatMapGroupsWithStateExec physical operator) is requested to callFunctionAndUpdateState . That executes the user-defined state function on a per-group state key object, value objects, and a GroupStateImpl .","title":" Internals"},{"location":"configuration-properties/","text":"Configuration Properties \u00b6 Configuration properties (aka settings ) allow you to fine-tune a Spark Structured Streaming application. The Internals of Spark SQL Learn more about Configuration Properties in The Internals of Spark SQL . spark.sql.streaming.commitProtocolClass \u00b6 (internal) FileCommitProtocol to use for writing out micro-batches in FileStreamSink . Default: org.apache.spark.sql.execution.streaming.ManifestFileCommitProtocol Use SQLConf.streamingFileCommitProtocolClass to access the current value. The Internals of Apache Spark Learn more on FileCommitProtocol in The Internals of Apache Spark . spark.sql.streaming.metricsEnabled \u00b6 Enables streaming metrics Default: false Use SQLConf.streamingMetricsEnabled to access the current value. spark.sql.streaming.fileSink.log.cleanupDelay \u00b6 (internal) How long (in millis) that a file is guaranteed to be visible for all readers. Default: 10 minutes Use SQLConf.fileSinkLogCleanupDelay to access the current value. spark.sql.streaming.fileSink.log.deletion \u00b6 (internal) Whether to delete the expired log files in file stream sink Default: true Use SQLConf.fileSinkLogDeletion to access the current value. spark.sql.streaming.fileSink.log.compactInterval \u00b6 (internal) Number of log files after which all the previous files are compacted into the next log file Default: 10 Use SQLConf.fileSinkLogCompactInterval to access the current value. spark.sql.streaming.minBatchesToRetain \u00b6 (internal) Minimum number of batches that must be retained and made recoverable Stream execution engines discard ( purge ) offsets from the offsets metadata log when the current batch ID (in MicroBatchExecution ) or the epoch committed (in ContinuousExecution ) is above the threshold. Default: 100 Use SQLConf.minBatchesToRetain to access the current value. spark.sql.streaming.aggregation.stateFormatVersion \u00b6 (internal) Version of the state format Default: 2 Supported values: 1 (for the legacy StreamingAggregationStateManagerImplV1 ) 2 (for the default StreamingAggregationStateManagerImplV2 ) Used when StatefulAggregationStrategy execution planning strategy is executed (and plans a streaming query with an aggregate that simply boils down to creating a StateStoreRestoreExec with the proper implementation version of StreamingAggregationStateManager ) Among the checkpointed properties that are not supposed to be overriden after a streaming query has once been started (and could later recover from a checkpoint after being restarted) spark.sql.streaming.checkpointFileManagerClass \u00b6 (internal) CheckpointFileManager to use to write checkpoint files atomically Default: FileContextBasedCheckpointFileManager (with FileSystemBasedCheckpointFileManager in case of unsupported file system used for storing metadata files) spark.sql.streaming.checkpointLocation \u00b6 Default checkpoint directory for storing checkpoint data Default: (empty) spark.sql.streaming.continuous.executorQueueSize \u00b6 (internal) The size (measured in number of rows) of the queue used in continuous execution to buffer the results of a ContinuousDataReader. Default: 1024 spark.sql.streaming.continuous.executorPollIntervalMs \u00b6 (internal) The interval (in millis) at which continuous execution readers will poll to check whether the epoch has advanced on the driver. Default: 100 (ms) spark.sql.streaming.disabledV2MicroBatchReaders \u00b6 (internal) A comma-separated list of fully-qualified class names of data source providers for which MicroBatchStream is disabled. Reads from these sources will fall back to the V1 Sources. Default: (empty) Use SQLConf.disabledV2StreamingMicroBatchReaders to get the current value. spark.sql.streaming.fileSource.log.cleanupDelay \u00b6 (internal) How long (in millis) a file is guaranteed to be visible for all readers. Default: 10 (minutes) Use SQLConf.fileSourceLogCleanupDelay to get the current value. spark.sql.streaming.fileSource.log.compactInterval \u00b6 (internal) Number of log files after which all the previous files are compacted into the next log file. Default: 10 Must be a positive value (greater than 0 ) Use SQLConf.fileSourceLogCompactInterval to get the current value. spark.sql.streaming.fileSource.log.deletion \u00b6 (internal) Whether to delete the expired log files in file stream source Default: true Use SQLConf.fileSourceLogDeletion to get the current value. spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion \u00b6 (internal) State format version used to create a StateManager for FlatMapGroupsWithStateExec physical operator Default: 2 Supported values: 1 2 Among the checkpointed properties that are not supposed to be overriden after a streaming query has once been started (and could later recover from a checkpoint after being restarted) spark.sql.streaming.maxBatchesToRetainInMemory \u00b6 (internal) The maximum number of batches which will be retained in memory to avoid loading from files. Default: 2 Maximum count of versions a State Store implementation should retain in memory. The value adjusts a trade-off between memory usage vs cache miss: 2 covers both success and direct failure cases 1 covers only success case 0 or negative value disables cache to maximize memory size of executors Used when HDFSBackedStateStoreProvider is requested to initialize . spark.sql.streaming.multipleWatermarkPolicy \u00b6 Global watermark policy that is the policy to calculate the global watermark value when there are multiple watermark operators in a streaming query Default: min Supported values: min - chooses the minimum watermark reported across multiple operators max - chooses the maximum across multiple operators Cannot be changed between query restarts from the same checkpoint location. spark.sql.streaming.noDataMicroBatches.enabled \u00b6 Flag to control whether the streaming micro-batch engine should execute batches with no data to process for eager state management for stateful streaming queries ( true ) or not ( false ). Default: true Use SQLConf.streamingNoDataMicroBatchesEnabled to get the current value spark.sql.streaming.noDataProgressEventInterval \u00b6 (internal) How long to wait between two progress events when there is no data (in millis) when ProgressReporter is requested to finish a trigger Default: 10000L Use SQLConf.streamingNoDataProgressEventInterval to get the current value spark.sql.streaming.numRecentProgressUpdates \u00b6 Number of StreamingQueryProgresses to retain in progressBuffer internal registry when ProgressReporter is requested to update progress of streaming query Default: 100 Use SQLConf.streamingProgressRetention to get the current value spark.sql.streaming.pollingDelay \u00b6 (internal) How long (in millis) to delay StreamExecution before polls for new data when no data was available in a batch Default: 10 (milliseconds) spark.sql.streaming.stateStore.maintenanceInterval \u00b6 The initial delay and how often to execute StateStore's maintenance task . Default: 60s spark.sql.streaming.stateStore.minDeltasForSnapshot \u00b6 (internal) Minimum number of state store delta files that need to be generated before HDFSBackedStateStore will consider generating a snapshot (consolidate the deltas into a snapshot) Default: 10 Use SQLConf.stateStoreMinDeltasForSnapshot to get the current value. spark.sql.streaming.stateStore.providerClass \u00b6 (internal) The fully-qualified class name of the StateStoreProvider implementation that manages state data in stateful streaming queries. This class must have a zero-arg constructor. Default: HDFSBackedStateStoreProvider Use SQLConf.stateStoreProviderClass to get the current value. spark.sql.streaming.unsupportedOperationCheck \u00b6 (internal) When enabled ( true ), StreamingQueryManager makes sure that the logical plan of a streaming query uses supported operations only Default: true","title":"Configuration Properties"},{"location":"configuration-properties/#configuration-properties","text":"Configuration properties (aka settings ) allow you to fine-tune a Spark Structured Streaming application. The Internals of Spark SQL Learn more about Configuration Properties in The Internals of Spark SQL .","title":"Configuration Properties"},{"location":"configuration-properties/#sparksqlstreamingcommitprotocolclass","text":"(internal) FileCommitProtocol to use for writing out micro-batches in FileStreamSink . Default: org.apache.spark.sql.execution.streaming.ManifestFileCommitProtocol Use SQLConf.streamingFileCommitProtocolClass to access the current value. The Internals of Apache Spark Learn more on FileCommitProtocol in The Internals of Apache Spark .","title":" spark.sql.streaming.commitProtocolClass"},{"location":"configuration-properties/#sparksqlstreamingmetricsenabled","text":"Enables streaming metrics Default: false Use SQLConf.streamingMetricsEnabled to access the current value.","title":" spark.sql.streaming.metricsEnabled"},{"location":"configuration-properties/#sparksqlstreamingfilesinklogcleanupdelay","text":"(internal) How long (in millis) that a file is guaranteed to be visible for all readers. Default: 10 minutes Use SQLConf.fileSinkLogCleanupDelay to access the current value.","title":" spark.sql.streaming.fileSink.log.cleanupDelay"},{"location":"configuration-properties/#sparksqlstreamingfilesinklogdeletion","text":"(internal) Whether to delete the expired log files in file stream sink Default: true Use SQLConf.fileSinkLogDeletion to access the current value.","title":" spark.sql.streaming.fileSink.log.deletion"},{"location":"configuration-properties/#sparksqlstreamingfilesinklogcompactinterval","text":"(internal) Number of log files after which all the previous files are compacted into the next log file Default: 10 Use SQLConf.fileSinkLogCompactInterval to access the current value.","title":" spark.sql.streaming.fileSink.log.compactInterval"},{"location":"configuration-properties/#sparksqlstreamingminbatchestoretain","text":"(internal) Minimum number of batches that must be retained and made recoverable Stream execution engines discard ( purge ) offsets from the offsets metadata log when the current batch ID (in MicroBatchExecution ) or the epoch committed (in ContinuousExecution ) is above the threshold. Default: 100 Use SQLConf.minBatchesToRetain to access the current value.","title":" spark.sql.streaming.minBatchesToRetain"},{"location":"configuration-properties/#sparksqlstreamingaggregationstateformatversion","text":"(internal) Version of the state format Default: 2 Supported values: 1 (for the legacy StreamingAggregationStateManagerImplV1 ) 2 (for the default StreamingAggregationStateManagerImplV2 ) Used when StatefulAggregationStrategy execution planning strategy is executed (and plans a streaming query with an aggregate that simply boils down to creating a StateStoreRestoreExec with the proper implementation version of StreamingAggregationStateManager ) Among the checkpointed properties that are not supposed to be overriden after a streaming query has once been started (and could later recover from a checkpoint after being restarted)","title":" spark.sql.streaming.aggregation.stateFormatVersion"},{"location":"configuration-properties/#sparksqlstreamingcheckpointfilemanagerclass","text":"(internal) CheckpointFileManager to use to write checkpoint files atomically Default: FileContextBasedCheckpointFileManager (with FileSystemBasedCheckpointFileManager in case of unsupported file system used for storing metadata files)","title":" spark.sql.streaming.checkpointFileManagerClass"},{"location":"configuration-properties/#sparksqlstreamingcheckpointlocation","text":"Default checkpoint directory for storing checkpoint data Default: (empty)","title":" spark.sql.streaming.checkpointLocation"},{"location":"configuration-properties/#sparksqlstreamingcontinuousexecutorqueuesize","text":"(internal) The size (measured in number of rows) of the queue used in continuous execution to buffer the results of a ContinuousDataReader. Default: 1024","title":" spark.sql.streaming.continuous.executorQueueSize"},{"location":"configuration-properties/#sparksqlstreamingcontinuousexecutorpollintervalms","text":"(internal) The interval (in millis) at which continuous execution readers will poll to check whether the epoch has advanced on the driver. Default: 100 (ms)","title":" spark.sql.streaming.continuous.executorPollIntervalMs"},{"location":"configuration-properties/#sparksqlstreamingdisabledv2microbatchreaders","text":"(internal) A comma-separated list of fully-qualified class names of data source providers for which MicroBatchStream is disabled. Reads from these sources will fall back to the V1 Sources. Default: (empty) Use SQLConf.disabledV2StreamingMicroBatchReaders to get the current value.","title":" spark.sql.streaming.disabledV2MicroBatchReaders"},{"location":"configuration-properties/#sparksqlstreamingfilesourcelogcleanupdelay","text":"(internal) How long (in millis) a file is guaranteed to be visible for all readers. Default: 10 (minutes) Use SQLConf.fileSourceLogCleanupDelay to get the current value.","title":" spark.sql.streaming.fileSource.log.cleanupDelay"},{"location":"configuration-properties/#sparksqlstreamingfilesourcelogcompactinterval","text":"(internal) Number of log files after which all the previous files are compacted into the next log file. Default: 10 Must be a positive value (greater than 0 ) Use SQLConf.fileSourceLogCompactInterval to get the current value.","title":" spark.sql.streaming.fileSource.log.compactInterval"},{"location":"configuration-properties/#sparksqlstreamingfilesourcelogdeletion","text":"(internal) Whether to delete the expired log files in file stream source Default: true Use SQLConf.fileSourceLogDeletion to get the current value.","title":" spark.sql.streaming.fileSource.log.deletion"},{"location":"configuration-properties/#sparksqlstreamingflatmapgroupswithstatestateformatversion","text":"(internal) State format version used to create a StateManager for FlatMapGroupsWithStateExec physical operator Default: 2 Supported values: 1 2 Among the checkpointed properties that are not supposed to be overriden after a streaming query has once been started (and could later recover from a checkpoint after being restarted)","title":" spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion"},{"location":"configuration-properties/#sparksqlstreamingmaxbatchestoretaininmemory","text":"(internal) The maximum number of batches which will be retained in memory to avoid loading from files. Default: 2 Maximum count of versions a State Store implementation should retain in memory. The value adjusts a trade-off between memory usage vs cache miss: 2 covers both success and direct failure cases 1 covers only success case 0 or negative value disables cache to maximize memory size of executors Used when HDFSBackedStateStoreProvider is requested to initialize .","title":" spark.sql.streaming.maxBatchesToRetainInMemory"},{"location":"configuration-properties/#sparksqlstreamingmultiplewatermarkpolicy","text":"Global watermark policy that is the policy to calculate the global watermark value when there are multiple watermark operators in a streaming query Default: min Supported values: min - chooses the minimum watermark reported across multiple operators max - chooses the maximum across multiple operators Cannot be changed between query restarts from the same checkpoint location.","title":" spark.sql.streaming.multipleWatermarkPolicy"},{"location":"configuration-properties/#sparksqlstreamingnodatamicrobatchesenabled","text":"Flag to control whether the streaming micro-batch engine should execute batches with no data to process for eager state management for stateful streaming queries ( true ) or not ( false ). Default: true Use SQLConf.streamingNoDataMicroBatchesEnabled to get the current value","title":" spark.sql.streaming.noDataMicroBatches.enabled"},{"location":"configuration-properties/#sparksqlstreamingnodataprogresseventinterval","text":"(internal) How long to wait between two progress events when there is no data (in millis) when ProgressReporter is requested to finish a trigger Default: 10000L Use SQLConf.streamingNoDataProgressEventInterval to get the current value","title":" spark.sql.streaming.noDataProgressEventInterval"},{"location":"configuration-properties/#sparksqlstreamingnumrecentprogressupdates","text":"Number of StreamingQueryProgresses to retain in progressBuffer internal registry when ProgressReporter is requested to update progress of streaming query Default: 100 Use SQLConf.streamingProgressRetention to get the current value","title":" spark.sql.streaming.numRecentProgressUpdates"},{"location":"configuration-properties/#sparksqlstreamingpollingdelay","text":"(internal) How long (in millis) to delay StreamExecution before polls for new data when no data was available in a batch Default: 10 (milliseconds)","title":" spark.sql.streaming.pollingDelay"},{"location":"configuration-properties/#sparksqlstreamingstatestoremaintenanceinterval","text":"The initial delay and how often to execute StateStore's maintenance task . Default: 60s","title":" spark.sql.streaming.stateStore.maintenanceInterval"},{"location":"configuration-properties/#sparksqlstreamingstatestoremindeltasforsnapshot","text":"(internal) Minimum number of state store delta files that need to be generated before HDFSBackedStateStore will consider generating a snapshot (consolidate the deltas into a snapshot) Default: 10 Use SQLConf.stateStoreMinDeltasForSnapshot to get the current value.","title":" spark.sql.streaming.stateStore.minDeltasForSnapshot"},{"location":"configuration-properties/#sparksqlstreamingstatestoreproviderclass","text":"(internal) The fully-qualified class name of the StateStoreProvider implementation that manages state data in stateful streaming queries. This class must have a zero-arg constructor. Default: HDFSBackedStateStoreProvider Use SQLConf.stateStoreProviderClass to get the current value.","title":" spark.sql.streaming.stateStore.providerClass"},{"location":"configuration-properties/#sparksqlstreamingunsupportedoperationcheck","text":"(internal) When enabled ( true ), StreamingQueryManager makes sure that the logical plan of a streaming query uses supported operations only Default: true","title":" spark.sql.streaming.unsupportedOperationCheck"},{"location":"offsets-and-metadata-checkpointing/","text":"Offsets and Metadata Checkpointing \u00b6 A streaming query can be started from scratch or from checkpoint (that gives fault-tolerance as the state is preserved even when a failure happens). Stream execution engines use checkpoint location to resume stream processing and get start offsets to start query processing from. StreamExecution resumes (populates the start offsets) from the latest checkpointed offsets from the Write-Ahead Log (WAL) of Offsets that may have already been processed (and, if so, committed to the Offset Commit Log ). Hadoop DFS-based metadata storage of OffsetSeq s OffsetSeq and StreamProgress StreamProgress and StreamExecutions ( committed and available offsets) Micro-Batch Stream Processing \u00b6 In Micro-Batch Stream Processing , the available offsets registry is populated with the latest offsets from the Write-Ahead Log (WAL) when MicroBatchExecution stream processing engine is requested to populate start offsets from checkpoint (if available) when MicroBatchExecution is requested to run an activated streaming query ( before the first \"zero\" micro-batch ). The available offsets are then added to the committed offsets when the latest batch ID available (as described above) is exactly the latest batch ID committed to the Offset Commit Log when MicroBatchExecution stream processing engine is requested to populate start offsets from checkpoint . When a streaming query is started from scratch (with no checkpoint that has offsets in the Offset Write-Ahead Log ), MicroBatchExecution prints out the following INFO message: Starting new streaming query. When a streaming query is resumed (restarted) from a checkpoint with offsets in the Offset Write-Ahead Log , MicroBatchExecution prints out the following INFO message: Resuming at batch [currentBatchId] with committed offsets [committedOffsets] and available offsets [availableOffsets] Every time MicroBatchExecution is requested to check whether a new data is available (in any of the streaming sources)...FIXME When MicroBatchExecution is requested to construct the next streaming micro-batch (when MicroBatchExecution requested to run the activated streaming query ), every streaming source is requested for the latest offset available that are added to the availableOffsets registry. Streaming sources report some offsets or none at all (if this source has never received any data). Streaming sources with no data are excluded ( filtered out ). MicroBatchExecution prints out the following TRACE message to the logs: noDataBatchesEnabled = [noDataBatchesEnabled], lastExecutionRequiresAnotherBatch = [lastExecutionRequiresAnotherBatch], isNewDataAvailable = [isNewDataAvailable], shouldConstructNextBatch = [shouldConstructNextBatch] With shouldConstructNextBatch internal flag enabled, MicroBatchExecution commits ( adds ) the available offsets for the batch to the Write-Ahead Log (WAL) and prints out the following INFO message to the logs: Committed offsets for batch [currentBatchId]. Metadata [offsetSeqMetadata] When running a single streaming micro-batch , MicroBatchExecution requests every Source and MicroBatchReader (in the availableOffsets registry) for unprocessed data (that has not been committed yet and so considered unprocessed). In the end (of running a single streaming micro-batch ), MicroBatchExecution commits ( adds ) the available offsets (to the committedOffsets registry) so they are considered processed already. MicroBatchExecution prints out the following DEBUG message to the logs: Completed batch [currentBatchId] Limitations (Assumptions) \u00b6 It is assumed that the order of streaming sources in a streaming query matches the order of the offsets of OffsetSeq (in offsetLog ) and availableOffsets . In other words, a streaming query can be modified and then restarted from a checkpoint (to maintain stream processing state) only when the number of streaming sources and their order are preserved across restarts.","title":"Offsets and Metadata Checkpointing"},{"location":"offsets-and-metadata-checkpointing/#offsets-and-metadata-checkpointing","text":"A streaming query can be started from scratch or from checkpoint (that gives fault-tolerance as the state is preserved even when a failure happens). Stream execution engines use checkpoint location to resume stream processing and get start offsets to start query processing from. StreamExecution resumes (populates the start offsets) from the latest checkpointed offsets from the Write-Ahead Log (WAL) of Offsets that may have already been processed (and, if so, committed to the Offset Commit Log ). Hadoop DFS-based metadata storage of OffsetSeq s OffsetSeq and StreamProgress StreamProgress and StreamExecutions ( committed and available offsets)","title":"Offsets and Metadata Checkpointing"},{"location":"offsets-and-metadata-checkpointing/#micro-batch-stream-processing","text":"In Micro-Batch Stream Processing , the available offsets registry is populated with the latest offsets from the Write-Ahead Log (WAL) when MicroBatchExecution stream processing engine is requested to populate start offsets from checkpoint (if available) when MicroBatchExecution is requested to run an activated streaming query ( before the first \"zero\" micro-batch ). The available offsets are then added to the committed offsets when the latest batch ID available (as described above) is exactly the latest batch ID committed to the Offset Commit Log when MicroBatchExecution stream processing engine is requested to populate start offsets from checkpoint . When a streaming query is started from scratch (with no checkpoint that has offsets in the Offset Write-Ahead Log ), MicroBatchExecution prints out the following INFO message: Starting new streaming query. When a streaming query is resumed (restarted) from a checkpoint with offsets in the Offset Write-Ahead Log , MicroBatchExecution prints out the following INFO message: Resuming at batch [currentBatchId] with committed offsets [committedOffsets] and available offsets [availableOffsets] Every time MicroBatchExecution is requested to check whether a new data is available (in any of the streaming sources)...FIXME When MicroBatchExecution is requested to construct the next streaming micro-batch (when MicroBatchExecution requested to run the activated streaming query ), every streaming source is requested for the latest offset available that are added to the availableOffsets registry. Streaming sources report some offsets or none at all (if this source has never received any data). Streaming sources with no data are excluded ( filtered out ). MicroBatchExecution prints out the following TRACE message to the logs: noDataBatchesEnabled = [noDataBatchesEnabled], lastExecutionRequiresAnotherBatch = [lastExecutionRequiresAnotherBatch], isNewDataAvailable = [isNewDataAvailable], shouldConstructNextBatch = [shouldConstructNextBatch] With shouldConstructNextBatch internal flag enabled, MicroBatchExecution commits ( adds ) the available offsets for the batch to the Write-Ahead Log (WAL) and prints out the following INFO message to the logs: Committed offsets for batch [currentBatchId]. Metadata [offsetSeqMetadata] When running a single streaming micro-batch , MicroBatchExecution requests every Source and MicroBatchReader (in the availableOffsets registry) for unprocessed data (that has not been committed yet and so considered unprocessed). In the end (of running a single streaming micro-batch ), MicroBatchExecution commits ( adds ) the available offsets (to the committedOffsets registry) so they are considered processed already. MicroBatchExecution prints out the following DEBUG message to the logs: Completed batch [currentBatchId]","title":"Micro-Batch Stream Processing"},{"location":"offsets-and-metadata-checkpointing/#limitations-assumptions","text":"It is assumed that the order of streaming sources in a streaming query matches the order of the offsets of OffsetSeq (in offsetLog ) and availableOffsets . In other words, a streaming query can be modified and then restarted from a checkpoint (to maintain stream processing state) only when the number of streaming sources and their order are preserved across restarts.","title":"Limitations (Assumptions)"},{"location":"overview/","text":"Spark Structured Streaming and Streaming Queries \u00b6 Spark Structured Streaming ( Structured Streaming or Spark Streams ) is the module of Apache Spark for stream processing using streaming queries . Streaming queries can be expressed using a high-level declarative streaming API ( Dataset API ) or good ol' SQL ( SQL over stream / streaming SQL ). The declarative streaming Dataset API and SQL are executed on the underlying highly-optimized Spark SQL engine. The semantics of the Structured Streaming model is as follows (see the article Structured Streaming In Apache Spark ): At any time, the output of a continuous application is equivalent to executing a batch job on a prefix of the data. As of Spark 2.2.0, Structured Streaming has been marked stable and ready for production use. With that the other older streaming module Spark Streaming is considered obsolete and not for developing new streaming applications with Apache Spark. Spark Structured Streaming comes with two stream execution engines for executing streaming queries: MicroBatchExecution for Micro-Batch Stream Processing ContinuousExecution for Continuous Stream Processing The goal of Spark Structured Streaming is to unify streaming, interactive, and batch queries over structured datasets for developing end-to-end stream processing applications dubbed continuous applications using Spark SQL's Datasets API with additional support for the following features: Streaming Aggregation Streaming Join Streaming Watermark Arbitrary Stateful Streaming Aggregation Stateful Stream Processing In Structured Streaming, Spark developers describe custom streaming computations in the same way as with Spark SQL. Internally, Structured Streaming applies the user-defined structured query to the continuously and indefinitely arriving data to analyze real-time streaming data. Structured Streaming introduces the concept of streaming datasets that are infinite datasets with primitives like input streaming data sources and output streaming data sinks . A Dataset is streaming when its logical plan is streaming. val batchQuery = spark . read . // <-- batch non-streaming query csv ( \"sales\" ) assert ( batchQuery . isStreaming == false ) val streamingQuery = spark . readStream . // <-- streaming query format ( \"rate\" ). load assert ( streamingQuery . isStreaming ) Note Read up on Spark SQL, Datasets and logical plans in The Internals of Spark SQL online book. Structured Streaming models a stream of data as an infinite (and hence continuous) table that could be changed every streaming batch. You can specify output mode of a streaming dataset which is what gets written to a streaming sink (i.e. the infinite result table) when there is a new data available. Streaming Datasets use streaming query plans (as opposed to regular batch Datasets that are based on batch query plans). Note From this perspective, batch queries can be considered streaming Datasets executed once only (and is why some batch queries, e.g. KafkaSource , can easily work in batch mode). val batchQuery = spark . read . format ( \"rate\" ). load assert ( batchQuery . isStreaming == false ) val streamingQuery = spark . readStream . format ( \"rate\" ). load assert ( streamingQuery . isStreaming ) With Structured Streaming, Spark 2 aims at simplifying streaming analytics with little to no need to reason about effective data streaming (trying to hide the unnecessary complexity in your streaming analytics architectures). Structured streaming is defined by the following data abstractions in org.apache.spark.sql.streaming package: StreamingQuery Streaming Source Streaming Sink StreamingQueryManager Structured Streaming follows micro-batch model and periodically fetches data from the data source (and uses the DataFrame data abstraction to represent the fetched data for a certain batch). With Datasets as Spark SQL's view of structured data, structured streaming checks input sources for new data every trigger (time) and executes the (continuous) queries. Note The feature has also been called Streaming Spark SQL Query , Streaming DataFrames , Continuous DataFrame or Continuous Query . There have been lots of names before the Spark project settled on Structured Streaming. References \u00b6 Articles \u00b6 SPARK-8360 Structured Streaming (aka Streaming DataFrames) The official Structured Streaming Programming Guide Structured Streaming In Apache Spark What Spark's Structured Streaming really means Videos \u00b6 The Future of Real Time in Spark from Spark Summit East 2016 in which Reynold Xin presents the concept of Streaming DataFrames Structuring Spark: DataFrames, Datasets, and Streaming A Deep Dive Into Structured Streaming by Tathagata \"TD\" Das from Spark Summit 2016 Arbitrary Stateful Aggregations in Structured Streaming in Apache Spark by Burak Yavuz","title":"Overview"},{"location":"overview/#spark-structured-streaming-and-streaming-queries","text":"Spark Structured Streaming ( Structured Streaming or Spark Streams ) is the module of Apache Spark for stream processing using streaming queries . Streaming queries can be expressed using a high-level declarative streaming API ( Dataset API ) or good ol' SQL ( SQL over stream / streaming SQL ). The declarative streaming Dataset API and SQL are executed on the underlying highly-optimized Spark SQL engine. The semantics of the Structured Streaming model is as follows (see the article Structured Streaming In Apache Spark ): At any time, the output of a continuous application is equivalent to executing a batch job on a prefix of the data. As of Spark 2.2.0, Structured Streaming has been marked stable and ready for production use. With that the other older streaming module Spark Streaming is considered obsolete and not for developing new streaming applications with Apache Spark. Spark Structured Streaming comes with two stream execution engines for executing streaming queries: MicroBatchExecution for Micro-Batch Stream Processing ContinuousExecution for Continuous Stream Processing The goal of Spark Structured Streaming is to unify streaming, interactive, and batch queries over structured datasets for developing end-to-end stream processing applications dubbed continuous applications using Spark SQL's Datasets API with additional support for the following features: Streaming Aggregation Streaming Join Streaming Watermark Arbitrary Stateful Streaming Aggregation Stateful Stream Processing In Structured Streaming, Spark developers describe custom streaming computations in the same way as with Spark SQL. Internally, Structured Streaming applies the user-defined structured query to the continuously and indefinitely arriving data to analyze real-time streaming data. Structured Streaming introduces the concept of streaming datasets that are infinite datasets with primitives like input streaming data sources and output streaming data sinks . A Dataset is streaming when its logical plan is streaming. val batchQuery = spark . read . // <-- batch non-streaming query csv ( \"sales\" ) assert ( batchQuery . isStreaming == false ) val streamingQuery = spark . readStream . // <-- streaming query format ( \"rate\" ). load assert ( streamingQuery . isStreaming ) Note Read up on Spark SQL, Datasets and logical plans in The Internals of Spark SQL online book. Structured Streaming models a stream of data as an infinite (and hence continuous) table that could be changed every streaming batch. You can specify output mode of a streaming dataset which is what gets written to a streaming sink (i.e. the infinite result table) when there is a new data available. Streaming Datasets use streaming query plans (as opposed to regular batch Datasets that are based on batch query plans). Note From this perspective, batch queries can be considered streaming Datasets executed once only (and is why some batch queries, e.g. KafkaSource , can easily work in batch mode). val batchQuery = spark . read . format ( \"rate\" ). load assert ( batchQuery . isStreaming == false ) val streamingQuery = spark . readStream . format ( \"rate\" ). load assert ( streamingQuery . isStreaming ) With Structured Streaming, Spark 2 aims at simplifying streaming analytics with little to no need to reason about effective data streaming (trying to hide the unnecessary complexity in your streaming analytics architectures). Structured streaming is defined by the following data abstractions in org.apache.spark.sql.streaming package: StreamingQuery Streaming Source Streaming Sink StreamingQueryManager Structured Streaming follows micro-batch model and periodically fetches data from the data source (and uses the DataFrame data abstraction to represent the fetched data for a certain batch). With Datasets as Spark SQL's view of structured data, structured streaming checks input sources for new data every trigger (time) and executes the (continuous) queries. Note The feature has also been called Streaming Spark SQL Query , Streaming DataFrames , Continuous DataFrame or Continuous Query . There have been lots of names before the Spark project settled on Structured Streaming.","title":"Spark Structured Streaming and Streaming Queries"},{"location":"overview/#references","text":"","title":"References"},{"location":"overview/#articles","text":"SPARK-8360 Structured Streaming (aka Streaming DataFrames) The official Structured Streaming Programming Guide Structured Streaming In Apache Spark What Spark's Structured Streaming really means","title":"Articles"},{"location":"overview/#videos","text":"The Future of Real Time in Spark from Spark Summit East 2016 in which Reynold Xin presents the concept of Streaming DataFrames Structuring Spark: DataFrames, Datasets, and Streaming A Deep Dive Into Structured Streaming by Tathagata \"TD\" Das from Spark Summit 2016 Arbitrary Stateful Aggregations in Structured Streaming in Apache Spark by Burak Yavuz","title":"Videos"},{"location":"spark-logging/","text":"Logging \u00b6 Tip Find out more on Logging in The Internals of Spark SQL online book.","title":"Logging"},{"location":"spark-logging/#logging","text":"Tip Find out more on Logging in The Internals of Spark SQL online book.","title":"Logging"},{"location":"spark-sql-streaming-ConsoleSinkProvider/","text":"ConsoleSinkProvider \u00b6 ConsoleSinkProvider is a DataSourceV2 for console data source format. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceV2.html[DataSourceV2 Contract] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. ConsoleSinkProvider is a < > and registers itself as the console data source format. [source, scala] \u00b6 import org.apache.spark.sql.streaming.Trigger val q = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") // \u2190 requests ConsoleSinkProvider for a sink .trigger(Trigger.Once) .start scala> println(q.lastProgress.sink) { \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@2392cfb1\" } [[CreatableRelationProvider]] ConsoleSinkProvider is a < >. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-CreatableRelationProvider.html[CreatableRelationProvider ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. === [[createRelation]] createRelation Method [source, scala] \u00b6 createRelation( sqlContext: SQLContext, mode: SaveMode, parameters: Map[String, String], data: DataFrame): BaseRelation NOTE: createRelation is part of the CreatableRelationProvider Contract to support writing a structured query (a DataFrame) per save mode. createRelation ...FIXME","title":"ConsoleSinkProvider"},{"location":"spark-sql-streaming-ConsoleSinkProvider/#consolesinkprovider","text":"ConsoleSinkProvider is a DataSourceV2 for console data source format. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceV2.html[DataSourceV2 Contract] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. ConsoleSinkProvider is a < > and registers itself as the console data source format.","title":"ConsoleSinkProvider"},{"location":"spark-sql-streaming-ConsoleSinkProvider/#source-scala","text":"import org.apache.spark.sql.streaming.Trigger val q = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") // \u2190 requests ConsoleSinkProvider for a sink .trigger(Trigger.Once) .start scala> println(q.lastProgress.sink) { \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@2392cfb1\" } [[CreatableRelationProvider]] ConsoleSinkProvider is a < >. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-CreatableRelationProvider.html[CreatableRelationProvider ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. === [[createRelation]] createRelation Method","title":"[source, scala]"},{"location":"spark-sql-streaming-ConsoleSinkProvider/#source-scala_1","text":"createRelation( sqlContext: SQLContext, mode: SaveMode, parameters: Map[String, String], data: DataFrame): BaseRelation NOTE: createRelation is part of the CreatableRelationProvider Contract to support writing a structured query (a DataFrame) per save mode. createRelation ...FIXME","title":"[source, scala]"},{"location":"spark-sql-streaming-ConsoleWriter/","text":"ConsoleWriter \u00b6 ConsoleWriter is a writer for console data source format.","title":"ConsoleWriter"},{"location":"spark-sql-streaming-ConsoleWriter/#consolewriter","text":"ConsoleWriter is a writer for console data source format.","title":"ConsoleWriter"},{"location":"spark-sql-streaming-FlatMapGroupsWithStateExecHelper/","text":"== [[FlatMapGroupsWithStateExecHelper]] FlatMapGroupsWithStateExecHelper FlatMapGroupsWithStateExecHelper is a utility with the main purpose of < > for FlatMapGroupsWithStateExec physical operator. === [[createStateManager]] Creating StateManager -- createStateManager Method [source, scala] \u00b6 createStateManager( stateEncoder: ExpressionEncoder[Any], shouldStoreTimestamp: Boolean, stateFormatVersion: Int): StateManager createStateManager simply creates a < > (with the stateEncoder and shouldStoreTimestamp flag) based on stateFormatVersion : < > for 1 < > for 2 createStateManager throws an IllegalArgumentException for stateFormatVersion not 1 or 2 : Version [stateFormatVersion] is invalid createStateManager is used for the StateManager for FlatMapGroupsWithStateExec physical operator.","title":"FlatMapGroupsWithStateExecHelper Helper Class"},{"location":"spark-sql-streaming-FlatMapGroupsWithStateExecHelper/#source-scala","text":"createStateManager( stateEncoder: ExpressionEncoder[Any], shouldStoreTimestamp: Boolean, stateFormatVersion: Int): StateManager createStateManager simply creates a < > (with the stateEncoder and shouldStoreTimestamp flag) based on stateFormatVersion : < > for 1 < > for 2 createStateManager throws an IllegalArgumentException for stateFormatVersion not 1 or 2 : Version [stateFormatVersion] is invalid createStateManager is used for the StateManager for FlatMapGroupsWithStateExec physical operator.","title":"[source, scala]"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/","text":"JoinStateWatermarkPredicate \u00b6 JoinStateWatermarkPredicate is the < > of < > that are described by a < > and < >. JoinStateWatermarkPredicate is created using StreamingSymmetricHashJoinHelper utility (for planning a StreamingSymmetricHashJoinExec physical operator for execution with execution-specific configuration ) JoinStateWatermarkPredicate is used to create a OneSideHashJoiner (and JoinStateWatermarkPredicates ). [[contract]] .JoinStateWatermarkPredicate Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | desc a| [[desc]] [source, scala] \u00b6 desc: String \u00b6 Used exclusively for the < > | expr a| [[expr]] [source, scala] \u00b6 expr: Expression \u00b6 A Catalyst Expression Used for the < > and a JoinStateWatermarkPredicates (for StreamingSymmetricHashJoinExec physical operator) |=== [[implementations]] .JoinStateWatermarkPredicates [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | JoinStateWatermarkPredicate | Description | JoinStateKeyWatermarkPredicate a| [[JoinStateKeyWatermarkPredicate]] Watermark predicate on state keys (i.e. when the streaming watermark is defined either on the left or right join keys) Created when StreamingSymmetricHashJoinHelper utility is requested for a JoinStateWatermarkPredicates for the left and right side of a stream-stream join (when IncrementalExecution is requested to optimize a query plan with a StreamingSymmetricHashJoinExec physical operator) Used when OneSideHashJoiner is requested for the stateKeyWatermarkPredicateFunc and then to remove an old state | JoinStateValueWatermarkPredicate | [[JoinStateValueWatermarkPredicate]] Watermark predicate on state values |=== NOTE: JoinStateWatermarkPredicate is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). === [[toString]] Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString uses the < > and < > for the string representation: [desc]: [expr]","title":"JoinStateWatermarkPredicate"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/#joinstatewatermarkpredicate","text":"JoinStateWatermarkPredicate is the < > of < > that are described by a < > and < >. JoinStateWatermarkPredicate is created using StreamingSymmetricHashJoinHelper utility (for planning a StreamingSymmetricHashJoinExec physical operator for execution with execution-specific configuration ) JoinStateWatermarkPredicate is used to create a OneSideHashJoiner (and JoinStateWatermarkPredicates ). [[contract]] .JoinStateWatermarkPredicate Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | desc a| [[desc]]","title":"JoinStateWatermarkPredicate"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/#desc-string","text":"Used exclusively for the < > | expr a| [[expr]]","title":"desc: String"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/#expr-expression","text":"A Catalyst Expression Used for the < > and a JoinStateWatermarkPredicates (for StreamingSymmetricHashJoinExec physical operator) |=== [[implementations]] .JoinStateWatermarkPredicates [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | JoinStateWatermarkPredicate | Description | JoinStateKeyWatermarkPredicate a| [[JoinStateKeyWatermarkPredicate]] Watermark predicate on state keys (i.e. when the streaming watermark is defined either on the left or right join keys) Created when StreamingSymmetricHashJoinHelper utility is requested for a JoinStateWatermarkPredicates for the left and right side of a stream-stream join (when IncrementalExecution is requested to optimize a query plan with a StreamingSymmetricHashJoinExec physical operator) Used when OneSideHashJoiner is requested for the stateKeyWatermarkPredicateFunc and then to remove an old state | JoinStateValueWatermarkPredicate | [[JoinStateValueWatermarkPredicate]] Watermark predicate on state values |=== NOTE: JoinStateWatermarkPredicate is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). === [[toString]] Textual Representation -- toString Method","title":"expr: Expression"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/#tostring-string","text":"NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString uses the < > and < > for the string representation: [desc]: [expr]","title":"toString: String"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicates/","text":"JoinStateWatermarkPredicates -- Watermark Predicates for State Removal \u00b6 [[creating-instance]] JoinStateWatermarkPredicates contains watermark predicates for state removal of the children of a StreamingSymmetricHashJoinExec physical operator: [[left]] < > for the left-hand side of a join (default: None ) [[right]] < > for the right-hand side of a join (default: None ) JoinStateWatermarkPredicates is < > for the following: < > physical operator is created (with the optional properties undefined, including < >) StreamingSymmetricHashJoinHelper utility is requested for one (for IncrementalExecution for the state preparation rule to optimize and specify the execution-specific configuration for a query plan with StreamingSymmetricHashJoinExec physical operators) === [[toString]] Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString uses the < > and < > predicates for the string representation: state cleanup [ left [left], right [right] ]","title":"JoinStateWatermarkPredicates"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicates/#joinstatewatermarkpredicates-watermark-predicates-for-state-removal","text":"[[creating-instance]] JoinStateWatermarkPredicates contains watermark predicates for state removal of the children of a StreamingSymmetricHashJoinExec physical operator: [[left]] < > for the left-hand side of a join (default: None ) [[right]] < > for the right-hand side of a join (default: None ) JoinStateWatermarkPredicates is < > for the following: < > physical operator is created (with the optional properties undefined, including < >) StreamingSymmetricHashJoinHelper utility is requested for one (for IncrementalExecution for the state preparation rule to optimize and specify the execution-specific configuration for a query plan with StreamingSymmetricHashJoinExec physical operators) === [[toString]] Textual Representation -- toString Method","title":"JoinStateWatermarkPredicates -- Watermark Predicates for State Removal"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicates/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicates/#tostring-string","text":"NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString uses the < > and < > predicates for the string representation: state cleanup [ left [left], right [right] ]","title":"toString: String"},{"location":"spark-sql-streaming-KeyToNumValuesStore/","text":"KeyToNumValuesStore \u00b6 KeyToNumValuesStore is a StateStoreHandler (of KeyToNumValuesType ) for SymmetricHashJoinStateManager to manage a < >. [[stateStore]] As a StateStoreHandler , KeyToNumValuesStore manages a state store (that is loaded ) with the join keys (per key schema ) and their count (per < >). [[longValueSchema]] KeyToNumValuesStore uses the schema for values in the < > with one field value (of type long ) that is the number of value rows (count). [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore=ALL Refer to < >. \u00b6 === [[get]] Looking Up Number Of Value Rows For Given Key (Value Count) -- get Method [source, scala] \u00b6 get(key: UnsafeRow): Long \u00b6 get requests the < > for the value for the given key and returns the long value at 0 th position (of the row found) or 0 . get is used when SymmetricHashJoinStateManager is requested for the values for a given key and append a new value to a given key . === [[put]] Storing Key Count For Given Key -- put Method [source, scala] \u00b6 put( key: UnsafeRow, numValues: Long): Unit put stores the numValues at the 0 th position (of the internal unsafe row) and requests the < > to store it with the given key . put requires that the numValues count is greater than 0 (or throws an IllegalArgumentException ). put is used when SymmetricHashJoinStateManager is requested for the append a new value to a given key and updateNumValueForCurrentKey . === [[iterator]] All State Keys and Values -- iterator Method [source, scala] \u00b6 iterator: Iterator[KeyAndNumValues] \u00b6 iterator simply requests the < > for all state keys and values . iterator is used when SymmetricHashJoinStateManager is requested to removeByKeyCondition and removeByValueCondition . === [[remove]] Removing State Key -- remove Method [source, scala] \u00b6 remove(key: UnsafeRow): Unit \u00b6 remove simply requests the < > to remove the given key . remove is used when...FIXME","title":"KeyToNumValuesStore"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#keytonumvaluesstore","text":"KeyToNumValuesStore is a StateStoreHandler (of KeyToNumValuesType ) for SymmetricHashJoinStateManager to manage a < >. [[stateStore]] As a StateStoreHandler , KeyToNumValuesStore manages a state store (that is loaded ) with the join keys (per key schema ) and their count (per < >). [[longValueSchema]] KeyToNumValuesStore uses the schema for values in the < > with one field value (of type long ) that is the number of value rows (count). [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore=ALL","title":"KeyToNumValuesStore"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#refer-to","text":"=== [[get]] Looking Up Number Of Value Rows For Given Key (Value Count) -- get Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-KeyToNumValuesStore/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#getkey-unsaferow-long","text":"get requests the < > for the value for the given key and returns the long value at 0 th position (of the row found) or 0 . get is used when SymmetricHashJoinStateManager is requested for the values for a given key and append a new value to a given key . === [[put]] Storing Key Count For Given Key -- put Method","title":"get(key: UnsafeRow): Long"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#source-scala_1","text":"put( key: UnsafeRow, numValues: Long): Unit put stores the numValues at the 0 th position (of the internal unsafe row) and requests the < > to store it with the given key . put requires that the numValues count is greater than 0 (or throws an IllegalArgumentException ). put is used when SymmetricHashJoinStateManager is requested for the append a new value to a given key and updateNumValueForCurrentKey . === [[iterator]] All State Keys and Values -- iterator Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#iterator-iteratorkeyandnumvalues","text":"iterator simply requests the < > for all state keys and values . iterator is used when SymmetricHashJoinStateManager is requested to removeByKeyCondition and removeByValueCondition . === [[remove]] Removing State Key -- remove Method","title":"iterator: Iterator[KeyAndNumValues]"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#removekey-unsaferow-unit","text":"remove simply requests the < > to remove the given key . remove is used when...FIXME","title":"remove(key: UnsafeRow): Unit"},{"location":"spark-sql-streaming-KeyValueGroupedDataset-flatMapGroupsWithState/","text":"flatMapGroupsWithState Operator -- Arbitrary Stateful Streaming Aggregation (with Explicit State Logic) \u00b6 KeyValueGroupedDataset [ K , V ]. flatMapGroupsWithState [ S : Encoder , U : Encoder ]( outputMode : OutputMode , timeoutConf : GroupStateTimeout )( func : ( K , Iterator [ V ], GroupState [ S ]) => Iterator [ U ]): Dataset [ U ] flatMapGroupsWithState operator is used for Arbitrary Stateful Streaming Aggregation (with Explicit State Logic). flatMapGroupsWithState requires that the given OutputMode is either Append or Update (and reports an IllegalArgumentException at runtime). Note An OutputMode is a required argument, but does not seem to be used at all. Check out the question What's the purpose of OutputMode in flatMapGroupsWithState? How/where is it used? on StackOverflow. Every time the state function func is executed for a key, the state (as GroupState[S] ) is for this key only. Note K is the type of the keys in KeyValueGroupedDataset V is the type of the values (per key) in KeyValueGroupedDataset S is the user-defined type of the state as maintained for each group U is the type of rows in the result Dataset flatMapGroupsWithState creates a new Dataset with FlatMapGroupsWithState unary logical operator.","title":"flatMapGroupsWithState Operator"},{"location":"spark-sql-streaming-KeyValueGroupedDataset-flatMapGroupsWithState/#flatmapgroupswithstate-operator-arbitrary-stateful-streaming-aggregation-with-explicit-state-logic","text":"KeyValueGroupedDataset [ K , V ]. flatMapGroupsWithState [ S : Encoder , U : Encoder ]( outputMode : OutputMode , timeoutConf : GroupStateTimeout )( func : ( K , Iterator [ V ], GroupState [ S ]) => Iterator [ U ]): Dataset [ U ] flatMapGroupsWithState operator is used for Arbitrary Stateful Streaming Aggregation (with Explicit State Logic). flatMapGroupsWithState requires that the given OutputMode is either Append or Update (and reports an IllegalArgumentException at runtime). Note An OutputMode is a required argument, but does not seem to be used at all. Check out the question What's the purpose of OutputMode in flatMapGroupsWithState? How/where is it used? on StackOverflow. Every time the state function func is executed for a key, the state (as GroupState[S] ) is for this key only. Note K is the type of the keys in KeyValueGroupedDataset V is the type of the values (per key) in KeyValueGroupedDataset S is the user-defined type of the state as maintained for each group U is the type of rows in the result Dataset flatMapGroupsWithState creates a new Dataset with FlatMapGroupsWithState unary logical operator.","title":"flatMapGroupsWithState Operator -- Arbitrary Stateful Streaming Aggregation (with Explicit State Logic)"},{"location":"spark-sql-streaming-KeyValueGroupedDataset-mapGroupsWithState/","text":"== [[mapGroupsWithState]] mapGroupsWithState Operator -- Stateful Streaming Aggregation (with Explicit State Logic) [source, scala] \u00b6 mapGroupsWithState S: Encoder, U: Encoder : Dataset[U] // <1> mapGroupsWithState S: Encoder, U: Encoder ( func: (K, Iterator[V], GroupState[S]) => U): Dataset[U] <1> Uses GroupStateTimeout.NoTimeout for timeoutConf mapGroupsWithState operator...FIXME Note mapGroupsWithState is a special case of flatMapGroupsWithState operator with the following: func being transformed to return a single-element Iterator Update output mode mapGroupsWithState also creates a FlatMapGroupsWithState with isMapGroupsWithState internal flag enabled. // numGroups defined at the beginning scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] import org.apache.spark.sql.streaming.GroupState def mappingFunc(key: Long, values: Iterator[(java.sql.Timestamp, Long)], state: GroupState[Long]): Long = { println(s\">>> key: $key => state: $state\") val newState = state.getOption.map(_ + values.size).getOrElse(0L) state.update(newState) key } import org.apache.spark.sql.streaming.GroupStateTimeout val longs = numGroups.mapGroupsWithState( timeoutConf = GroupStateTimeout.ProcessingTimeTimeout)( func = mappingFunc) import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = longs. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). // <-- required for mapGroupsWithState start // Note GroupState ------------------------------------------- Batch: 1 ------------------------------------------- >>> key: 0 => state: GroupState(<undefined>) >>> key: 1 => state: GroupState(<undefined>) +-----+ |value| +-----+ | 0| | 1| +-----+ ------------------------------------------- Batch: 2 ------------------------------------------- >>> key: 0 => state: GroupState(0) >>> key: 1 => state: GroupState(0) +-----+ |value| +-----+ | 0| | 1| +-----+ ------------------------------------------- Batch: 3 ------------------------------------------- >>> key: 0 => state: GroupState(4) >>> key: 1 => state: GroupState(4) +-----+ |value| +-----+ | 0| | 1| +-----+ // in the end spark.streams.active.foreach(_.stop)","title":"mapGroupsWithState Operator"},{"location":"spark-sql-streaming-KeyValueGroupedDataset-mapGroupsWithState/#source-scala","text":"mapGroupsWithState S: Encoder, U: Encoder : Dataset[U] // <1> mapGroupsWithState S: Encoder, U: Encoder ( func: (K, Iterator[V], GroupState[S]) => U): Dataset[U] <1> Uses GroupStateTimeout.NoTimeout for timeoutConf mapGroupsWithState operator...FIXME Note mapGroupsWithState is a special case of flatMapGroupsWithState operator with the following: func being transformed to return a single-element Iterator Update output mode mapGroupsWithState also creates a FlatMapGroupsWithState with isMapGroupsWithState internal flag enabled. // numGroups defined at the beginning scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] import org.apache.spark.sql.streaming.GroupState def mappingFunc(key: Long, values: Iterator[(java.sql.Timestamp, Long)], state: GroupState[Long]): Long = { println(s\">>> key: $key => state: $state\") val newState = state.getOption.map(_ + values.size).getOrElse(0L) state.update(newState) key } import org.apache.spark.sql.streaming.GroupStateTimeout val longs = numGroups.mapGroupsWithState( timeoutConf = GroupStateTimeout.ProcessingTimeTimeout)( func = mappingFunc) import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = longs. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). // <-- required for mapGroupsWithState start // Note GroupState ------------------------------------------- Batch: 1 ------------------------------------------- >>> key: 0 => state: GroupState(<undefined>) >>> key: 1 => state: GroupState(<undefined>) +-----+ |value| +-----+ | 0| | 1| +-----+ ------------------------------------------- Batch: 2 ------------------------------------------- >>> key: 0 => state: GroupState(0) >>> key: 1 => state: GroupState(0) +-----+ |value| +-----+ | 0| | 1| +-----+ ------------------------------------------- Batch: 3 ------------------------------------------- >>> key: 0 => state: GroupState(4) >>> key: 1 => state: GroupState(4) +-----+ |value| +-----+ | 0| | 1| +-----+ // in the end spark.streams.active.foreach(_.stop)","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/","text":"KeyWithIndexToValueStore \u00b6 KeyWithIndexToValueStore is a StateStoreHandler (of KeyWithIndexToValueType ) for SymmetricHashJoinStateManager to manage a < >. .KeyToNumValuesStore, KeyWithIndexToValueStore and Stream-Stream Join image::images/KeyToNumValuesStore-KeyWithIndexToValueStore.png[align=\"center\"] [[stateStore]] As a StateStoreHandler , KeyWithIndexToValueStore manages a state store (that is loaded ) for keys and values per the < > and input values schemas, respectively. [[keyWithIndexSchema]] KeyWithIndexToValueStore uses a schema (for the < >) that is the key schema (of the parent SymmetricHashJoinStateManager ) with an extra field index of type long . [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyWithIndexToValueStore to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyWithIndexToValueStore=ALL Refer to < >. \u00b6 === [[get]] Looking Up State Row For Given Key and Index -- get Method [source, scala] \u00b6 get( key: UnsafeRow, valueIndex: Long): UnsafeRow get simply requests the internal state store to look up the value for the given < >. get is used when SymmetricHashJoinStateManager is requested to removeByValueCondition . === [[getAll]] Retrieving (Given Number of) Values for Key -- getAll Method [source, scala] \u00b6 getAll( key: UnsafeRow, numValues: Long): Iterator[KeyWithIndexAndValue] getAll ...FIXME getAll is used when SymmetricHashJoinStateManager is requested to get values for a given key and removeByKeyCondition . === [[put]] Storing State Row For Given Key and Index -- put Method [source, scala] \u00b6 put( key: UnsafeRow, valueIndex: Long, value: UnsafeRow): Unit put ...FIXME put is used when SymmetricHashJoinStateManager is requested to append a new value to a given key and removeByKeyCondition . === [[remove]] remove Method [source, scala] \u00b6 remove( key: UnsafeRow, valueIndex: Long): Unit remove ...FIXME remove is used when SymmetricHashJoinStateManager is requested to removeByKeyCondition and removeByValueCondition . === [[keyWithIndexRow]] keyWithIndexRow Internal Method [source, scala] \u00b6 keyWithIndexRow( key: UnsafeRow, valueIndex: Long): UnsafeRow keyWithIndexRow uses the < > to generate an UnsafeRow for the key and sets the valueIndex at the < > position. NOTE: keyWithIndexRow is used when KeyWithIndexToValueStore is requested to < >, < >, < >, < > and < >. === [[removeAllValues]] removeAllValues Method [source, scala] \u00b6 removeAllValues( key: UnsafeRow, numValues: Long): Unit removeAllValues ...FIXME NOTE: removeAllValues does not seem to be used at all. === [[iterator]] iterator Method [source, scala] \u00b6 iterator: Iterator[KeyWithIndexAndValue] \u00b6 iterator ...FIXME NOTE: iterator does not seem to be used at all. Internal Properties \u00b6 [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | indexOrdinalInKeyWithIndexRow a| [[indexOrdinalInKeyWithIndexRow]] Position of the index in the key row (which corresponds to the number of the key attributes ) Used exclusively in the < > | keyWithIndexExprs a| [[keyWithIndexExprs]] keyAttributes with Literal(1L) expression appended Used exclusively for the < > projection | keyWithIndexRowGenerator a| [[keyWithIndexRowGenerator]] UnsafeProjection for the < > bound to the keyAttributes Used exclusively in < > |===","title":"KeyWithIndexToValueStore"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#keywithindextovaluestore","text":"KeyWithIndexToValueStore is a StateStoreHandler (of KeyWithIndexToValueType ) for SymmetricHashJoinStateManager to manage a < >. .KeyToNumValuesStore, KeyWithIndexToValueStore and Stream-Stream Join image::images/KeyToNumValuesStore-KeyWithIndexToValueStore.png[align=\"center\"] [[stateStore]] As a StateStoreHandler , KeyWithIndexToValueStore manages a state store (that is loaded ) for keys and values per the < > and input values schemas, respectively. [[keyWithIndexSchema]] KeyWithIndexToValueStore uses a schema (for the < >) that is the key schema (of the parent SymmetricHashJoinStateManager ) with an extra field index of type long . [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyWithIndexToValueStore to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyWithIndexToValueStore=ALL","title":"KeyWithIndexToValueStore"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#refer-to","text":"=== [[get]] Looking Up State Row For Given Key and Index -- get Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#source-scala","text":"get( key: UnsafeRow, valueIndex: Long): UnsafeRow get simply requests the internal state store to look up the value for the given < >. get is used when SymmetricHashJoinStateManager is requested to removeByValueCondition . === [[getAll]] Retrieving (Given Number of) Values for Key -- getAll Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#source-scala_1","text":"getAll( key: UnsafeRow, numValues: Long): Iterator[KeyWithIndexAndValue] getAll ...FIXME getAll is used when SymmetricHashJoinStateManager is requested to get values for a given key and removeByKeyCondition . === [[put]] Storing State Row For Given Key and Index -- put Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#source-scala_2","text":"put( key: UnsafeRow, valueIndex: Long, value: UnsafeRow): Unit put ...FIXME put is used when SymmetricHashJoinStateManager is requested to append a new value to a given key and removeByKeyCondition . === [[remove]] remove Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#source-scala_3","text":"remove( key: UnsafeRow, valueIndex: Long): Unit remove ...FIXME remove is used when SymmetricHashJoinStateManager is requested to removeByKeyCondition and removeByValueCondition . === [[keyWithIndexRow]] keyWithIndexRow Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#source-scala_4","text":"keyWithIndexRow( key: UnsafeRow, valueIndex: Long): UnsafeRow keyWithIndexRow uses the < > to generate an UnsafeRow for the key and sets the valueIndex at the < > position. NOTE: keyWithIndexRow is used when KeyWithIndexToValueStore is requested to < >, < >, < >, < > and < >. === [[removeAllValues]] removeAllValues Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#source-scala_5","text":"removeAllValues( key: UnsafeRow, numValues: Long): Unit removeAllValues ...FIXME NOTE: removeAllValues does not seem to be used at all. === [[iterator]] iterator Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#iterator-iteratorkeywithindexandvalue","text":"iterator ...FIXME NOTE: iterator does not seem to be used at all.","title":"iterator: Iterator[KeyWithIndexAndValue]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#internal-properties","text":"[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | indexOrdinalInKeyWithIndexRow a| [[indexOrdinalInKeyWithIndexRow]] Position of the index in the key row (which corresponds to the number of the key attributes ) Used exclusively in the < > | keyWithIndexExprs a| [[keyWithIndexExprs]] keyAttributes with Literal(1L) expression appended Used exclusively for the < > projection | keyWithIndexRowGenerator a| [[keyWithIndexRowGenerator]] UnsafeProjection for the < > bound to the keyAttributes Used exclusively in < > |===","title":"Internal Properties"},{"location":"spark-sql-streaming-MemoryStreamWriter/","text":"== [[MemoryStreamWriter]] MemoryStreamWriter MemoryStreamWriter is...FIXME","title":"MemoryStreamWriter"},{"location":"spark-sql-streaming-RateSourceProvider/","text":"RateSourceProvider \u00b6 RateSourceProvider is a StreamSourceProvider for RateStreamSource (that acts as the source for rate format). RateSourceProvider is also a DataSourceRegister . [[shortName]] The short name of the data source is rate . [[sourceSchema]]","title":"RateSourceProvider"},{"location":"spark-sql-streaming-RateSourceProvider/#ratesourceprovider","text":"RateSourceProvider is a StreamSourceProvider for RateStreamSource (that acts as the source for rate format). RateSourceProvider is also a DataSourceRegister . [[shortName]] The short name of the data source is rate . [[sourceSchema]]","title":"RateSourceProvider"},{"location":"spark-sql-streaming-RateStreamMicroBatchReader/","text":"== [[RateStreamMicroBatchReader]] RateStreamMicroBatchReader RateStreamMicroBatchReader is...FIXME","title":"RateStreamMicroBatchReader"},{"location":"spark-sql-streaming-RateStreamSource/","text":"RateStreamSource \u00b6 RateStreamSource is a streaming source that generates < > that can be useful for testing and PoCs. RateStreamSource < > for rate format (that is registered by spark-sql-streaming-RateSourceProvider.md[RateSourceProvider]). [source, scala] \u00b6 val rates = spark .readStream .format(\"rate\") // \u2190 use RateStreamSource .option(\"rowsPerSecond\", 1) .load [[options]] .RateStreamSource's Options [cols=\"1m,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default Value | Description | numPartitions | (default parallelism) | [[numPartitions]] Number of partitions to use | rampUpTime | 0 (seconds) | [[rampUpTime]] | rowsPerSecond | 1 | [[rowsPerSecond]] Number of rows to generate per second (has to be greater than 0 ) |=== [[schema]] RateStreamSource uses a predefined schema that cannot be changed. [source, scala] \u00b6 val schema = rates.schema scala> println(schema.treeString) root |-- timestamp: timestamp (nullable = true) |-- value: long (nullable = true) .RateStreamSource's Dataset Schema (in the positional order) [cols=\"1m,1m\",options=\"header\",width=\"100%\"] |=== | Name | Type | timestamp | TimestampType | value | LongType |=== [[internal-registries]] .RateStreamSource's Internal Registries and Counters [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | clock | [[clock]] | lastTimeMs | [[lastTimeMs]] | maxSeconds | [[maxSeconds]] | startTimeMs | [[startTimeMs]] |=== [[logging]] [TIP] ==== Enable INFO or DEBUG logging levels for org.apache.spark.sql.execution.streaming.RateStreamSource to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.RateStreamSource=DEBUG Refer to spark-sql-streaming-spark-logging.md[Logging]. \u00b6 === [[getBatch]] Generating DataFrame for Streaming Batch -- getBatch Method [source, scala] \u00b6 getBatch(start: Option[Offset], end: Offset): DataFrame \u00b6 getBatch is a part of the Source abstraction. Internally, getBatch calculates the seconds to start from and end at (from the input start and end offsets) or assumes 0 . getBatch then calculates the values to generate for the start and end seconds. You should see the following DEBUG message in the logs: DEBUG RateStreamSource: startSeconds: [startSeconds], endSeconds: [endSeconds], rangeStart: [rangeStart], rangeEnd: [rangeEnd] If the start and end ranges are equal, getBatch creates an empty DataFrame (with the < >) and returns. Otherwise, when the ranges are different, getBatch creates a DataFrame using SparkContext.range operator (for the start and end ranges and < > partitions). === [[creating-instance]] Creating RateStreamSource Instance RateStreamSource takes the following when created: [[sqlContext]] SQLContext [[metadataPath]] Path to the metadata [[rowsPerSecond]] Rows per second [[rampUpTimeSeconds]] RampUp time in seconds [[numPartitions]] Number of partitions [[useManualClock]] Flag to whether to use ManualClock ( true ) or SystemClock ( false ) RateStreamSource initializes the < >.","title":"RateStreamSource"},{"location":"spark-sql-streaming-RateStreamSource/#ratestreamsource","text":"RateStreamSource is a streaming source that generates < > that can be useful for testing and PoCs. RateStreamSource < > for rate format (that is registered by spark-sql-streaming-RateSourceProvider.md[RateSourceProvider]).","title":"RateStreamSource"},{"location":"spark-sql-streaming-RateStreamSource/#source-scala","text":"val rates = spark .readStream .format(\"rate\") // \u2190 use RateStreamSource .option(\"rowsPerSecond\", 1) .load [[options]] .RateStreamSource's Options [cols=\"1m,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default Value | Description | numPartitions | (default parallelism) | [[numPartitions]] Number of partitions to use | rampUpTime | 0 (seconds) | [[rampUpTime]] | rowsPerSecond | 1 | [[rowsPerSecond]] Number of rows to generate per second (has to be greater than 0 ) |=== [[schema]] RateStreamSource uses a predefined schema that cannot be changed.","title":"[source, scala]"},{"location":"spark-sql-streaming-RateStreamSource/#source-scala_1","text":"val schema = rates.schema scala> println(schema.treeString) root |-- timestamp: timestamp (nullable = true) |-- value: long (nullable = true) .RateStreamSource's Dataset Schema (in the positional order) [cols=\"1m,1m\",options=\"header\",width=\"100%\"] |=== | Name | Type | timestamp | TimestampType | value | LongType |=== [[internal-registries]] .RateStreamSource's Internal Registries and Counters [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | clock | [[clock]] | lastTimeMs | [[lastTimeMs]] | maxSeconds | [[maxSeconds]] | startTimeMs | [[startTimeMs]] |=== [[logging]] [TIP] ==== Enable INFO or DEBUG logging levels for org.apache.spark.sql.execution.streaming.RateStreamSource to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.RateStreamSource=DEBUG","title":"[source, scala]"},{"location":"spark-sql-streaming-RateStreamSource/#refer-to-spark-sql-streaming-spark-loggingmdlogging","text":"=== [[getBatch]] Generating DataFrame for Streaming Batch -- getBatch Method","title":"Refer to spark-sql-streaming-spark-logging.md[Logging]."},{"location":"spark-sql-streaming-RateStreamSource/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-RateStreamSource/#getbatchstart-optionoffset-end-offset-dataframe","text":"getBatch is a part of the Source abstraction. Internally, getBatch calculates the seconds to start from and end at (from the input start and end offsets) or assumes 0 . getBatch then calculates the values to generate for the start and end seconds. You should see the following DEBUG message in the logs: DEBUG RateStreamSource: startSeconds: [startSeconds], endSeconds: [endSeconds], rangeStart: [rangeStart], rangeEnd: [rangeEnd] If the start and end ranges are equal, getBatch creates an empty DataFrame (with the < >) and returns. Otherwise, when the ranges are different, getBatch creates a DataFrame using SparkContext.range operator (for the start and end ranges and < > partitions). === [[creating-instance]] Creating RateStreamSource Instance RateStreamSource takes the following when created: [[sqlContext]] SQLContext [[metadataPath]] Path to the metadata [[rowsPerSecond]] Rows per second [[rampUpTimeSeconds]] RampUp time in seconds [[numPartitions]] Number of partitions [[useManualClock]] Flag to whether to use ManualClock ( true ) or SystemClock ( false ) RateStreamSource initializes the < >.","title":"getBatch(start: Option[Offset], end: Offset): DataFrame"},{"location":"spark-sql-streaming-StateManager/","text":"StateManager \u00b6 StateManager is the < > of < > that act as middlemen between state stores and the FlatMapGroupsWithStateExec physical operator used in Arbitrary Stateful Streaming Aggregation . [[contract]] .StateManager Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | getAllState a| [[getAllState]] [source, scala] \u00b6 getAllState(store: StateStore): Iterator[StateData] \u00b6 Retrieves all state data (for all keys) from the StateStore Used when InputProcessor is requested to processTimedOutState | getState a| [[getState]] [source, scala] \u00b6 getState( store: StateStore, keyRow: UnsafeRow): StateData Gets the state data for the key from the StateStore Used exclusively when InputProcessor is requested to processNewData | putState a| [[putState]] [source, scala] \u00b6 putState( store: StateStore, keyRow: UnsafeRow, state: Any, timeoutTimestamp: Long): Unit Persists ( puts ) the state value for the key in the StateStore Used exclusively when InputProcessor is requested to callFunctionAndUpdateState ( right after all rows have been processed ) | removeState a| [[removeState]] [source, scala] \u00b6 removeState( store: StateStore, keyRow: UnsafeRow): Unit Removes the state for the key from the StateStore Used exclusively when InputProcessor is requested to callFunctionAndUpdateState ( right after all rows have been processed ) | stateSchema a| [[stateSchema]] [source, scala] \u00b6 stateSchema: StructType \u00b6 State schema Note It looks like (in StateManager of the FlatMapGroupsWithStateExec physical operator) stateSchema is used for the schema of state value objects (not state keys as they are described by the grouping attributes instead). Used when: FlatMapGroupsWithStateExec physical operator is executed StateManagerImplBase is requested for the stateDeserializerFunc |=== [[implementations]] NOTE: < > is the one and only known direct implementation of the < > in Spark Structured Streaming. NOTE: StateManager is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file).","title":"StateManager"},{"location":"spark-sql-streaming-StateManager/#statemanager","text":"StateManager is the < > of < > that act as middlemen between state stores and the FlatMapGroupsWithStateExec physical operator used in Arbitrary Stateful Streaming Aggregation . [[contract]] .StateManager Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | getAllState a| [[getAllState]]","title":"StateManager"},{"location":"spark-sql-streaming-StateManager/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManager/#getallstatestore-statestore-iteratorstatedata","text":"Retrieves all state data (for all keys) from the StateStore Used when InputProcessor is requested to processTimedOutState | getState a| [[getState]]","title":"getAllState(store: StateStore): Iterator[StateData]"},{"location":"spark-sql-streaming-StateManager/#source-scala_1","text":"getState( store: StateStore, keyRow: UnsafeRow): StateData Gets the state data for the key from the StateStore Used exclusively when InputProcessor is requested to processNewData | putState a| [[putState]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManager/#source-scala_2","text":"putState( store: StateStore, keyRow: UnsafeRow, state: Any, timeoutTimestamp: Long): Unit Persists ( puts ) the state value for the key in the StateStore Used exclusively when InputProcessor is requested to callFunctionAndUpdateState ( right after all rows have been processed ) | removeState a| [[removeState]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManager/#source-scala_3","text":"removeState( store: StateStore, keyRow: UnsafeRow): Unit Removes the state for the key from the StateStore Used exclusively when InputProcessor is requested to callFunctionAndUpdateState ( right after all rows have been processed ) | stateSchema a| [[stateSchema]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManager/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManager/#stateschema-structtype","text":"State schema Note It looks like (in StateManager of the FlatMapGroupsWithStateExec physical operator) stateSchema is used for the schema of state value objects (not state keys as they are described by the grouping attributes instead). Used when: FlatMapGroupsWithStateExec physical operator is executed StateManagerImplBase is requested for the stateDeserializerFunc |=== [[implementations]] NOTE: < > is the one and only known direct implementation of the < > in Spark Structured Streaming. NOTE: StateManager is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file).","title":"stateSchema: StructType"},{"location":"spark-sql-streaming-StateManagerImplBase/","text":"== [[StateManagerImplBase]] StateManagerImplBase StateManagerImplBase is the < > of the < > for < > of FlatMapGroupsWithStateExec physical operator with the following features: Use Catalyst expressions for < > and < > Use < > when < > with the < > flag on [[contract]] .StateManagerImplBase Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | stateDeserializerExpr a| [[stateDeserializerExpr]] [source, scala] \u00b6 stateDeserializerExpr: Expression \u00b6 State deserializer , i.e. a Catalyst expression to deserialize a state object from a row ( UnsafeRow ) Used exclusively for the < > | stateSerializerExprs a| [[stateSerializerExprs]] [source, scala] \u00b6 stateSerializerExprs: Seq[Expression] \u00b6 State serializer , i.e. Catalyst expressions to serialize a state object to a row ( UnsafeRow ) Used exclusively for the < > | timeoutTimestampOrdinalInRow a| [[timeoutTimestampOrdinalInRow]] [source, scala] \u00b6 timeoutTimestampOrdinalInRow: Int \u00b6 Position of the timeout timestamp in a state row Used when StateManagerImplBase is requested to < > and < > |=== [[implementations]] .StateManagerImplBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | StateManagerImplBase | Description | < > | [[StateManagerImplV1]] Legacy < > | < > | [[StateManagerImplV2]] Default < > |=== === [[creating-instance]][[shouldStoreTimestamp]] Creating StateManagerImplBase Instance StateManagerImplBase takes a single shouldStoreTimestamp flag to be created (that is set when the < > are created). NOTE: StateManagerImplBase is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. StateManagerImplBase initializes the < >. === [[getState]] Getting State Data for Key from StateStore -- getState Method [source, scala] \u00b6 getState( store: StateStore, keyRow: UnsafeRow): StateData getState is part of the StateManager abstraction. getState ...FIXME === [[putState]] Persisting State Value for Key in StateStore -- putState Method [source, scala] \u00b6 putState( store: StateStore, key: UnsafeRow, state: Any, timestamp: Long): Unit putState is part of the StateManager abstraction. putState ...FIXME === [[removeState]] Removing State for Key from StateStore -- removeState Method [source, scala] \u00b6 removeState( store: StateStore, keyRow: UnsafeRow): Unit removeState is part of the StateManager abstraction. removeState ...FIXME === [[getAllState]] Getting All State Data (for All Keys) from StateStore -- getAllState Method [source, scala] \u00b6 getAllState(store: StateStore): Iterator[StateData] \u00b6 getAllState is part of the StateManager abstraction. getAllState ...FIXME === [[getStateObject]] getStateObject Internal Method [source, scala] \u00b6 getStateObject(row: UnsafeRow): Any \u00b6 getStateObject ...FIXME getStateObject is used when...FIXME === [[getStateRow]] getStateRow Internal Method [source, scala] \u00b6 getStateRow(obj: Any): UnsafeRow \u00b6 getStateRow ...FIXME getStateRow is used when...FIXME === [[getTimestamp]] Getting Timeout Timestamp (from State Row) -- getTimestamp Internal Method [source, scala] \u00b6 getTimestamp(stateRow: UnsafeRow): Long \u00b6 getTimestamp ...FIXME getTimestamp is used when...FIXME === [[setTimestamp]] Setting Timeout Timestamp (to State Row) -- setTimestamp Internal Method [source, scala] \u00b6 setTimestamp( stateRow: UnsafeRow, timeoutTimestamps: Long): Unit setTimestamp ...FIXME setTimestamp is used when...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | stateSerializerFunc a| [[stateSerializerFunc]] State object serializer (of type Any => UnsafeRow ) to serialize a state object (for a per-group state key) to a row ( UnsafeRow ) The serialization expression (incl. the type) is specified as the < > Used exclusively in < > | stateDeserializerFunc a| [[stateDeserializerFunc]] State object deserializer (of type InternalRow => Any ) to deserialize a row (for a per-group state value) to a Scala value The deserialization expression (incl. the type) is specified as the < > Used exclusively in < > | stateDataForGets a| [[stateDataForGets]] Empty StateData to share ( reuse ) between < > calls (to avoid high use of memory with many StateData objects) |===","title":"StateManagerImplBase"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#statedeserializerexpr-expression","text":"State deserializer , i.e. a Catalyst expression to deserialize a state object from a row ( UnsafeRow ) Used exclusively for the < > | stateSerializerExprs a| [[stateSerializerExprs]]","title":"stateDeserializerExpr: Expression"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#stateserializerexprs-seqexpression","text":"State serializer , i.e. Catalyst expressions to serialize a state object to a row ( UnsafeRow ) Used exclusively for the < > | timeoutTimestampOrdinalInRow a| [[timeoutTimestampOrdinalInRow]]","title":"stateSerializerExprs: Seq[Expression]"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#timeouttimestampordinalinrow-int","text":"Position of the timeout timestamp in a state row Used when StateManagerImplBase is requested to < > and < > |=== [[implementations]] .StateManagerImplBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | StateManagerImplBase | Description | < > | [[StateManagerImplV1]] Legacy < > | < > | [[StateManagerImplV2]] Default < > |=== === [[creating-instance]][[shouldStoreTimestamp]] Creating StateManagerImplBase Instance StateManagerImplBase takes a single shouldStoreTimestamp flag to be created (that is set when the < > are created). NOTE: StateManagerImplBase is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. StateManagerImplBase initializes the < >. === [[getState]] Getting State Data for Key from StateStore -- getState Method","title":"timeoutTimestampOrdinalInRow: Int"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_3","text":"getState( store: StateStore, keyRow: UnsafeRow): StateData getState is part of the StateManager abstraction. getState ...FIXME === [[putState]] Persisting State Value for Key in StateStore -- putState Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_4","text":"putState( store: StateStore, key: UnsafeRow, state: Any, timestamp: Long): Unit putState is part of the StateManager abstraction. putState ...FIXME === [[removeState]] Removing State for Key from StateStore -- removeState Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_5","text":"removeState( store: StateStore, keyRow: UnsafeRow): Unit removeState is part of the StateManager abstraction. removeState ...FIXME === [[getAllState]] Getting All State Data (for All Keys) from StateStore -- getAllState Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#getallstatestore-statestore-iteratorstatedata","text":"getAllState is part of the StateManager abstraction. getAllState ...FIXME === [[getStateObject]] getStateObject Internal Method","title":"getAllState(store: StateStore): Iterator[StateData]"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_7","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#getstateobjectrow-unsaferow-any","text":"getStateObject ...FIXME getStateObject is used when...FIXME === [[getStateRow]] getStateRow Internal Method","title":"getStateObject(row: UnsafeRow): Any"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_8","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#getstaterowobj-any-unsaferow","text":"getStateRow ...FIXME getStateRow is used when...FIXME === [[getTimestamp]] Getting Timeout Timestamp (from State Row) -- getTimestamp Internal Method","title":"getStateRow(obj: Any): UnsafeRow"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_9","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#gettimestampstaterow-unsaferow-long","text":"getTimestamp ...FIXME getTimestamp is used when...FIXME === [[setTimestamp]] Setting Timeout Timestamp (to State Row) -- setTimestamp Internal Method","title":"getTimestamp(stateRow: UnsafeRow): Long"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_10","text":"setTimestamp( stateRow: UnsafeRow, timeoutTimestamps: Long): Unit setTimestamp ...FIXME setTimestamp is used when...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | stateSerializerFunc a| [[stateSerializerFunc]] State object serializer (of type Any => UnsafeRow ) to serialize a state object (for a per-group state key) to a row ( UnsafeRow ) The serialization expression (incl. the type) is specified as the < > Used exclusively in < > | stateDeserializerFunc a| [[stateDeserializerFunc]] State object deserializer (of type InternalRow => Any ) to deserialize a row (for a per-group state value) to a Scala value The deserialization expression (incl. the type) is specified as the < > Used exclusively in < > | stateDataForGets a| [[stateDataForGets]] Empty StateData to share ( reuse ) between < > calls (to avoid high use of memory with many StateData objects) |===","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplV1/","text":"== [[StateManagerImplV1]] StateManagerImplV1 StateManagerImplV1 is...FIXME","title":"StateManagerImplV1"},{"location":"spark-sql-streaming-StateManagerImplV2/","text":"StateManagerImplV2 \u00b6 StateManagerImplV2 is a concrete < > (as a < >) that is used by default in FlatMapGroupsWithStateExec physical operator (per spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion internal configuration property). StateManagerImplV2 is < > exclusively when FlatMapGroupsWithStateExecHelper utility is requested for a < > (when the stateFormatVersion is 2 ). === [[creating-instance]] Creating StateManagerImplV2 Instance StateManagerImplV2 takes the following to be created: [[stateEncoder]] State encoder ( ExpressionEncoder[Any] ) [[shouldStoreTimestamp]] shouldStoreTimestamp flag StateManagerImplV2 initializes the < >. === [[stateSchema]] State Schema -- stateSchema Value [source, scala] \u00b6 stateSchema: StructType \u00b6 NOTE: stateSchema is part of the < > for the schema of the state. stateSchema ...FIXME === [[stateSerializerExprs]] State Serializer -- stateSerializerExprs Value [source, scala] \u00b6 stateSerializerExprs: Seq[Expression] \u00b6 NOTE: stateSerializerExprs is part of the < > for the state serializer, i.e. Catalyst expressions to serialize a state object to a row ( UnsafeRow ). stateSerializerExprs ...FIXME === [[stateDeserializerExpr]] State Deserializer -- stateDeserializerExpr Value [source, scala] \u00b6 stateDeserializerExpr: Expression \u00b6 NOTE: stateDeserializerExpr is part of the < > for the state deserializer, i.e. a Catalyst expression to deserialize a state object from a row ( UnsafeRow ). stateDeserializerExpr ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | nestedStateOrdinal a| [[nestedStateOrdinal]] Position of the state in a state row ( 0 ) Used when...FIXME | timeoutTimestampOrdinalInRow a| [[timeoutTimestampOrdinalInRow]] Position of the timeout timestamp in a state row ( 1 ) Used when...FIXME |===","title":"StateManagerImplV2"},{"location":"spark-sql-streaming-StateManagerImplV2/#statemanagerimplv2","text":"StateManagerImplV2 is a concrete < > (as a < >) that is used by default in FlatMapGroupsWithStateExec physical operator (per spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion internal configuration property). StateManagerImplV2 is < > exclusively when FlatMapGroupsWithStateExecHelper utility is requested for a < > (when the stateFormatVersion is 2 ). === [[creating-instance]] Creating StateManagerImplV2 Instance StateManagerImplV2 takes the following to be created: [[stateEncoder]] State encoder ( ExpressionEncoder[Any] ) [[shouldStoreTimestamp]] shouldStoreTimestamp flag StateManagerImplV2 initializes the < >. === [[stateSchema]] State Schema -- stateSchema Value","title":"StateManagerImplV2"},{"location":"spark-sql-streaming-StateManagerImplV2/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplV2/#stateschema-structtype","text":"NOTE: stateSchema is part of the < > for the schema of the state. stateSchema ...FIXME === [[stateSerializerExprs]] State Serializer -- stateSerializerExprs Value","title":"stateSchema: StructType"},{"location":"spark-sql-streaming-StateManagerImplV2/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplV2/#stateserializerexprs-seqexpression","text":"NOTE: stateSerializerExprs is part of the < > for the state serializer, i.e. Catalyst expressions to serialize a state object to a row ( UnsafeRow ). stateSerializerExprs ...FIXME === [[stateDeserializerExpr]] State Deserializer -- stateDeserializerExpr Value","title":"stateSerializerExprs: Seq[Expression]"},{"location":"spark-sql-streaming-StateManagerImplV2/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplV2/#statedeserializerexpr-expression","text":"NOTE: stateDeserializerExpr is part of the < > for the state deserializer, i.e. a Catalyst expression to deserialize a state object from a row ( UnsafeRow ). stateDeserializerExpr ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | nestedStateOrdinal a| [[nestedStateOrdinal]] Position of the state in a state row ( 0 ) Used when...FIXME | timeoutTimestampOrdinalInRow a| [[timeoutTimestampOrdinalInRow]] Position of the timeout timestamp in a state row ( 1 ) Used when...FIXME |===","title":"stateDeserializerExpr: Expression"},{"location":"spark-sql-streaming-StateStoreAwareZipPartitionsHelper/","text":"StateStoreAwareZipPartitionsHelper \u00b6 StateStoreAwareZipPartitionsHelper is a Scala implicit class of a data RDD (of type RDD[T] ) to create a StateStoreAwareZipPartitionsRDD for StreamingSymmetricHashJoinExec physical operator. Note Implicit Classes are a language feature in Scala for implicit conversions with extension methods for existing types. Creating StateStoreAwareZipPartitionsRDD \u00b6 stateStoreAwareZipPartitions [ U : ClassTag , V : ClassTag ]( dataRDD2 : RDD [ U ], stateInfo : StatefulOperatorStateInfo , storeNames : Seq [ String ], storeCoordinator : StateStoreCoordinatorRef )( f : ( Iterator [ T ], Iterator [ U ]) => Iterator [ V ]): RDD [ V ] stateStoreAwareZipPartitions simply creates a new StateStoreAwareZipPartitionsRDD . stateStoreAwareZipPartitions is used when StreamingSymmetricHashJoinExec physical operator is executed.","title":"StateStoreAwareZipPartitionsHelper"},{"location":"spark-sql-streaming-StateStoreAwareZipPartitionsHelper/#statestoreawarezippartitionshelper","text":"StateStoreAwareZipPartitionsHelper is a Scala implicit class of a data RDD (of type RDD[T] ) to create a StateStoreAwareZipPartitionsRDD for StreamingSymmetricHashJoinExec physical operator. Note Implicit Classes are a language feature in Scala for implicit conversions with extension methods for existing types.","title":"StateStoreAwareZipPartitionsHelper"},{"location":"spark-sql-streaming-StateStoreAwareZipPartitionsHelper/#creating-statestoreawarezippartitionsrdd","text":"stateStoreAwareZipPartitions [ U : ClassTag , V : ClassTag ]( dataRDD2 : RDD [ U ], stateInfo : StatefulOperatorStateInfo , storeNames : Seq [ String ], storeCoordinator : StateStoreCoordinatorRef )( f : ( Iterator [ T ], Iterator [ U ]) => Iterator [ V ]): RDD [ V ] stateStoreAwareZipPartitions simply creates a new StateStoreAwareZipPartitionsRDD . stateStoreAwareZipPartitions is used when StreamingSymmetricHashJoinExec physical operator is executed.","title":" Creating StateStoreAwareZipPartitionsRDD"},{"location":"spark-sql-streaming-StateStoreCustomMetric/","text":"== [[StateStoreCustomMetric]] StateStoreCustomMetric Contract StateStoreCustomMetric is the < > of < > that a state store may wish to expose (as < > or < >). StateStoreCustomMetric is used when: StateStoreProvider is requested for the < > StateStoreMetrics is < > [[contract]] .StateStoreCustomMetric Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | desc a| [[desc]] [source, scala] \u00b6 desc: String \u00b6 Description of the custom metrics | name a| [[name]] [source, scala] \u00b6 name: String \u00b6 Name of the custom metrics |=== [[implementations]] .StateStoreCustomMetrics [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | StateStoreCustomMetric | Description | StateStoreCustomSizeMetric | [[StateStoreCustomSizeMetric]] | StateStoreCustomSumMetric | [[StateStoreCustomSumMetric]] | StateStoreCustomTimingMetric | [[StateStoreCustomTimingMetric]] |===","title":"StateStoreCustomMetric"},{"location":"spark-sql-streaming-StateStoreCustomMetric/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreCustomMetric/#desc-string","text":"Description of the custom metrics | name a| [[name]]","title":"desc: String"},{"location":"spark-sql-streaming-StateStoreCustomMetric/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreCustomMetric/#name-string","text":"Name of the custom metrics |=== [[implementations]] .StateStoreCustomMetrics [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | StateStoreCustomMetric | Description | StateStoreCustomSizeMetric | [[StateStoreCustomSizeMetric]] | StateStoreCustomSumMetric | [[StateStoreCustomSumMetric]] | StateStoreCustomTimingMetric | [[StateStoreCustomTimingMetric]] |===","title":"name: String"},{"location":"spark-sql-streaming-StateStoreHandler/","text":"StateStoreHandler \u00b6 StateStoreHandler is the internal < > of < > that manage a < > (i.e. < >, < > and < >). [[stateStoreType]] StateStoreHandler takes a single StateStoreType to be created: [[KeyToNumValuesType]] KeyToNumValuesType for < > (alias: keyToNumValues ) [[KeyWithIndexToValueType]] KeyWithIndexToValueType for < > (alias: keyWithIndexToValue ) NOTE: StateStoreHandler is a Scala private abstract class and cannot be < > directly. It is created indirectly for the < >. [[contract]] .StateStoreHandler Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | stateStore a| [[stateStore]] [source, scala] \u00b6 stateStore: StateStore \u00b6 StateStore |=== [[extensions]] .StateStoreHandlers [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | StateStoreHandler | Description | < > | [[KeyToNumValuesStore]] StateStoreHandler of < > | < > | [[KeyWithIndexToValueStore]] |=== [[logging]] [TIP] ==== Enable ALL logging levels for org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.StateStoreHandler logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.StateStoreHandler=ALL Refer to < >. \u00b6 === [[metrics]] Performance Metrics -- metrics Method [source, scala] \u00b6 metrics: StateStoreMetrics \u00b6 metrics simply requests the StateStore for the StateStoreMetrics . metrics is used when SymmetricHashJoinStateManager is requested for the metrics . === [[commit]] Committing State (Changes to State Store) -- commit Method [source, scala] \u00b6 commit(): Unit \u00b6 commit ...FIXME NOTE: commit is used when...FIXME === [[abortIfNeeded]] abortIfNeeded Method [source, scala] \u00b6 abortIfNeeded(): Unit \u00b6 abortIfNeeded ...FIXME NOTE: abortIfNeeded is used when...FIXME === [[getStateStore]] Loading State Store (By Key and Value Schemas) -- getStateStore Method [source, scala] \u00b6 getStateStore( keySchema: StructType, valueSchema: StructType): StateStore getStateStore creates a new < > (for the StatefulOperatorStateInfo of the owning SymmetricHashJoinStateManager , the partition ID from the execution context, and the name of the state store for the JoinSide and < >). getStateStore uses the StateStore utility to look up a StateStore for the StateStoreProviderId . In the end, getStateStore prints out the following INFO message to the logs: Loaded store [storeId] getStateStore is used when KeyToNumValuesStore and < > state store handlers are created (for SymmetricHashJoinStateManager ). === [[StateStoreType]] StateStoreType Contract (Sealed Trait) StateStoreType is required to create a < >. [[StateStoreType-implementations]] .StateStoreTypes [cols=\"1m,1m,2\",options=\"header\",width=\"100%\"] |=== | StateStoreType | toString | Description | KeyToNumValuesType | keyToNumValues | [[KeyToNumValuesType]] | KeyWithIndexToValueType | keyWithIndexToValue | [[KeyWithIndexToValueType]] |=== NOTE: StateStoreType is a Scala private sealed trait which means that all the < > are in the same compilation unit (a single file).","title":"StateStoreHandler"},{"location":"spark-sql-streaming-StateStoreHandler/#statestorehandler","text":"StateStoreHandler is the internal < > of < > that manage a < > (i.e. < >, < > and < >). [[stateStoreType]] StateStoreHandler takes a single StateStoreType to be created: [[KeyToNumValuesType]] KeyToNumValuesType for < > (alias: keyToNumValues ) [[KeyWithIndexToValueType]] KeyWithIndexToValueType for < > (alias: keyWithIndexToValue ) NOTE: StateStoreHandler is a Scala private abstract class and cannot be < > directly. It is created indirectly for the < >. [[contract]] .StateStoreHandler Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | stateStore a| [[stateStore]]","title":"StateStoreHandler"},{"location":"spark-sql-streaming-StateStoreHandler/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreHandler/#statestore-statestore","text":"StateStore |=== [[extensions]] .StateStoreHandlers [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | StateStoreHandler | Description | < > | [[KeyToNumValuesStore]] StateStoreHandler of < > | < > | [[KeyWithIndexToValueStore]] |=== [[logging]] [TIP] ==== Enable ALL logging levels for org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.StateStoreHandler logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.StateStoreHandler=ALL","title":"stateStore: StateStore"},{"location":"spark-sql-streaming-StateStoreHandler/#refer-to","text":"=== [[metrics]] Performance Metrics -- metrics Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-StateStoreHandler/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreHandler/#metrics-statestoremetrics","text":"metrics simply requests the StateStore for the StateStoreMetrics . metrics is used when SymmetricHashJoinStateManager is requested for the metrics . === [[commit]] Committing State (Changes to State Store) -- commit Method","title":"metrics: StateStoreMetrics"},{"location":"spark-sql-streaming-StateStoreHandler/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreHandler/#commit-unit","text":"commit ...FIXME NOTE: commit is used when...FIXME === [[abortIfNeeded]] abortIfNeeded Method","title":"commit(): Unit"},{"location":"spark-sql-streaming-StateStoreHandler/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreHandler/#abortifneeded-unit","text":"abortIfNeeded ...FIXME NOTE: abortIfNeeded is used when...FIXME === [[getStateStore]] Loading State Store (By Key and Value Schemas) -- getStateStore Method","title":"abortIfNeeded(): Unit"},{"location":"spark-sql-streaming-StateStoreHandler/#source-scala_4","text":"getStateStore( keySchema: StructType, valueSchema: StructType): StateStore getStateStore creates a new < > (for the StatefulOperatorStateInfo of the owning SymmetricHashJoinStateManager , the partition ID from the execution context, and the name of the state store for the JoinSide and < >). getStateStore uses the StateStore utility to look up a StateStore for the StateStoreProviderId . In the end, getStateStore prints out the following INFO message to the logs: Loaded store [storeId] getStateStore is used when KeyToNumValuesStore and < > state store handlers are created (for SymmetricHashJoinStateManager ). === [[StateStoreType]] StateStoreType Contract (Sealed Trait) StateStoreType is required to create a < >. [[StateStoreType-implementations]] .StateStoreTypes [cols=\"1m,1m,2\",options=\"header\",width=\"100%\"] |=== | StateStoreType | toString | Description | KeyToNumValuesType | keyToNumValues | [[KeyToNumValuesType]] | KeyWithIndexToValueType | keyWithIndexToValue | [[KeyWithIndexToValueType]] |=== NOTE: StateStoreType is a Scala private sealed trait which means that all the < > are in the same compilation unit (a single file).","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreId/","text":"StateStoreId \u00b6 [[creating-instance]] StateStoreId is a unique identifier of a StateStore with the following attributes: [[checkpointRootLocation]] Checkpoint Root Location - the root directory for state checkpointing [[operatorId]] Operator ID - a unique ID of the stateful operator [[partitionId]] Partition ID - the index of the partition [[storeName]] Store Name - the name of the state store (default: < >) StateStoreId is < > when: StateStoreRDD is requested for the preferred locations of a partition (executed on the driver) and to compute it (later on an executor) StateStoreProviderId helper object is requested to create a < > (with a < > and the run ID of a streaming query) that is then used for the preferred locations of a partition of a StateStoreAwareZipPartitionsRDD (executed on the driver) and to...FIXME [[DEFAULT_STORE_NAME]] The name of the default state store (for reading state store data that was generated before store names were used, i.e. in Spark 2.2 and earlier) is default . === [[storeCheckpointLocation]] State Checkpoint Base Directory of Stateful Operator -- storeCheckpointLocation Method [source, scala] \u00b6 storeCheckpointLocation(): Path \u00b6 storeCheckpointLocation is Hadoop DFS's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the checkpoint location (for the stateful operator by < >, the partition by the < > in the < >). If the < > is used (for Spark 2.2 and earlier), the < > is not included in the path. storeCheckpointLocation is used when HDFSBackedStateStoreProvider is requested for the state checkpoint base directory .","title":"StateStoreId"},{"location":"spark-sql-streaming-StateStoreId/#statestoreid","text":"[[creating-instance]] StateStoreId is a unique identifier of a StateStore with the following attributes: [[checkpointRootLocation]] Checkpoint Root Location - the root directory for state checkpointing [[operatorId]] Operator ID - a unique ID of the stateful operator [[partitionId]] Partition ID - the index of the partition [[storeName]] Store Name - the name of the state store (default: < >) StateStoreId is < > when: StateStoreRDD is requested for the preferred locations of a partition (executed on the driver) and to compute it (later on an executor) StateStoreProviderId helper object is requested to create a < > (with a < > and the run ID of a streaming query) that is then used for the preferred locations of a partition of a StateStoreAwareZipPartitionsRDD (executed on the driver) and to...FIXME [[DEFAULT_STORE_NAME]] The name of the default state store (for reading state store data that was generated before store names were used, i.e. in Spark 2.2 and earlier) is default . === [[storeCheckpointLocation]] State Checkpoint Base Directory of Stateful Operator -- storeCheckpointLocation Method","title":"StateStoreId"},{"location":"spark-sql-streaming-StateStoreId/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreId/#storecheckpointlocation-path","text":"storeCheckpointLocation is Hadoop DFS's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the checkpoint location (for the stateful operator by < >, the partition by the < > in the < >). If the < > is used (for Spark 2.2 and earlier), the < > is not included in the path. storeCheckpointLocation is used when HDFSBackedStateStoreProvider is requested for the state checkpoint base directory .","title":"storeCheckpointLocation(): Path"},{"location":"spark-sql-streaming-StateStoreMetrics/","text":"StateStoreMetrics \u00b6 [[creating-instance]] StateStoreMetrics holds the performance metrics of a state store : [[numKeys]] Number of keys [[memoryUsedBytes]] Memory used (in bytes) [[customMetrics]] < > with their current values ( Map[StateStoreCustomMetric, Long] ) StateStoreMetrics is used (and < >) when the following are requested for the performance metrics: StateStore StateStoreHandler SymmetricHashJoinStateManager","title":"StateStoreMetrics"},{"location":"spark-sql-streaming-StateStoreMetrics/#statestoremetrics","text":"[[creating-instance]] StateStoreMetrics holds the performance metrics of a state store : [[numKeys]] Number of keys [[memoryUsedBytes]] Memory used (in bytes) [[customMetrics]] < > with their current values ( Map[StateStoreCustomMetric, Long] ) StateStoreMetrics is used (and < >) when the following are requested for the performance metrics: StateStore StateStoreHandler SymmetricHashJoinStateManager","title":"StateStoreMetrics"},{"location":"spark-sql-streaming-StateStoreProvider/","text":"StateStoreProvider \u00b6 StateStoreProvider is the < > of < > that manage < > in < > (e.g. for persisting running aggregates in Streaming Aggregation ) in stateful streaming queries. Note StateStoreProvider utility uses spark.sql.streaming.stateStore.providerClass internal configuration property for the name of the class of the default < >. [[implementations]] NOTE: HDFSBackedStateStoreProvider is the default and only known StateStoreProvider in Spark Structured Streaming. [[contract]] .StateStoreProvider Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | close a| [[close]] [source, scala] \u00b6 close(): Unit \u00b6 Closes the state store provider Used exclusively when StateStore helper object is requested to unload a state store provider | doMaintenance a| [[doMaintenance]] [source, scala] \u00b6 doMaintenance(): Unit = {} \u00b6 Optional state maintenance Used exclusively when StateStore utility is requested to perform maintenance of registered state store providers (on a separate MaintenanceTask daemon thread ) | getStore a| [[getStore]] [source, scala] \u00b6 getStore( version: Long): StateStore Finds the StateStore for the specified version Used exclusively when StateStore utility is requested to look up the StateStore by a given provider ID | init a| [[init]] [source, scala] \u00b6 init( stateStoreId: StateStoreId, keySchema: StructType, valueSchema: StructType, keyIndexOrdinal: Option[Int], storeConfs: StateStoreConf, hadoopConf: Configuration): Unit Initializes the state store provider Used exclusively when StateStoreProvider helper object is requested to < > for a given StateStoreId (when StateStore helper object is requested to retrieve a StateStore by ID and version ) | stateStoreId a| [[stateStoreId]] [source, scala] \u00b6 stateStoreId: StateStoreId \u00b6 StateStoreId associated with the provider (at < >) Used when: HDFSBackedStateStore is requested for the unique id HDFSBackedStateStoreProvider is created and requested for the textual representation | supportedCustomMetrics a| [[supportedCustomMetrics]] [source, scala] \u00b6 supportedCustomMetrics: Seq[StateStoreCustomMetric] \u00b6 < > of the state store provider Used when: StateStoreWriter stateful physical operators are requested for the stateStoreCustomMetrics (when requested for the metrics and getProgress ) HDFSBackedStateStore is requested for the performance metrics |=== === [[createAndInit]] Creating and Initializing StateStoreProvider -- createAndInit Object Method [source, scala] \u00b6 createAndInit( stateStoreId: StateStoreId, keySchema: StructType, valueSchema: StructType, indexOrdinal: Option[Int], storeConf: StateStoreConf, hadoopConf: Configuration): StateStoreProvider createAndInit creates a new < > (per spark.sql.streaming.stateStore.providerClass internal configuration property). createAndInit requests the StateStoreProvider to < >. createAndInit is used when StateStore utility is requested for the StateStore by given provider ID and version .","title":"StateStoreProvider"},{"location":"spark-sql-streaming-StateStoreProvider/#statestoreprovider","text":"StateStoreProvider is the < > of < > that manage < > in < > (e.g. for persisting running aggregates in Streaming Aggregation ) in stateful streaming queries. Note StateStoreProvider utility uses spark.sql.streaming.stateStore.providerClass internal configuration property for the name of the class of the default < >. [[implementations]] NOTE: HDFSBackedStateStoreProvider is the default and only known StateStoreProvider in Spark Structured Streaming. [[contract]] .StateStoreProvider Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | close a| [[close]]","title":"StateStoreProvider"},{"location":"spark-sql-streaming-StateStoreProvider/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProvider/#close-unit","text":"Closes the state store provider Used exclusively when StateStore helper object is requested to unload a state store provider | doMaintenance a| [[doMaintenance]]","title":"close(): Unit"},{"location":"spark-sql-streaming-StateStoreProvider/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProvider/#domaintenance-unit","text":"Optional state maintenance Used exclusively when StateStore utility is requested to perform maintenance of registered state store providers (on a separate MaintenanceTask daemon thread ) | getStore a| [[getStore]]","title":"doMaintenance(): Unit = {}"},{"location":"spark-sql-streaming-StateStoreProvider/#source-scala_2","text":"getStore( version: Long): StateStore Finds the StateStore for the specified version Used exclusively when StateStore utility is requested to look up the StateStore by a given provider ID | init a| [[init]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProvider/#source-scala_3","text":"init( stateStoreId: StateStoreId, keySchema: StructType, valueSchema: StructType, keyIndexOrdinal: Option[Int], storeConfs: StateStoreConf, hadoopConf: Configuration): Unit Initializes the state store provider Used exclusively when StateStoreProvider helper object is requested to < > for a given StateStoreId (when StateStore helper object is requested to retrieve a StateStore by ID and version ) | stateStoreId a| [[stateStoreId]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProvider/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProvider/#statestoreid-statestoreid","text":"StateStoreId associated with the provider (at < >) Used when: HDFSBackedStateStore is requested for the unique id HDFSBackedStateStoreProvider is created and requested for the textual representation | supportedCustomMetrics a| [[supportedCustomMetrics]]","title":"stateStoreId: StateStoreId"},{"location":"spark-sql-streaming-StateStoreProvider/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProvider/#supportedcustommetrics-seqstatestorecustommetric","text":"< > of the state store provider Used when: StateStoreWriter stateful physical operators are requested for the stateStoreCustomMetrics (when requested for the metrics and getProgress ) HDFSBackedStateStore is requested for the performance metrics |=== === [[createAndInit]] Creating and Initializing StateStoreProvider -- createAndInit Object Method","title":"supportedCustomMetrics: Seq[StateStoreCustomMetric]"},{"location":"spark-sql-streaming-StateStoreProvider/#source-scala_6","text":"createAndInit( stateStoreId: StateStoreId, keySchema: StructType, valueSchema: StructType, indexOrdinal: Option[Int], storeConf: StateStoreConf, hadoopConf: Configuration): StateStoreProvider createAndInit creates a new < > (per spark.sql.streaming.stateStore.providerClass internal configuration property). createAndInit requests the StateStoreProvider to < >. createAndInit is used when StateStore utility is requested for the StateStore by given provider ID and version .","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProviderId/","text":"StateStoreProviderId \u00b6 [[creating-instance]] StateStoreProviderId is a unique identifier of a StateStoreProvider with the following properties: [[storeId]] StateStoreId [[queryRunId]] Run ID of a streaming query ( java.util.UUID ) In other words, StateStoreProviderId is a < > with the < > that is different every restart. StateStoreProviderId is used by the following execution components: StateStoreCoordinator to track the executors of state store providers (on the driver) StateStore object to manage state store providers (on executors) StateStoreProviderId is < > (directly or using < > factory method) when: StateStoreRDD is requested for the placement preferences of a partition and to compute a partition StateStoreAwareZipPartitionsRDD is requested for the preferred locations of a partition StateStoreHandler is requested to look up a state store Creating StateStoreProviderId \u00b6 apply ( stateInfo : StatefulOperatorStateInfo , partitionIndex : Int , storeName : String ): StateStoreProviderId apply simply creates a < > for the StatefulOperatorStateInfo , the partition and the store name. Internally, apply requests the StatefulOperatorStateInfo for the checkpoint directory ( checkpointLocation ) and the stateful operator ID and creates a new StateStoreId (with the partitionIndex and storeName ). In the end, apply requests the StatefulOperatorStateInfo for the run ID of a streaming query and creates a < > (together with the run ID). apply is used when: StateStoreAwareZipPartitionsRDD is requested for the preferred locations of a partition StateStoreHandler is requested to look up a state store","title":"StateStoreProviderId"},{"location":"spark-sql-streaming-StateStoreProviderId/#statestoreproviderid","text":"[[creating-instance]] StateStoreProviderId is a unique identifier of a StateStoreProvider with the following properties: [[storeId]] StateStoreId [[queryRunId]] Run ID of a streaming query ( java.util.UUID ) In other words, StateStoreProviderId is a < > with the < > that is different every restart. StateStoreProviderId is used by the following execution components: StateStoreCoordinator to track the executors of state store providers (on the driver) StateStore object to manage state store providers (on executors) StateStoreProviderId is < > (directly or using < > factory method) when: StateStoreRDD is requested for the placement preferences of a partition and to compute a partition StateStoreAwareZipPartitionsRDD is requested for the preferred locations of a partition StateStoreHandler is requested to look up a state store","title":"StateStoreProviderId"},{"location":"spark-sql-streaming-StateStoreProviderId/#creating-statestoreproviderid","text":"apply ( stateInfo : StatefulOperatorStateInfo , partitionIndex : Int , storeName : String ): StateStoreProviderId apply simply creates a < > for the StatefulOperatorStateInfo , the partition and the store name. Internally, apply requests the StatefulOperatorStateInfo for the checkpoint directory ( checkpointLocation ) and the stateful operator ID and creates a new StateStoreId (with the partitionIndex and storeName ). In the end, apply requests the StatefulOperatorStateInfo for the run ID of a streaming query and creates a < > (together with the run ID). apply is used when: StateStoreAwareZipPartitionsRDD is requested for the preferred locations of a partition StateStoreHandler is requested to look up a state store","title":" Creating StateStoreProviderId"},{"location":"spark-sql-streaming-StateStoreUpdater/","text":"== [[StateStoreUpdater]] StateStoreUpdater StateStoreUpdater is...FIXME === [[updateStateForKeysWithData]] updateStateForKeysWithData Method CAUTION: FIXME === [[updateStateForTimedOutKeys]] updateStateForTimedOutKeys Method CAUTION: FIXME","title":"StateStoreUpdater"},{"location":"spark-sql-streaming-deduplication/","text":"== Streaming Deduplication Streaming Deduplication is...FIXME","title":"Streaming Deduplication"},{"location":"spark-sql-streaming-extending-new-data-sources/","text":"Extending Structured Streaming with New Data Sources \u00b6 Spark Structured Streaming uses Spark SQL for planning streaming queries ( preparing for execution ). Structured Streaming supports two stream execution engines (i.e. Micro-Batch and Continuous ) with their own APIs. Micro-Batch Stream Processing supports the old Data Source API V1 and the new modern Data Source API V2 with micro-batch-specific APIs for streaming sources and sinks. Continuous Stream Processing supports the new modern Data Source API V2 only with continuous-specific APIs for streaming sources and sinks. The following are the questions to think of (and answer) while considering development of a new data source for Structured Streaming. They are supposed to give you a sense of how much work and time it takes as well as what Spark version to support (e.g. 2.2 vs 2.4). Data Source API V1 Data Source API V2 Micro-Batch Stream Processing Continuous Stream Processing","title":"Extending Structured Streaming with New Data Sources"},{"location":"spark-sql-streaming-extending-new-data-sources/#extending-structured-streaming-with-new-data-sources","text":"Spark Structured Streaming uses Spark SQL for planning streaming queries ( preparing for execution ). Structured Streaming supports two stream execution engines (i.e. Micro-Batch and Continuous ) with their own APIs. Micro-Batch Stream Processing supports the old Data Source API V1 and the new modern Data Source API V2 with micro-batch-specific APIs for streaming sources and sinks. Continuous Stream Processing supports the new modern Data Source API V2 only with continuous-specific APIs for streaming sources and sinks. The following are the questions to think of (and answer) while considering development of a new data source for Structured Streaming. They are supposed to give you a sense of how much work and time it takes as well as what Spark version to support (e.g. 2.2 vs 2.4). Data Source API V1 Data Source API V2 Micro-Batch Stream Processing Continuous Stream Processing","title":"Extending Structured Streaming with New Data Sources"},{"location":"spark-sql-streaming-join/","text":"Streaming Join \u00b6 [[operators]] In Spark Structured Streaming, a streaming join is a streaming query that was described ( build ) using the high-level streaming operators : Dataset.crossJoin Dataset.join Dataset.joinWith SQL's JOIN clause Streaming joins can be stateless or < >: Joins of a streaming query and a batch query ( stream-static joins ) are stateless and no state management is required Joins of two streaming queries (< >) are stateful and require streaming state (with an optional < >). Stream-Stream Joins \u00b6 Spark Structured Streaming supports stream-stream joins with the following: Equality predicate ( equi-joins that use only equality comparisons in the join predicate) Inner , LeftOuter , and RightOuter < > Stream-stream equi-joins are planned as < > physical operators of two ShuffleExchangeExec physical operators (per < >). === [[join-state-watermark]] Join State Watermark for State Removal Stream-stream joins may optionally define Join State Watermark for state removal (cf. < >). A join state watermark can be specified on the following: . < > ( key state ) . < > ( value state ) A join state watermark can be specified on key state, value state or both. === [[IncrementalExecution]] IncrementalExecution -- QueryExecution of Streaming Queries Under the covers, the < > create a logical query plan with one or more Join logical operators. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-Join.html[Join Logical Operator] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. In Spark Structured Streaming IncrementalExecution is responsible for planning streaming queries for execution. At query planning , IncrementalExecution uses the StreamingJoinStrategy execution planning strategy for planning stream-stream joins as StreamingSymmetricHashJoinExec physical operators. Demos \u00b6 StreamStreamJoinApp Further Reading Or Watching \u00b6 Stream-stream Joins in the official documentation of Apache Spark for Structured Streaming Introducing Stream-Stream Joins in Apache Spark 2.3 by Databricks (video) Deep Dive into Stateful Stream Processing in Structured Streaming by Tathagata Das","title":"Streaming Join"},{"location":"spark-sql-streaming-join/#streaming-join","text":"[[operators]] In Spark Structured Streaming, a streaming join is a streaming query that was described ( build ) using the high-level streaming operators : Dataset.crossJoin Dataset.join Dataset.joinWith SQL's JOIN clause Streaming joins can be stateless or < >: Joins of a streaming query and a batch query ( stream-static joins ) are stateless and no state management is required Joins of two streaming queries (< >) are stateful and require streaming state (with an optional < >).","title":"Streaming Join"},{"location":"spark-sql-streaming-join/#stream-stream-joins","text":"Spark Structured Streaming supports stream-stream joins with the following: Equality predicate ( equi-joins that use only equality comparisons in the join predicate) Inner , LeftOuter , and RightOuter < > Stream-stream equi-joins are planned as < > physical operators of two ShuffleExchangeExec physical operators (per < >). === [[join-state-watermark]] Join State Watermark for State Removal Stream-stream joins may optionally define Join State Watermark for state removal (cf. < >). A join state watermark can be specified on the following: . < > ( key state ) . < > ( value state ) A join state watermark can be specified on key state, value state or both. === [[IncrementalExecution]] IncrementalExecution -- QueryExecution of Streaming Queries Under the covers, the < > create a logical query plan with one or more Join logical operators. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-Join.html[Join Logical Operator] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. In Spark Structured Streaming IncrementalExecution is responsible for planning streaming queries for execution. At query planning , IncrementalExecution uses the StreamingJoinStrategy execution planning strategy for planning stream-stream joins as StreamingSymmetricHashJoinExec physical operators.","title":"Stream-Stream Joins"},{"location":"spark-sql-streaming-join/#demos","text":"StreamStreamJoinApp","title":"Demos"},{"location":"spark-sql-streaming-join/#further-reading-or-watching","text":"Stream-stream Joins in the official documentation of Apache Spark for Structured Streaming Introducing Stream-Stream Joins in Apache Spark 2.3 by Databricks (video) Deep Dive into Stateful Stream Processing in Structured Streaming by Tathagata Das","title":"Further Reading Or Watching"},{"location":"spark-sql-streaming-limit/","text":"== Streaming Limit Streaming Limit is...FIXME","title":"Streaming Limit"},{"location":"spark-sql-streaming-stateful-stream-processing/","text":"Stateful Stream Processing \u00b6 Stateful Stream Processing is a stream processing with state (implicit or explicit). In Spark Structured Streaming, a streaming query is stateful when is one of the following (that makes use of StateStores ): Streaming Aggregation Arbitrary Stateful Streaming Aggregation Stream-Stream Join Streaming Deduplication Streaming Limit Versioned State, StateStores and StateStoreProviders \u00b6 Spark Structured Streaming uses StateStore s for versioned and fault-tolerant key-value state stores. State stores are checkpointed incrementally to avoid state loss and for increased performance. State stores are managed by StateStoreProvider s with HDFSBackedStateStoreProvider being the default and only known implementation. HDFSBackedStateStoreProvider uses Hadoop DFS-compliant file system for state checkpointing and fault-tolerance . State store providers manage versioned state per stateful operator (and partition it operates on) . The lifecycle of a StateStoreProvider begins when StateStore utility (on a Spark executor) is requested for the StateStore by provider ID and version . Important It is worth to notice that since StateStore and StateStoreProvider utilities are Scala objects that makes it possible that there can only be one instance of StateStore and StateStoreProvider on a single JVM. Scala objects are (sort of) singletons which means that there will be exactly one instance of each per JVM and that is exactly the JVM of a Spark executor. As long as the executor is up and running state versions are cached and no Hadoop DFS is used (except for the initial load). When requested for a StateStore , StateStore utility is given the version of a state store to look up. The version is either the < > (in Continuous Stream Processing ) or the current batch ID (in Micro-Batch Stream Processing ). StateStore utility requests StateStoreProvider utility to < > that creates the StateStoreProvider implementation (based on spark.sql.streaming.stateStore.providerClass internal configuration property) and requests it to < >. The initialized StateStoreProvider is cached in loadedProviders internal lookup table (for a < >) for later lookups. StateStoreProvider utility then requests the StateStoreProvider for the < >. (e.g. a HDFSBackedStateStore in case of HDFSBackedStateStoreProvider ). An instance of StateStoreProvider is requested to < > or < > (when < >) in < > that runs periodically every spark.sql.streaming.stateStore.maintenanceInterval configuration property. IncrementalExecution \u2014 QueryExecution of Streaming Queries \u00b6 Regardless of the query language (Dataset API or SQL), any structured query (incl. streaming queries) becomes a logical query plan. In Spark Structured Streaming it is IncrementalExecution that plans streaming queries for execution. While planning a streaming query for execution (aka query planning ), IncrementalExecution uses the state preparation rule . The rule fills out the following physical operators with the execution-specific configuration (with StatefulOperatorStateInfo being the most important for stateful stream processing): FlatMapGroupsWithStateExec StateStoreRestoreExec StateStoreSaveExec StreamingDeduplicateExec StreamingGlobalLimitExec StreamingSymmetricHashJoinExec ==== [[IncrementalExecution-shouldRunAnotherBatch]] Micro-Batch Stream Processing and Extra Non-Data Batch for StateStoreWriter Stateful Operators In Micro-Batch Stream Processing (with MicroBatchExecution engine), IncrementalExecution uses shouldRunAnotherBatch flag that allows StateStoreWriters stateful physical operators to indicate whether the last batch execution requires another non-data batch . The following StateStoreWriters redefine shouldRunAnotherBatch flag. [[StateStoreWriters-shouldRunAnotherBatch]] .StateStoreWriters and shouldRunAnotherBatch Flag [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | StateStoreWriter | shouldRunAnotherBatch Flag | FlatMapGroupsWithStateExec a| [[shouldRunAnotherBatch-FlatMapGroupsWithStateExec]] Based on GroupStateTimeout | < > a| [[shouldRunAnotherBatch-StateStoreSaveExec]] Based on < > | < > a| [[shouldRunAnotherBatch-StreamingDeduplicateExec]] Based on < > | < > a| [[shouldRunAnotherBatch-StreamingSymmetricHashJoinExec]] Based on < > |=== StateStoreRDD \u00b6 Right after query planning , a stateful streaming query (a single micro-batch actually) becomes an RDD with one or more StateStoreRDD s. You can find the StateStoreRDDs of a streaming query in the RDD lineage. scala> :type streamingQuery org.apache.spark.sql.streaming.StreamingQuery scala> streamingQuery.explain == Physical Plan == *(4) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[count(1)]) +- StateStoreSave [window#13-T0ms, value#3L], state info [ checkpoint = file:/tmp/checkpoint-counts/state, runId = 1dec2d81-f2d0-45b9-8f16-39ede66e13e7, opId = 0, ver = 1, numPartitions = 1], Append, 10000, 2 +- *(3) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[merge_count(1)]) +- StateStoreRestore [window#13-T0ms, value#3L], state info [ checkpoint = file:/tmp/checkpoint-counts/state, runId = 1dec2d81-f2d0-45b9-8f16-39ede66e13e7, opId = 0, ver = 1, numPartitions = 1], 2 +- *(2) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[merge_count(1)]) +- Exchange hashpartitioning(window#13-T0ms, value#3L, 1) +- *(1) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[partial_count(1)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#13-T0ms, value#3L] +- *(1) Filter isnotnull(time#2-T0ms) +- EventTimeWatermark time#2: timestamp, interval +- LocalTableScan <empty>, [time#2, value#3L] import org.apache.spark.sql.execution.streaming.{StreamExecution, StreamingQueryWrapper} val se = streamingQuery.asInstanceOf[StreamingQueryWrapper].streamingQuery scala> :type se org.apache.spark.sql.execution.streaming.StreamExecution scala> :type se.lastExecution org.apache.spark.sql.execution.streaming.IncrementalExecution val rdd = se.lastExecution.toRdd scala> rdd.toDebugString res3: String = (1) MapPartitionsRDD[39] at toRdd at <console>:40 [] | StateStoreRDD[38] at toRdd at <console>:40 [] // <-- here | MapPartitionsRDD[37] at toRdd at <console>:40 [] | StateStoreRDD[36] at toRdd at <console>:40 [] // <-- here | MapPartitionsRDD[35] at toRdd at <console>:40 [] | ShuffledRowRDD[17] at start at <pastie>:67 [] +-(1) MapPartitionsRDD[16] at start at <pastie>:67 [] | MapPartitionsRDD[15] at start at <pastie>:67 [] | MapPartitionsRDD[14] at start at <pastie>:67 [] | MapPartitionsRDD[13] at start at <pastie>:67 [] | ParallelCollectionRDD[12] at start at <pastie>:67 [] StateStoreCoordinator RPC Endpoint, StateStoreRDD and Preferred Locations \u00b6 Since execution of a stateful streaming query happens on Spark executors whereas planning is on the driver, Spark Structured Streaming uses RPC environment for tracking locations of the state stores in use. That makes the tasks (of a structured query) to be scheduled where the state (of a partition) is. When planned for execution, the StateStoreRDD is first asked for the preferred locations of a partition (which happens on the driver) that are later used to compute it (on Spark executors). Spark Structured Streaming uses RPC environment to keep track of StateStore s (their StateStoreProvider actually) for RDD planning. Every time StateStoreRDD is requested for the preferred locations of a partition , it communicates with the StateStoreCoordinator RPC endpoint that knows the locations of the required StateStores (per host and executor ID). StateStoreRDD uses StateStoreProviderId with StateStoreId to uniquely identify the state store to use for ( associate with ) a stateful operator and a partition. State Management \u00b6 The state in a stateful streaming query can be implicit or explicit.","title":"Stateful Stream Processing"},{"location":"spark-sql-streaming-stateful-stream-processing/#stateful-stream-processing","text":"Stateful Stream Processing is a stream processing with state (implicit or explicit). In Spark Structured Streaming, a streaming query is stateful when is one of the following (that makes use of StateStores ): Streaming Aggregation Arbitrary Stateful Streaming Aggregation Stream-Stream Join Streaming Deduplication Streaming Limit","title":"Stateful Stream Processing"},{"location":"spark-sql-streaming-stateful-stream-processing/#versioned-state-statestores-and-statestoreproviders","text":"Spark Structured Streaming uses StateStore s for versioned and fault-tolerant key-value state stores. State stores are checkpointed incrementally to avoid state loss and for increased performance. State stores are managed by StateStoreProvider s with HDFSBackedStateStoreProvider being the default and only known implementation. HDFSBackedStateStoreProvider uses Hadoop DFS-compliant file system for state checkpointing and fault-tolerance . State store providers manage versioned state per stateful operator (and partition it operates on) . The lifecycle of a StateStoreProvider begins when StateStore utility (on a Spark executor) is requested for the StateStore by provider ID and version . Important It is worth to notice that since StateStore and StateStoreProvider utilities are Scala objects that makes it possible that there can only be one instance of StateStore and StateStoreProvider on a single JVM. Scala objects are (sort of) singletons which means that there will be exactly one instance of each per JVM and that is exactly the JVM of a Spark executor. As long as the executor is up and running state versions are cached and no Hadoop DFS is used (except for the initial load). When requested for a StateStore , StateStore utility is given the version of a state store to look up. The version is either the < > (in Continuous Stream Processing ) or the current batch ID (in Micro-Batch Stream Processing ). StateStore utility requests StateStoreProvider utility to < > that creates the StateStoreProvider implementation (based on spark.sql.streaming.stateStore.providerClass internal configuration property) and requests it to < >. The initialized StateStoreProvider is cached in loadedProviders internal lookup table (for a < >) for later lookups. StateStoreProvider utility then requests the StateStoreProvider for the < >. (e.g. a HDFSBackedStateStore in case of HDFSBackedStateStoreProvider ). An instance of StateStoreProvider is requested to < > or < > (when < >) in < > that runs periodically every spark.sql.streaming.stateStore.maintenanceInterval configuration property.","title":" Versioned State, StateStores and StateStoreProviders"},{"location":"spark-sql-streaming-stateful-stream-processing/#incrementalexecution-queryexecution-of-streaming-queries","text":"Regardless of the query language (Dataset API or SQL), any structured query (incl. streaming queries) becomes a logical query plan. In Spark Structured Streaming it is IncrementalExecution that plans streaming queries for execution. While planning a streaming query for execution (aka query planning ), IncrementalExecution uses the state preparation rule . The rule fills out the following physical operators with the execution-specific configuration (with StatefulOperatorStateInfo being the most important for stateful stream processing): FlatMapGroupsWithStateExec StateStoreRestoreExec StateStoreSaveExec StreamingDeduplicateExec StreamingGlobalLimitExec StreamingSymmetricHashJoinExec ==== [[IncrementalExecution-shouldRunAnotherBatch]] Micro-Batch Stream Processing and Extra Non-Data Batch for StateStoreWriter Stateful Operators In Micro-Batch Stream Processing (with MicroBatchExecution engine), IncrementalExecution uses shouldRunAnotherBatch flag that allows StateStoreWriters stateful physical operators to indicate whether the last batch execution requires another non-data batch . The following StateStoreWriters redefine shouldRunAnotherBatch flag. [[StateStoreWriters-shouldRunAnotherBatch]] .StateStoreWriters and shouldRunAnotherBatch Flag [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | StateStoreWriter | shouldRunAnotherBatch Flag | FlatMapGroupsWithStateExec a| [[shouldRunAnotherBatch-FlatMapGroupsWithStateExec]] Based on GroupStateTimeout | < > a| [[shouldRunAnotherBatch-StateStoreSaveExec]] Based on < > | < > a| [[shouldRunAnotherBatch-StreamingDeduplicateExec]] Based on < > | < > a| [[shouldRunAnotherBatch-StreamingSymmetricHashJoinExec]] Based on < > |===","title":" IncrementalExecution &mdash; QueryExecution of Streaming Queries"},{"location":"spark-sql-streaming-stateful-stream-processing/#statestorerdd","text":"Right after query planning , a stateful streaming query (a single micro-batch actually) becomes an RDD with one or more StateStoreRDD s. You can find the StateStoreRDDs of a streaming query in the RDD lineage. scala> :type streamingQuery org.apache.spark.sql.streaming.StreamingQuery scala> streamingQuery.explain == Physical Plan == *(4) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[count(1)]) +- StateStoreSave [window#13-T0ms, value#3L], state info [ checkpoint = file:/tmp/checkpoint-counts/state, runId = 1dec2d81-f2d0-45b9-8f16-39ede66e13e7, opId = 0, ver = 1, numPartitions = 1], Append, 10000, 2 +- *(3) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[merge_count(1)]) +- StateStoreRestore [window#13-T0ms, value#3L], state info [ checkpoint = file:/tmp/checkpoint-counts/state, runId = 1dec2d81-f2d0-45b9-8f16-39ede66e13e7, opId = 0, ver = 1, numPartitions = 1], 2 +- *(2) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[merge_count(1)]) +- Exchange hashpartitioning(window#13-T0ms, value#3L, 1) +- *(1) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[partial_count(1)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#13-T0ms, value#3L] +- *(1) Filter isnotnull(time#2-T0ms) +- EventTimeWatermark time#2: timestamp, interval +- LocalTableScan <empty>, [time#2, value#3L] import org.apache.spark.sql.execution.streaming.{StreamExecution, StreamingQueryWrapper} val se = streamingQuery.asInstanceOf[StreamingQueryWrapper].streamingQuery scala> :type se org.apache.spark.sql.execution.streaming.StreamExecution scala> :type se.lastExecution org.apache.spark.sql.execution.streaming.IncrementalExecution val rdd = se.lastExecution.toRdd scala> rdd.toDebugString res3: String = (1) MapPartitionsRDD[39] at toRdd at <console>:40 [] | StateStoreRDD[38] at toRdd at <console>:40 [] // <-- here | MapPartitionsRDD[37] at toRdd at <console>:40 [] | StateStoreRDD[36] at toRdd at <console>:40 [] // <-- here | MapPartitionsRDD[35] at toRdd at <console>:40 [] | ShuffledRowRDD[17] at start at <pastie>:67 [] +-(1) MapPartitionsRDD[16] at start at <pastie>:67 [] | MapPartitionsRDD[15] at start at <pastie>:67 [] | MapPartitionsRDD[14] at start at <pastie>:67 [] | MapPartitionsRDD[13] at start at <pastie>:67 [] | ParallelCollectionRDD[12] at start at <pastie>:67 []","title":" StateStoreRDD"},{"location":"spark-sql-streaming-stateful-stream-processing/#statestorecoordinator-rpc-endpoint-statestorerdd-and-preferred-locations","text":"Since execution of a stateful streaming query happens on Spark executors whereas planning is on the driver, Spark Structured Streaming uses RPC environment for tracking locations of the state stores in use. That makes the tasks (of a structured query) to be scheduled where the state (of a partition) is. When planned for execution, the StateStoreRDD is first asked for the preferred locations of a partition (which happens on the driver) that are later used to compute it (on Spark executors). Spark Structured Streaming uses RPC environment to keep track of StateStore s (their StateStoreProvider actually) for RDD planning. Every time StateStoreRDD is requested for the preferred locations of a partition , it communicates with the StateStoreCoordinator RPC endpoint that knows the locations of the required StateStores (per host and executor ID). StateStoreRDD uses StateStoreProviderId with StateStoreId to uniquely identify the state store to use for ( associate with ) a stateful operator and a partition.","title":" StateStoreCoordinator RPC Endpoint, StateStoreRDD and Preferred Locations"},{"location":"spark-sql-streaming-stateful-stream-processing/#state-management","text":"The state in a stateful streaming query can be implicit or explicit.","title":"State Management"},{"location":"spark-sql-streaming-window/","text":"== [[window]] window Function -- Stream Time Windows window is a standard function that generates tumbling , sliding or delayed stream time window ranges (on a timestamp column). [source, scala] \u00b6 window( timeColumn: Column, windowDuration: String): Column // <1> window( timeColumn: Column, windowDuration: String, slideDuration: String): Column // <2> window( timeColumn: Column, windowDuration: String, slideDuration: String, startTime: String): Column // <3> <1> Creates a tumbling time window with slideDuration as windowDuration and 0 second for startTime <2> Creates a sliding time window with 0 second for startTime <3> Creates a delayed time window [NOTE] \u00b6 From https://msdn.microsoft.com/en-us/library/azure/dn835055.aspx[Tumbling Window (Azure Stream Analytics)]: > Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals. \u00b6 [NOTE] \u00b6 From https://flink.apache.org/news/2015/12/04/Introducing-windows.html[Introducing Stream Windows in Apache Flink]: Tumbling windows group elements of a stream into finite sets where each set corresponds to an interval. > Tumbling windows discretize a stream into non-overlapping windows. \u00b6 [source, scala] \u00b6 scala> val timeColumn = window($\"time\", \"5 seconds\") timeColumn: org.apache.spark.sql.Column = timewindow(time, 5000000, 5000000, 0) AS window timeColumn should be of TimestampType , i.e. with https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html[java.sql.Timestamp ] values. TIP: Use ++ https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html#from-java.time.Instant-++[java.sql.Timestamp.from ] or ++ https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html#valueOf-java.time.LocalDateTime-++[java.sql.Timestamp.valueOf ] factory methods to create Timestamp instances. [source, scala] \u00b6 // https://docs.oracle.com/javase/8/docs/api/java/time/LocalDateTime.html import java.time.LocalDateTime // https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html import java.sql.Timestamp val levels = Seq( // (year, month, dayOfMonth, hour, minute, second) ((2012, 12, 12, 12, 12, 12), 5), ((2012, 12, 12, 12, 12, 14), 9), ((2012, 12, 12, 13, 13, 14), 4), ((2016, 8, 13, 0, 0, 0), 10), ((2017, 5, 27, 0, 0, 0), 15)). map { case ((yy, mm, dd, h, m, s), a) => (LocalDateTime.of(yy, mm, dd, h, m, s), a) }. map { case (ts, a) => (Timestamp.valueOf(ts), a) }. toDF(\"time\", \"level\") scala> levels.show +-------------------+-----+ | time|level| +-------------------+-----+ |2012-12-12 12:12:12| 5| |2012-12-12 12:12:14| 9| |2012-12-12 13:13:14| 4| |2016-08-13 00:00:00| 10| |2017-05-27 00:00:00| 15| +-------------------+-----+ val q = levels.select(window($\"time\", \"5 seconds\"), $\"level\") scala> q.show(truncate = false) +---------------------------------------------+-----+ |window |level| +---------------------------------------------+-----+ |[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|5 | |[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|9 | |[2012-12-12 13:13:10.0,2012-12-12 13:13:15.0]|4 | |[2016-08-13 00:00:00.0,2016-08-13 00:00:05.0]|10 | |[2017-05-27 00:00:00.0,2017-05-27 00:00:05.0]|15 | +---------------------------------------------+-----+ scala> q.printSchema root |-- window: struct (nullable = true) | |-- start: timestamp (nullable = true) | |-- end: timestamp (nullable = true) |-- level: integer (nullable = false) // calculating the sum of levels every 5 seconds val sums = levels. groupBy(window($\"time\", \"5 seconds\")). agg(sum(\"level\") as \"level_sum\"). select(\"window.start\", \"window.end\", \"level_sum\") scala> sums.show +-------------------+-------------------+---------+ | start| end|level_sum| +-------------------+-------------------+---------+ |2012-12-12 13:13:10|2012-12-12 13:13:15| 4| |2012-12-12 12:12:10|2012-12-12 12:12:15| 14| |2016-08-13 00:00:00|2016-08-13 00:00:05| 10| |2017-05-27 00:00:00|2017-05-27 00:00:05| 15| +-------------------+-------------------+---------+ windowDuration and slideDuration are strings specifying the width of the window for duration and sliding identifiers, respectively. TIP: Use CalendarInterval for valid window identifiers. There are a couple of rules governing the durations: The window duration must be greater than 0 The slide duration must be greater than 0. The start time must be greater than or equal to 0. The slide duration must be less than or equal to the window duration. The start time must be less than the slide duration. NOTE: Only one window expression is supported in a query. NOTE: null values are filtered out in window expression. Internally, window creates a spark-sql-Column.md[Column] with TimeWindow Catalyst expression under window alias. [source, scala] \u00b6 scala> val timeColumn = window($\"time\", \"5 seconds\") timeColumn: org.apache.spark.sql.Column = timewindow(time, 5000000, 5000000, 0) AS window val windowExpr = timeColumn.expr scala> println(windowExpr.numberedTreeString) 00 timewindow('time, 5000000, 5000000, 0) AS window#23 01 +- timewindow('time, 5000000, 5000000, 0) 02 +- 'time Internally, TimeWindow Catalyst expression is simply a struct type with two fields, i.e. start and end , both of TimestampType type. [source, scala] \u00b6 scala> println(windowExpr.dataType) StructType(StructField(start,TimestampType,true), StructField(end,TimestampType,true)) scala> println(windowExpr.dataType.prettyJson) { \"type\" : \"struct\", \"fields\" : [ { \"name\" : \"start\", \"type\" : \"timestamp\", \"nullable\" : true, \"metadata\" : { } }, { \"name\" : \"end\", \"type\" : \"timestamp\", \"nullable\" : true, \"metadata\" : { } } ] } [NOTE] \u00b6 TimeWindow time window Catalyst expression is planned (i.e. converted ) in TimeWindowing logical optimization rule (i.e. Rule[LogicalPlan] ) of the Spark SQL logical query plan analyzer. Find more about the\u2009Spark SQL logical query plan analyzer in https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-Analyzer.html[Mastering Apache Spark 2] gitbook. \u00b6 ==== [[window-example]] Example -- Traffic Sensor NOTE: The example is borrowed from https://flink.apache.org/news/2015/12/04/Introducing-windows.html[Introducing Stream Windows in Apache Flink]. The example shows how to use window function to model a traffic sensor that counts every 15 seconds the number of vehicles passing a certain location.","title":"window Function"},{"location":"spark-sql-streaming-window/#source-scala","text":"window( timeColumn: Column, windowDuration: String): Column // <1> window( timeColumn: Column, windowDuration: String, slideDuration: String): Column // <2> window( timeColumn: Column, windowDuration: String, slideDuration: String, startTime: String): Column // <3> <1> Creates a tumbling time window with slideDuration as windowDuration and 0 second for startTime <2> Creates a sliding time window with 0 second for startTime <3> Creates a delayed time window","title":"[source, scala]"},{"location":"spark-sql-streaming-window/#note","text":"From https://msdn.microsoft.com/en-us/library/azure/dn835055.aspx[Tumbling Window (Azure Stream Analytics)]:","title":"[NOTE]"},{"location":"spark-sql-streaming-window/#tumbling-windows-are-a-series-of-fixed-sized-non-overlapping-and-contiguous-time-intervals","text":"","title":"&gt; Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals."},{"location":"spark-sql-streaming-window/#note_1","text":"From https://flink.apache.org/news/2015/12/04/Introducing-windows.html[Introducing Stream Windows in Apache Flink]: Tumbling windows group elements of a stream into finite sets where each set corresponds to an interval.","title":"[NOTE]"},{"location":"spark-sql-streaming-window/#tumbling-windows-discretize-a-stream-into-non-overlapping-windows","text":"","title":"&gt; Tumbling windows discretize a stream into non-overlapping windows."},{"location":"spark-sql-streaming-window/#source-scala_1","text":"scala> val timeColumn = window($\"time\", \"5 seconds\") timeColumn: org.apache.spark.sql.Column = timewindow(time, 5000000, 5000000, 0) AS window timeColumn should be of TimestampType , i.e. with https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html[java.sql.Timestamp ] values. TIP: Use ++ https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html#from-java.time.Instant-++[java.sql.Timestamp.from ] or ++ https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html#valueOf-java.time.LocalDateTime-++[java.sql.Timestamp.valueOf ] factory methods to create Timestamp instances.","title":"[source, scala]"},{"location":"spark-sql-streaming-window/#source-scala_2","text":"// https://docs.oracle.com/javase/8/docs/api/java/time/LocalDateTime.html import java.time.LocalDateTime // https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html import java.sql.Timestamp val levels = Seq( // (year, month, dayOfMonth, hour, minute, second) ((2012, 12, 12, 12, 12, 12), 5), ((2012, 12, 12, 12, 12, 14), 9), ((2012, 12, 12, 13, 13, 14), 4), ((2016, 8, 13, 0, 0, 0), 10), ((2017, 5, 27, 0, 0, 0), 15)). map { case ((yy, mm, dd, h, m, s), a) => (LocalDateTime.of(yy, mm, dd, h, m, s), a) }. map { case (ts, a) => (Timestamp.valueOf(ts), a) }. toDF(\"time\", \"level\") scala> levels.show +-------------------+-----+ | time|level| +-------------------+-----+ |2012-12-12 12:12:12| 5| |2012-12-12 12:12:14| 9| |2012-12-12 13:13:14| 4| |2016-08-13 00:00:00| 10| |2017-05-27 00:00:00| 15| +-------------------+-----+ val q = levels.select(window($\"time\", \"5 seconds\"), $\"level\") scala> q.show(truncate = false) +---------------------------------------------+-----+ |window |level| +---------------------------------------------+-----+ |[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|5 | |[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|9 | |[2012-12-12 13:13:10.0,2012-12-12 13:13:15.0]|4 | |[2016-08-13 00:00:00.0,2016-08-13 00:00:05.0]|10 | |[2017-05-27 00:00:00.0,2017-05-27 00:00:05.0]|15 | +---------------------------------------------+-----+ scala> q.printSchema root |-- window: struct (nullable = true) | |-- start: timestamp (nullable = true) | |-- end: timestamp (nullable = true) |-- level: integer (nullable = false) // calculating the sum of levels every 5 seconds val sums = levels. groupBy(window($\"time\", \"5 seconds\")). agg(sum(\"level\") as \"level_sum\"). select(\"window.start\", \"window.end\", \"level_sum\") scala> sums.show +-------------------+-------------------+---------+ | start| end|level_sum| +-------------------+-------------------+---------+ |2012-12-12 13:13:10|2012-12-12 13:13:15| 4| |2012-12-12 12:12:10|2012-12-12 12:12:15| 14| |2016-08-13 00:00:00|2016-08-13 00:00:05| 10| |2017-05-27 00:00:00|2017-05-27 00:00:05| 15| +-------------------+-------------------+---------+ windowDuration and slideDuration are strings specifying the width of the window for duration and sliding identifiers, respectively. TIP: Use CalendarInterval for valid window identifiers. There are a couple of rules governing the durations: The window duration must be greater than 0 The slide duration must be greater than 0. The start time must be greater than or equal to 0. The slide duration must be less than or equal to the window duration. The start time must be less than the slide duration. NOTE: Only one window expression is supported in a query. NOTE: null values are filtered out in window expression. Internally, window creates a spark-sql-Column.md[Column] with TimeWindow Catalyst expression under window alias.","title":"[source, scala]"},{"location":"spark-sql-streaming-window/#source-scala_3","text":"scala> val timeColumn = window($\"time\", \"5 seconds\") timeColumn: org.apache.spark.sql.Column = timewindow(time, 5000000, 5000000, 0) AS window val windowExpr = timeColumn.expr scala> println(windowExpr.numberedTreeString) 00 timewindow('time, 5000000, 5000000, 0) AS window#23 01 +- timewindow('time, 5000000, 5000000, 0) 02 +- 'time Internally, TimeWindow Catalyst expression is simply a struct type with two fields, i.e. start and end , both of TimestampType type.","title":"[source, scala]"},{"location":"spark-sql-streaming-window/#source-scala_4","text":"scala> println(windowExpr.dataType) StructType(StructField(start,TimestampType,true), StructField(end,TimestampType,true)) scala> println(windowExpr.dataType.prettyJson) { \"type\" : \"struct\", \"fields\" : [ { \"name\" : \"start\", \"type\" : \"timestamp\", \"nullable\" : true, \"metadata\" : { } }, { \"name\" : \"end\", \"type\" : \"timestamp\", \"nullable\" : true, \"metadata\" : { } } ] }","title":"[source, scala]"},{"location":"spark-sql-streaming-window/#note_2","text":"TimeWindow time window Catalyst expression is planned (i.e. converted ) in TimeWindowing logical optimization rule (i.e. Rule[LogicalPlan] ) of the Spark SQL logical query plan analyzer.","title":"[NOTE]"},{"location":"spark-sql-streaming-window/#find-more-about-the-spark-sql-logical-query-plan-analyzer-in-httpsjaceklaskowskigitbooksiomastering-apache-sparkspark-sql-analyzerhtmlmastering-apache-spark-2-gitbook","text":"==== [[window-example]] Example -- Traffic Sensor NOTE: The example is borrowed from https://flink.apache.org/news/2015/12/04/Introducing-windows.html[Introducing Stream Windows in Apache Flink]. The example shows how to use window function to model a traffic sensor that counts every 15 seconds the number of vehicles passing a certain location.","title":"Find more about the\u2009Spark SQL logical query plan analyzer in https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-Analyzer.html[Mastering Apache Spark 2] gitbook."},{"location":"spark-structured-streaming-batch-processing-time/","text":"Batch Processing Time \u00b6 Batch Processing Time (aka Batch Timeout Threshold ) is the processing time ( processing timestamp ) of the current streaming batch. The following standard functions (and their Catalyst expressions) allow accessing the batch processing time in Micro-Batch Stream Processing : now , current_timestamp , and unix_timestamp functions ( CurrentTimestamp ) current_date function ( CurrentDate ) Note CurrentTimestamp or CurrentDate expressions are not supported in Continuous Stream Processing . Internals \u00b6 GroupStateImpl is given the batch processing time when created for a streaming query (that is actually the batch processing time of the FlatMapGroupsWithStateExec physical operator). When created, FlatMapGroupsWithStateExec physical operator has the processing time undefined and set to the current timestamp in the state preparation rule every streaming batch. The current timestamp (and other batch-specific configurations) is given as the OffsetSeqMetadata (as part of the query planning phase) when a stream execution engine does the following: MicroBatchExecution is requested to construct a next streaming micro-batch in Micro-Batch Stream Processing In Continuous Stream Processing the base StreamExecution is requested to run stream processing and initializes OffsetSeqMetadata to 0 s.","title":"Batch Processing Time"},{"location":"spark-structured-streaming-batch-processing-time/#batch-processing-time","text":"Batch Processing Time (aka Batch Timeout Threshold ) is the processing time ( processing timestamp ) of the current streaming batch. The following standard functions (and their Catalyst expressions) allow accessing the batch processing time in Micro-Batch Stream Processing : now , current_timestamp , and unix_timestamp functions ( CurrentTimestamp ) current_date function ( CurrentDate ) Note CurrentTimestamp or CurrentDate expressions are not supported in Continuous Stream Processing .","title":"Batch Processing Time"},{"location":"spark-structured-streaming-batch-processing-time/#internals","text":"GroupStateImpl is given the batch processing time when created for a streaming query (that is actually the batch processing time of the FlatMapGroupsWithStateExec physical operator). When created, FlatMapGroupsWithStateExec physical operator has the processing time undefined and set to the current timestamp in the state preparation rule every streaming batch. The current timestamp (and other batch-specific configurations) is given as the OffsetSeqMetadata (as part of the query planning phase) when a stream execution engine does the following: MicroBatchExecution is requested to construct a next streaming micro-batch in Micro-Batch Stream Processing In Continuous Stream Processing the base StreamExecution is requested to run stream processing and initializes OffsetSeqMetadata to 0 s.","title":"Internals"},{"location":"spark-structured-streaming-internals/","text":"Internals of Streaming Queries \u00b6 The page is to keep notes about how to guide readers through the codebase and may disappear if merged with the other pages or become an intro page. DataStreamReader and Streaming Data Source Data Source Resolution, Streaming Dataset and Logical Query Plan Dataset API \u2014 High-Level DSL to Build Logical Query Plan DataStreamWriter and Streaming Data Sink StreamingQuery StreamingQueryManager DataStreamReader and Streaming Data Source \u00b6 It all starts with SparkSession.readStream method which lets you define a streaming source in a stream processing pipeline ( streaming processing graph or dataflow graph ). import org . apache . spark . sql . SparkSession assert ( spark . isInstanceOf [ SparkSession ]) val reader = spark . readStream import org . apache . spark . sql . streaming . DataStreamReader assert ( reader . isInstanceOf [ DataStreamReader ]) SparkSession.readStream method creates a DataStreamReader . The fluent API of DataStreamReader allows you to describe the input data source (e.g. DataStreamReader.format and DataStreamReader.options ) using method chaining (with the goal of making the readability of the source code close to that of ordinary written prose, essentially creating a domain-specific language within the interface. See Fluent interface article in Wikipedia). reader . format ( \"csv\" ) . option ( \"delimiter\" , \"|\" ) There are a couple of built-in data source formats. Their names are the names of the corresponding DataStreamReader methods and so act like shortcuts of DataStreamReader.format (where you have to specify the format by name), i.e. csv , json , orc , parquet and text , followed by DataStreamReader.load . You may also want to use DataStreamReader.schema method to specify the schema of the streaming data source. reader . schema ( \"a INT, b STRING\" ) In the end, you use DataStreamReader.load method that simply creates a streaming Dataset (the good ol' Dataset that you may have already used in Spark SQL). val input = reader . format ( \"csv\" ) . option ( \"delimiter\" , \"\\t\" ) . schema ( \"word STRING, num INT\" ) . load ( \"data/streaming\" ) import org . apache . spark . sql . DataFrame assert ( input . isInstanceOf [ DataFrame ]) The Dataset has the isStreaming property enabled that is basically the only way you could distinguish streaming Datasets from regular, batch Datasets. assert ( input . isStreaming ) In other words, Spark Structured Streaming is designed to extend the features of Spark SQL and let your structured queries be streaming queries. Data Source Resolution, Streaming Dataset and Logical Query Plan \u00b6 Whenever you create a Dataset (be it batch in Spark SQL or streaming in Spark Structured Streaming) is when you create a logical query plan using the High-Level Dataset DSL . A logical query plan is made up of logical operators. Spark Structured Streaming gives you two logical operators to represent streaming sources ( StreamingRelationV2 and StreamingRelation ). When DataStreamReader.load method is executed, load first looks up the requested data source (that you specified using DataStreamReader.format ) and creates an instance of it ( instantiation ). That'd be data source resolution step (that I described in...FIXME). DataStreamReader.load is where you can find the intersection of the former Micro-Batch Stream Processing V1 API with the new Continuous Stream Processing V2 API. V2 Code Path \u00b6 For MicroBatchStream or ContinuousReadSupport data sources, DataStreamReader.load creates a logical query plan with a StreamingRelationV2 leaf logical operator. That is the new V2 code path . // rate data source is V2 val rates = spark.readStream.format(\"rate\").load val plan = rates.queryExecution.logical scala> println(plan.numberedTreeString) 00 StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2ed03b1a, rate, [timestamp#12, value#13L] V1 Code Path \u00b6 For all other types of streaming data sources, DataStreamReader.load creates a logical query plan with a StreamingRelation leaf logical operator. That is the former V1 code path . // text data source is V1 val texts = spark.readStream.format(\"text\").load(\"data/streaming\") val plan = texts.queryExecution.logical scala> println(plan.numberedTreeString) 00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@35edd886,text,List(),None,List(),None,Map(path -> data/streaming),None), FileSource[data/streaming], [value#18] Dataset API \u2014 High-Level DSL to Build Logical Query Plan \u00b6 With a streaming Dataset created, you can now use all the methods of Dataset API, including but not limited to the following operators: Dataset.dropDuplicates for streaming deduplication Dataset.groupBy and Dataset.groupByKey for streaming aggregation Dataset.withWatermark for event time watermark Please note that a streaming Dataset is a regular Dataset ( with some streaming-related limitations ). val rates = spark .readStream .format(\"rate\") .load val countByTime = rates .withWatermark(\"timestamp\", \"10 seconds\") .groupBy($\"timestamp\") .agg(count(\"*\") as \"count\") import org.apache.spark.sql.Dataset assert(countByTime.isInstanceOf[Dataset[_]]) The point is to understand that the Dataset API is a domain-specific language (DSL) to build a more sophisticated stream processing pipeline that you could also build using the low-level logical operators directly. Use Dataset.explain to learn the underlying logical and physical query plans. assert(countByTime.isStreaming) scala> countByTime.explain(extended = true) == Parsed Logical Plan == 'Aggregate ['timestamp], [unresolvedalias('timestamp, None), count(1) AS count#131L] +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] == Analyzed Logical Plan == timestamp: timestamp, count: bigint Aggregate [timestamp#88-T10000ms], [timestamp#88-T10000ms, count(1) AS count#131L] +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] == Optimized Logical Plan == Aggregate [timestamp#88-T10000ms], [timestamp#88-T10000ms, count(1) AS count#131L] +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- Project [timestamp#88] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] == Physical Plan == *(5) HashAggregate(keys=[timestamp#88-T10000ms], functions=[count(1)], output=[timestamp#88-T10000ms, count#131L]) +- StateStoreSave [timestamp#88-T10000ms], state info [ checkpoint = <unknown>, runId = 28606ba5-9c7f-4f1f-ae41-e28d75c4d948, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2 +- *(4) HashAggregate(keys=[timestamp#88-T10000ms], functions=[merge_count(1)], output=[timestamp#88-T10000ms, count#136L]) +- StateStoreRestore [timestamp#88-T10000ms], state info [ checkpoint = <unknown>, runId = 28606ba5-9c7f-4f1f-ae41-e28d75c4d948, opId = 0, ver = 0, numPartitions = 200], 2 +- *(3) HashAggregate(keys=[timestamp#88-T10000ms], functions=[merge_count(1)], output=[timestamp#88-T10000ms, count#136L]) +- Exchange hashpartitioning(timestamp#88-T10000ms, 200) +- *(2) HashAggregate(keys=[timestamp#88-T10000ms], functions=[partial_count(1)], output=[timestamp#88-T10000ms, count#136L]) +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- *(1) Project [timestamp#88] +- StreamingRelation rate, [timestamp#88, value#89L] Or go pro and talk to QueryExecution directly. val plan = countByTime.queryExecution.logical scala> println(plan.numberedTreeString) 00 'Aggregate ['timestamp], [unresolvedalias('timestamp, None), count(1) AS count#131L] 01 +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds 02 +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] Please note that most of the stream processing operators you may also have used in batch structured queries in Spark SQL. Again, the distinction between Spark SQL and Spark Structured Streaming is very thin from a developer's point of view. DataStreamWriter and Streaming Data Sink \u00b6 Once you're satisfied with building a stream processing pipeline (using the APIs of DataStreamReader , Dataset , RelationalGroupedDataset and KeyValueGroupedDataset ), you should define how and when the result of the streaming query is persisted in ( sent out to ) an external data system using a streaming sink . You should use Dataset.writeStream method that simply creates a DataStreamWriter . // Not only is this a Dataset, but it is also streaming assert(countByTime.isStreaming) val writer = countByTime.writeStream import org.apache.spark.sql.streaming.DataStreamWriter assert(writer.isInstanceOf[DataStreamWriter[_]]) The fluent API of DataStreamWriter allows you to describe the output data sink ( DataStreamWriter.format and DataStreamWriter.options ) using method chaining (with the goal of making the readability of the source code close to that of ordinary written prose, essentially creating a domain-specific language within the interface. See Fluent interface article in Wikipedia). writer .format(\"csv\") .option(\"delimiter\", \"\\t\") Like in DataStreamReader data source formats, there are a couple of built-in data sink formats. Unlike data source formats, their names do not have corresponding DataStreamWriter methods. The reason is that you will use DataStreamWriter.start to create and immediately start a StreamingQuery . There are however two special output formats that do have corresponding DataStreamWriter methods, i.e. DataStreamWriter.foreach and DataStreamWriter.foreachBatch , that allow for persisting query results to external data systems that do not have streaming sinks available. They give you a trade-off between developing a full-blown streaming sink and simply using the methods (that lay the basis of what a custom sink would have to do anyway). DataStreamWriter API defines two new concepts (that are not available in the \"base\" Spark SQL): OutputMode that you specify using DataStreamWriter.outputMode method Trigger that you specify using DataStreamWriter.trigger method You may also want to give a streaming query a name using DataStreamWriter.queryName method. In the end, you use DataStreamWriter.start method to create and immediately start a StreamingQuery . import org.apache.spark.sql.streaming.OutputMode import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = writer .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", \"/tmp/csv-to-csv-checkpoint\") .outputMode(OutputMode.Append) .trigger(Trigger.ProcessingTime(30.seconds)) .queryName(\"csv-to-csv\") .start(\"/tmp\") import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) When DataStreamWriter is requested to start a streaming query , it allows for the following data source formats: memory with MemorySinkV2 (with ContinuousTrigger ) or MemorySink foreach with ForeachWriterProvider sink foreachBatch with ForeachBatchSink sink (that does not support ContinuousTrigger ) Any DataSourceRegister data source Custom data sources specified by their fully-qualified class names or [name].DefaultSource avro , kafka and some others (see DataSource.lookupDataSource object method) DataSource is requested to create a streaming sink that accepts StreamSinkProvider or FileFormat data sources only With a streaming sink, DataStreamWriter requests the StreamingQueryManager to start a streaming query . StreamingQuery \u00b6 When a stream processing pipeline is started (using DataStreamWriter.start method), DataStreamWriter creates a StreamingQuery and requests the StreamingQueryManager to start a streaming query . StreamingQueryManager \u00b6 StreamingQueryManager is used to manage streaming queries.","title":"Internals of Streaming Queries"},{"location":"spark-structured-streaming-internals/#internals-of-streaming-queries","text":"The page is to keep notes about how to guide readers through the codebase and may disappear if merged with the other pages or become an intro page. DataStreamReader and Streaming Data Source Data Source Resolution, Streaming Dataset and Logical Query Plan Dataset API \u2014 High-Level DSL to Build Logical Query Plan DataStreamWriter and Streaming Data Sink StreamingQuery StreamingQueryManager","title":"Internals of Streaming Queries"},{"location":"spark-structured-streaming-internals/#datastreamreader-and-streaming-data-source","text":"It all starts with SparkSession.readStream method which lets you define a streaming source in a stream processing pipeline ( streaming processing graph or dataflow graph ). import org . apache . spark . sql . SparkSession assert ( spark . isInstanceOf [ SparkSession ]) val reader = spark . readStream import org . apache . spark . sql . streaming . DataStreamReader assert ( reader . isInstanceOf [ DataStreamReader ]) SparkSession.readStream method creates a DataStreamReader . The fluent API of DataStreamReader allows you to describe the input data source (e.g. DataStreamReader.format and DataStreamReader.options ) using method chaining (with the goal of making the readability of the source code close to that of ordinary written prose, essentially creating a domain-specific language within the interface. See Fluent interface article in Wikipedia). reader . format ( \"csv\" ) . option ( \"delimiter\" , \"|\" ) There are a couple of built-in data source formats. Their names are the names of the corresponding DataStreamReader methods and so act like shortcuts of DataStreamReader.format (where you have to specify the format by name), i.e. csv , json , orc , parquet and text , followed by DataStreamReader.load . You may also want to use DataStreamReader.schema method to specify the schema of the streaming data source. reader . schema ( \"a INT, b STRING\" ) In the end, you use DataStreamReader.load method that simply creates a streaming Dataset (the good ol' Dataset that you may have already used in Spark SQL). val input = reader . format ( \"csv\" ) . option ( \"delimiter\" , \"\\t\" ) . schema ( \"word STRING, num INT\" ) . load ( \"data/streaming\" ) import org . apache . spark . sql . DataFrame assert ( input . isInstanceOf [ DataFrame ]) The Dataset has the isStreaming property enabled that is basically the only way you could distinguish streaming Datasets from regular, batch Datasets. assert ( input . isStreaming ) In other words, Spark Structured Streaming is designed to extend the features of Spark SQL and let your structured queries be streaming queries.","title":" DataStreamReader and Streaming Data Source"},{"location":"spark-structured-streaming-internals/#data-source-resolution-streaming-dataset-and-logical-query-plan","text":"Whenever you create a Dataset (be it batch in Spark SQL or streaming in Spark Structured Streaming) is when you create a logical query plan using the High-Level Dataset DSL . A logical query plan is made up of logical operators. Spark Structured Streaming gives you two logical operators to represent streaming sources ( StreamingRelationV2 and StreamingRelation ). When DataStreamReader.load method is executed, load first looks up the requested data source (that you specified using DataStreamReader.format ) and creates an instance of it ( instantiation ). That'd be data source resolution step (that I described in...FIXME). DataStreamReader.load is where you can find the intersection of the former Micro-Batch Stream Processing V1 API with the new Continuous Stream Processing V2 API.","title":" Data Source Resolution, Streaming Dataset and Logical Query Plan"},{"location":"spark-structured-streaming-internals/#v2-code-path","text":"For MicroBatchStream or ContinuousReadSupport data sources, DataStreamReader.load creates a logical query plan with a StreamingRelationV2 leaf logical operator. That is the new V2 code path . // rate data source is V2 val rates = spark.readStream.format(\"rate\").load val plan = rates.queryExecution.logical scala> println(plan.numberedTreeString) 00 StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2ed03b1a, rate, [timestamp#12, value#13L]","title":"V2 Code Path"},{"location":"spark-structured-streaming-internals/#v1-code-path","text":"For all other types of streaming data sources, DataStreamReader.load creates a logical query plan with a StreamingRelation leaf logical operator. That is the former V1 code path . // text data source is V1 val texts = spark.readStream.format(\"text\").load(\"data/streaming\") val plan = texts.queryExecution.logical scala> println(plan.numberedTreeString) 00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@35edd886,text,List(),None,List(),None,Map(path -> data/streaming),None), FileSource[data/streaming], [value#18]","title":"V1 Code Path"},{"location":"spark-structured-streaming-internals/#dataset-api-high-level-dsl-to-build-logical-query-plan","text":"With a streaming Dataset created, you can now use all the methods of Dataset API, including but not limited to the following operators: Dataset.dropDuplicates for streaming deduplication Dataset.groupBy and Dataset.groupByKey for streaming aggregation Dataset.withWatermark for event time watermark Please note that a streaming Dataset is a regular Dataset ( with some streaming-related limitations ). val rates = spark .readStream .format(\"rate\") .load val countByTime = rates .withWatermark(\"timestamp\", \"10 seconds\") .groupBy($\"timestamp\") .agg(count(\"*\") as \"count\") import org.apache.spark.sql.Dataset assert(countByTime.isInstanceOf[Dataset[_]]) The point is to understand that the Dataset API is a domain-specific language (DSL) to build a more sophisticated stream processing pipeline that you could also build using the low-level logical operators directly. Use Dataset.explain to learn the underlying logical and physical query plans. assert(countByTime.isStreaming) scala> countByTime.explain(extended = true) == Parsed Logical Plan == 'Aggregate ['timestamp], [unresolvedalias('timestamp, None), count(1) AS count#131L] +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] == Analyzed Logical Plan == timestamp: timestamp, count: bigint Aggregate [timestamp#88-T10000ms], [timestamp#88-T10000ms, count(1) AS count#131L] +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] == Optimized Logical Plan == Aggregate [timestamp#88-T10000ms], [timestamp#88-T10000ms, count(1) AS count#131L] +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- Project [timestamp#88] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] == Physical Plan == *(5) HashAggregate(keys=[timestamp#88-T10000ms], functions=[count(1)], output=[timestamp#88-T10000ms, count#131L]) +- StateStoreSave [timestamp#88-T10000ms], state info [ checkpoint = <unknown>, runId = 28606ba5-9c7f-4f1f-ae41-e28d75c4d948, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2 +- *(4) HashAggregate(keys=[timestamp#88-T10000ms], functions=[merge_count(1)], output=[timestamp#88-T10000ms, count#136L]) +- StateStoreRestore [timestamp#88-T10000ms], state info [ checkpoint = <unknown>, runId = 28606ba5-9c7f-4f1f-ae41-e28d75c4d948, opId = 0, ver = 0, numPartitions = 200], 2 +- *(3) HashAggregate(keys=[timestamp#88-T10000ms], functions=[merge_count(1)], output=[timestamp#88-T10000ms, count#136L]) +- Exchange hashpartitioning(timestamp#88-T10000ms, 200) +- *(2) HashAggregate(keys=[timestamp#88-T10000ms], functions=[partial_count(1)], output=[timestamp#88-T10000ms, count#136L]) +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- *(1) Project [timestamp#88] +- StreamingRelation rate, [timestamp#88, value#89L] Or go pro and talk to QueryExecution directly. val plan = countByTime.queryExecution.logical scala> println(plan.numberedTreeString) 00 'Aggregate ['timestamp], [unresolvedalias('timestamp, None), count(1) AS count#131L] 01 +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds 02 +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] Please note that most of the stream processing operators you may also have used in batch structured queries in Spark SQL. Again, the distinction between Spark SQL and Spark Structured Streaming is very thin from a developer's point of view.","title":" Dataset API &mdash; High-Level DSL to Build Logical Query Plan"},{"location":"spark-structured-streaming-internals/#datastreamwriter-and-streaming-data-sink","text":"Once you're satisfied with building a stream processing pipeline (using the APIs of DataStreamReader , Dataset , RelationalGroupedDataset and KeyValueGroupedDataset ), you should define how and when the result of the streaming query is persisted in ( sent out to ) an external data system using a streaming sink . You should use Dataset.writeStream method that simply creates a DataStreamWriter . // Not only is this a Dataset, but it is also streaming assert(countByTime.isStreaming) val writer = countByTime.writeStream import org.apache.spark.sql.streaming.DataStreamWriter assert(writer.isInstanceOf[DataStreamWriter[_]]) The fluent API of DataStreamWriter allows you to describe the output data sink ( DataStreamWriter.format and DataStreamWriter.options ) using method chaining (with the goal of making the readability of the source code close to that of ordinary written prose, essentially creating a domain-specific language within the interface. See Fluent interface article in Wikipedia). writer .format(\"csv\") .option(\"delimiter\", \"\\t\") Like in DataStreamReader data source formats, there are a couple of built-in data sink formats. Unlike data source formats, their names do not have corresponding DataStreamWriter methods. The reason is that you will use DataStreamWriter.start to create and immediately start a StreamingQuery . There are however two special output formats that do have corresponding DataStreamWriter methods, i.e. DataStreamWriter.foreach and DataStreamWriter.foreachBatch , that allow for persisting query results to external data systems that do not have streaming sinks available. They give you a trade-off between developing a full-blown streaming sink and simply using the methods (that lay the basis of what a custom sink would have to do anyway). DataStreamWriter API defines two new concepts (that are not available in the \"base\" Spark SQL): OutputMode that you specify using DataStreamWriter.outputMode method Trigger that you specify using DataStreamWriter.trigger method You may also want to give a streaming query a name using DataStreamWriter.queryName method. In the end, you use DataStreamWriter.start method to create and immediately start a StreamingQuery . import org.apache.spark.sql.streaming.OutputMode import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = writer .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", \"/tmp/csv-to-csv-checkpoint\") .outputMode(OutputMode.Append) .trigger(Trigger.ProcessingTime(30.seconds)) .queryName(\"csv-to-csv\") .start(\"/tmp\") import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) When DataStreamWriter is requested to start a streaming query , it allows for the following data source formats: memory with MemorySinkV2 (with ContinuousTrigger ) or MemorySink foreach with ForeachWriterProvider sink foreachBatch with ForeachBatchSink sink (that does not support ContinuousTrigger ) Any DataSourceRegister data source Custom data sources specified by their fully-qualified class names or [name].DefaultSource avro , kafka and some others (see DataSource.lookupDataSource object method) DataSource is requested to create a streaming sink that accepts StreamSinkProvider or FileFormat data sources only With a streaming sink, DataStreamWriter requests the StreamingQueryManager to start a streaming query .","title":" DataStreamWriter and Streaming Data Sink"},{"location":"spark-structured-streaming-internals/#streamingquery","text":"When a stream processing pipeline is started (using DataStreamWriter.start method), DataStreamWriter creates a StreamingQuery and requests the StreamingQueryManager to start a streaming query .","title":" StreamingQuery"},{"location":"spark-structured-streaming-internals/#streamingquerymanager","text":"StreamingQueryManager is used to manage streaming queries.","title":" StreamingQueryManager"},{"location":"streaming-aggregation/","text":"Streaming Aggregation \u00b6 In Spark Structured Streaming, a streaming aggregation is a streaming query that was described ( build ) using the following high-level streaming operators : Dataset.groupBy , Dataset.rollup , Dataset.cube (that simply create a RelationalGroupedDataset ) Dataset.groupByKey (that simply creates a KeyValueGroupedDataset ) SQL's GROUP BY clause (including WITH CUBE and WITH ROLLUP ) Streaming aggregation belongs to the category of Stateful Stream Processing . === [[IncrementalExecution]] IncrementalExecution -- QueryExecution of Streaming Queries Under the covers, the high-level operators create a logical query plan with one or more Aggregate logical operators. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-Aggregate.html[Aggregate ] logical operator in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. In Spark Structured Streaming IncrementalExecution is responsible for planning streaming queries for execution. At query planning , IncrementalExecution uses the StatefulAggregationStrategy execution planning strategy for planning streaming aggregations ( Aggregate unary logical operators) as pairs of StateStoreRestoreExec and StateStoreSaveExec physical operators. // input data from a data source // it's rate data source // but that does not really matter // We need a streaming Dataset val input = spark .readStream .format(\"rate\") .load // Streaming aggregation with groupBy val counts = input .groupBy($\"value\" % 2) .count counts.explain(extended = true) /** == Parsed Logical Plan == 'Aggregate [('value % 2)], [('value % 2) AS (value % 2)#23, count(1) AS count#22L] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L] == Analyzed Logical Plan == (value % 2): bigint, count: bigint Aggregate [(value#16L % cast(2 as bigint))], [(value#16L % cast(2 as bigint)) AS (value % 2)#23L, count(1) AS count#22L] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L] == Optimized Logical Plan == Aggregate [(value#16L % 2)], [(value#16L % 2) AS (value % 2)#23L, count(1) AS count#22L] +- Project [value#16L] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L] == Physical Plan == *(4) HashAggregate(keys=[(value#16L % 2)#27L], functions=[count(1)], output=[(value % 2)#23L, count#22L]) +- StateStoreSave [(value#16L % 2)#27L], state info [ checkpoint = <unknown>, runId = 8c0ae2be-5eaa-4038-bc29-a176abfaf885, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2 +- *(3) HashAggregate(keys=[(value#16L % 2)#27L], functions=[merge_count(1)], output=[(value#16L % 2)#27L, count#29L]) +- StateStoreRestore [(value#16L % 2)#27L], state info [ checkpoint = <unknown>, runId = 8c0ae2be-5eaa-4038-bc29-a176abfaf885, opId = 0, ver = 0, numPartitions = 200], 2 +- *(2) HashAggregate(keys=[(value#16L % 2)#27L], functions=[merge_count(1)], output=[(value#16L % 2)#27L, count#29L]) +- Exchange hashpartitioning((value#16L % 2)#27L, 200) +- *(1) HashAggregate(keys=[(value#16L % 2) AS (value#16L % 2)#27L], functions=[partial_count(1)], output=[(value#16L % 2)#27L, count#29L]) +- *(1) Project [value#16L] +- StreamingRelation rate, [timestamp#15, value#16L] */ Demos \u00b6 Learn more in the following demos: Streaming Watermark with Aggregation in Append Output Mode Streaming Query for Running Counts (Socket Source and Complete Output Mode) Streaming Aggregation with Kafka Data Source groupByKey Streaming Aggregation in Update Mode","title":"Streaming Aggregation"},{"location":"streaming-aggregation/#streaming-aggregation","text":"In Spark Structured Streaming, a streaming aggregation is a streaming query that was described ( build ) using the following high-level streaming operators : Dataset.groupBy , Dataset.rollup , Dataset.cube (that simply create a RelationalGroupedDataset ) Dataset.groupByKey (that simply creates a KeyValueGroupedDataset ) SQL's GROUP BY clause (including WITH CUBE and WITH ROLLUP ) Streaming aggregation belongs to the category of Stateful Stream Processing . === [[IncrementalExecution]] IncrementalExecution -- QueryExecution of Streaming Queries Under the covers, the high-level operators create a logical query plan with one or more Aggregate logical operators. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-Aggregate.html[Aggregate ] logical operator in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. In Spark Structured Streaming IncrementalExecution is responsible for planning streaming queries for execution. At query planning , IncrementalExecution uses the StatefulAggregationStrategy execution planning strategy for planning streaming aggregations ( Aggregate unary logical operators) as pairs of StateStoreRestoreExec and StateStoreSaveExec physical operators. // input data from a data source // it's rate data source // but that does not really matter // We need a streaming Dataset val input = spark .readStream .format(\"rate\") .load // Streaming aggregation with groupBy val counts = input .groupBy($\"value\" % 2) .count counts.explain(extended = true) /** == Parsed Logical Plan == 'Aggregate [('value % 2)], [('value % 2) AS (value % 2)#23, count(1) AS count#22L] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L] == Analyzed Logical Plan == (value % 2): bigint, count: bigint Aggregate [(value#16L % cast(2 as bigint))], [(value#16L % cast(2 as bigint)) AS (value % 2)#23L, count(1) AS count#22L] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L] == Optimized Logical Plan == Aggregate [(value#16L % 2)], [(value#16L % 2) AS (value % 2)#23L, count(1) AS count#22L] +- Project [value#16L] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L] == Physical Plan == *(4) HashAggregate(keys=[(value#16L % 2)#27L], functions=[count(1)], output=[(value % 2)#23L, count#22L]) +- StateStoreSave [(value#16L % 2)#27L], state info [ checkpoint = <unknown>, runId = 8c0ae2be-5eaa-4038-bc29-a176abfaf885, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2 +- *(3) HashAggregate(keys=[(value#16L % 2)#27L], functions=[merge_count(1)], output=[(value#16L % 2)#27L, count#29L]) +- StateStoreRestore [(value#16L % 2)#27L], state info [ checkpoint = <unknown>, runId = 8c0ae2be-5eaa-4038-bc29-a176abfaf885, opId = 0, ver = 0, numPartitions = 200], 2 +- *(2) HashAggregate(keys=[(value#16L % 2)#27L], functions=[merge_count(1)], output=[(value#16L % 2)#27L, count#29L]) +- Exchange hashpartitioning((value#16L % 2)#27L, 200) +- *(1) HashAggregate(keys=[(value#16L % 2) AS (value#16L % 2)#27L], functions=[partial_count(1)], output=[(value#16L % 2)#27L, count#29L]) +- *(1) Project [value#16L] +- StreamingRelation rate, [timestamp#15, value#16L] */","title":"Streaming Aggregation"},{"location":"streaming-aggregation/#demos","text":"Learn more in the following demos: Streaming Watermark with Aggregation in Append Output Mode Streaming Query for Running Counts (Socket Source and Complete Output Mode) Streaming Aggregation with Kafka Data Source groupByKey Streaming Aggregation in Update Mode","title":"Demos"},{"location":"streaming-watermark/","text":"Streaming Watermark \u00b6 Streaming Watermark of a stateful streaming query is how long to wait for late and possibly out-of-order events until a streaming state can be considered final and not to change. Streaming watermark is used to mark events (modeled as a row in the streaming Dataset) that are older than the threshold as \"too late\", and not \"interesting\" to update partial non-final streaming state. In Spark Structured Streaming, streaming watermark is defined using Dataset.withWatermark high-level operator. withWatermark ( eventTime : String , delayThreshold : String ): Dataset [ T ] In Dataset.withWatermark operator, eventTime is the name of the column to use to monitor event time whereas delayThreshold is a delay threshold. Watermark Delay says how late and possibly out-of-order events are still acceptable and contribute to the final result of a stateful streaming query. Event-time watermark delay is used to calculate the difference between the event time of an event and the time in the past. Event-Time Watermark is then a time threshold ( point in time ) that is the minimum acceptable time of an event (modeled as a row in the streaming Dataset) that is accepted in a stateful streaming query. With streaming watermark, memory usage of a streaming state can be controlled as late events can easily be dropped, and old state (e.g. aggregates or join) that are never going to be updated removed. That avoids unbounded streaming state that would inevitably use up all the available memory of long-running streaming queries and end up in out of memory errors. In Append output mode the current event-time streaming watermark is used for the following: Output saved state rows that became expired ( Expired events in the demo) Dropping late events, i.e. don't save them to a state store or include in aggregation ( Late events in the demo) Streaming watermark is required for a streaming aggregation in append output mode. Streaming Aggregation \u00b6 In streaming aggregation , a streaming watermark has to be defined on one or many grouping expressions of a streaming aggregation (directly or using window standard function). Note Dataset.withWatermark operator has to be used before an aggregation operator (for the watermark to have an effect). Streaming Join \u00b6 In streaming join , a streaming watermark can be defined on join keys or any of the join sides . Demos \u00b6 Use the following demos to learn more: Demo: Streaming Watermark with Aggregation in Append Output Mode Internals \u00b6 Under the covers, Dataset.withWatermark high-level operator creates a logical query plan with EventTimeWatermark logical operator. EventTimeWatermark logical operator is planned to EventTimeWatermarkExec physical operator that extracts the event times (from the data being processed) and adds them to an accumulator. Since the execution (data processing) happens on Spark executors, using the accumulator is the only Spark-approved way for communication between the tasks (on the executors) and the driver. Using accumulator updates the driver with the current event-time watermark. During the query planning phase (in MicroBatchExecution and ContinuousExecution ) that also happens on the driver, IncrementalExecution is given the current OffsetSeqMetadata with the current event-time watermark. Further Reading Or Watching \u00b6 SPARK-18124 Observed delay based event time watermarks","title":"Streaming Watermark"},{"location":"streaming-watermark/#streaming-watermark","text":"Streaming Watermark of a stateful streaming query is how long to wait for late and possibly out-of-order events until a streaming state can be considered final and not to change. Streaming watermark is used to mark events (modeled as a row in the streaming Dataset) that are older than the threshold as \"too late\", and not \"interesting\" to update partial non-final streaming state. In Spark Structured Streaming, streaming watermark is defined using Dataset.withWatermark high-level operator. withWatermark ( eventTime : String , delayThreshold : String ): Dataset [ T ] In Dataset.withWatermark operator, eventTime is the name of the column to use to monitor event time whereas delayThreshold is a delay threshold. Watermark Delay says how late and possibly out-of-order events are still acceptable and contribute to the final result of a stateful streaming query. Event-time watermark delay is used to calculate the difference between the event time of an event and the time in the past. Event-Time Watermark is then a time threshold ( point in time ) that is the minimum acceptable time of an event (modeled as a row in the streaming Dataset) that is accepted in a stateful streaming query. With streaming watermark, memory usage of a streaming state can be controlled as late events can easily be dropped, and old state (e.g. aggregates or join) that are never going to be updated removed. That avoids unbounded streaming state that would inevitably use up all the available memory of long-running streaming queries and end up in out of memory errors. In Append output mode the current event-time streaming watermark is used for the following: Output saved state rows that became expired ( Expired events in the demo) Dropping late events, i.e. don't save them to a state store or include in aggregation ( Late events in the demo) Streaming watermark is required for a streaming aggregation in append output mode.","title":"Streaming Watermark"},{"location":"streaming-watermark/#streaming-aggregation","text":"In streaming aggregation , a streaming watermark has to be defined on one or many grouping expressions of a streaming aggregation (directly or using window standard function). Note Dataset.withWatermark operator has to be used before an aggregation operator (for the watermark to have an effect).","title":"Streaming Aggregation"},{"location":"streaming-watermark/#streaming-join","text":"In streaming join , a streaming watermark can be defined on join keys or any of the join sides .","title":"Streaming Join"},{"location":"streaming-watermark/#demos","text":"Use the following demos to learn more: Demo: Streaming Watermark with Aggregation in Append Output Mode","title":"Demos"},{"location":"streaming-watermark/#internals","text":"Under the covers, Dataset.withWatermark high-level operator creates a logical query plan with EventTimeWatermark logical operator. EventTimeWatermark logical operator is planned to EventTimeWatermarkExec physical operator that extracts the event times (from the data being processed) and adds them to an accumulator. Since the execution (data processing) happens on Spark executors, using the accumulator is the only Spark-approved way for communication between the tasks (on the executors) and the driver. Using accumulator updates the driver with the current event-time watermark. During the query planning phase (in MicroBatchExecution and ContinuousExecution ) that also happens on the driver, IncrementalExecution is given the current OffsetSeqMetadata with the current event-time watermark.","title":"Internals"},{"location":"streaming-watermark/#further-reading-or-watching","text":"SPARK-18124 Observed delay based event time watermarks","title":"Further Reading Or Watching"},{"location":"webui/","text":"Web UI \u00b6 Web UI...FIXME CAUTION: FIXME What's visible on the plan diagram in the SQL tab of the UI","title":"Web UI"},{"location":"webui/#web-ui","text":"Web UI...FIXME CAUTION: FIXME What's visible on the plan diagram in the SQL tab of the UI","title":"Web UI"},{"location":"continuous-execution/","text":"Continuous Stream Processing \u00b6 Continuous Stream Processing is a stream processing engine in Spark Structured Streaming used for execution of structured streaming queries with Trigger.Continuous trigger. Continuous Stream Processing execution engine uses the novel Data Source API V2 (Spark SQL) and for the very first time makes stream processing truly continuous (not micro-batch). Tip Read up on Data Source API V2 in The Internals of Spark SQL online book. Because of the two innovative changes Continuous Stream Processing is often referred to as Structured Streaming V2 . Under the covers, Continuous Stream Processing uses ContinuousExecution stream execution engine. When requested to run an activated streaming query , ContinuousExecution adds WriteToContinuousDataSourceExec physical operator as the top-level operator in the physical query plan of the streaming query. scala> :type sq org.apache.spark.sql.streaming.StreamingQuery scala> sq.explain == Physical Plan == WriteToContinuousDataSource ConsoleWriter[numRows=20, truncate=false] +- *(1) Project [timestamp#758, value#759L] +- *(1) ScanV2 rate[timestamp#758, value#759L] From now on, you may think of a streaming query as a soon-to-be-generated ContinuousWriteRDD - an RDD data structure that Spark developers use to describe a distributed computation. When the streaming query is started (and the top-level WriteToContinuousDataSourceExec physical operator is requested to execute ), it simply requests the underlying ContinuousWriteRDD to collect. That collect operator is how a Spark job is run (as tasks over all partitions of the RDD) as described by the ContinuousWriteRDD.compute \"protocol\" (a recipe for the tasks to be scheduled to run on Spark executors). .Creating Instance of StreamExecution image::images/webui-spark-job-streaming-query-started.png[align=\"center\"] While the tasks are computing partitions (of the ContinuousWriteRDD ), they keep running until killed or completed . And that's the ingenious design trick of how the streaming query (as a Spark job with the distributed tasks running on executors) runs continuously and indefinitely. When DataStreamReader is requested to create a streaming query for a ContinuousReadSupport data source , it creates...FIXME Demo \u00b6 import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .option(\"truncate\", false) .trigger(Trigger.Continuous(15.seconds)) // <-- Uses ContinuousExecution for execution .queryName(\"rate2console\") .start scala> :type sq org.apache.spark.sql.streaming.StreamingQuery assert(sq.isActive) // sq.stop","title":"Continuous Stream Processing"},{"location":"continuous-execution/#continuous-stream-processing","text":"Continuous Stream Processing is a stream processing engine in Spark Structured Streaming used for execution of structured streaming queries with Trigger.Continuous trigger. Continuous Stream Processing execution engine uses the novel Data Source API V2 (Spark SQL) and for the very first time makes stream processing truly continuous (not micro-batch). Tip Read up on Data Source API V2 in The Internals of Spark SQL online book. Because of the two innovative changes Continuous Stream Processing is often referred to as Structured Streaming V2 . Under the covers, Continuous Stream Processing uses ContinuousExecution stream execution engine. When requested to run an activated streaming query , ContinuousExecution adds WriteToContinuousDataSourceExec physical operator as the top-level operator in the physical query plan of the streaming query. scala> :type sq org.apache.spark.sql.streaming.StreamingQuery scala> sq.explain == Physical Plan == WriteToContinuousDataSource ConsoleWriter[numRows=20, truncate=false] +- *(1) Project [timestamp#758, value#759L] +- *(1) ScanV2 rate[timestamp#758, value#759L] From now on, you may think of a streaming query as a soon-to-be-generated ContinuousWriteRDD - an RDD data structure that Spark developers use to describe a distributed computation. When the streaming query is started (and the top-level WriteToContinuousDataSourceExec physical operator is requested to execute ), it simply requests the underlying ContinuousWriteRDD to collect. That collect operator is how a Spark job is run (as tasks over all partitions of the RDD) as described by the ContinuousWriteRDD.compute \"protocol\" (a recipe for the tasks to be scheduled to run on Spark executors). .Creating Instance of StreamExecution image::images/webui-spark-job-streaming-query-started.png[align=\"center\"] While the tasks are computing partitions (of the ContinuousWriteRDD ), they keep running until killed or completed . And that's the ingenious design trick of how the streaming query (as a Spark job with the distributed tasks running on executors) runs continuously and indefinitely. When DataStreamReader is requested to create a streaming query for a ContinuousReadSupport data source , it creates...FIXME","title":"Continuous Stream Processing"},{"location":"continuous-execution/#demo","text":"import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .option(\"truncate\", false) .trigger(Trigger.Continuous(15.seconds)) // <-- Uses ContinuousExecution for execution .queryName(\"rate2console\") .start scala> :type sq org.apache.spark.sql.streaming.StreamingQuery assert(sq.isActive) // sq.stop","title":"Demo"},{"location":"continuous-execution/ContinuousExecution/","text":"ContinuousExecution \u00b6 ContinuousExecution is the stream execution engine of Continuous Stream Processing . ContinuousExecution can only run streaming queries with StreamingRelationV2 leaf logical operators with ContinuousReadSupport data source. When created (for a streaming query), ContinuousExecution is given the < >. The analyzed logical plan is immediately transformed to include a ContinuousExecutionRelation for every StreamingRelationV2 leaf logical operator with ContinuousReadSupport data source (and is the logical plan internally). Note ContinuousExecution uses the same instance of ContinuousExecutionRelation for the same instances of StreamingRelationV2 with ContinuousReadSupport data source. When requested to < >, ContinuousExecution collects ContinuousReadSupport data sources (inside ContinuousExecutionRelation ) from the < > and requests each and every ContinuousReadSupport to create a ContinuousReader (that are stored in < > internal registry). Local Properties \u00b6 __epoch_coordinator_id \u00b6 ContinuousExecution uses __epoch_coordinator_id local property for...FIXME __continuous_start_epoch \u00b6 ContinuousExecution uses __continuous_start_epoch local property for...FIXME __continuous_epoch_interval \u00b6 ContinuousExecution uses __continuous_epoch_interval local property for...FIXME TriggerExecutor \u00b6 TriggerExecutor for the Trigger : ProcessingTimeExecutor for ContinuousTrigger Used when...FIXME Note StreamExecution throws an IllegalStateException when the Trigger is not a ContinuousTrigger . Running Activated Streaming Query \u00b6 runActivatedStream ( sparkSessionForStream : SparkSession ): Unit runActivatedStream simply runs the streaming query in continuous mode as long as the state is ACTIVE . runActivatedStream is part of StreamExecution abstraction. Running Streaming Query in Continuous Mode \u00b6 runContinuous ( sparkSessionForQuery : SparkSession ): Unit runContinuous initializes the continuousSources internal registry by traversing the analyzed logical plan to find ContinuousExecutionRelation leaf logical operators and requests their ContinuousReadSupport data sources to create a ContinuousReader (with the sources metadata directory under the checkpoint directory ). runContinuous initializes the uniqueSources internal registry to be the continuousSources distinct. runContinuous gets the start offsets (they may or may not be available). runContinuous transforms the analyzed logical plan . For every ContinuousExecutionRelation runContinuous finds the corresponding ContinuousReader (in the continuousSources ), requests it to deserialize the start offsets (from their JSON representation), and then setStartOffset . In the end, runContinuous creates a StreamingDataSourceV2Relation (with the read schema of the ContinuousReader and the ContinuousReader itself). runContinuous rewires the transformed plan (with the StreamingDataSourceV2Relation ) to use the new attributes from the source (the reader). Important CurrentTimestamp and CurrentDate expressions are not supported for continuous processing. runContinuous ...FIXME runContinuous finds the only ContinuousReader (of the only StreamingDataSourceV2Relation ) in the query plan with the WriteToContinuousDataSource . queryPlanning Phase \u00b6 In queryPlanning time-tracking section , runContinuous creates an IncrementalExecution (that becomes the lastExecution ) that is immediately executed (the entire query execution pipeline is executed up to and including executedPlan ). runContinuous sets the following local properties: __is_continuous_processing as true __continuous_start_epoch as the currentBatchId __epoch_coordinator_id as the currentEpochCoordinatorId , i.e. runId followed by -- with a random UUID __continuous_epoch_interval as the interval of the ContinuousTrigger runContinuous uses the EpochCoordinatorRef helper to create a remote reference to the EpochCoordinator RPC endpoint (with the ContinuousReader , the currentEpochCoordinatorId , and the currentBatchId ). runContinuous creates a daemon epoch update thread and starts it immediately. runContinuous Phase \u00b6 In runContinuous time-tracking section , runContinuous requests the physical query plan (of the IncrementalExecution ) to execute (that simply requests the physical operator to doExecute and generate an RDD[InternalRow] ). runContinuous is used when ContinuousExecution is requested to run an activated streaming query . ==== [[runContinuous-epoch-update-thread]] Epoch Update Thread runContinuous creates an epoch update thread that...FIXME ==== [[getStartOffsets]] Getting Start Offsets From Checkpoint -- getStartOffsets Internal Method [source, scala] \u00b6 getStartOffsets(sparkSessionToRunBatches: SparkSession): OffsetSeq \u00b6 getStartOffsets ...FIXME NOTE: getStartOffsets is used exclusively when ContinuousExecution is requested to < >. Committing Epoch \u00b6 commit ( epoch : Long ): Unit In essence, commit adds the given epoch to commit log and the committedOffsets , and requests the < > to commit the corresponding offset . In the end, commit removes old log entries from the offset and commit logs (to keep spark.sql.streaming.minBatchesToRetain entries only). Internally, commit recordTriggerOffsets (with the from and to offsets as the committedOffsets and availableOffsets , respectively). At this point, commit may simply return when the stream execution thread is no longer alive (died). commit requests the commit log to store a metadata for the epoch. commit requests the single < > to deserialize the offset for the epoch (from the offset write-ahead log ). commit adds the single < > and the offset (for the epoch) to the committedOffsets registry. commit requests the single < > to commit the offset . commit requests the offset and commit logs to remove log entries to keep spark.sql.streaming.minBatchesToRetain only. commit then acquires the awaitProgressLock , wakes up all threads waiting for the awaitProgressLockCondition and in the end releases the awaitProgressLock . NOTE: commit supports only one continuous source (registered in the < > internal registry). commit asserts that the given epoch is available in the offsetLog internal registry (i.e. the offset for the given epoch has been reported before). commit is used when EpochCoordinator is requested to commitEpoch . === [[addOffset]] addOffset Method [source, scala] \u00b6 addOffset( epoch: Long, reader: ContinuousReader, partitionOffsets: Seq[PartitionOffset]): Unit In essense, addOffset requests the given ContinuousReader to mergeOffsets (with the given PartitionOffsets ) and then requests the OffsetSeqLog to register the offset with the given epoch . Internally, addOffset requests the given ContinuousReader to mergeOffsets (with the given PartitionOffsets ) and to get the current \"global\" offset back. addOffset then requests the OffsetSeqLog to add the current \"global\" offset for the given epoch . addOffset requests the OffsetSeqLog for the offset at the previous epoch . If the offsets at the current and previous epochs are the same, addOffset turns the noNewData internal flag on. addOffset then acquires the awaitProgressLock , wakes up all threads waiting for the awaitProgressLockCondition and in the end releases the awaitProgressLock . NOTE: addOffset supports exactly one continuous source . addOffset is used when EpochCoordinator is requested to handle a ReportPartitionOffset message. Analyzed Logical Plan of Streaming Query \u00b6 logicalPlan : LogicalPlan logicalPlan resolves StreamingRelationV2 leaf logical operators (with a ContinuousReadSupport source) to ContinuousExecutionRelation leaf logical operators. Internally, logicalPlan transforms the < > as follows: . For every StreamingRelationV2 leaf logical operator with a ContinuousReadSupport source, logicalPlan looks it up for the corresponding ContinuousExecutionRelation (if available in the internal lookup registry) or creates a ContinuousExecutionRelation (with the ContinuousReadSupport source, the options and the output attributes of the StreamingRelationV2 operator) . For any other StreamingRelationV2 , logicalPlan throws an UnsupportedOperationException : + Data source [name] does not support continuous processing. logicalPlan is part of the StreamExecution abstraction. Creating Instance \u00b6 ContinuousExecution takes the following when created: [[sparkSession]] SparkSession [[name]] The name of the structured query [[checkpointRoot]] Path to the checkpoint directory (aka metadata directory ) [[analyzedPlan]] Analyzed logical query plan ( LogicalPlan ) [[trigger]] Trigger [[triggerClock]] Clock [[outputMode]] OutputMode [[extraOptions]] Options ( Map[String, String] ) [[deleteCheckpointOnStop]] deleteCheckpointOnStop flag to control whether to delete the checkpoint directory on stop ContinuousExecution is created when StreamingQueryManager is requested to create a streaming query with a StreamWriteSupport sink and a ContinuousTrigger (when DataStreamWriter is requested to start an execution of the streaming query ). Stopping Stream Processing \u00b6 stop (): Unit stop is part of the StreamingQuery abstraction. stop transitions the streaming query to TERMINATED state. If the queryExecutionThread is alive (i.e. it has been started and has not yet died), stop interrupts it and waits for this thread to die. In the end, stop prints out the following INFO message to the logs: Query [prettyIdString] was stopped Note prettyIdString is in the format of queryName [id = [id], runId = [runId]] . === [[awaitEpoch]] awaitEpoch Internal Method [source, scala] \u00b6 awaitEpoch(epoch: Long): Unit \u00b6 awaitEpoch ...FIXME NOTE: awaitEpoch seems to be used exclusively in tests. continuousSources \u00b6 continuousSources : Seq [ ContinuousReader ] Registry of ContinuousReader s (in the analyzed logical plan of the streaming query ) As asserted in < > and < > there could only be exactly one ContinuousReaders registered. Used when ContinuousExecution is requested to < >, < >, and < > Use < > to access the current value sources \u00b6 ContinuousExecution supports one < > only in a < > (and asserts it when < > and < >). When requested for available streaming sources , ContinuousExecution simply gives the < >. import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .option(\"truncate\", false) .trigger(../Trigger.Continuous(1.minute)) // <-- Gives ContinuousExecution .queryName(\"rate2console\") .start import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) // The following gives access to the internals // And to ContinuousExecution import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val engine = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery import org.apache.spark.sql.execution.streaming.StreamExecution assert(engine.isInstanceOf[StreamExecution]) import org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution val continuousEngine = engine.asInstanceOf[ContinuousExecution] assert(continuousEngine.trigger == Trigger.Continuous(1.minute)) Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution=ALL Refer to Logging .","title":"ContinuousExecution"},{"location":"continuous-execution/ContinuousExecution/#continuousexecution","text":"ContinuousExecution is the stream execution engine of Continuous Stream Processing . ContinuousExecution can only run streaming queries with StreamingRelationV2 leaf logical operators with ContinuousReadSupport data source. When created (for a streaming query), ContinuousExecution is given the < >. The analyzed logical plan is immediately transformed to include a ContinuousExecutionRelation for every StreamingRelationV2 leaf logical operator with ContinuousReadSupport data source (and is the logical plan internally). Note ContinuousExecution uses the same instance of ContinuousExecutionRelation for the same instances of StreamingRelationV2 with ContinuousReadSupport data source. When requested to < >, ContinuousExecution collects ContinuousReadSupport data sources (inside ContinuousExecutionRelation ) from the < > and requests each and every ContinuousReadSupport to create a ContinuousReader (that are stored in < > internal registry).","title":"ContinuousExecution"},{"location":"continuous-execution/ContinuousExecution/#local-properties","text":"","title":"Local Properties"},{"location":"continuous-execution/ContinuousExecution/#__epoch_coordinator_id","text":"ContinuousExecution uses __epoch_coordinator_id local property for...FIXME","title":" __epoch_coordinator_id"},{"location":"continuous-execution/ContinuousExecution/#__continuous_start_epoch","text":"ContinuousExecution uses __continuous_start_epoch local property for...FIXME","title":" __continuous_start_epoch"},{"location":"continuous-execution/ContinuousExecution/#__continuous_epoch_interval","text":"ContinuousExecution uses __continuous_epoch_interval local property for...FIXME","title":" __continuous_epoch_interval"},{"location":"continuous-execution/ContinuousExecution/#triggerexecutor","text":"TriggerExecutor for the Trigger : ProcessingTimeExecutor for ContinuousTrigger Used when...FIXME Note StreamExecution throws an IllegalStateException when the Trigger is not a ContinuousTrigger .","title":" TriggerExecutor"},{"location":"continuous-execution/ContinuousExecution/#running-activated-streaming-query","text":"runActivatedStream ( sparkSessionForStream : SparkSession ): Unit runActivatedStream simply runs the streaming query in continuous mode as long as the state is ACTIVE . runActivatedStream is part of StreamExecution abstraction.","title":" Running Activated Streaming Query"},{"location":"continuous-execution/ContinuousExecution/#running-streaming-query-in-continuous-mode","text":"runContinuous ( sparkSessionForQuery : SparkSession ): Unit runContinuous initializes the continuousSources internal registry by traversing the analyzed logical plan to find ContinuousExecutionRelation leaf logical operators and requests their ContinuousReadSupport data sources to create a ContinuousReader (with the sources metadata directory under the checkpoint directory ). runContinuous initializes the uniqueSources internal registry to be the continuousSources distinct. runContinuous gets the start offsets (they may or may not be available). runContinuous transforms the analyzed logical plan . For every ContinuousExecutionRelation runContinuous finds the corresponding ContinuousReader (in the continuousSources ), requests it to deserialize the start offsets (from their JSON representation), and then setStartOffset . In the end, runContinuous creates a StreamingDataSourceV2Relation (with the read schema of the ContinuousReader and the ContinuousReader itself). runContinuous rewires the transformed plan (with the StreamingDataSourceV2Relation ) to use the new attributes from the source (the reader). Important CurrentTimestamp and CurrentDate expressions are not supported for continuous processing. runContinuous ...FIXME runContinuous finds the only ContinuousReader (of the only StreamingDataSourceV2Relation ) in the query plan with the WriteToContinuousDataSource .","title":" Running Streaming Query in Continuous Mode"},{"location":"continuous-execution/ContinuousExecution/#queryplanning-phase","text":"In queryPlanning time-tracking section , runContinuous creates an IncrementalExecution (that becomes the lastExecution ) that is immediately executed (the entire query execution pipeline is executed up to and including executedPlan ). runContinuous sets the following local properties: __is_continuous_processing as true __continuous_start_epoch as the currentBatchId __epoch_coordinator_id as the currentEpochCoordinatorId , i.e. runId followed by -- with a random UUID __continuous_epoch_interval as the interval of the ContinuousTrigger runContinuous uses the EpochCoordinatorRef helper to create a remote reference to the EpochCoordinator RPC endpoint (with the ContinuousReader , the currentEpochCoordinatorId , and the currentBatchId ). runContinuous creates a daemon epoch update thread and starts it immediately.","title":" queryPlanning Phase"},{"location":"continuous-execution/ContinuousExecution/#runcontinuous-phase","text":"In runContinuous time-tracking section , runContinuous requests the physical query plan (of the IncrementalExecution ) to execute (that simply requests the physical operator to doExecute and generate an RDD[InternalRow] ). runContinuous is used when ContinuousExecution is requested to run an activated streaming query . ==== [[runContinuous-epoch-update-thread]] Epoch Update Thread runContinuous creates an epoch update thread that...FIXME ==== [[getStartOffsets]] Getting Start Offsets From Checkpoint -- getStartOffsets Internal Method","title":" runContinuous Phase"},{"location":"continuous-execution/ContinuousExecution/#source-scala","text":"","title":"[source, scala]"},{"location":"continuous-execution/ContinuousExecution/#getstartoffsetssparksessiontorunbatches-sparksession-offsetseq","text":"getStartOffsets ...FIXME NOTE: getStartOffsets is used exclusively when ContinuousExecution is requested to < >.","title":"getStartOffsets(sparkSessionToRunBatches: SparkSession): OffsetSeq"},{"location":"continuous-execution/ContinuousExecution/#committing-epoch","text":"commit ( epoch : Long ): Unit In essence, commit adds the given epoch to commit log and the committedOffsets , and requests the < > to commit the corresponding offset . In the end, commit removes old log entries from the offset and commit logs (to keep spark.sql.streaming.minBatchesToRetain entries only). Internally, commit recordTriggerOffsets (with the from and to offsets as the committedOffsets and availableOffsets , respectively). At this point, commit may simply return when the stream execution thread is no longer alive (died). commit requests the commit log to store a metadata for the epoch. commit requests the single < > to deserialize the offset for the epoch (from the offset write-ahead log ). commit adds the single < > and the offset (for the epoch) to the committedOffsets registry. commit requests the single < > to commit the offset . commit requests the offset and commit logs to remove log entries to keep spark.sql.streaming.minBatchesToRetain only. commit then acquires the awaitProgressLock , wakes up all threads waiting for the awaitProgressLockCondition and in the end releases the awaitProgressLock . NOTE: commit supports only one continuous source (registered in the < > internal registry). commit asserts that the given epoch is available in the offsetLog internal registry (i.e. the offset for the given epoch has been reported before). commit is used when EpochCoordinator is requested to commitEpoch . === [[addOffset]] addOffset Method","title":" Committing Epoch"},{"location":"continuous-execution/ContinuousExecution/#source-scala_1","text":"addOffset( epoch: Long, reader: ContinuousReader, partitionOffsets: Seq[PartitionOffset]): Unit In essense, addOffset requests the given ContinuousReader to mergeOffsets (with the given PartitionOffsets ) and then requests the OffsetSeqLog to register the offset with the given epoch . Internally, addOffset requests the given ContinuousReader to mergeOffsets (with the given PartitionOffsets ) and to get the current \"global\" offset back. addOffset then requests the OffsetSeqLog to add the current \"global\" offset for the given epoch . addOffset requests the OffsetSeqLog for the offset at the previous epoch . If the offsets at the current and previous epochs are the same, addOffset turns the noNewData internal flag on. addOffset then acquires the awaitProgressLock , wakes up all threads waiting for the awaitProgressLockCondition and in the end releases the awaitProgressLock . NOTE: addOffset supports exactly one continuous source . addOffset is used when EpochCoordinator is requested to handle a ReportPartitionOffset message.","title":"[source, scala]"},{"location":"continuous-execution/ContinuousExecution/#analyzed-logical-plan-of-streaming-query","text":"logicalPlan : LogicalPlan logicalPlan resolves StreamingRelationV2 leaf logical operators (with a ContinuousReadSupport source) to ContinuousExecutionRelation leaf logical operators. Internally, logicalPlan transforms the < > as follows: . For every StreamingRelationV2 leaf logical operator with a ContinuousReadSupport source, logicalPlan looks it up for the corresponding ContinuousExecutionRelation (if available in the internal lookup registry) or creates a ContinuousExecutionRelation (with the ContinuousReadSupport source, the options and the output attributes of the StreamingRelationV2 operator) . For any other StreamingRelationV2 , logicalPlan throws an UnsupportedOperationException : + Data source [name] does not support continuous processing. logicalPlan is part of the StreamExecution abstraction.","title":" Analyzed Logical Plan of Streaming Query"},{"location":"continuous-execution/ContinuousExecution/#creating-instance","text":"ContinuousExecution takes the following when created: [[sparkSession]] SparkSession [[name]] The name of the structured query [[checkpointRoot]] Path to the checkpoint directory (aka metadata directory ) [[analyzedPlan]] Analyzed logical query plan ( LogicalPlan ) [[trigger]] Trigger [[triggerClock]] Clock [[outputMode]] OutputMode [[extraOptions]] Options ( Map[String, String] ) [[deleteCheckpointOnStop]] deleteCheckpointOnStop flag to control whether to delete the checkpoint directory on stop ContinuousExecution is created when StreamingQueryManager is requested to create a streaming query with a StreamWriteSupport sink and a ContinuousTrigger (when DataStreamWriter is requested to start an execution of the streaming query ).","title":"Creating Instance"},{"location":"continuous-execution/ContinuousExecution/#stopping-stream-processing","text":"stop (): Unit stop is part of the StreamingQuery abstraction. stop transitions the streaming query to TERMINATED state. If the queryExecutionThread is alive (i.e. it has been started and has not yet died), stop interrupts it and waits for this thread to die. In the end, stop prints out the following INFO message to the logs: Query [prettyIdString] was stopped Note prettyIdString is in the format of queryName [id = [id], runId = [runId]] . === [[awaitEpoch]] awaitEpoch Internal Method","title":" Stopping Stream Processing"},{"location":"continuous-execution/ContinuousExecution/#source-scala_2","text":"","title":"[source, scala]"},{"location":"continuous-execution/ContinuousExecution/#awaitepochepoch-long-unit","text":"awaitEpoch ...FIXME NOTE: awaitEpoch seems to be used exclusively in tests.","title":"awaitEpoch(epoch: Long): Unit"},{"location":"continuous-execution/ContinuousExecution/#continuoussources","text":"continuousSources : Seq [ ContinuousReader ] Registry of ContinuousReader s (in the analyzed logical plan of the streaming query ) As asserted in < > and < > there could only be exactly one ContinuousReaders registered. Used when ContinuousExecution is requested to < >, < >, and < > Use < > to access the current value","title":" continuousSources"},{"location":"continuous-execution/ContinuousExecution/#sources","text":"ContinuousExecution supports one < > only in a < > (and asserts it when < > and < >). When requested for available streaming sources , ContinuousExecution simply gives the < >. import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .option(\"truncate\", false) .trigger(../Trigger.Continuous(1.minute)) // <-- Gives ContinuousExecution .queryName(\"rate2console\") .start import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) // The following gives access to the internals // And to ContinuousExecution import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val engine = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery import org.apache.spark.sql.execution.streaming.StreamExecution assert(engine.isInstanceOf[StreamExecution]) import org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution val continuousEngine = engine.asInstanceOf[ContinuousExecution] assert(continuousEngine.trigger == Trigger.Continuous(1.minute))","title":" sources"},{"location":"continuous-execution/ContinuousExecution/#logging","text":"Enable ALL logging level for org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution=ALL Refer to Logging .","title":"Logging"},{"location":"continuous-execution/ContinuousReadSupport/","text":"ContinuousReadSupport \u00b6 ContinuousReadSupport is the < > of the DataSourceV2 for < > with a < > for Continuous Stream Processing . [[contract]][[createContinuousReader]] ContinuousReadSupport defines a single createContinuousReader method to create a ContinuousReader . ContinuousReader createContinuousReader ( Optional < StructType > schema , String checkpointLocation , DataSourceOptions options ) createContinuousReader is used when: ContinuousExecution is requested to run a streaming query (and finds ContinuousExecutionRelations in the analyzed logical plan ) DataStreamReader is requested to create a streaming query for a ContinuousReadSupport data source [[implementations]] .ContinuousReadSupports [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | ContinuousReadSupport | Description | ContinuousMemoryStream | [[ContinuousMemoryStream]] Data source provider for memory format | KafkaSourceProvider | [[KafkaSourceProvider]] Data source provider for kafka format | RateStreamProvider | [[RateStreamProvider]] Data source provider for rate format | TextSocketSourceProvider | [[TextSocketSourceProvider]] Data source provider for socket format |===","title":"ContinuousReadSupport"},{"location":"continuous-execution/ContinuousReadSupport/#continuousreadsupport","text":"ContinuousReadSupport is the < > of the DataSourceV2 for < > with a < > for Continuous Stream Processing . [[contract]][[createContinuousReader]] ContinuousReadSupport defines a single createContinuousReader method to create a ContinuousReader . ContinuousReader createContinuousReader ( Optional < StructType > schema , String checkpointLocation , DataSourceOptions options ) createContinuousReader is used when: ContinuousExecution is requested to run a streaming query (and finds ContinuousExecutionRelations in the analyzed logical plan ) DataStreamReader is requested to create a streaming query for a ContinuousReadSupport data source [[implementations]] .ContinuousReadSupports [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | ContinuousReadSupport | Description | ContinuousMemoryStream | [[ContinuousMemoryStream]] Data source provider for memory format | KafkaSourceProvider | [[KafkaSourceProvider]] Data source provider for kafka format | RateStreamProvider | [[RateStreamProvider]] Data source provider for rate format | TextSocketSourceProvider | [[TextSocketSourceProvider]] Data source provider for socket format |===","title":"ContinuousReadSupport"},{"location":"continuous-execution/ContinuousReader/","text":"ContinuousReader \u2014 Data Source Readers in Continuous Stream Processing \u00b6 ContinuousReader is the < > of Spark SQL's DataSourceReader abstraction for < > in Continuous Stream Processing . ContinuousReader is part of the novel Data Source API V2 in Spark SQL. Tip Read up on Data Source API V2 in The Internals of Spark SQL online book. [[contract]] .ContinuousReader Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | commit a| [[commit]] [source, java] \u00b6 void commit(Offset end) \u00b6 Commits the specified offset Used exclusively when ContinuousExecution is requested to commit an epoch | deserializeOffset a| [[deserializeOffset]] [source, java] \u00b6 Offset deserializeOffset(String json) \u00b6 Deserializes an offset from JSON representation Used when ContinuousExecution is requested to run a streaming query and commit an epoch | getStartOffset a| [[getStartOffset]] [source, java] \u00b6 Offset getStartOffset() \u00b6 NOTE: Used exclusively in tests. | mergeOffsets a| [[mergeOffsets]] [source, java] \u00b6 Offset mergeOffsets(PartitionOffset[] offsets) \u00b6 Used exclusively when ContinuousExecution is requested to addOffset | needsReconfiguration a| [[needsReconfiguration]] [source, java] \u00b6 boolean needsReconfiguration() \u00b6 Indicates that the reader needs reconfiguration (e.g. to generate new input partitions) Used exclusively when ContinuousExecution is requested to run a streaming query in continuous mode | setStartOffset a| [[setStartOffset]] [source, java] \u00b6 void setStartOffset(Optional start) \u00b6 Used exclusively when ContinuousExecution is requested to run the streaming query in continuous mode . |=== [[implementations]] .ContinuousReaders [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ContinuousReader | Description | ContinuousMemoryStream | [[ContinuousMemoryStream]] | KafkaContinuousReader | [[KafkaContinuousReader]] | RateStreamContinuousReader | [[RateStreamContinuousReader]] | TextSocketContinuousReader | [[TextSocketContinuousReader]] |===","title":"ContinuousReader"},{"location":"continuous-execution/ContinuousReader/#continuousreader-data-source-readers-in-continuous-stream-processing","text":"ContinuousReader is the < > of Spark SQL's DataSourceReader abstraction for < > in Continuous Stream Processing . ContinuousReader is part of the novel Data Source API V2 in Spark SQL. Tip Read up on Data Source API V2 in The Internals of Spark SQL online book. [[contract]] .ContinuousReader Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | commit a| [[commit]]","title":"ContinuousReader &mdash; Data Source Readers in Continuous Stream Processing"},{"location":"continuous-execution/ContinuousReader/#source-java","text":"","title":"[source, java]"},{"location":"continuous-execution/ContinuousReader/#void-commitoffset-end","text":"Commits the specified offset Used exclusively when ContinuousExecution is requested to commit an epoch | deserializeOffset a| [[deserializeOffset]]","title":"void commit(Offset end)"},{"location":"continuous-execution/ContinuousReader/#source-java_1","text":"","title":"[source, java]"},{"location":"continuous-execution/ContinuousReader/#offset-deserializeoffsetstring-json","text":"Deserializes an offset from JSON representation Used when ContinuousExecution is requested to run a streaming query and commit an epoch | getStartOffset a| [[getStartOffset]]","title":"Offset deserializeOffset(String json)"},{"location":"continuous-execution/ContinuousReader/#source-java_2","text":"","title":"[source, java]"},{"location":"continuous-execution/ContinuousReader/#offset-getstartoffset","text":"NOTE: Used exclusively in tests. | mergeOffsets a| [[mergeOffsets]]","title":"Offset getStartOffset()"},{"location":"continuous-execution/ContinuousReader/#source-java_3","text":"","title":"[source, java]"},{"location":"continuous-execution/ContinuousReader/#offset-mergeoffsetspartitionoffset-offsets","text":"Used exclusively when ContinuousExecution is requested to addOffset | needsReconfiguration a| [[needsReconfiguration]]","title":"Offset mergeOffsets(PartitionOffset[] offsets)"},{"location":"continuous-execution/ContinuousReader/#source-java_4","text":"","title":"[source, java]"},{"location":"continuous-execution/ContinuousReader/#boolean-needsreconfiguration","text":"Indicates that the reader needs reconfiguration (e.g. to generate new input partitions) Used exclusively when ContinuousExecution is requested to run a streaming query in continuous mode | setStartOffset a| [[setStartOffset]]","title":"boolean needsReconfiguration()"},{"location":"continuous-execution/ContinuousReader/#source-java_5","text":"","title":"[source, java]"},{"location":"continuous-execution/ContinuousReader/#void-setstartoffsetoptional-start","text":"Used exclusively when ContinuousExecution is requested to run the streaming query in continuous mode . |=== [[implementations]] .ContinuousReaders [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ContinuousReader | Description | ContinuousMemoryStream | [[ContinuousMemoryStream]] | KafkaContinuousReader | [[KafkaContinuousReader]] | RateStreamContinuousReader | [[RateStreamContinuousReader]] | TextSocketContinuousReader | [[TextSocketContinuousReader]] |===","title":"void setStartOffset(Optional start)"},{"location":"datasources/","text":"Data Sources \u00b6 Spark Structured Streaming comes with the following streaming sources and sinks: File Kafka Text Socket Rate Console Foreach ForeachBatchSink Memory","title":"Data Sources"},{"location":"datasources/#data-sources","text":"Spark Structured Streaming comes with the following streaming sources and sinks: File Kafka Text Socket Rate Console Foreach ForeachBatchSink Memory","title":"Data Sources"},{"location":"datasources/ForeachBatchSink/","text":"ForeachBatchSink \u00b6 ForeachBatchSink is a streaming sink that represents DataStreamWriter.foreachBatch streaming operator at runtime. Type Constructor ForeachBatchSink[T] is a Scala type constructor with the type parameter T . ForeachBatchSink was added in Spark 2.4.0 as part of SPARK-24565 Add API for in Structured Streaming for exposing output rows of each microbatch as a DataFrame . import org.apache.spark.sql.Dataset val q = spark.readStream .format(\"rate\") .load .writeStream .foreachBatch { (output: Dataset[_], batchId: Long) => // <-- creates a ForeachBatchSink println(s\"Batch ID: $batchId\") output.show } .start // q.stop scala> println(q.lastProgress.sink.description) ForeachBatchSink Creating Instance \u00b6 ForeachBatchSink takes the following when created: Batch Writer Function ( (Dataset[T], Long) => Unit ) Encoder of type T ( ExpressionEncoder[T] ) ForeachBatchSink is created when DataStreamWriter is requested to start execution of the streaming query (with the foreachBatch source) for DataStreamWriter.foreachBatch streaming operator. Adding Batch \u00b6 addBatch ( batchId : Long , data : DataFrame ): Unit addBatch requests the encoder to resolveAndBind (using the output of the analyzed logical plan of the given DataFrame ) that creates a \"resolved\" encoder. addBatch requests the resolved encoder to create an Deserializer (to convert a Spark SQL Row objects into objects of type T ). addBatch requests the QueryExecution (of the given DataFrame ) for RDD[InternalRow] ( executes the query plan ) and applies map operator to convert rows to Scala objects. Important At this point the \"old\" DataFrame is no longer a DataFrame but an RDD[InternalRow] . One of the \"side-effects\" is that whatever logical and physical optimizations may have been applied to the given DataFrame it is over now. addBatch creates a new Dataset (for the RDD) and executes batchWriter function (passing the Dataset and the batchId ). addBatch is a part of the Sink abstraction. Text Representation \u00b6 ForeachBatchSink uses ForeachBatchSink name.","title":"ForeachBatchSink"},{"location":"datasources/ForeachBatchSink/#foreachbatchsink","text":"ForeachBatchSink is a streaming sink that represents DataStreamWriter.foreachBatch streaming operator at runtime. Type Constructor ForeachBatchSink[T] is a Scala type constructor with the type parameter T . ForeachBatchSink was added in Spark 2.4.0 as part of SPARK-24565 Add API for in Structured Streaming for exposing output rows of each microbatch as a DataFrame . import org.apache.spark.sql.Dataset val q = spark.readStream .format(\"rate\") .load .writeStream .foreachBatch { (output: Dataset[_], batchId: Long) => // <-- creates a ForeachBatchSink println(s\"Batch ID: $batchId\") output.show } .start // q.stop scala> println(q.lastProgress.sink.description) ForeachBatchSink","title":"ForeachBatchSink"},{"location":"datasources/ForeachBatchSink/#creating-instance","text":"ForeachBatchSink takes the following when created: Batch Writer Function ( (Dataset[T], Long) => Unit ) Encoder of type T ( ExpressionEncoder[T] ) ForeachBatchSink is created when DataStreamWriter is requested to start execution of the streaming query (with the foreachBatch source) for DataStreamWriter.foreachBatch streaming operator.","title":"Creating Instance"},{"location":"datasources/ForeachBatchSink/#adding-batch","text":"addBatch ( batchId : Long , data : DataFrame ): Unit addBatch requests the encoder to resolveAndBind (using the output of the analyzed logical plan of the given DataFrame ) that creates a \"resolved\" encoder. addBatch requests the resolved encoder to create an Deserializer (to convert a Spark SQL Row objects into objects of type T ). addBatch requests the QueryExecution (of the given DataFrame ) for RDD[InternalRow] ( executes the query plan ) and applies map operator to convert rows to Scala objects. Important At this point the \"old\" DataFrame is no longer a DataFrame but an RDD[InternalRow] . One of the \"side-effects\" is that whatever logical and physical optimizations may have been applied to the given DataFrame it is over now. addBatch creates a new Dataset (for the RDD) and executes batchWriter function (passing the Dataset and the batchId ). addBatch is a part of the Sink abstraction.","title":" Adding Batch"},{"location":"datasources/ForeachBatchSink/#text-representation","text":"ForeachBatchSink uses ForeachBatchSink name.","title":" Text Representation"},{"location":"datasources/ForeachSink/","text":"ForeachSink \u00b6 ForeachSink is a typed streaming sink that passes rows (of the type T ) to ForeachWriter (one record at a time per partition). Note ForeachSink is assigned a ForeachWriter when DataStreamWriter is started . ForeachSink is used exclusively in foreach operator. [source, scala] \u00b6 val records = spark. readStream format(\"text\"). load(\"server-logs/*.out\"). as[String] import org.apache.spark.sql.ForeachWriter val writer = new ForeachWriter[String] { override def open(partitionId: Long, version: Long) = true override def process(value: String) = println(value) override def close(errorOrNull: Throwable) = {} } records.writeStream .queryName(\"server-logs processor\") .foreach(writer) .start Internally, addBatch (the only method from the < >) takes records from the input spark-sql-dataframe.md[DataFrame] (as data ), transforms them to expected type T (of this ForeachSink ) and (now as a spark-sql-dataset.md[Dataset]) spark-sql-dataset.md#foreachPartition[processes each partition]. [source, scala] \u00b6 addBatch(batchId: Long, data: DataFrame): Unit \u00b6 addBatch then opens the constructor's datasources/ForeachWriter.md[ForeachWriter] (for the spark-taskscheduler-taskcontext.md#getPartitionId[current partition] and the input batch) and passes the records to process (one at a time per partition). CAUTION: FIXME Why does Spark track whether the writer failed or not? Why couldn't it finally and do close ? CAUTION: FIXME Can we have a constant for \"foreach\" for source in DataStreamWriter ?","title":"ForeachSink"},{"location":"datasources/ForeachSink/#foreachsink","text":"ForeachSink is a typed streaming sink that passes rows (of the type T ) to ForeachWriter (one record at a time per partition). Note ForeachSink is assigned a ForeachWriter when DataStreamWriter is started . ForeachSink is used exclusively in foreach operator.","title":"ForeachSink"},{"location":"datasources/ForeachSink/#source-scala","text":"val records = spark. readStream format(\"text\"). load(\"server-logs/*.out\"). as[String] import org.apache.spark.sql.ForeachWriter val writer = new ForeachWriter[String] { override def open(partitionId: Long, version: Long) = true override def process(value: String) = println(value) override def close(errorOrNull: Throwable) = {} } records.writeStream .queryName(\"server-logs processor\") .foreach(writer) .start Internally, addBatch (the only method from the < >) takes records from the input spark-sql-dataframe.md[DataFrame] (as data ), transforms them to expected type T (of this ForeachSink ) and (now as a spark-sql-dataset.md[Dataset]) spark-sql-dataset.md#foreachPartition[processes each partition].","title":"[source, scala]"},{"location":"datasources/ForeachSink/#source-scala_1","text":"","title":"[source, scala]"},{"location":"datasources/ForeachSink/#addbatchbatchid-long-data-dataframe-unit","text":"addBatch then opens the constructor's datasources/ForeachWriter.md[ForeachWriter] (for the spark-taskscheduler-taskcontext.md#getPartitionId[current partition] and the input batch) and passes the records to process (one at a time per partition). CAUTION: FIXME Why does Spark track whether the writer failed or not? Why couldn't it finally and do close ? CAUTION: FIXME Can we have a constant for \"foreach\" for source in DataStreamWriter ?","title":"addBatch(batchId: Long, data: DataFrame): Unit"},{"location":"datasources/ForeachWriter/","text":"ForeachWriter \u00b6 ForeachWriter is the < > for a foreach writer that is a streaming format that controls streaming writes. Note ForeachWriter is set using DataStreamWriter.foreach operator. val foreachWriter = new ForeachWriter [ String ] { ... } streamingQuery . writeStream . foreach ( foreachWriter ). start === [[contract]] ForeachWriter Contract [source, scala] \u00b6 package org.apache.spark.sql abstract class ForeachWriter[T] { def open(partitionId: Long, version: Long): Boolean def process(value: T): Unit def close(errorOrNull: Throwable): Unit } .ForeachWriter Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | [[open]] open | Used when... | [[process]] process | Used when... | [[close]] close | Used when... |===","title":"ForeachWriter"},{"location":"datasources/ForeachWriter/#foreachwriter","text":"ForeachWriter is the < > for a foreach writer that is a streaming format that controls streaming writes. Note ForeachWriter is set using DataStreamWriter.foreach operator. val foreachWriter = new ForeachWriter [ String ] { ... } streamingQuery . writeStream . foreach ( foreachWriter ). start === [[contract]] ForeachWriter Contract","title":"ForeachWriter"},{"location":"datasources/ForeachWriter/#source-scala","text":"package org.apache.spark.sql abstract class ForeachWriter[T] { def open(partitionId: Long, version: Long): Boolean def process(value: T): Unit def close(errorOrNull: Throwable): Unit } .ForeachWriter Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | [[open]] open | Used when... | [[process]] process | Used when... | [[close]] close | Used when... |===","title":"[source, scala]"},{"location":"datasources/ForeachWriterProvider/","text":"== [[ForeachWriterProvider]] ForeachWriterProvider ForeachWriterProvider is...FIXME","title":"ForeachWriterProvider"},{"location":"datasources/file/","text":"File Data Source \u00b6 File Data Source comes with the following main abstractions: FileStreamSource FileStreamSink FileStreamSink uses FileStreamSinkLog for tracking valid files per micro-batch (as part of ManifestFileCommitProtocol ).","title":"File Data Source"},{"location":"datasources/file/#file-data-source","text":"File Data Source comes with the following main abstractions: FileStreamSource FileStreamSink FileStreamSink uses FileStreamSinkLog for tracking valid files per micro-batch (as part of ManifestFileCommitProtocol ).","title":"File Data Source"},{"location":"datasources/file/CompactibleFileStreamLog/","text":"CompactibleFileStreamLog \u00b6 CompactibleFileStreamLog is an extension of the HDFSMetadataLog abstraction for metadata logs that can compact logs at regular intervals . Creating Instance \u00b6 CompactibleFileStreamLog takes the following to be created: Version of the Metadata Log SparkSession Path of the Metadata Log Abstract Class CompactibleFileStreamLog is an abstract class and cannot be created directly. It is created indirectly for the concrete CompactibleFileStreamLogs . Contract \u00b6 Filtering Out Obsolete Logs \u00b6 compactLogs ( logs : Seq [ T ]): Seq [ T ] Used when storing metadata and for all files (except deleted) Important compactLogs does nothing important in the available implementations . Consider this method a noop. Default Compact Interval \u00b6 defaultCompactInterval : Int Used for the compact interval File Cleanup Delay \u00b6 fileCleanupDelayMs : Long Used for delete expired log entries isDeletingExpiredLog \u00b6 isDeletingExpiredLog : Boolean Used to store metadata Implementations \u00b6 FileStreamSinkLog FileStreamSourceLog Compaction \u00b6 compact ( batchId : Long , logs : Array [ T ]): Boolean compact finds valid metadata files for compaction (for the given compaction batchId and compact interval ) and makes sure that they are all available . compact tracks elapsed time ( loadElapsedMs ). compact filters out obsolete logs among the valid metadata files and the input logs (which actually does nothing important given the note in compactLogs ). compact stores the metadata (the filtered metadata files and the input logs ) for the input batchId . compact tracks elapsed time ( writeElapsedMs ). compact prints out the following DEBUG message (only when the total elapsed time of loadElapsedMs and writeElapsedMs are below the unconfigurable 2000 ms): Compacting took [elapsedMs] ms (load: [loadElapsedMs] ms, write: [writeElapsedMs] ms) for compact batch [batchId] In case the total epased time is above the unconfigurable 2000 ms, compact prints out the following WARN messages: Compacting took [elapsedMs] ms (load: [loadElapsedMs] ms, write: [writeElapsedMs] ms) for compact batch [batchId] Loaded [allLogs] entries (estimated [allLogs] bytes in memory), and wrote [compactedLogs] entries for compact batch [batchId] compact throws an IllegalStateException when one of the metadata files to compact is not valid (not accessible on a file system or of incorrect format): [batchIdToPath] doesn't exist when compacting batch [batchId] (compactInterval: [compactInterval]) compact is used while storing metadata for streaming batch . spark.sql.streaming.fileSink.log.cleanupDelay \u00b6 CompactibleFileStreamLog uses spark.sql.streaming.fileSink.log.cleanupDelay configuration property to delete expired log entries . compact File Suffix \u00b6 CompactibleFileStreamLog uses .compact file suffix for batchIdToPath , getBatchIdFromFileName , and the compactInterval . Storing Metadata for Streaming Batch \u00b6 add ( batchId : Long , logs : Array [ T ]): Boolean add checks whether the given batchId is compaction batch or not (alongside compact interval ). add ...FIXME add is part of the MetadataLog abstraction. Deleting Expired Log Entries \u00b6 deleteExpiredLog ( currentBatchId : Long ): Unit deleteExpiredLog ...FIXME deleteExpiredLog does nothing and simply returns when the current batch ID incremented ( currentBatchId + 1 ) is below the compact interval plus the minBatchesToRetain . Compact Interval \u00b6 compactInterval : Int compactInterval is the number of metadata log files between compactions. Lazy Value compactInterval is a Scala lazy value which means that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards. compactInterval finds compacted IDs and determines the compact interval. compactInterval requests the CheckpointFileManager for the files in the metadataPath that are batch (and possibly compacted ). compactInterval takes the compacted files only (if available), converts them to batch IDs and sorts in descending order. compactInterval starts with the default compact interval . If there are two compacted IDs, their difference is the compact interval If there is one compacted ID only, compactInterval \"derives\" the compact interval (FIXME) compactInterval asserts that the compact interval is a positive value or throws an AssertionError . compactInterval prints out the following INFO message to the logs (with the defaultCompactInterval ): Set the compact interval to [interval] [defaultCompactInterval: [defaultCompactInterval]] All Files (Except Deleted) \u00b6 allFiles (): Array [ T ] allFiles ...FIXME allFiles is used when: FileStreamSource is created MetadataLogFileIndex is created Converting Batch Id to Hadoop Path \u00b6 batchIdToPath ( batchId : Long ): Path batchIdToPath ...FIXME batchIdToPath is part of the HDFSMetadataLog abstraction. Converting Hadoop Path to Batch Id \u00b6 pathToBatchId ( path : Path ): Long pathToBatchId ...FIXME pathToBatchId is part of the HDFSMetadataLog abstraction. isBatchFile \u00b6 isBatchFile ( path : Path ): Boolean isBatchFile is true when successful to get the batchId for the given path. Otherwise is false . isBatchFile is part of the HDFSMetadataLog abstraction. Serializing Metadata (Writing Metadata in Serialized Format) \u00b6 serialize ( logData : Array [ T ], out : OutputStream ): Unit serialize writes the version header ( v and the < >) out to the given output stream (in UTF_8 ). serialize then writes the log data (serialized using Json4s (with Jackson binding) library). Entries are separated by new lines. serialize is part of the HDFSMetadataLog abstraction. Deserializing Metadata \u00b6 deserialize ( in : InputStream ): Array [ T ] deserialize ...FIXME deserialize is part of the HDFSMetadataLog abstraction. Utilities \u00b6 getBatchIdFromFileName \u00b6 getBatchIdFromFileName ( fileName : String ): Long getBatchIdFromFileName simply removes the .compact suffix from the given fileName and converts the remaining part to a number. getBatchIdFromFileName is used for pathToBatchId , isBatchFile , and delete expired log entries . getValidBatchesBeforeCompactionBatch \u00b6 getValidBatchesBeforeCompactionBatch ( compactionBatchId : Long , compactInterval : Int ): Seq [ Long ] getValidBatchesBeforeCompactionBatch ...FIXME getBatchIdFromFileName is used for compaction .","title":"CompactibleFileStreamLog"},{"location":"datasources/file/CompactibleFileStreamLog/#compactiblefilestreamlog","text":"CompactibleFileStreamLog is an extension of the HDFSMetadataLog abstraction for metadata logs that can compact logs at regular intervals .","title":"CompactibleFileStreamLog"},{"location":"datasources/file/CompactibleFileStreamLog/#creating-instance","text":"CompactibleFileStreamLog takes the following to be created: Version of the Metadata Log SparkSession Path of the Metadata Log Abstract Class CompactibleFileStreamLog is an abstract class and cannot be created directly. It is created indirectly for the concrete CompactibleFileStreamLogs .","title":"Creating Instance"},{"location":"datasources/file/CompactibleFileStreamLog/#contract","text":"","title":"Contract"},{"location":"datasources/file/CompactibleFileStreamLog/#filtering-out-obsolete-logs","text":"compactLogs ( logs : Seq [ T ]): Seq [ T ] Used when storing metadata and for all files (except deleted) Important compactLogs does nothing important in the available implementations . Consider this method a noop.","title":" Filtering Out Obsolete Logs"},{"location":"datasources/file/CompactibleFileStreamLog/#default-compact-interval","text":"defaultCompactInterval : Int Used for the compact interval","title":" Default Compact Interval"},{"location":"datasources/file/CompactibleFileStreamLog/#file-cleanup-delay","text":"fileCleanupDelayMs : Long Used for delete expired log entries","title":" File Cleanup Delay"},{"location":"datasources/file/CompactibleFileStreamLog/#isdeletingexpiredlog","text":"isDeletingExpiredLog : Boolean Used to store metadata","title":" isDeletingExpiredLog"},{"location":"datasources/file/CompactibleFileStreamLog/#implementations","text":"FileStreamSinkLog FileStreamSourceLog","title":"Implementations"},{"location":"datasources/file/CompactibleFileStreamLog/#compaction","text":"compact ( batchId : Long , logs : Array [ T ]): Boolean compact finds valid metadata files for compaction (for the given compaction batchId and compact interval ) and makes sure that they are all available . compact tracks elapsed time ( loadElapsedMs ). compact filters out obsolete logs among the valid metadata files and the input logs (which actually does nothing important given the note in compactLogs ). compact stores the metadata (the filtered metadata files and the input logs ) for the input batchId . compact tracks elapsed time ( writeElapsedMs ). compact prints out the following DEBUG message (only when the total elapsed time of loadElapsedMs and writeElapsedMs are below the unconfigurable 2000 ms): Compacting took [elapsedMs] ms (load: [loadElapsedMs] ms, write: [writeElapsedMs] ms) for compact batch [batchId] In case the total epased time is above the unconfigurable 2000 ms, compact prints out the following WARN messages: Compacting took [elapsedMs] ms (load: [loadElapsedMs] ms, write: [writeElapsedMs] ms) for compact batch [batchId] Loaded [allLogs] entries (estimated [allLogs] bytes in memory), and wrote [compactedLogs] entries for compact batch [batchId] compact throws an IllegalStateException when one of the metadata files to compact is not valid (not accessible on a file system or of incorrect format): [batchIdToPath] doesn't exist when compacting batch [batchId] (compactInterval: [compactInterval]) compact is used while storing metadata for streaming batch .","title":" Compaction"},{"location":"datasources/file/CompactibleFileStreamLog/#sparksqlstreamingfilesinklogcleanupdelay","text":"CompactibleFileStreamLog uses spark.sql.streaming.fileSink.log.cleanupDelay configuration property to delete expired log entries .","title":" spark.sql.streaming.fileSink.log.cleanupDelay"},{"location":"datasources/file/CompactibleFileStreamLog/#compact-file-suffix","text":"CompactibleFileStreamLog uses .compact file suffix for batchIdToPath , getBatchIdFromFileName , and the compactInterval .","title":" compact File Suffix"},{"location":"datasources/file/CompactibleFileStreamLog/#storing-metadata-for-streaming-batch","text":"add ( batchId : Long , logs : Array [ T ]): Boolean add checks whether the given batchId is compaction batch or not (alongside compact interval ). add ...FIXME add is part of the MetadataLog abstraction.","title":" Storing Metadata for Streaming Batch"},{"location":"datasources/file/CompactibleFileStreamLog/#deleting-expired-log-entries","text":"deleteExpiredLog ( currentBatchId : Long ): Unit deleteExpiredLog ...FIXME deleteExpiredLog does nothing and simply returns when the current batch ID incremented ( currentBatchId + 1 ) is below the compact interval plus the minBatchesToRetain .","title":" Deleting Expired Log Entries"},{"location":"datasources/file/CompactibleFileStreamLog/#compact-interval","text":"compactInterval : Int compactInterval is the number of metadata log files between compactions. Lazy Value compactInterval is a Scala lazy value which means that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards. compactInterval finds compacted IDs and determines the compact interval. compactInterval requests the CheckpointFileManager for the files in the metadataPath that are batch (and possibly compacted ). compactInterval takes the compacted files only (if available), converts them to batch IDs and sorts in descending order. compactInterval starts with the default compact interval . If there are two compacted IDs, their difference is the compact interval If there is one compacted ID only, compactInterval \"derives\" the compact interval (FIXME) compactInterval asserts that the compact interval is a positive value or throws an AssertionError . compactInterval prints out the following INFO message to the logs (with the defaultCompactInterval ): Set the compact interval to [interval] [defaultCompactInterval: [defaultCompactInterval]]","title":" Compact Interval"},{"location":"datasources/file/CompactibleFileStreamLog/#all-files-except-deleted","text":"allFiles (): Array [ T ] allFiles ...FIXME allFiles is used when: FileStreamSource is created MetadataLogFileIndex is created","title":" All Files (Except Deleted)"},{"location":"datasources/file/CompactibleFileStreamLog/#converting-batch-id-to-hadoop-path","text":"batchIdToPath ( batchId : Long ): Path batchIdToPath ...FIXME batchIdToPath is part of the HDFSMetadataLog abstraction.","title":" Converting Batch Id to Hadoop Path"},{"location":"datasources/file/CompactibleFileStreamLog/#converting-hadoop-path-to-batch-id","text":"pathToBatchId ( path : Path ): Long pathToBatchId ...FIXME pathToBatchId is part of the HDFSMetadataLog abstraction.","title":" Converting Hadoop Path to Batch Id"},{"location":"datasources/file/CompactibleFileStreamLog/#isbatchfile","text":"isBatchFile ( path : Path ): Boolean isBatchFile is true when successful to get the batchId for the given path. Otherwise is false . isBatchFile is part of the HDFSMetadataLog abstraction.","title":" isBatchFile"},{"location":"datasources/file/CompactibleFileStreamLog/#serializing-metadata-writing-metadata-in-serialized-format","text":"serialize ( logData : Array [ T ], out : OutputStream ): Unit serialize writes the version header ( v and the < >) out to the given output stream (in UTF_8 ). serialize then writes the log data (serialized using Json4s (with Jackson binding) library). Entries are separated by new lines. serialize is part of the HDFSMetadataLog abstraction.","title":" Serializing Metadata (Writing Metadata in Serialized Format)"},{"location":"datasources/file/CompactibleFileStreamLog/#deserializing-metadata","text":"deserialize ( in : InputStream ): Array [ T ] deserialize ...FIXME deserialize is part of the HDFSMetadataLog abstraction.","title":" Deserializing Metadata"},{"location":"datasources/file/CompactibleFileStreamLog/#utilities","text":"","title":"Utilities"},{"location":"datasources/file/CompactibleFileStreamLog/#getbatchidfromfilename","text":"getBatchIdFromFileName ( fileName : String ): Long getBatchIdFromFileName simply removes the .compact suffix from the given fileName and converts the remaining part to a number. getBatchIdFromFileName is used for pathToBatchId , isBatchFile , and delete expired log entries .","title":" getBatchIdFromFileName"},{"location":"datasources/file/CompactibleFileStreamLog/#getvalidbatchesbeforecompactionbatch","text":"getValidBatchesBeforeCompactionBatch ( compactionBatchId : Long , compactInterval : Int ): Seq [ Long ] getValidBatchesBeforeCompactionBatch ...FIXME getBatchIdFromFileName is used for compaction .","title":" getValidBatchesBeforeCompactionBatch"},{"location":"datasources/file/FileStreamSink/","text":"FileStreamSink \u00b6 FileStreamSink is a streaming sink that writes data out to files (in a given file format and a directory ). FileStreamSink can only be used with Append output mode. Tip Learn more in Demo: Deep Dive into FileStreamSink . Creating Instance \u00b6 FileStreamSink takes the following to be created: SparkSession Path FileFormat Names of the Partition Columns (if any) Options ( Map[String, String] ) FileStreamSink is created when DataSource is requested to create a streaming sink for FileFormat data sources. Metadata Log Directory \u00b6 FileStreamSink uses _spark_metadata directory (under the path ) as the Metadata Log Directory to store metadata indicating which files are valid and can be read (and skipping already committed batch). Metadata Log Directory is managed by FileStreamSinkLog . Hadoop Path of Metadata Log \u00b6 logPath : Path logPath is the location of the Metadata Log (as a Hadoop Path ). FileStreamSinkLog \u00b6 fileLog : FileStreamSinkLog fileLog is a FileStreamSinkLog (for the version 1 and the metadata log path ) Used for \"adding\" batch . Text Representation \u00b6 FileStreamSink uses the path for the text representation ( toString ): FileSink[path] \"Adding\" Batch of Data to Sink \u00b6 addBatch ( batchId : Long , data : DataFrame ): Unit addBatch requests the FileStreamSinkLog for the latest committed batch ID . With a newer batchId , addBatch creates a FileCommitProtocol based on spark.sql.streaming.commitProtocolClass configuration property. The Internals of Apache Spark Learn more on FileCommitProtocol in The Internals of Apache Spark . For a ManifestFileCommitProtocol , addBatch requests it to setupManifestOptions (with the FileStreamSinkLog and the given batchId ). In the end, addBatch writes out the data using FileFormatWriter.write workflow (with the FileCommitProtocol and BasicWriteJobStatsTracker ). The Internals of Spark SQL Learn more on FileFormatWriter in The Internals of Spark SQL . addBatch prints out the following INFO message to the logs when the given batchId is below the latest committed batch ID: Skipping already committed batch [batchId] addBatch is a part of the Sink abstraction. Creating BasicWriteJobStatsTracker \u00b6 basicWriteJobStatsTracker : BasicWriteJobStatsTracker basicWriteJobStatsTracker creates a BasicWriteJobStatsTracker with the basic metrics: number of written files bytes of written output number of output rows number of dynamic partitions Tip Learn more about BasicWriteJobStatsTracker in The Internals of Spark SQL online book. basicWriteJobStatsTracker is used when FileStreamSink is requested to addBatch . hasMetadata Utility \u00b6 hasMetadata ( path : Seq [ String ], hadoopConf : Configuration ): Boolean hasMetadata ...FIXME hasMetadata is used (to short-circut listing files using MetadataLogFileIndex instead of using HDFS API) when: DataSource (Spark SQL) is requested to resolve a FileFormat relation FileTable (Spark SQL) is requested for a PartitioningAwareFileIndex FileStreamSource is requested to fetchAllFiles Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.execution.streaming.FileStreamSink logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.FileStreamSink=ALL Refer to Logging .","title":"FileStreamSink"},{"location":"datasources/file/FileStreamSink/#filestreamsink","text":"FileStreamSink is a streaming sink that writes data out to files (in a given file format and a directory ). FileStreamSink can only be used with Append output mode. Tip Learn more in Demo: Deep Dive into FileStreamSink .","title":"FileStreamSink"},{"location":"datasources/file/FileStreamSink/#creating-instance","text":"FileStreamSink takes the following to be created: SparkSession Path FileFormat Names of the Partition Columns (if any) Options ( Map[String, String] ) FileStreamSink is created when DataSource is requested to create a streaming sink for FileFormat data sources.","title":"Creating Instance"},{"location":"datasources/file/FileStreamSink/#metadata-log-directory","text":"FileStreamSink uses _spark_metadata directory (under the path ) as the Metadata Log Directory to store metadata indicating which files are valid and can be read (and skipping already committed batch). Metadata Log Directory is managed by FileStreamSinkLog .","title":" Metadata Log Directory"},{"location":"datasources/file/FileStreamSink/#hadoop-path-of-metadata-log","text":"logPath : Path logPath is the location of the Metadata Log (as a Hadoop Path ).","title":" Hadoop Path of Metadata Log"},{"location":"datasources/file/FileStreamSink/#filestreamsinklog","text":"fileLog : FileStreamSinkLog fileLog is a FileStreamSinkLog (for the version 1 and the metadata log path ) Used for \"adding\" batch .","title":" FileStreamSinkLog"},{"location":"datasources/file/FileStreamSink/#text-representation","text":"FileStreamSink uses the path for the text representation ( toString ): FileSink[path]","title":" Text Representation"},{"location":"datasources/file/FileStreamSink/#adding-batch-of-data-to-sink","text":"addBatch ( batchId : Long , data : DataFrame ): Unit addBatch requests the FileStreamSinkLog for the latest committed batch ID . With a newer batchId , addBatch creates a FileCommitProtocol based on spark.sql.streaming.commitProtocolClass configuration property. The Internals of Apache Spark Learn more on FileCommitProtocol in The Internals of Apache Spark . For a ManifestFileCommitProtocol , addBatch requests it to setupManifestOptions (with the FileStreamSinkLog and the given batchId ). In the end, addBatch writes out the data using FileFormatWriter.write workflow (with the FileCommitProtocol and BasicWriteJobStatsTracker ). The Internals of Spark SQL Learn more on FileFormatWriter in The Internals of Spark SQL . addBatch prints out the following INFO message to the logs when the given batchId is below the latest committed batch ID: Skipping already committed batch [batchId] addBatch is a part of the Sink abstraction.","title":" \"Adding\" Batch of Data to Sink"},{"location":"datasources/file/FileStreamSink/#creating-basicwritejobstatstracker","text":"basicWriteJobStatsTracker : BasicWriteJobStatsTracker basicWriteJobStatsTracker creates a BasicWriteJobStatsTracker with the basic metrics: number of written files bytes of written output number of output rows number of dynamic partitions Tip Learn more about BasicWriteJobStatsTracker in The Internals of Spark SQL online book. basicWriteJobStatsTracker is used when FileStreamSink is requested to addBatch .","title":" Creating BasicWriteJobStatsTracker"},{"location":"datasources/file/FileStreamSink/#hasmetadata-utility","text":"hasMetadata ( path : Seq [ String ], hadoopConf : Configuration ): Boolean hasMetadata ...FIXME hasMetadata is used (to short-circut listing files using MetadataLogFileIndex instead of using HDFS API) when: DataSource (Spark SQL) is requested to resolve a FileFormat relation FileTable (Spark SQL) is requested for a PartitioningAwareFileIndex FileStreamSource is requested to fetchAllFiles","title":" hasMetadata Utility"},{"location":"datasources/file/FileStreamSink/#logging","text":"Enable ALL logging level for org.apache.spark.sql.execution.streaming.FileStreamSink logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.FileStreamSink=ALL Refer to Logging .","title":"Logging"},{"location":"datasources/file/FileStreamSinkLog/","text":"FileStreamSinkLog \u00b6 FileStreamSinkLog is a CompactibleFileStreamLog (of SinkFileStatus es) for FileStreamSink and MetadataLogFileIndex . FileStreamSinkLog concatenates metadata logs to a single compact file after defined compact interval. Creating Instance \u00b6 FileStreamSinkLog (like the parent CompactibleFileStreamLog ) takes the following to be created: Version of the Metadata Log SparkSession Path of the Metadata Log Configuration Properties \u00b6 spark.sql.streaming.fileSink.log.cleanupDelay \u00b6 FileStreamSinkLog uses spark.sql.streaming.fileSink.log.cleanupDelay configuration property for fileCleanupDelayMs . spark.sql.streaming.fileSink.log.compactInterval \u00b6 FileStreamSinkLog uses spark.sql.streaming.fileSink.log.compactInterval configuration property for defaultCompactInterval . spark.sql.streaming.fileSink.log.deletion \u00b6 FileStreamSinkLog uses spark.sql.streaming.fileSink.log.deletion configuration property for isDeletingExpiredLog . Compacting Logs \u00b6 compactLogs ( logs : Seq [ SinkFileStatus ]): Seq [ SinkFileStatus ] compactLogs finds delete actions in the given collection of SinkFileStatus es. If there are no deletes, compactLogs gives the SinkFileStatus es back (unmodified). Otherwise, compactLogs removes the deleted paths from the SinkFileStatus es. compactLogs is part of the CompactibleFileStreamLog abstraction. Version \u00b6 FileStreamSinkLog uses 1 for the version. Actions \u00b6 Add \u00b6 FileStreamSinkLog uses add action to create new metadata logs . Delete \u00b6 FileStreamSinkLog uses delete action to mark status files to be excluded from compaction . Important Delete action is not used in Spark Structured Streaming and will be removed in 3.1.0 .","title":"FileStreamSinkLog"},{"location":"datasources/file/FileStreamSinkLog/#filestreamsinklog","text":"FileStreamSinkLog is a CompactibleFileStreamLog (of SinkFileStatus es) for FileStreamSink and MetadataLogFileIndex . FileStreamSinkLog concatenates metadata logs to a single compact file after defined compact interval.","title":"FileStreamSinkLog"},{"location":"datasources/file/FileStreamSinkLog/#creating-instance","text":"FileStreamSinkLog (like the parent CompactibleFileStreamLog ) takes the following to be created: Version of the Metadata Log SparkSession Path of the Metadata Log","title":"Creating Instance"},{"location":"datasources/file/FileStreamSinkLog/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"datasources/file/FileStreamSinkLog/#sparksqlstreamingfilesinklogcleanupdelay","text":"FileStreamSinkLog uses spark.sql.streaming.fileSink.log.cleanupDelay configuration property for fileCleanupDelayMs .","title":" spark.sql.streaming.fileSink.log.cleanupDelay"},{"location":"datasources/file/FileStreamSinkLog/#sparksqlstreamingfilesinklogcompactinterval","text":"FileStreamSinkLog uses spark.sql.streaming.fileSink.log.compactInterval configuration property for defaultCompactInterval .","title":" spark.sql.streaming.fileSink.log.compactInterval"},{"location":"datasources/file/FileStreamSinkLog/#sparksqlstreamingfilesinklogdeletion","text":"FileStreamSinkLog uses spark.sql.streaming.fileSink.log.deletion configuration property for isDeletingExpiredLog .","title":" spark.sql.streaming.fileSink.log.deletion"},{"location":"datasources/file/FileStreamSinkLog/#compacting-logs","text":"compactLogs ( logs : Seq [ SinkFileStatus ]): Seq [ SinkFileStatus ] compactLogs finds delete actions in the given collection of SinkFileStatus es. If there are no deletes, compactLogs gives the SinkFileStatus es back (unmodified). Otherwise, compactLogs removes the deleted paths from the SinkFileStatus es. compactLogs is part of the CompactibleFileStreamLog abstraction.","title":" Compacting Logs"},{"location":"datasources/file/FileStreamSinkLog/#version","text":"FileStreamSinkLog uses 1 for the version.","title":" Version"},{"location":"datasources/file/FileStreamSinkLog/#actions","text":"","title":"Actions"},{"location":"datasources/file/FileStreamSinkLog/#add","text":"FileStreamSinkLog uses add action to create new metadata logs .","title":" Add"},{"location":"datasources/file/FileStreamSinkLog/#delete","text":"FileStreamSinkLog uses delete action to mark status files to be excluded from compaction . Important Delete action is not used in Spark Structured Streaming and will be removed in 3.1.0 .","title":" Delete"},{"location":"datasources/file/FileStreamSource/","text":"FileStreamSource \u00b6 FileStreamSource is a streaming source that reads files (in a given file format ) from a directory . FileStreamSource is used by DataSource.createSource for FileFormat . Tip Learn more in Demo: Using File Streaming Source . Creating Instance \u00b6 FileStreamSource takes the following to be created: SparkSession Path Class Name of FileFormat Schema Names of the Partition Columns (if any) Metadata Path Options ( Map[String, String] ) FileStreamSource is created when DataSource is requested to create a streaming source for FileFormat data sources. While being created, FileStreamSource prints out the following INFO message to the logs (with the maxFilesPerBatch and maxFileAgeMs options): maxFilesPerBatch = [maxFilesPerBatch], maxFileAgeMs = [maxFileAgeMs] FileStreamSource requests the FileStreamSourceLog for all files that are added to seenFiles internal registry. FileStreamSource requests the seenFiles internal registry to purge (remove aged entries). Options \u00b6 Options are case-insensitive (so cleanSource and CLEANSOURCE are equivalent). cleanSource \u00b6 How to clean up completed files. Available modes: archive delete off fileNameOnly \u00b6 Whether to check for new files on on the filename only ( true ) or the full path ( false ) Default: false When enabled, FileStreamSource prints out the following WARN message to the logs: 'fileNameOnly' is enabled. Make sure your file names are unique (e.g. using UUID), otherwise, files with the same name but under different paths will be considered the same and causes data lost. latestFirst \u00b6 Whether to scan latest files first ( true ) or not ( false ) Default: false When enabled, FileStreamSource prints out the following WARN message to the logs: 'latestFirst' is true. New files will be processed first, which may affect the watermark value. In addition, 'maxFileAge' will be ignored. maxFileAgeMs \u00b6 Maximum age of a file that can be found in this directory, before being ignored Default: 7d Uses time suffices: us , ms , s , m , min , h , d . No suffix is assumed to be in ms. maxFilesPerTrigger \u00b6 Maximum number of files per trigger (batch) sourceArchiveDir \u00b6 Archive directory to move completed files to (for cleanSource set to archive ) SupportsAdmissionControl \u00b6 FileStreamSource is a SupportsAdmissionControl and controls the rate of data ingested. FileStreamSourceCleaner \u00b6 FileStreamSource may create a FileStreamSourceCleaner based on cleanSource option. FileStreamSourceLog \u00b6 FileStreamSource uses FileStreamSourceLog (for the given metadataPath ). Latest Offset \u00b6 FileStreamSource tracks the latest offset in metadataLogCurrentOffset internal registry. Seen Files Registry \u00b6 seenFiles : SeenFilesMap seenFiles is...FIXME seenFiles is used for...FIXME Committing \u00b6 commit ( end : Offset ): Unit commit is...FIXME commit is part of the Source abstraction. getDefaultReadLimit \u00b6 getDefaultReadLimit : ReadLimit getDefaultReadLimit is...FIXME getDefaultReadLimit is part of the SupportsAdmissionControl abstraction. getOffset \u00b6 getOffset : Option [ Offset ] getOffset simply throws an UnsupportedOperationException : latestOffset(Offset, ReadLimit) should be called instead of this method getOffset is part of the Source abstraction. Generating DataFrame for Streaming Batch \u00b6 getBatch ( start : Option [ Offset ], end : Offset ): DataFrame getBatch ...FIXME FileStreamSource.getBatch asks < > for the batch. You should see the following INFO and DEBUG messages in the logs: Processing ${files.length} files from ${startId + 1}:$endId Streaming ${files.mkString(\", \")} The method to create a result batch is given at instantiation time (as dataFrameBuilder constructor parameter). getBatch is part of the Source abstraction. fetchMaxOffset \u00b6 fetchMaxOffset ( limit : ReadLimit ): FileStreamSourceOffset fetchMaxOffset ...FIXME fetchMaxOffset is used for latestOffset . fetchAllFiles \u00b6 fetchAllFiles (): Seq [( String , Long )] fetchAllFiles ...FIXME fetchAllFiles is used for fetchMaxOffset . latestOffset \u00b6 latestOffset ( startOffset : streaming . Offset , limit : ReadLimit ): streaming . Offset latestOffset ...FIXME latestOffset is part of the SparkDataStream abstraction. Stopping Streaming Source \u00b6 stop (): Unit stop ...FIXME stop is part of the SupportsAdmissionControl abstraction. allFilesUsingInMemoryFileIndex \u00b6 allFilesUsingInMemoryFileIndex (): Seq [ FileStatus ] allFilesUsingInMemoryFileIndex is...FIXME allFilesUsingInMemoryFileIndex is used for fetchAllFiles . allFilesUsingMetadataLogFileIndex \u00b6 allFilesUsingMetadataLogFileIndex (): Seq [ FileStatus ] allFilesUsingMetadataLogFileIndex is...FIXME allFilesUsingMetadataLogFileIndex is used for fetchAllFiles Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.execution.streaming.FileStreamSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.FileStreamSource=ALL Refer to Logging .","title":"FileStreamSource"},{"location":"datasources/file/FileStreamSource/#filestreamsource","text":"FileStreamSource is a streaming source that reads files (in a given file format ) from a directory . FileStreamSource is used by DataSource.createSource for FileFormat . Tip Learn more in Demo: Using File Streaming Source .","title":"FileStreamSource"},{"location":"datasources/file/FileStreamSource/#creating-instance","text":"FileStreamSource takes the following to be created: SparkSession Path Class Name of FileFormat Schema Names of the Partition Columns (if any) Metadata Path Options ( Map[String, String] ) FileStreamSource is created when DataSource is requested to create a streaming source for FileFormat data sources. While being created, FileStreamSource prints out the following INFO message to the logs (with the maxFilesPerBatch and maxFileAgeMs options): maxFilesPerBatch = [maxFilesPerBatch], maxFileAgeMs = [maxFileAgeMs] FileStreamSource requests the FileStreamSourceLog for all files that are added to seenFiles internal registry. FileStreamSource requests the seenFiles internal registry to purge (remove aged entries).","title":"Creating Instance"},{"location":"datasources/file/FileStreamSource/#options","text":"Options are case-insensitive (so cleanSource and CLEANSOURCE are equivalent).","title":" Options"},{"location":"datasources/file/FileStreamSource/#cleansource","text":"How to clean up completed files. Available modes: archive delete off","title":" cleanSource"},{"location":"datasources/file/FileStreamSource/#filenameonly","text":"Whether to check for new files on on the filename only ( true ) or the full path ( false ) Default: false When enabled, FileStreamSource prints out the following WARN message to the logs: 'fileNameOnly' is enabled. Make sure your file names are unique (e.g. using UUID), otherwise, files with the same name but under different paths will be considered the same and causes data lost.","title":" fileNameOnly"},{"location":"datasources/file/FileStreamSource/#latestfirst","text":"Whether to scan latest files first ( true ) or not ( false ) Default: false When enabled, FileStreamSource prints out the following WARN message to the logs: 'latestFirst' is true. New files will be processed first, which may affect the watermark value. In addition, 'maxFileAge' will be ignored.","title":" latestFirst"},{"location":"datasources/file/FileStreamSource/#maxfileagems","text":"Maximum age of a file that can be found in this directory, before being ignored Default: 7d Uses time suffices: us , ms , s , m , min , h , d . No suffix is assumed to be in ms.","title":" maxFileAgeMs"},{"location":"datasources/file/FileStreamSource/#maxfilespertrigger","text":"Maximum number of files per trigger (batch)","title":" maxFilesPerTrigger"},{"location":"datasources/file/FileStreamSource/#sourcearchivedir","text":"Archive directory to move completed files to (for cleanSource set to archive )","title":" sourceArchiveDir"},{"location":"datasources/file/FileStreamSource/#supportsadmissioncontrol","text":"FileStreamSource is a SupportsAdmissionControl and controls the rate of data ingested.","title":"SupportsAdmissionControl"},{"location":"datasources/file/FileStreamSource/#filestreamsourcecleaner","text":"FileStreamSource may create a FileStreamSourceCleaner based on cleanSource option.","title":" FileStreamSourceCleaner"},{"location":"datasources/file/FileStreamSource/#filestreamsourcelog","text":"FileStreamSource uses FileStreamSourceLog (for the given metadataPath ).","title":" FileStreamSourceLog"},{"location":"datasources/file/FileStreamSource/#latest-offset","text":"FileStreamSource tracks the latest offset in metadataLogCurrentOffset internal registry.","title":" Latest Offset"},{"location":"datasources/file/FileStreamSource/#seen-files-registry","text":"seenFiles : SeenFilesMap seenFiles is...FIXME seenFiles is used for...FIXME","title":" Seen Files Registry"},{"location":"datasources/file/FileStreamSource/#committing","text":"commit ( end : Offset ): Unit commit is...FIXME commit is part of the Source abstraction.","title":" Committing"},{"location":"datasources/file/FileStreamSource/#getdefaultreadlimit","text":"getDefaultReadLimit : ReadLimit getDefaultReadLimit is...FIXME getDefaultReadLimit is part of the SupportsAdmissionControl abstraction.","title":" getDefaultReadLimit"},{"location":"datasources/file/FileStreamSource/#getoffset","text":"getOffset : Option [ Offset ] getOffset simply throws an UnsupportedOperationException : latestOffset(Offset, ReadLimit) should be called instead of this method getOffset is part of the Source abstraction.","title":" getOffset"},{"location":"datasources/file/FileStreamSource/#generating-dataframe-for-streaming-batch","text":"getBatch ( start : Option [ Offset ], end : Offset ): DataFrame getBatch ...FIXME FileStreamSource.getBatch asks < > for the batch. You should see the following INFO and DEBUG messages in the logs: Processing ${files.length} files from ${startId + 1}:$endId Streaming ${files.mkString(\", \")} The method to create a result batch is given at instantiation time (as dataFrameBuilder constructor parameter). getBatch is part of the Source abstraction.","title":" Generating DataFrame for Streaming Batch"},{"location":"datasources/file/FileStreamSource/#fetchmaxoffset","text":"fetchMaxOffset ( limit : ReadLimit ): FileStreamSourceOffset fetchMaxOffset ...FIXME fetchMaxOffset is used for latestOffset .","title":" fetchMaxOffset"},{"location":"datasources/file/FileStreamSource/#fetchallfiles","text":"fetchAllFiles (): Seq [( String , Long )] fetchAllFiles ...FIXME fetchAllFiles is used for fetchMaxOffset .","title":" fetchAllFiles"},{"location":"datasources/file/FileStreamSource/#latestoffset","text":"latestOffset ( startOffset : streaming . Offset , limit : ReadLimit ): streaming . Offset latestOffset ...FIXME latestOffset is part of the SparkDataStream abstraction.","title":" latestOffset"},{"location":"datasources/file/FileStreamSource/#stopping-streaming-source","text":"stop (): Unit stop ...FIXME stop is part of the SupportsAdmissionControl abstraction.","title":" Stopping Streaming Source"},{"location":"datasources/file/FileStreamSource/#allfilesusinginmemoryfileindex","text":"allFilesUsingInMemoryFileIndex (): Seq [ FileStatus ] allFilesUsingInMemoryFileIndex is...FIXME allFilesUsingInMemoryFileIndex is used for fetchAllFiles .","title":" allFilesUsingInMemoryFileIndex"},{"location":"datasources/file/FileStreamSource/#allfilesusingmetadatalogfileindex","text":"allFilesUsingMetadataLogFileIndex (): Seq [ FileStatus ] allFilesUsingMetadataLogFileIndex is...FIXME allFilesUsingMetadataLogFileIndex is used for fetchAllFiles","title":" allFilesUsingMetadataLogFileIndex"},{"location":"datasources/file/FileStreamSource/#logging","text":"Enable ALL logging level for org.apache.spark.sql.execution.streaming.FileStreamSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.FileStreamSource=ALL Refer to Logging .","title":"Logging"},{"location":"datasources/file/FileStreamSourceCleaner/","text":"FileStreamSourceCleaner \u00b6 FileStreamSourceCleaner is...FIXME","title":"FileStreamSourceCleaner"},{"location":"datasources/file/FileStreamSourceCleaner/#filestreamsourcecleaner","text":"FileStreamSourceCleaner is...FIXME","title":"FileStreamSourceCleaner"},{"location":"datasources/file/FileStreamSourceLog/","text":"FileStreamSourceLog \u00b6 FileStreamSourceLog is a concrete CompactibleFileStreamLog (of FileEntry metadata) of FileStreamSource . FileStreamSourceLog uses a fixed-size < > of metadata of compaction batches. [[defaultCompactInterval]] FileStreamSourceLog uses < > configuration property (default: 10 ) for the default compaction interval . [[fileCleanupDelayMs]] FileStreamSourceLog uses < > configuration property (default: 10 minutes) for the fileCleanupDelayMs . [[isDeletingExpiredLog]] FileStreamSourceLog uses < > configuration property (default: true ) for the isDeletingExpiredLog . Creating Instance \u00b6 FileStreamSourceLog (like the parent CompactibleFileStreamLog ) takes the following to be created: [[metadataLogVersion]] Metadata version [[sparkSession]] SparkSession [[path]] Path of the metadata log directory === [[add]] Storing (Adding) Metadata of Streaming Batch -- add Method [source, scala] \u00b6 add( batchId: Long, logs: Array[FileEntry]): Boolean add requests the parent CompactibleFileStreamLog to store metadata (possibly compacting logs if the batch is compaction ). If so (and this is a compation batch), add adds the batch and the logs to < > internal registry (and possibly removing the eldest entry if the size is above the < >). add is part of the MetadataLog abstraction. === [[get]][[get-range]] get Method [source, scala] \u00b6 get( startId: Option[Long], endId: Option[Long]): Array[(Long, Array[FileEntry])] get ...FIXME get is part of the MetadataLog abstraction. Internal Properties \u00b6 [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | cacheSize a| [[cacheSize]] Size of the < > that is exactly the compact interval Used when the < > is requested to add a new entry in < > and < > a compaction batch | fileEntryCache a| [[fileEntryCache]] Metadata of a streaming batch ( FileEntry ) per batch ID ( LinkedHashMap[Long, Array[FileEntry]] ) of size configured using the < > New entry added for a compaction batch when < > Used when < > (for a compaction batch ) |===","title":"FileStreamSourceLog"},{"location":"datasources/file/FileStreamSourceLog/#filestreamsourcelog","text":"FileStreamSourceLog is a concrete CompactibleFileStreamLog (of FileEntry metadata) of FileStreamSource . FileStreamSourceLog uses a fixed-size < > of metadata of compaction batches. [[defaultCompactInterval]] FileStreamSourceLog uses < > configuration property (default: 10 ) for the default compaction interval . [[fileCleanupDelayMs]] FileStreamSourceLog uses < > configuration property (default: 10 minutes) for the fileCleanupDelayMs . [[isDeletingExpiredLog]] FileStreamSourceLog uses < > configuration property (default: true ) for the isDeletingExpiredLog .","title":"FileStreamSourceLog"},{"location":"datasources/file/FileStreamSourceLog/#creating-instance","text":"FileStreamSourceLog (like the parent CompactibleFileStreamLog ) takes the following to be created: [[metadataLogVersion]] Metadata version [[sparkSession]] SparkSession [[path]] Path of the metadata log directory === [[add]] Storing (Adding) Metadata of Streaming Batch -- add Method","title":"Creating Instance"},{"location":"datasources/file/FileStreamSourceLog/#source-scala","text":"add( batchId: Long, logs: Array[FileEntry]): Boolean add requests the parent CompactibleFileStreamLog to store metadata (possibly compacting logs if the batch is compaction ). If so (and this is a compation batch), add adds the batch and the logs to < > internal registry (and possibly removing the eldest entry if the size is above the < >). add is part of the MetadataLog abstraction. === [[get]][[get-range]] get Method","title":"[source, scala]"},{"location":"datasources/file/FileStreamSourceLog/#source-scala_1","text":"get( startId: Option[Long], endId: Option[Long]): Array[(Long, Array[FileEntry])] get ...FIXME get is part of the MetadataLog abstraction.","title":"[source, scala]"},{"location":"datasources/file/FileStreamSourceLog/#internal-properties","text":"[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | cacheSize a| [[cacheSize]] Size of the < > that is exactly the compact interval Used when the < > is requested to add a new entry in < > and < > a compaction batch | fileEntryCache a| [[fileEntryCache]] Metadata of a streaming batch ( FileEntry ) per batch ID ( LinkedHashMap[Long, Array[FileEntry]] ) of size configured using the < > New entry added for a compaction batch when < > Used when < > (for a compaction batch ) |===","title":"Internal Properties"},{"location":"datasources/file/ManifestFileCommitProtocol/","text":"ManifestFileCommitProtocol \u00b6 ManifestFileCommitProtocol is a FileCommitProtocol for tracking valid files (per micro-batch) in FileStreamSinkLog . The Internals of Apache Spark Learn more on FileCommitProtocol in The Internals of Apache Spark . Creating Instance \u00b6 ManifestFileCommitProtocol takes the following to be created: Job ID ( unused ) Path to write the output to ManifestFileCommitProtocol is created when FileStreamSink is requested to add a batch (which is every micro-batch). FileStreamSinkLog \u00b6 ManifestFileCommitProtocol is given a FileStreamSinkLog when setting up the manifest options for a micro-batch (right after having been created ). FileStreamSinkLog is used to add the SinkFileStatus es (in a micro-batch) when ManifestFileCommitProtocol is requested to commit a write job . Setting Up Manifest Options \u00b6 setupManifestOptions ( fileLog : FileStreamSinkLog , batchId : Long ): Unit setupManifestOptions assigns the FileStreamSinkLog and batchId . setupManifestOptions is used when FileStreamSink is requested to add a batch (right after having been created ). Setting Up Job \u00b6 setupJob ( jobContext : JobContext ): Unit setupJob initializes pendingCommitFiles to be an empty collection of Hadoop Path s. setupJob is part of the FileCommitProtocol ( Spark SQL ) abstraction. Setting Up Task \u00b6 setupTask ( taskContext : TaskAttemptContext ): Unit setupTask initializes addedFiles to be an empty collection of file locations (?) setupTask is part of the FileCommitProtocol ( Spark SQL ) abstraction. newTaskTempFile \u00b6 newTaskTempFile ( taskContext : TaskAttemptContext , dir : Option [ String ], ext : String ): String newTaskTempFile creates a temporary file part-[split]-[uuid][ext] in the optional dir location or the path and adds it to addedFiles internal registry. newTaskTempFile is part of the FileCommitProtocol ( Spark SQL ) abstraction. Task Committed \u00b6 onTaskCommit ( taskCommit : TaskCommitMessage ): Unit onTaskCommit adds the SinkFileStatus s from the given taskCommits to pendingCommitFiles internal registry. onTaskCommit is part of the FileCommitProtocol ( Spark SQL ) abstraction. Committing Task \u00b6 commitTask ( taskContext : TaskAttemptContext ): TaskCommitMessage commitTask creates a TaskCommitMessage with SinkFileStatus es for every added file . commitTask is part of the FileCommitProtocol ( Spark SQL ) abstraction. Aborting Task \u00b6 abortTask ( taskContext : TaskAttemptContext ): Unit abortTask deletes added files . abortTask is part of the FileCommitProtocol ( Spark SQL ) abstraction. Committing Job \u00b6 commitJob ( jobContext : JobContext , taskCommits : Seq [ TaskCommitMessage ]): Unit commitJob takes SinkFileStatus s from the given taskCommits . In the end, commitJob requests the FileStreamSinkLog to add the SinkFileStatus s as the batchId . If successful ( true ), commitJob prints out the following INFO message to the logs: Committed batch [batchId] Otherwise, when failed ( false ), commitJob throws an IllegalStateException : Race while writing batch [batchId] commitJob is part of the FileCommitProtocol ( Spark SQL ) abstraction. Aborting Job \u00b6 abortJob ( jobContext : JobContext ): Unit abortJob simply tries to remove all pendingCommitFiles if there are any and clear it up. abortJob is part of the FileCommitProtocol ( Spark SQL ) abstraction.","title":"ManifestFileCommitProtocol"},{"location":"datasources/file/ManifestFileCommitProtocol/#manifestfilecommitprotocol","text":"ManifestFileCommitProtocol is a FileCommitProtocol for tracking valid files (per micro-batch) in FileStreamSinkLog . The Internals of Apache Spark Learn more on FileCommitProtocol in The Internals of Apache Spark .","title":"ManifestFileCommitProtocol"},{"location":"datasources/file/ManifestFileCommitProtocol/#creating-instance","text":"ManifestFileCommitProtocol takes the following to be created: Job ID ( unused ) Path to write the output to ManifestFileCommitProtocol is created when FileStreamSink is requested to add a batch (which is every micro-batch).","title":"Creating Instance"},{"location":"datasources/file/ManifestFileCommitProtocol/#filestreamsinklog","text":"ManifestFileCommitProtocol is given a FileStreamSinkLog when setting up the manifest options for a micro-batch (right after having been created ). FileStreamSinkLog is used to add the SinkFileStatus es (in a micro-batch) when ManifestFileCommitProtocol is requested to commit a write job .","title":" FileStreamSinkLog"},{"location":"datasources/file/ManifestFileCommitProtocol/#setting-up-manifest-options","text":"setupManifestOptions ( fileLog : FileStreamSinkLog , batchId : Long ): Unit setupManifestOptions assigns the FileStreamSinkLog and batchId . setupManifestOptions is used when FileStreamSink is requested to add a batch (right after having been created ).","title":" Setting Up Manifest Options"},{"location":"datasources/file/ManifestFileCommitProtocol/#setting-up-job","text":"setupJob ( jobContext : JobContext ): Unit setupJob initializes pendingCommitFiles to be an empty collection of Hadoop Path s. setupJob is part of the FileCommitProtocol ( Spark SQL ) abstraction.","title":" Setting Up Job"},{"location":"datasources/file/ManifestFileCommitProtocol/#setting-up-task","text":"setupTask ( taskContext : TaskAttemptContext ): Unit setupTask initializes addedFiles to be an empty collection of file locations (?) setupTask is part of the FileCommitProtocol ( Spark SQL ) abstraction.","title":" Setting Up Task"},{"location":"datasources/file/ManifestFileCommitProtocol/#newtasktempfile","text":"newTaskTempFile ( taskContext : TaskAttemptContext , dir : Option [ String ], ext : String ): String newTaskTempFile creates a temporary file part-[split]-[uuid][ext] in the optional dir location or the path and adds it to addedFiles internal registry. newTaskTempFile is part of the FileCommitProtocol ( Spark SQL ) abstraction.","title":" newTaskTempFile"},{"location":"datasources/file/ManifestFileCommitProtocol/#task-committed","text":"onTaskCommit ( taskCommit : TaskCommitMessage ): Unit onTaskCommit adds the SinkFileStatus s from the given taskCommits to pendingCommitFiles internal registry. onTaskCommit is part of the FileCommitProtocol ( Spark SQL ) abstraction.","title":" Task Committed"},{"location":"datasources/file/ManifestFileCommitProtocol/#committing-task","text":"commitTask ( taskContext : TaskAttemptContext ): TaskCommitMessage commitTask creates a TaskCommitMessage with SinkFileStatus es for every added file . commitTask is part of the FileCommitProtocol ( Spark SQL ) abstraction.","title":" Committing Task"},{"location":"datasources/file/ManifestFileCommitProtocol/#aborting-task","text":"abortTask ( taskContext : TaskAttemptContext ): Unit abortTask deletes added files . abortTask is part of the FileCommitProtocol ( Spark SQL ) abstraction.","title":" Aborting Task"},{"location":"datasources/file/ManifestFileCommitProtocol/#committing-job","text":"commitJob ( jobContext : JobContext , taskCommits : Seq [ TaskCommitMessage ]): Unit commitJob takes SinkFileStatus s from the given taskCommits . In the end, commitJob requests the FileStreamSinkLog to add the SinkFileStatus s as the batchId . If successful ( true ), commitJob prints out the following INFO message to the logs: Committed batch [batchId] Otherwise, when failed ( false ), commitJob throws an IllegalStateException : Race while writing batch [batchId] commitJob is part of the FileCommitProtocol ( Spark SQL ) abstraction.","title":" Committing Job"},{"location":"datasources/file/ManifestFileCommitProtocol/#aborting-job","text":"abortJob ( jobContext : JobContext ): Unit abortJob simply tries to remove all pendingCommitFiles if there are any and clear it up. abortJob is part of the FileCommitProtocol ( Spark SQL ) abstraction.","title":" Aborting Job"},{"location":"datasources/file/MetadataLogFileIndex/","text":"MetadataLogFileIndex \u00b6 MetadataLogFileIndex is a PartitioningAwareFileIndex of metadata log files (generated by FileStreamSink ). Tip Learn more about PartitioningAwareFileIndex in The Internals of Spark SQL online book. Creating Instance \u00b6 MetadataLogFileIndex takes the following to be created: SparkSession Hadoop Path Parameters ( Map[String, String] ) User-Defined Schema ( Option[StructType] ) MetadataLogFileIndex is created when: DataSource is requested to resolveRelation (for FileFormat streaming data sources) FileTable is requested for a PartitioningAwareFileIndex (for FileFormat streaming data sources) FileStreamSource is requested to allFilesUsingMetadataLogFileIndex While being created, MetadataLogFileIndex prints out the following INFO message to the logs (with the metadataDirectory ): Reading streaming file log from [metadataDirectory] Metadata Directory \u00b6 metadataDirectory : Path metadataDirectory is a Hadoop Path of Metadata Directory . metadataDirectory is a _spark_metadata directory in the given path . metadataDirectory is used to create a FileStreamSinkLog . FileStreamSinkLog \u00b6 metadataLog : FileStreamSinkLog metadataLog is a FileStreamSinkLog with the Metadata Directory . metadataLog is used for metadata log files . Metadata Log Files \u00b6 allFilesFromLog : Array [ FileStatus ] allFilesFromLog requests the FileStreamSinkLog for all files that are in turn requested for their representation as a Hadoop FileStatus . allFilesFromLog is used for leafFiles and leafDirToChildrenFiles . Leaf Files \u00b6 leafFiles : mutable . LinkedHashMap [ Path , FileStatus ] leafFiles ...FIXME leafFiles is part of the PartitioningAwareFileIndex abstraction ( Spark SQL ). leafDirToChildrenFiles \u00b6 leafDirToChildrenFiles : Map [ Path , Array [ FileStatus ]] leafDirToChildrenFiles ...FIXME leafDirToChildrenFiles is part of the PartitioningAwareFileIndex abstraction ( Spark SQL ). Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.execution.streaming.MetadataLogFileIndex logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.MetadataLogFileIndex=ALL Refer to Logging .","title":"MetadataLogFileIndex"},{"location":"datasources/file/MetadataLogFileIndex/#metadatalogfileindex","text":"MetadataLogFileIndex is a PartitioningAwareFileIndex of metadata log files (generated by FileStreamSink ). Tip Learn more about PartitioningAwareFileIndex in The Internals of Spark SQL online book.","title":"MetadataLogFileIndex"},{"location":"datasources/file/MetadataLogFileIndex/#creating-instance","text":"MetadataLogFileIndex takes the following to be created: SparkSession Hadoop Path Parameters ( Map[String, String] ) User-Defined Schema ( Option[StructType] ) MetadataLogFileIndex is created when: DataSource is requested to resolveRelation (for FileFormat streaming data sources) FileTable is requested for a PartitioningAwareFileIndex (for FileFormat streaming data sources) FileStreamSource is requested to allFilesUsingMetadataLogFileIndex While being created, MetadataLogFileIndex prints out the following INFO message to the logs (with the metadataDirectory ): Reading streaming file log from [metadataDirectory]","title":"Creating Instance"},{"location":"datasources/file/MetadataLogFileIndex/#metadata-directory","text":"metadataDirectory : Path metadataDirectory is a Hadoop Path of Metadata Directory . metadataDirectory is a _spark_metadata directory in the given path . metadataDirectory is used to create a FileStreamSinkLog .","title":" Metadata Directory"},{"location":"datasources/file/MetadataLogFileIndex/#filestreamsinklog","text":"metadataLog : FileStreamSinkLog metadataLog is a FileStreamSinkLog with the Metadata Directory . metadataLog is used for metadata log files .","title":" FileStreamSinkLog"},{"location":"datasources/file/MetadataLogFileIndex/#metadata-log-files","text":"allFilesFromLog : Array [ FileStatus ] allFilesFromLog requests the FileStreamSinkLog for all files that are in turn requested for their representation as a Hadoop FileStatus . allFilesFromLog is used for leafFiles and leafDirToChildrenFiles .","title":" Metadata Log Files"},{"location":"datasources/file/MetadataLogFileIndex/#leaf-files","text":"leafFiles : mutable . LinkedHashMap [ Path , FileStatus ] leafFiles ...FIXME leafFiles is part of the PartitioningAwareFileIndex abstraction ( Spark SQL ).","title":" Leaf Files"},{"location":"datasources/file/MetadataLogFileIndex/#leafdirtochildrenfiles","text":"leafDirToChildrenFiles : Map [ Path , Array [ FileStatus ]] leafDirToChildrenFiles ...FIXME leafDirToChildrenFiles is part of the PartitioningAwareFileIndex abstraction ( Spark SQL ).","title":" leafDirToChildrenFiles"},{"location":"datasources/file/MetadataLogFileIndex/#logging","text":"Enable ALL logging level for org.apache.spark.sql.execution.streaming.MetadataLogFileIndex logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.MetadataLogFileIndex=ALL Refer to Logging .","title":"Logging"},{"location":"datasources/file/SinkFileStatus/","text":"SinkFileStatus \u00b6 [[creating-instance]] SinkFileStatus represents the status of files of FileStreamSink (and the type of the metadata of FileStreamSinkLog ): [[path]] Path [[size]] Size [[isDir]] isDir flag [[modificationTime]] Modification time [[blockReplication]] Block replication [[blockSize]] Block size [[action]] Action (either add or delete ) === [[toFileStatus]] toFileStatus Method [source, scala] \u00b6 toFileStatus: FileStatus \u00b6 toFileStatus simply creates a new Hadoop FileStatus . NOTE: toFileStatus is used exclusively when MetadataLogFileIndex is created . === [[apply]] Creating SinkFileStatus Instance [source, scala] \u00b6 apply(f: FileStatus): SinkFileStatus \u00b6 apply simply creates a new < > (with add action). apply is used when ManifestFileCommitProtocol is requested to commitTask .","title":"SinkFileStatus"},{"location":"datasources/file/SinkFileStatus/#sinkfilestatus","text":"[[creating-instance]] SinkFileStatus represents the status of files of FileStreamSink (and the type of the metadata of FileStreamSinkLog ): [[path]] Path [[size]] Size [[isDir]] isDir flag [[modificationTime]] Modification time [[blockReplication]] Block replication [[blockSize]] Block size [[action]] Action (either add or delete ) === [[toFileStatus]] toFileStatus Method","title":"SinkFileStatus"},{"location":"datasources/file/SinkFileStatus/#source-scala","text":"","title":"[source, scala]"},{"location":"datasources/file/SinkFileStatus/#tofilestatus-filestatus","text":"toFileStatus simply creates a new Hadoop FileStatus . NOTE: toFileStatus is used exclusively when MetadataLogFileIndex is created . === [[apply]] Creating SinkFileStatus Instance","title":"toFileStatus: FileStatus"},{"location":"datasources/file/SinkFileStatus/#source-scala_1","text":"","title":"[source, scala]"},{"location":"datasources/file/SinkFileStatus/#applyf-filestatus-sinkfilestatus","text":"apply simply creates a new < > (with add action). apply is used when ManifestFileCommitProtocol is requested to commitTask .","title":"apply(f: FileStatus): SinkFileStatus"},{"location":"datasources/kafka/","text":"Kafka Data Source \u00b6 Kafka Data Source is the streaming data source for Apache Kafka in Spark Structured Streaming. Kafka Data Source provides a streaming source and a streaming sink for micro-batch and continuous stream processing. spark-sql-kafka-0-10 External Module \u00b6 Kafka Data Source is part of the spark-sql-kafka-0-10 external module that is distributed with the official distribution of Apache Spark, but it is not on the CLASSPATH by default. You should define spark-sql-kafka-0-10 module as part of the build definition in your Spark project, e.g. as a libraryDependency in build.sbt for sbt: libraryDependencies += \"org.apache.spark\" %% \"spark-sql-kafka-0-10\" % \"3.1.1\" For Spark environments like spark-submit (and \"derivatives\" like spark-shell ), you should use --packages command-line option: ./bin/spark-shell \\ --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 Note Replace the version of spark-sql-kafka-0-10 module (e.g. 3.1.1 above) with one of the available versions found at The Central Repository's Search that matches your version of Apache Spark. Streaming Source \u00b6 Kafka data source can load streaming data (reading records) from one or more Kafka topics. val records = spark . readStream . format ( \"kafka\" ) . option ( \"subscribePattern\" , \"\"\"topic-\\d{2}\"\"\" ) // topics with two digits at the end . option ( \"kafka.bootstrap.servers\" , \":9092\" ) . load Kafka data source supports many options for reading. Kafka data source for reading is available through KafkaSourceProvider that is a MicroBatchStream and ContinuousReadSupport for micro-batch and continuous stream processing, respectively. Predefined (Fixed) Schema \u00b6 Kafka Data Source uses a predefined (fixed) schema. Name Type key BinaryType value BinaryType topic StringType partition IntegerType offset LongType timestamp TimestampType timestampType IntegerType scala> records.printSchema root |-- key: binary (nullable = true) |-- value: binary (nullable = true) |-- topic: string (nullable = true) |-- partition: integer (nullable = true) |-- offset: long (nullable = true) |-- timestamp: timestamp (nullable = true) |-- timestampType: integer (nullable = true) Internally, the fixed schema is defined as part of the DataSourceReader abstraction through MicroBatchReader and ContinuousReader extension contracts for micro-batch and continuous stream processing, respectively. Column.cast Operator \u00b6 Use Column.cast operator to cast BinaryType to a StringType (for key and value columns). scala> :type records org.apache.spark.sql.DataFrame val values = records .select($\"value\" cast \"string\") // deserializing values scala> values.printSchema root |-- value: string (nullable = true) Streaming Sink \u00b6 Kafka data source can write streaming data (the result of executing a streaming query) to one or more Kafka topics. val sq = records . writeStream . format ( \"kafka\" ) . option ( \"kafka.bootstrap.servers\" , \":9092\" ) . option ( \"topic\" , \"kafka2console-output\" ) . option ( \"checkpointLocation\" , \"checkpointLocation-kafka2console\" ) . start Internally, the kafka data source format for writing is available through KafkaSourceProvider . Micro-Batch Stream Processing \u00b6 Kafka Data Source supports Micro-Batch Stream Processing using KafkaMicroBatchReader . import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"kafka\") .option(\"subscribepattern\", \"kafka2console.*\") .option(\"kafka.bootstrap.servers\", \":9092\") .load .withColumn(\"value\", $\"value\" cast \"string\") // deserializing values .writeStream .format(\"console\") .option(\"truncate\", false) // format-specific option .option(\"checkpointLocation\", \"checkpointLocation-kafka2console\") // generic query option .trigger(Trigger.ProcessingTime(30.seconds)) .queryName(\"kafka2console-microbatch\") .start // In the end, stop the streaming query sq.stop Kafka Data Source can assign a single task per Kafka partition (using KafkaOffsetRangeCalculator in Micro-Batch Stream Processing ). Kafka Data Source can reuse a Kafka consumer (using KafkaMicroBatchReader in Micro-Batch Stream Processing ). Continuous Stream Processing \u00b6 Kafka Data Source supports Continuous Stream Processing using KafkaContinuousReader . import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"kafka\") .option(\"subscribepattern\", \"kafka2console.*\") .option(\"kafka.bootstrap.servers\", \":9092\") .load .withColumn(\"value\", $\"value\" cast \"string\") // convert bytes to string for display purposes .writeStream .format(\"console\") .option(\"truncate\", false) // format-specific option .option(\"checkpointLocation\", \"checkpointLocation-kafka2console\") // generic query option .queryName(\"kafka2console-continuous\") .trigger(Trigger.Continuous(10.seconds)) .start // In the end, stop the streaming query sq.stop Configuration Options \u00b6 Options with kafka. prefix (e.g. kafka.bootstrap.servers ) are considered configuration properties for the Kafka consumers used on the driver and executors . assign \u00b6 Topic subscription strategy that accepts a JSON with topic names and partitions, e.g. {\"topicA\":[0,1],\"topicB\":[0,1]} Exactly one topic subscription strategy is allowed (that KafkaSourceProvider validates before creating KafkaSource ). failOnDataLoss \u00b6 Default: true Used when KafkaSourceProvider is requested for failOnDataLoss configuration property kafka.bootstrap.servers \u00b6 (required) bootstrap.servers configuration property of the Kafka consumers used on the driver and executors Default: (empty) kafkaConsumer.pollTimeoutMs \u00b6 The time (in milliseconds) spent waiting in Consumer.poll if data is not available in the buffer. Default: spark.network.timeout or 120s maxOffsetsPerTrigger \u00b6 Number of records to fetch per trigger (to limit the number of records to fetch). Default: (undefined) Unless defined, KafkaSource requests KafkaOffsetReader for the latest offsets . minPartitions \u00b6 Minimum number of partitions per executor (given Kafka partitions) Default: (undefined) Must be undefined (default) or greater than 0 When undefined (default) or smaller than the number of TopicPartitions with records to consume from, KafkaMicroBatchReader uses KafkaOffsetRangeCalculator to find the preferred executor for every TopicPartition (and the available executors ). startingOffsets \u00b6 Starting offsets Default: latest Possible values: latest earliest JSON with topics, partitions and their starting offsets, e.g. { \"topicA\" :{ \"part\" : o ffset , \"p1\" : -1 }, \"topicB\" :{ \"0\" : -2 }} Tip Use Scala's tripple quotes for the JSON for topics, partitions and offsets. option( \"startingOffsets\", \"\"\"{\"topic1\":{\"0\":5,\"4\":-1},\"topic2\":{\"0\":-2}}\"\"\") subscribe \u00b6 Topic subscription strategy that accepts topic names as a comma-separated string, e.g. topic1,topic2,topic3 Exactly one topic subscription strategy is allowed (that KafkaSourceProvider validates before creating KafkaSource ). subscribepattern \u00b6 Topic subscription strategy that uses Java's java.util.regex.Pattern for the topic subscription regex pattern of topics to subscribe to, e.g. topic\\d Tip Use Scala's tripple quotes for the regular expression for topic subscription regex pattern. option(\"subscribepattern\", \"\"\"topic\\d\"\"\") Exactly one topic subscription strategy is allowed (that KafkaSourceProvider validates before creating KafkaSource ). topic \u00b6 Optional topic name to use for writing a streaming query Default: (empty) Unless defined, Kafka data source uses the topic names as defined in the topic field in the incoming data. Logical Query Plan for Reading \u00b6 When DataStreamReader is requested to load a dataset with kafka data source format, it creates a DataFrame with a StreamingRelationV2 leaf logical operator. scala> records.explain(extended = true) == Parsed Logical Plan == StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1a366d0, kafka, Map(maxOffsetsPerTrigger -> 1, startingOffsets -> latest, subscribepattern -> topic\\d, kafka.bootstrap.servers -> :9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@39b3de87,kafka,List(),None,List(),None,Map(maxOffsetsPerTrigger -> 1, startingOffsets -> latest, subscribepattern -> topic\\d, kafka.bootstrap.servers -> :9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] ... Logical Query Plan for Writing \u00b6 When DataStreamWriter is requested to start a streaming query with kafka data source format for writing, it requests the StreamingQueryManager to create a streaming query that in turn creates (a StreamingQueryWrapper with) a ContinuousExecution or a MicroBatchExecution for continuous and micro-batch stream processing, respectively. scala> sq.explain(extended = true) == Parsed Logical Plan == WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@bf98b73 +- Project [key#28 AS key#7, value#29 AS value#8, topic#30 AS topic#9, partition#31 AS partition#10, offset#32L AS offset#11L, timestamp#33 AS timestamp#12, timestampType#34 AS timestampType#13] +- Streaming RelationV2 kafka[key#28, value#29, topic#30, partition#31, offset#32L, timestamp#33, timestampType#34] (Options: [subscribepattern=kafka2console.*,kafka.bootstrap.servers=:9092]) Demo: Streaming Aggregation with Kafka Data Source \u00b6 Check out Demo: Streaming Aggregation with Kafka Data Source . Use the following to publish events to Kafka. // 1st streaming batch $ cat /tmp/1 1,1,1 15,2,1 $ kafkacat -P -b localhost:9092 -t topic1 -l /tmp/1 // Alternatively (and slower due to JVM bootup) $ cat /tmp/1 | ./bin/kafka-console-producer.sh --topic topic1 --broker-list localhost:9092","title":"Kafka Data Source"},{"location":"datasources/kafka/#kafka-data-source","text":"Kafka Data Source is the streaming data source for Apache Kafka in Spark Structured Streaming. Kafka Data Source provides a streaming source and a streaming sink for micro-batch and continuous stream processing.","title":"Kafka Data Source"},{"location":"datasources/kafka/#spark-sql-kafka-0-10-external-module","text":"Kafka Data Source is part of the spark-sql-kafka-0-10 external module that is distributed with the official distribution of Apache Spark, but it is not on the CLASSPATH by default. You should define spark-sql-kafka-0-10 module as part of the build definition in your Spark project, e.g. as a libraryDependency in build.sbt for sbt: libraryDependencies += \"org.apache.spark\" %% \"spark-sql-kafka-0-10\" % \"3.1.1\" For Spark environments like spark-submit (and \"derivatives\" like spark-shell ), you should use --packages command-line option: ./bin/spark-shell \\ --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 Note Replace the version of spark-sql-kafka-0-10 module (e.g. 3.1.1 above) with one of the available versions found at The Central Repository's Search that matches your version of Apache Spark.","title":" spark-sql-kafka-0-10 External Module"},{"location":"datasources/kafka/#streaming-source","text":"Kafka data source can load streaming data (reading records) from one or more Kafka topics. val records = spark . readStream . format ( \"kafka\" ) . option ( \"subscribePattern\" , \"\"\"topic-\\d{2}\"\"\" ) // topics with two digits at the end . option ( \"kafka.bootstrap.servers\" , \":9092\" ) . load Kafka data source supports many options for reading. Kafka data source for reading is available through KafkaSourceProvider that is a MicroBatchStream and ContinuousReadSupport for micro-batch and continuous stream processing, respectively.","title":"Streaming Source"},{"location":"datasources/kafka/#predefined-fixed-schema","text":"Kafka Data Source uses a predefined (fixed) schema. Name Type key BinaryType value BinaryType topic StringType partition IntegerType offset LongType timestamp TimestampType timestampType IntegerType scala> records.printSchema root |-- key: binary (nullable = true) |-- value: binary (nullable = true) |-- topic: string (nullable = true) |-- partition: integer (nullable = true) |-- offset: long (nullable = true) |-- timestamp: timestamp (nullable = true) |-- timestampType: integer (nullable = true) Internally, the fixed schema is defined as part of the DataSourceReader abstraction through MicroBatchReader and ContinuousReader extension contracts for micro-batch and continuous stream processing, respectively.","title":" Predefined (Fixed) Schema"},{"location":"datasources/kafka/#columncast-operator","text":"Use Column.cast operator to cast BinaryType to a StringType (for key and value columns). scala> :type records org.apache.spark.sql.DataFrame val values = records .select($\"value\" cast \"string\") // deserializing values scala> values.printSchema root |-- value: string (nullable = true)","title":"Column.cast Operator"},{"location":"datasources/kafka/#streaming-sink","text":"Kafka data source can write streaming data (the result of executing a streaming query) to one or more Kafka topics. val sq = records . writeStream . format ( \"kafka\" ) . option ( \"kafka.bootstrap.servers\" , \":9092\" ) . option ( \"topic\" , \"kafka2console-output\" ) . option ( \"checkpointLocation\" , \"checkpointLocation-kafka2console\" ) . start Internally, the kafka data source format for writing is available through KafkaSourceProvider .","title":"Streaming Sink"},{"location":"datasources/kafka/#micro-batch-stream-processing","text":"Kafka Data Source supports Micro-Batch Stream Processing using KafkaMicroBatchReader . import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"kafka\") .option(\"subscribepattern\", \"kafka2console.*\") .option(\"kafka.bootstrap.servers\", \":9092\") .load .withColumn(\"value\", $\"value\" cast \"string\") // deserializing values .writeStream .format(\"console\") .option(\"truncate\", false) // format-specific option .option(\"checkpointLocation\", \"checkpointLocation-kafka2console\") // generic query option .trigger(Trigger.ProcessingTime(30.seconds)) .queryName(\"kafka2console-microbatch\") .start // In the end, stop the streaming query sq.stop Kafka Data Source can assign a single task per Kafka partition (using KafkaOffsetRangeCalculator in Micro-Batch Stream Processing ). Kafka Data Source can reuse a Kafka consumer (using KafkaMicroBatchReader in Micro-Batch Stream Processing ).","title":"Micro-Batch Stream Processing"},{"location":"datasources/kafka/#continuous-stream-processing","text":"Kafka Data Source supports Continuous Stream Processing using KafkaContinuousReader . import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"kafka\") .option(\"subscribepattern\", \"kafka2console.*\") .option(\"kafka.bootstrap.servers\", \":9092\") .load .withColumn(\"value\", $\"value\" cast \"string\") // convert bytes to string for display purposes .writeStream .format(\"console\") .option(\"truncate\", false) // format-specific option .option(\"checkpointLocation\", \"checkpointLocation-kafka2console\") // generic query option .queryName(\"kafka2console-continuous\") .trigger(Trigger.Continuous(10.seconds)) .start // In the end, stop the streaming query sq.stop","title":"Continuous Stream Processing"},{"location":"datasources/kafka/#configuration-options","text":"Options with kafka. prefix (e.g. kafka.bootstrap.servers ) are considered configuration properties for the Kafka consumers used on the driver and executors .","title":" Configuration Options"},{"location":"datasources/kafka/#assign","text":"Topic subscription strategy that accepts a JSON with topic names and partitions, e.g. {\"topicA\":[0,1],\"topicB\":[0,1]} Exactly one topic subscription strategy is allowed (that KafkaSourceProvider validates before creating KafkaSource ).","title":" assign"},{"location":"datasources/kafka/#failondataloss","text":"Default: true Used when KafkaSourceProvider is requested for failOnDataLoss configuration property","title":" failOnDataLoss"},{"location":"datasources/kafka/#kafkabootstrapservers","text":"(required) bootstrap.servers configuration property of the Kafka consumers used on the driver and executors Default: (empty)","title":" kafka.bootstrap.servers"},{"location":"datasources/kafka/#kafkaconsumerpolltimeoutms","text":"The time (in milliseconds) spent waiting in Consumer.poll if data is not available in the buffer. Default: spark.network.timeout or 120s","title":" kafkaConsumer.pollTimeoutMs"},{"location":"datasources/kafka/#maxoffsetspertrigger","text":"Number of records to fetch per trigger (to limit the number of records to fetch). Default: (undefined) Unless defined, KafkaSource requests KafkaOffsetReader for the latest offsets .","title":" maxOffsetsPerTrigger"},{"location":"datasources/kafka/#minpartitions","text":"Minimum number of partitions per executor (given Kafka partitions) Default: (undefined) Must be undefined (default) or greater than 0 When undefined (default) or smaller than the number of TopicPartitions with records to consume from, KafkaMicroBatchReader uses KafkaOffsetRangeCalculator to find the preferred executor for every TopicPartition (and the available executors ).","title":" minPartitions"},{"location":"datasources/kafka/#startingoffsets","text":"Starting offsets Default: latest Possible values: latest earliest JSON with topics, partitions and their starting offsets, e.g. { \"topicA\" :{ \"part\" : o ffset , \"p1\" : -1 }, \"topicB\" :{ \"0\" : -2 }} Tip Use Scala's tripple quotes for the JSON for topics, partitions and offsets. option( \"startingOffsets\", \"\"\"{\"topic1\":{\"0\":5,\"4\":-1},\"topic2\":{\"0\":-2}}\"\"\")","title":" startingOffsets"},{"location":"datasources/kafka/#subscribe","text":"Topic subscription strategy that accepts topic names as a comma-separated string, e.g. topic1,topic2,topic3 Exactly one topic subscription strategy is allowed (that KafkaSourceProvider validates before creating KafkaSource ).","title":" subscribe"},{"location":"datasources/kafka/#subscribepattern","text":"Topic subscription strategy that uses Java's java.util.regex.Pattern for the topic subscription regex pattern of topics to subscribe to, e.g. topic\\d Tip Use Scala's tripple quotes for the regular expression for topic subscription regex pattern. option(\"subscribepattern\", \"\"\"topic\\d\"\"\") Exactly one topic subscription strategy is allowed (that KafkaSourceProvider validates before creating KafkaSource ).","title":" subscribepattern"},{"location":"datasources/kafka/#topic","text":"Optional topic name to use for writing a streaming query Default: (empty) Unless defined, Kafka data source uses the topic names as defined in the topic field in the incoming data.","title":" topic"},{"location":"datasources/kafka/#logical-query-plan-for-reading","text":"When DataStreamReader is requested to load a dataset with kafka data source format, it creates a DataFrame with a StreamingRelationV2 leaf logical operator. scala> records.explain(extended = true) == Parsed Logical Plan == StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1a366d0, kafka, Map(maxOffsetsPerTrigger -> 1, startingOffsets -> latest, subscribepattern -> topic\\d, kafka.bootstrap.servers -> :9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@39b3de87,kafka,List(),None,List(),None,Map(maxOffsetsPerTrigger -> 1, startingOffsets -> latest, subscribepattern -> topic\\d, kafka.bootstrap.servers -> :9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] ...","title":"Logical Query Plan for Reading"},{"location":"datasources/kafka/#logical-query-plan-for-writing","text":"When DataStreamWriter is requested to start a streaming query with kafka data source format for writing, it requests the StreamingQueryManager to create a streaming query that in turn creates (a StreamingQueryWrapper with) a ContinuousExecution or a MicroBatchExecution for continuous and micro-batch stream processing, respectively. scala> sq.explain(extended = true) == Parsed Logical Plan == WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@bf98b73 +- Project [key#28 AS key#7, value#29 AS value#8, topic#30 AS topic#9, partition#31 AS partition#10, offset#32L AS offset#11L, timestamp#33 AS timestamp#12, timestampType#34 AS timestampType#13] +- Streaming RelationV2 kafka[key#28, value#29, topic#30, partition#31, offset#32L, timestamp#33, timestampType#34] (Options: [subscribepattern=kafka2console.*,kafka.bootstrap.servers=:9092])","title":"Logical Query Plan for Writing"},{"location":"datasources/kafka/#demo-streaming-aggregation-with-kafka-data-source","text":"Check out Demo: Streaming Aggregation with Kafka Data Source . Use the following to publish events to Kafka. // 1st streaming batch $ cat /tmp/1 1,1,1 15,2,1 $ kafkacat -P -b localhost:9092 -t topic1 -l /tmp/1 // Alternatively (and slower due to JVM bootup) $ cat /tmp/1 | ./bin/kafka-console-producer.sh --topic topic1 --broker-list localhost:9092","title":" Demo: Streaming Aggregation with Kafka Data Source"},{"location":"datasources/kafka/CachedKafkaConsumer/","text":"== [[CachedKafkaConsumer]] CachedKafkaConsumer CAUTION: FIXME === [[poll]] poll Internal Method CAUTION: FIXME === [[fetchData]] fetchData Internal Method CAUTION: FIXME","title":"CachedKafkaConsumer"},{"location":"datasources/kafka/ConsumerStrategy/","text":"== [[ConsumerStrategy]] ConsumerStrategy Contract for KafkaConsumer Providers ConsumerStrategy is the < > for components that can < > using the given Kafka parameters. [[contract]] [[createConsumer]] [source, scala] createConsumer(kafkaParams: java.util.Map[String, Object]): Consumer[Array[Byte], Array[Byte]] \u00b6 [[available-consumerstrategies]] .Available ConsumerStrategies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ConsumerStrategy | createConsumer | [[AssignStrategy]] AssignStrategy | Uses ++ http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#assign(java.util.Collection)++[KafkaConsumer.assign(Collection partitions)] | [[SubscribeStrategy]] SubscribeStrategy | Uses ++ http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe(java.util.Collection)++[KafkaConsumer.subscribe(Collection topics)] | [[SubscribePatternStrategy]] SubscribePatternStrategy a| Uses ++ http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe(java.util.regex.Pattern,%20org.apache.kafka.clients.consumer.ConsumerRebalanceListener)++[KafkaConsumer.subscribe(Pattern pattern, ConsumerRebalanceListener listener)] with NoOpConsumerRebalanceListener . TIP: Refer to http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[java.util.regex.Pattern ] for the format of supported topic subscription regex patterns. |===","title":"ConsumerStrategy"},{"location":"datasources/kafka/ConsumerStrategy/#createconsumerkafkaparams-javautilmapstring-object-consumerarraybyte-arraybyte","text":"[[available-consumerstrategies]] .Available ConsumerStrategies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ConsumerStrategy | createConsumer | [[AssignStrategy]] AssignStrategy | Uses ++ http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#assign(java.util.Collection)++[KafkaConsumer.assign(Collection partitions)] | [[SubscribeStrategy]] SubscribeStrategy | Uses ++ http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe(java.util.Collection)++[KafkaConsumer.subscribe(Collection topics)] | [[SubscribePatternStrategy]] SubscribePatternStrategy a| Uses ++ http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe(java.util.regex.Pattern,%20org.apache.kafka.clients.consumer.ConsumerRebalanceListener)++[KafkaConsumer.subscribe(Pattern pattern, ConsumerRebalanceListener listener)] with NoOpConsumerRebalanceListener . TIP: Refer to http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[java.util.regex.Pattern ] for the format of supported topic subscription regex patterns. |===","title":"createConsumer(kafkaParams: java.util.Map[String, Object]): Consumer[Array[Byte], Array[Byte]]"},{"location":"datasources/kafka/KafkaBatch/","text":"KafkaBatch \u00b6 KafkaBatch is...FIXME","title":"KafkaBatch"},{"location":"datasources/kafka/KafkaBatch/#kafkabatch","text":"KafkaBatch is...FIXME","title":"KafkaBatch"},{"location":"datasources/kafka/KafkaContinuousInputPartition/","text":"== [[KafkaContinuousInputPartition]] KafkaContinuousInputPartition KafkaContinuousInputPartition is...FIXME","title":"KafkaContinuousInputPartition"},{"location":"datasources/kafka/KafkaContinuousReader/","text":"KafkaContinuousReader \u00b6 KafkaContinuousReader is a ContinuousReader for Kafka Data Source in Continuous Stream Processing . KafkaContinuousReader is < > exclusively when KafkaSourceProvider is requested to create a ContinuousReader . [[pollTimeoutMs]] [[kafkaConsumer.pollTimeoutMs]] KafkaContinuousReader uses kafkaConsumer.pollTimeoutMs configuration parameter (default: 512 ) for KafkaContinuousInputPartitions when requested to < >. [[logging]] [TIP] ==== Enable INFO or WARN logging levels for org.apache.spark.sql.kafka010.KafkaContinuousReader to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaContinuousReader=INFO Refer to spark-sql-streaming-spark-logging.md[Logging]. \u00b6 Creating Instance \u00b6 KafkaContinuousReader takes the following to be created: [[offsetReader]] KafkaOffsetReader [[kafkaParams]] Kafka parameters (as java.util.Map[String, Object] ) [[sourceOptions]] Source options (as Map[String, String] ) [[metadataPath]] Metadata path [[initialOffsets]] Initial offsets [[failOnDataLoss]] failOnDataLoss flag === [[planInputPartitions]] Plan Input Partitions -- planInputPartitions Method [source, scala] \u00b6 planInputPartitions(): java.util.List[InputPartition[InternalRow]] \u00b6 NOTE: planInputPartitions is part of the DataSourceReader contract in Spark SQL for the number of InputPartitions to use as RDD partitions (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). planInputPartitions ...FIXME === [[setStartOffset]] setStartOffset Method [source, java] \u00b6 setStartOffset( start: Optional[Offset]): Unit setStartOffset is part of the ContinuousReader abstraction. setStartOffset ...FIXME === [[deserializeOffset]] deserializeOffset Method [source, java] \u00b6 deserializeOffset( json: String): Offset deserializeOffset is part of the ContinuousReader abstraction. deserializeOffset ...FIXME === [[mergeOffsets]] mergeOffsets Method [source, java] \u00b6 mergeOffsets( offsets: Array[PartitionOffset]): Offset mergeOffsets is part of the ContinuousReader abstraction. mergeOffsets ...FIXME","title":"KafkaContinuousReader"},{"location":"datasources/kafka/KafkaContinuousReader/#kafkacontinuousreader","text":"KafkaContinuousReader is a ContinuousReader for Kafka Data Source in Continuous Stream Processing . KafkaContinuousReader is < > exclusively when KafkaSourceProvider is requested to create a ContinuousReader . [[pollTimeoutMs]] [[kafkaConsumer.pollTimeoutMs]] KafkaContinuousReader uses kafkaConsumer.pollTimeoutMs configuration parameter (default: 512 ) for KafkaContinuousInputPartitions when requested to < >. [[logging]] [TIP] ==== Enable INFO or WARN logging levels for org.apache.spark.sql.kafka010.KafkaContinuousReader to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaContinuousReader=INFO","title":"KafkaContinuousReader"},{"location":"datasources/kafka/KafkaContinuousReader/#refer-to-spark-sql-streaming-spark-loggingmdlogging","text":"","title":"Refer to spark-sql-streaming-spark-logging.md[Logging]."},{"location":"datasources/kafka/KafkaContinuousReader/#creating-instance","text":"KafkaContinuousReader takes the following to be created: [[offsetReader]] KafkaOffsetReader [[kafkaParams]] Kafka parameters (as java.util.Map[String, Object] ) [[sourceOptions]] Source options (as Map[String, String] ) [[metadataPath]] Metadata path [[initialOffsets]] Initial offsets [[failOnDataLoss]] failOnDataLoss flag === [[planInputPartitions]] Plan Input Partitions -- planInputPartitions Method","title":"Creating Instance"},{"location":"datasources/kafka/KafkaContinuousReader/#source-scala","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaContinuousReader/#planinputpartitions-javautillistinputpartitioninternalrow","text":"NOTE: planInputPartitions is part of the DataSourceReader contract in Spark SQL for the number of InputPartitions to use as RDD partitions (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). planInputPartitions ...FIXME === [[setStartOffset]] setStartOffset Method","title":"planInputPartitions(): java.util.List[InputPartition[InternalRow]]"},{"location":"datasources/kafka/KafkaContinuousReader/#source-java","text":"setStartOffset( start: Optional[Offset]): Unit setStartOffset is part of the ContinuousReader abstraction. setStartOffset ...FIXME === [[deserializeOffset]] deserializeOffset Method","title":"[source, java]"},{"location":"datasources/kafka/KafkaContinuousReader/#source-java_1","text":"deserializeOffset( json: String): Offset deserializeOffset is part of the ContinuousReader abstraction. deserializeOffset ...FIXME === [[mergeOffsets]] mergeOffsets Method","title":"[source, java]"},{"location":"datasources/kafka/KafkaContinuousReader/#source-java_2","text":"mergeOffsets( offsets: Array[PartitionOffset]): Offset mergeOffsets is part of the ContinuousReader abstraction. mergeOffsets ...FIXME","title":"[source, java]"},{"location":"datasources/kafka/KafkaDataConsumer/","text":"== [[KafkaDataConsumer]] KafkaDataConsumer KafkaDataConsumer is the < > of < > that use < > that can be < >. [[contract]] .KafkaDataConsumer Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | internalConsumer a| [[internalConsumer]] [source, scala] \u00b6 internalConsumer: InternalKafkaConsumer \u00b6 Used when...FIXME | release a| [[release]] [source, scala] \u00b6 release(): Unit \u00b6 Used when...FIXME |=== [[implementations]] .KafkaDataConsumers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | KafkaDataConsumer | Description | CachedKafkaDataConsumer | [[CachedKafkaDataConsumer]] | NonCachedKafkaDataConsumer | [[NonCachedKafkaDataConsumer]] |=== === [[acquire]] Acquiring Cached KafkaDataConsumer for Partition -- acquire Object Method [source, scala] \u00b6 acquire( topicPartition: TopicPartition, kafkaParams: ju.Map[String, Object], useCache: Boolean ): KafkaDataConsumer acquire ...FIXME NOTE: acquire is used when...FIXME === [[get]] Getting Kafka Record -- get Method [source, scala] \u00b6 get( offset: Long, untilOffset: Long, pollTimeoutMs: Long, failOnDataLoss: Boolean ): ConsumerRecord[Array[Byte], Array[Byte]] get ...FIXME NOTE: get is used when...FIXME","title":"KafkaDataConsumer"},{"location":"datasources/kafka/KafkaDataConsumer/#source-scala","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaDataConsumer/#internalconsumer-internalkafkaconsumer","text":"Used when...FIXME | release a| [[release]]","title":"internalConsumer: InternalKafkaConsumer"},{"location":"datasources/kafka/KafkaDataConsumer/#source-scala_1","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaDataConsumer/#release-unit","text":"Used when...FIXME |=== [[implementations]] .KafkaDataConsumers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | KafkaDataConsumer | Description | CachedKafkaDataConsumer | [[CachedKafkaDataConsumer]] | NonCachedKafkaDataConsumer | [[NonCachedKafkaDataConsumer]] |=== === [[acquire]] Acquiring Cached KafkaDataConsumer for Partition -- acquire Object Method","title":"release(): Unit"},{"location":"datasources/kafka/KafkaDataConsumer/#source-scala_2","text":"acquire( topicPartition: TopicPartition, kafkaParams: ju.Map[String, Object], useCache: Boolean ): KafkaDataConsumer acquire ...FIXME NOTE: acquire is used when...FIXME === [[get]] Getting Kafka Record -- get Method","title":"[source, scala]"},{"location":"datasources/kafka/KafkaDataConsumer/#source-scala_3","text":"get( offset: Long, untilOffset: Long, pollTimeoutMs: Long, failOnDataLoss: Boolean ): ConsumerRecord[Array[Byte], Array[Byte]] get ...FIXME NOTE: get is used when...FIXME","title":"[source, scala]"},{"location":"datasources/kafka/KafkaMicroBatchInputPartition/","text":"KafkaMicroBatchInputPartition \u00b6 KafkaMicroBatchInputPartition is an InputPartition (of InternalRows ) that is used (< >) exclusively when KafkaMicroBatchReader is requested for input partitions (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). [[creating-instance]] KafkaMicroBatchInputPartition takes the following to be created: [[offsetRange]] KafkaOffsetRange [[executorKafkaParams]] Kafka parameters used for Kafka clients on executors ( Map[String, Object] ) [[pollTimeoutMs]] Poll timeout (in ms) [[failOnDataLoss]] failOnDataLoss flag [[reuseKafkaConsumer]] reuseKafkaConsumer flag [[createPartitionReader]] KafkaMicroBatchInputPartition creates a KafkaMicroBatchInputPartitionReader when requested for a InputPartitionReader[InternalRow] (as a part of the InputPartition contract). [[preferredLocations]] KafkaMicroBatchInputPartition simply requests the given < > for the optional preferredLoc when requested for preferredLocations (as a part of the InputPartition contract).","title":"KafkaMicroBatchInputPartition"},{"location":"datasources/kafka/KafkaMicroBatchInputPartition/#kafkamicrobatchinputpartition","text":"KafkaMicroBatchInputPartition is an InputPartition (of InternalRows ) that is used (< >) exclusively when KafkaMicroBatchReader is requested for input partitions (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). [[creating-instance]] KafkaMicroBatchInputPartition takes the following to be created: [[offsetRange]] KafkaOffsetRange [[executorKafkaParams]] Kafka parameters used for Kafka clients on executors ( Map[String, Object] ) [[pollTimeoutMs]] Poll timeout (in ms) [[failOnDataLoss]] failOnDataLoss flag [[reuseKafkaConsumer]] reuseKafkaConsumer flag [[createPartitionReader]] KafkaMicroBatchInputPartition creates a KafkaMicroBatchInputPartitionReader when requested for a InputPartitionReader[InternalRow] (as a part of the InputPartition contract). [[preferredLocations]] KafkaMicroBatchInputPartition simply requests the given < > for the optional preferredLoc when requested for preferredLocations (as a part of the InputPartition contract).","title":"KafkaMicroBatchInputPartition"},{"location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/","text":"KafkaMicroBatchInputPartitionReader \u00b6 KafkaMicroBatchInputPartitionReader is an InputPartitionReader (of InternalRows ) that is < > exclusively when KafkaMicroBatchInputPartition is requested for one (as a part of the InputPartition contract). Creating Instance \u00b6 KafkaMicroBatchInputPartitionReader takes the following to be created: [[offsetRange]] KafkaOffsetRange [[executorKafkaParams]] Kafka parameters used for Kafka clients on executors ( Map[String, Object] ) [[pollTimeoutMs]] Poll timeout (in ms) [[failOnDataLoss]] failOnDataLoss flag [[reuseKafkaConsumer]] reuseKafkaConsumer flag NOTE: All the input arguments to create a KafkaMicroBatchInputPartitionReader are exactly the input arguments used to create a KafkaMicroBatchInputPartition . KafkaMicroBatchInputPartitionReader initializes the < >. === [[next]] next Method [source, scala] \u00b6 next(): Boolean \u00b6 NOTE: next is part of the InputPartitionReader contract to proceed to next record if available ( true ). next checks whether the < > should < > or < > (i.e. < > is smaller than the untilOffset of the < >). ==== [[next-poll]] next Method -- KafkaDataConsumer Polls Records If so, next requests the < > to get ( poll ) records in the range of < > and the untilOffset (of the < >) with the given < > and < >. With a new record, next requests the < > to convert ( toUnsafeRow ) the record to be the < >. next sets the < > as the offset of the record incremented. next returns true . With no new record, next simply returns false . ==== [[next-no-poll]] next Method -- No Polling If the < > is equal or larger than the untilOffset (of the < >), next simply returns false . === [[close]] Closing (Releasing KafkaDataConsumer) -- close Method [source, scala] \u00b6 close(): Unit \u00b6 NOTE: close is part of the Java Closeable contract to release resources. close simply requests the < > to release . === [[resolveRange]] resolveRange Internal Method [source, scala] \u00b6 resolveRange( range: KafkaOffsetRange): KafkaOffsetRange resolveRange ...FIXME NOTE: resolveRange is used exclusively when KafkaMicroBatchInputPartitionReader is < > (and initializes the < > internal property). === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | consumer a| [[consumer]] KafkaDataConsumer for the partition (per < >) Used in < >, < >, and < > | converter a| [[converter]] KafkaRecordToUnsafeRowConverter | nextOffset a| [[nextOffset]] Next offset | nextRow a| [[nextRow]] Next UnsafeRow | rangeToRead a| [[rangeToRead]] KafkaOffsetRange |===","title":"KafkaMicroBatchInputPartitionReader"},{"location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/#kafkamicrobatchinputpartitionreader","text":"KafkaMicroBatchInputPartitionReader is an InputPartitionReader (of InternalRows ) that is < > exclusively when KafkaMicroBatchInputPartition is requested for one (as a part of the InputPartition contract).","title":"KafkaMicroBatchInputPartitionReader"},{"location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/#creating-instance","text":"KafkaMicroBatchInputPartitionReader takes the following to be created: [[offsetRange]] KafkaOffsetRange [[executorKafkaParams]] Kafka parameters used for Kafka clients on executors ( Map[String, Object] ) [[pollTimeoutMs]] Poll timeout (in ms) [[failOnDataLoss]] failOnDataLoss flag [[reuseKafkaConsumer]] reuseKafkaConsumer flag NOTE: All the input arguments to create a KafkaMicroBatchInputPartitionReader are exactly the input arguments used to create a KafkaMicroBatchInputPartition . KafkaMicroBatchInputPartitionReader initializes the < >. === [[next]] next Method","title":"Creating Instance"},{"location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/#source-scala","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/#next-boolean","text":"NOTE: next is part of the InputPartitionReader contract to proceed to next record if available ( true ). next checks whether the < > should < > or < > (i.e. < > is smaller than the untilOffset of the < >). ==== [[next-poll]] next Method -- KafkaDataConsumer Polls Records If so, next requests the < > to get ( poll ) records in the range of < > and the untilOffset (of the < >) with the given < > and < >. With a new record, next requests the < > to convert ( toUnsafeRow ) the record to be the < >. next sets the < > as the offset of the record incremented. next returns true . With no new record, next simply returns false . ==== [[next-no-poll]] next Method -- No Polling If the < > is equal or larger than the untilOffset (of the < >), next simply returns false . === [[close]] Closing (Releasing KafkaDataConsumer) -- close Method","title":"next(): Boolean"},{"location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/#source-scala_1","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/#close-unit","text":"NOTE: close is part of the Java Closeable contract to release resources. close simply requests the < > to release . === [[resolveRange]] resolveRange Internal Method","title":"close(): Unit"},{"location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/#source-scala_2","text":"resolveRange( range: KafkaOffsetRange): KafkaOffsetRange resolveRange ...FIXME NOTE: resolveRange is used exclusively when KafkaMicroBatchInputPartitionReader is < > (and initializes the < > internal property). === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | consumer a| [[consumer]] KafkaDataConsumer for the partition (per < >) Used in < >, < >, and < > | converter a| [[converter]] KafkaRecordToUnsafeRowConverter | nextOffset a| [[nextOffset]] Next offset | nextRow a| [[nextRow]] Next UnsafeRow | rangeToRead a| [[rangeToRead]] KafkaOffsetRange |===","title":"[source, scala]"},{"location":"datasources/kafka/KafkaMicroBatchReader/","text":"KafkaMicroBatchReader \u00b6 KafkaMicroBatchReader is the MicroBatchReader for kafka data source for Micro-Batch Stream Processing . KafkaMicroBatchReader is created when KafkaSourceProvider is requested to create a MicroBatchReader . [[pollTimeoutMs]] KafkaMicroBatchReader uses the DataSourceOptions to access the kafkaConsumer.pollTimeoutMs option (default: spark.network.timeout or 120s ). [[maxOffsetsPerTrigger]] KafkaMicroBatchReader uses the DataSourceOptions to access the maxOffsetsPerTrigger option (default: (undefined) ). KafkaMicroBatchReader uses the Kafka properties for executors to create KafkaMicroBatchInputPartitions when requested to planInputPartitions . Creating Instance \u00b6 KafkaMicroBatchReader takes the following to be created: [[kafkaOffsetReader]] KafkaOffsetReader [[executorKafkaParams]] Kafka properties for executors ( Map[String, Object] ) [[options]] DataSourceOptions [[metadataPath]] Metadata Path [[startingOffsets]] Desired starting KafkaOffsetRangeLimit [[failOnDataLoss]] failOnDataLoss option KafkaMicroBatchReader initializes the < >. === [[readSchema]] readSchema Method [source, scala] \u00b6 readSchema(): StructType \u00b6 NOTE: readSchema is part of the DataSourceReader contract to...FIXME. readSchema simply returns the predefined fixed schema . === [[planInputPartitions]] Plan Input Partitions -- planInputPartitions Method [source, scala] \u00b6 planInputPartitions(): java.util.List[InputPartition[InternalRow]] \u00b6 NOTE: planInputPartitions is part of the DataSourceReader contract in Spark SQL for the number of InputPartitions to use as RDD partitions (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). planInputPartitions first finds the new partitions ( TopicPartitions that are in the < > but not in the < >) and requests the < > to fetch their earliest offsets . planInputPartitions prints out the following INFO message to the logs: Partitions added: [newPartitionInitialOffsets] planInputPartitions then prints out the following DEBUG message to the logs: TopicPartitions: [comma-separated list of TopicPartitions] planInputPartitions requests the < > for < > (given the < > and the newly-calculated newPartitionInitialOffsets as the fromOffsets , the < > as the untilOffsets , and the < >). In the end, planInputPartitions creates a KafkaMicroBatchInputPartition for every offset range (with the < >, the < >, the < > flag and whether to reuse a Kafka consumer among Spark tasks). planInputPartitions < > when...FIXME === [[getSortedExecutorList]] Available Executors in Spark Cluster (Sorted By Host and Executor ID in Descending Order) -- getSortedExecutorList Internal Method [source, scala] \u00b6 getSortedExecutorList(): Array[String] \u00b6 getSortedExecutorList requests the BlockManager to request the BlockManagerMaster to get the peers (the other nodes in a Spark cluster), creates a ExecutorCacheTaskLocation for every pair of host and executor ID, and in the end, sort it in descending order. NOTE: getSortedExecutorList is used exclusively when KafkaMicroBatchReader is requested to < > (and calculates offset ranges). === [[getOrCreateInitialPartitionOffsets]] getOrCreateInitialPartitionOffsets Internal Method [source, scala] \u00b6 getOrCreateInitialPartitionOffsets(): PartitionOffsetMap \u00b6 getOrCreateInitialPartitionOffsets ...FIXME NOTE: getOrCreateInitialPartitionOffsets is used exclusively for the < > internal registry. === [[getStartOffset]] getStartOffset Method [source, scala] \u00b6 getStartOffset: Offset \u00b6 getStartOffset is part of the MicroBatchReader abstraction. getStartOffset ...FIXME === [[getEndOffset]] getEndOffset Method [source, scala] \u00b6 getEndOffset: Offset \u00b6 getEndOffset is part of the MicroBatchReader abstraction. getEndOffset ...FIXME === [[deserializeOffset]] deserializeOffset Method [source, scala] \u00b6 deserializeOffset(json: String): Offset \u00b6 deserializeOffset is part of the MicroBatchReader abstraction. deserializeOffset ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | endPartitionOffsets a| [[endPartitionOffsets]] Ending offsets for the assigned partitions ( Map[TopicPartition, Long] ) Used when...FIXME | initialPartitionOffsets a| [[initialPartitionOffsets]] [source, scala] \u00b6 initialPartitionOffsets: Map[TopicPartition, Long] \u00b6 | rangeCalculator a| [[rangeCalculator]] KafkaOffsetRangeCalculator (for the given < >) Used exclusively when KafkaMicroBatchReader is requested to < > (to calculate offset ranges) | startPartitionOffsets a| [[startPartitionOffsets]] Starting offsets for the assigned partitions ( Map[TopicPartition, Long] ) Used when...FIXME |=== Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.kafka010.KafkaMicroBatchReader logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaMicroBatchReader=ALL Refer to Logging .","title":"KafkaMicroBatchReader"},{"location":"datasources/kafka/KafkaMicroBatchReader/#kafkamicrobatchreader","text":"KafkaMicroBatchReader is the MicroBatchReader for kafka data source for Micro-Batch Stream Processing . KafkaMicroBatchReader is created when KafkaSourceProvider is requested to create a MicroBatchReader . [[pollTimeoutMs]] KafkaMicroBatchReader uses the DataSourceOptions to access the kafkaConsumer.pollTimeoutMs option (default: spark.network.timeout or 120s ). [[maxOffsetsPerTrigger]] KafkaMicroBatchReader uses the DataSourceOptions to access the maxOffsetsPerTrigger option (default: (undefined) ). KafkaMicroBatchReader uses the Kafka properties for executors to create KafkaMicroBatchInputPartitions when requested to planInputPartitions .","title":"KafkaMicroBatchReader"},{"location":"datasources/kafka/KafkaMicroBatchReader/#creating-instance","text":"KafkaMicroBatchReader takes the following to be created: [[kafkaOffsetReader]] KafkaOffsetReader [[executorKafkaParams]] Kafka properties for executors ( Map[String, Object] ) [[options]] DataSourceOptions [[metadataPath]] Metadata Path [[startingOffsets]] Desired starting KafkaOffsetRangeLimit [[failOnDataLoss]] failOnDataLoss option KafkaMicroBatchReader initializes the < >. === [[readSchema]] readSchema Method","title":"Creating Instance"},{"location":"datasources/kafka/KafkaMicroBatchReader/#source-scala","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaMicroBatchReader/#readschema-structtype","text":"NOTE: readSchema is part of the DataSourceReader contract to...FIXME. readSchema simply returns the predefined fixed schema . === [[planInputPartitions]] Plan Input Partitions -- planInputPartitions Method","title":"readSchema(): StructType"},{"location":"datasources/kafka/KafkaMicroBatchReader/#source-scala_1","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaMicroBatchReader/#planinputpartitions-javautillistinputpartitioninternalrow","text":"NOTE: planInputPartitions is part of the DataSourceReader contract in Spark SQL for the number of InputPartitions to use as RDD partitions (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). planInputPartitions first finds the new partitions ( TopicPartitions that are in the < > but not in the < >) and requests the < > to fetch their earliest offsets . planInputPartitions prints out the following INFO message to the logs: Partitions added: [newPartitionInitialOffsets] planInputPartitions then prints out the following DEBUG message to the logs: TopicPartitions: [comma-separated list of TopicPartitions] planInputPartitions requests the < > for < > (given the < > and the newly-calculated newPartitionInitialOffsets as the fromOffsets , the < > as the untilOffsets , and the < >). In the end, planInputPartitions creates a KafkaMicroBatchInputPartition for every offset range (with the < >, the < >, the < > flag and whether to reuse a Kafka consumer among Spark tasks). planInputPartitions < > when...FIXME === [[getSortedExecutorList]] Available Executors in Spark Cluster (Sorted By Host and Executor ID in Descending Order) -- getSortedExecutorList Internal Method","title":"planInputPartitions(): java.util.List[InputPartition[InternalRow]]"},{"location":"datasources/kafka/KafkaMicroBatchReader/#source-scala_2","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaMicroBatchReader/#getsortedexecutorlist-arraystring","text":"getSortedExecutorList requests the BlockManager to request the BlockManagerMaster to get the peers (the other nodes in a Spark cluster), creates a ExecutorCacheTaskLocation for every pair of host and executor ID, and in the end, sort it in descending order. NOTE: getSortedExecutorList is used exclusively when KafkaMicroBatchReader is requested to < > (and calculates offset ranges). === [[getOrCreateInitialPartitionOffsets]] getOrCreateInitialPartitionOffsets Internal Method","title":"getSortedExecutorList(): Array[String]"},{"location":"datasources/kafka/KafkaMicroBatchReader/#source-scala_3","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaMicroBatchReader/#getorcreateinitialpartitionoffsets-partitionoffsetmap","text":"getOrCreateInitialPartitionOffsets ...FIXME NOTE: getOrCreateInitialPartitionOffsets is used exclusively for the < > internal registry. === [[getStartOffset]] getStartOffset Method","title":"getOrCreateInitialPartitionOffsets(): PartitionOffsetMap"},{"location":"datasources/kafka/KafkaMicroBatchReader/#source-scala_4","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaMicroBatchReader/#getstartoffset-offset","text":"getStartOffset is part of the MicroBatchReader abstraction. getStartOffset ...FIXME === [[getEndOffset]] getEndOffset Method","title":"getStartOffset: Offset"},{"location":"datasources/kafka/KafkaMicroBatchReader/#source-scala_5","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaMicroBatchReader/#getendoffset-offset","text":"getEndOffset is part of the MicroBatchReader abstraction. getEndOffset ...FIXME === [[deserializeOffset]] deserializeOffset Method","title":"getEndOffset: Offset"},{"location":"datasources/kafka/KafkaMicroBatchReader/#source-scala_6","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaMicroBatchReader/#deserializeoffsetjson-string-offset","text":"deserializeOffset is part of the MicroBatchReader abstraction. deserializeOffset ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | endPartitionOffsets a| [[endPartitionOffsets]] Ending offsets for the assigned partitions ( Map[TopicPartition, Long] ) Used when...FIXME | initialPartitionOffsets a| [[initialPartitionOffsets]]","title":"deserializeOffset(json: String): Offset"},{"location":"datasources/kafka/KafkaMicroBatchReader/#source-scala_7","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaMicroBatchReader/#initialpartitionoffsets-maptopicpartition-long","text":"| rangeCalculator a| [[rangeCalculator]] KafkaOffsetRangeCalculator (for the given < >) Used exclusively when KafkaMicroBatchReader is requested to < > (to calculate offset ranges) | startPartitionOffsets a| [[startPartitionOffsets]] Starting offsets for the assigned partitions ( Map[TopicPartition, Long] ) Used when...FIXME |===","title":"initialPartitionOffsets: Map[TopicPartition, Long]"},{"location":"datasources/kafka/KafkaMicroBatchReader/#logging","text":"Enable ALL logging level for org.apache.spark.sql.kafka010.KafkaMicroBatchReader logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaMicroBatchReader=ALL Refer to Logging .","title":"Logging"},{"location":"datasources/kafka/KafkaMicroBatchStream/","text":"KafkaMicroBatchStream \u00b6 KafkaMicroBatchStream is...FIXME","title":"KafkaMicroBatchStream"},{"location":"datasources/kafka/KafkaMicroBatchStream/#kafkamicrobatchstream","text":"KafkaMicroBatchStream is...FIXME","title":"KafkaMicroBatchStream"},{"location":"datasources/kafka/KafkaOffsetRangeCalculator/","text":"KafkaOffsetRangeCalculator \u00b6 KafkaOffsetRangeCalculator is < > for KafkaMicroBatchReader to < > (when KafkaMicroBatchReader is requested to planInputPartitions ). [[minPartitions]][[creating-instance]] KafkaOffsetRangeCalculator takes an optional minimum number of partitions per executor ( minPartitions ) to be created (that can either be undefined or greater than 0 ). [[apply]] When created with a DataSourceOptions , KafkaOffsetRangeCalculator uses minPartitions option for the < >. === [[getRanges]] Offset Ranges -- getRanges Method [source, scala] \u00b6 getRanges( fromOffsets: PartitionOffsetMap, untilOffsets: PartitionOffsetMap, executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] getRanges finds the common TopicPartitions that are the keys that are used in the fromOffsets and untilOffsets collections ( intersection ). For every common TopicPartition , getRanges creates a < > with the from and until offsets from the fromOffsets and untilOffsets collections (and the < > undefined). getRanges filters out the TopicPartitions that < > (i.e. the difference between until and from offsets is not greater than 0 ). At this point, getRanges knows the TopicPartitions with records to consume. getRanges branches off based on the defined < > and the number of KafkaOffsetRanges ( TopicPartitions with records to consume). For the < > undefined or smaller than the number of KafkaOffsetRanges ( TopicPartitions to consume records from), getRanges updates every KafkaOffsetRange with the < > based on the TopicPartition and the executorLocations ). Otherwise (with the < > defined and greater than the number of KafkaOffsetRanges ), getRanges splits KafkaOffsetRanges into smaller ones. NOTE: getRanges is used exclusively when KafkaMicroBatchReader is requested to planInputPartitions . === [[KafkaOffsetRange]] KafkaOffsetRange -- TopicPartition with From and Until Offsets and Optional Preferred Location KafkaOffsetRange is a case class with the following attributes: [[topicPartition]] TopicPartition [[fromOffset]] fromOffset offset [[untilOffset]] untilOffset offset [[preferredLoc]] Optional preferred location [[size]] KafkaOffsetRange knows the size, i.e. the number of records between the < > and < > offsets. === [[getLocation]] Selecting Preferred Executor for TopicPartition -- getLocation Internal Method [source, scala] \u00b6 getLocation( tp: TopicPartition, executorLocations: Seq[String]): Option[String] getLocation ...FIXME NOTE: getLocation is used exclusively when KafkaOffsetRangeCalculator is requested to < >.","title":"KafkaOffsetRangeCalculator"},{"location":"datasources/kafka/KafkaOffsetRangeCalculator/#kafkaoffsetrangecalculator","text":"KafkaOffsetRangeCalculator is < > for KafkaMicroBatchReader to < > (when KafkaMicroBatchReader is requested to planInputPartitions ). [[minPartitions]][[creating-instance]] KafkaOffsetRangeCalculator takes an optional minimum number of partitions per executor ( minPartitions ) to be created (that can either be undefined or greater than 0 ). [[apply]] When created with a DataSourceOptions , KafkaOffsetRangeCalculator uses minPartitions option for the < >. === [[getRanges]] Offset Ranges -- getRanges Method","title":"KafkaOffsetRangeCalculator"},{"location":"datasources/kafka/KafkaOffsetRangeCalculator/#source-scala","text":"getRanges( fromOffsets: PartitionOffsetMap, untilOffsets: PartitionOffsetMap, executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] getRanges finds the common TopicPartitions that are the keys that are used in the fromOffsets and untilOffsets collections ( intersection ). For every common TopicPartition , getRanges creates a < > with the from and until offsets from the fromOffsets and untilOffsets collections (and the < > undefined). getRanges filters out the TopicPartitions that < > (i.e. the difference between until and from offsets is not greater than 0 ). At this point, getRanges knows the TopicPartitions with records to consume. getRanges branches off based on the defined < > and the number of KafkaOffsetRanges ( TopicPartitions with records to consume). For the < > undefined or smaller than the number of KafkaOffsetRanges ( TopicPartitions to consume records from), getRanges updates every KafkaOffsetRange with the < > based on the TopicPartition and the executorLocations ). Otherwise (with the < > defined and greater than the number of KafkaOffsetRanges ), getRanges splits KafkaOffsetRanges into smaller ones. NOTE: getRanges is used exclusively when KafkaMicroBatchReader is requested to planInputPartitions . === [[KafkaOffsetRange]] KafkaOffsetRange -- TopicPartition with From and Until Offsets and Optional Preferred Location KafkaOffsetRange is a case class with the following attributes: [[topicPartition]] TopicPartition [[fromOffset]] fromOffset offset [[untilOffset]] untilOffset offset [[preferredLoc]] Optional preferred location [[size]] KafkaOffsetRange knows the size, i.e. the number of records between the < > and < > offsets. === [[getLocation]] Selecting Preferred Executor for TopicPartition -- getLocation Internal Method","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetRangeCalculator/#source-scala_1","text":"getLocation( tp: TopicPartition, executorLocations: Seq[String]): Option[String] getLocation ...FIXME NOTE: getLocation is used exclusively when KafkaOffsetRangeCalculator is requested to < >.","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetRangeLimit/","text":"KafkaOffsetRangeLimit \u2014 Desired Offset Range Limits \u00b6 KafkaOffsetRangeLimit represents the desired offset range limits for starting, ending, and specific offsets in Kafka Data Source . [[implementations]] .KafkaOffsetRangeLimits [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | KafkaOffsetRangeLimit | Description | EarliestOffsetRangeLimit | [[EarliestOffsetRangeLimit]] Intent to bind to the earliest offset | LatestOffsetRangeLimit | [[LatestOffsetRangeLimit]] Intent to bind to the latest offset | SpecificOffsetRangeLimit a| [[SpecificOffsetRangeLimit]] Intent to bind to specific offsets with the following special offset \"magic\" numbers: [[LATEST]] -1 or KafkaOffsetRangeLimit.LATEST - the latest offset [[EARLIEST]] -2 or KafkaOffsetRangeLimit.EARLIEST - the earliest offset |=== NOTE: KafkaOffsetRangeLimit is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). KafkaOffsetRangeLimit is often used in a text-based representation and is converted to from latest , earliest or a JSON-formatted text using KafkaSourceProvider.getKafkaOffsetRangeLimit utility. NOTE: A JSON-formatted text is of the following format {\"topicName\":{\"partition\":offset},...} , e.g. {\"topicA\":{\"0\":23,\"1\":-1},\"topicB\":{\"0\":-2}} . KafkaOffsetRangeLimit is used when: KafkaContinuousReader is created (with the initial offsets ) KafkaMicroBatchReader is created (with the starting offsets ) KafkaRelation is created (with the starting and ending offsets) KafkaSource is created (with the starting offsets ) KafkaSourceProvider is requested to convert configuration options to KafkaOffsetRangeLimits","title":"KafkaOffsetRangeLimit"},{"location":"datasources/kafka/KafkaOffsetRangeLimit/#kafkaoffsetrangelimit-desired-offset-range-limits","text":"KafkaOffsetRangeLimit represents the desired offset range limits for starting, ending, and specific offsets in Kafka Data Source . [[implementations]] .KafkaOffsetRangeLimits [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | KafkaOffsetRangeLimit | Description | EarliestOffsetRangeLimit | [[EarliestOffsetRangeLimit]] Intent to bind to the earliest offset | LatestOffsetRangeLimit | [[LatestOffsetRangeLimit]] Intent to bind to the latest offset | SpecificOffsetRangeLimit a| [[SpecificOffsetRangeLimit]] Intent to bind to specific offsets with the following special offset \"magic\" numbers: [[LATEST]] -1 or KafkaOffsetRangeLimit.LATEST - the latest offset [[EARLIEST]] -2 or KafkaOffsetRangeLimit.EARLIEST - the earliest offset |=== NOTE: KafkaOffsetRangeLimit is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). KafkaOffsetRangeLimit is often used in a text-based representation and is converted to from latest , earliest or a JSON-formatted text using KafkaSourceProvider.getKafkaOffsetRangeLimit utility. NOTE: A JSON-formatted text is of the following format {\"topicName\":{\"partition\":offset},...} , e.g. {\"topicA\":{\"0\":23,\"1\":-1},\"topicB\":{\"0\":-2}} . KafkaOffsetRangeLimit is used when: KafkaContinuousReader is created (with the initial offsets ) KafkaMicroBatchReader is created (with the starting offsets ) KafkaRelation is created (with the starting and ending offsets) KafkaSource is created (with the starting offsets ) KafkaSourceProvider is requested to convert configuration options to KafkaOffsetRangeLimits","title":"KafkaOffsetRangeLimit &mdash; Desired Offset Range Limits"},{"location":"datasources/kafka/KafkaOffsetReader/","text":"KafkaOffsetReader \u00b6 KafkaOffsetReader relies on the ConsumerStrategy to < >. KafkaOffsetReader < > with group.id ( ConsumerConfig.GROUP_ID_CONFIG ) configuration explicitly set to < > (i.e. the given < > followed by < >). KafkaOffsetReader is < > when: KafkaRelation is requested to build a distributed data scan with column pruning KafkaSourceProvider is requested to create a KafkaSource , createMicroBatchReader , and createContinuousReader [[options]] .KafkaOffsetReader's Options [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | fetchOffset.numRetries a| [[fetchOffset.numRetries]] Default: 3 | fetchOffset.retryIntervalMs a| [[fetchOffset.retryIntervalMs]] How long to wait before retries Default: 1000 |=== [[kafkaSchema]] KafkaOffsetReader defines the predefined fixed schema . [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.kafka010.KafkaOffsetReader to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaOffsetReader=ALL Refer to < >. \u00b6 === [[creating-instance]] Creating KafkaOffsetReader Instance KafkaOffsetReader takes the following to be created: [[consumerStrategy]] ConsumerStrategy [[driverKafkaParams]] Kafka parameters (as name-value pairs that are used exclusively to < > [[readerOptions]] Options (as name-value pairs) [[driverGroupIdPrefix]] Prefix of the group ID KafkaOffsetReader initializes the < >. === [[nextGroupId]] nextGroupId Internal Method [source, scala] \u00b6 nextGroupId(): String \u00b6 nextGroupId sets the < > to be the < >, - followed by the < > (i.e. [driverGroupIdPrefix]-[nextId] ). In the end, nextGroupId increments the < > and returns the < >. NOTE: nextGroupId is used exclusively when KafkaOffsetReader is requested for a < >. === [[resetConsumer]] resetConsumer Internal Method [source, scala] \u00b6 resetConsumer(): Unit \u00b6 resetConsumer ...FIXME NOTE: resetConsumer is used when...FIXME === [[fetchTopicPartitions]] fetchTopicPartitions Method [source, scala] \u00b6 fetchTopicPartitions(): Set[TopicPartition] \u00b6 CAUTION: FIXME fetchTopicPartitions is used when KafkaRelation is requested for getPartitionOffsets . === [[fetchEarliestOffsets]] Fetching Earliest Offsets -- fetchEarliestOffsets Method [source, scala] \u00b6 fetchEarliestOffsets(): Map[TopicPartition, Long] fetchEarliestOffsets(newPartitions: Seq[TopicPartition]): Map[TopicPartition, Long] CAUTION: FIXME NOTE: fetchEarliestOffsets is used when KafkaSource rateLimit and generates a DataFrame for a batch (when new partitions have been assigned). === [[fetchLatestOffsets]] Fetching Latest Offsets -- fetchLatestOffsets Method [source, scala] \u00b6 fetchLatestOffsets(): Map[TopicPartition, Long] \u00b6 CAUTION: FIXME NOTE: fetchLatestOffsets is used when KafkaSource gets offsets or initialPartitionOffsets is initialized . === [[withRetriesWithoutInterrupt]] withRetriesWithoutInterrupt Internal Method [source, scala] \u00b6 withRetriesWithoutInterrupt( body: => Map[TopicPartition, Long]): Map[TopicPartition, Long] withRetriesWithoutInterrupt ...FIXME NOTE: withRetriesWithoutInterrupt is used when...FIXME === [[fetchSpecificOffsets]] Fetching Offsets for Selected TopicPartitions -- fetchSpecificOffsets Method [source, scala] \u00b6 fetchSpecificOffsets( partitionOffsets: Map[TopicPartition, Long], reportDataLoss: String => Unit): KafkaSourceOffset .KafkaOffsetReader's fetchSpecificOffsets image::images/KafkaOffsetReader-fetchSpecificOffsets.png[align=\"center\"] fetchSpecificOffsets requests the < > to poll(0) . fetchSpecificOffsets requests the < > for assigned partitions (using Consumer.assignment() ). fetchSpecificOffsets requests the < > to pause(partitions) . You should see the following DEBUG message in the logs: DEBUG KafkaOffsetReader: Partitions assigned to consumer: [partitions]. Seeking to [partitionOffsets] For every partition offset in the input partitionOffsets , fetchSpecificOffsets requests the < > to: seekToEnd for the latest (aka -1 ) seekToBeginning for the earliest (aka -2 ) seek for other offsets In the end, fetchSpecificOffsets creates a collection of Kafka's TopicPartition and position (using the < >). fetchSpecificOffsets is used when KafkaSource fetches and verifies initial partition offsets . === [[createConsumer]] Creating Kafka Consumer -- createConsumer Internal Method [source, scala] \u00b6 createConsumer(): Consumer[Array[Byte], Array[Byte]] \u00b6 createConsumer requests < > to create a Kafka Consumer with < > and < >. NOTE: createConsumer is used when KafkaOffsetReader is < > (and initializes < >) and < > === [[consumer]] Creating Kafka Consumer (Unless Already Available) -- consumer Method [source, scala] \u00b6 consumer: Consumer[Array[Byte], Array[Byte]] \u00b6 consumer gives the cached <<_consumer, Kafka Consumer>> or creates one itself. NOTE: Since consumer method is used (to access the internal <<_consumer, Kafka Consumer>>) in the fetch methods that gives the property of creating a new Kafka Consumer whenever the internal <<_consumer, Kafka Consumer>> reference become null , i.e. as in < >. consumer ...FIXME NOTE: consumer is used when KafkaOffsetReader is requested to < >, < >, < >, and < >. === [[close]] Closing -- close Method [source, scala] \u00b6 close(): Unit \u00b6 close < > (if the <<_consumer, Kafka Consumer>> is available). close requests the < > to shut down. close is used when: KafkaContinuousReader , KafkaMicroBatchReader , and KafkaSource are requested to stop a streaming reader or source KafkaRelation is requested to build a distributed data scan with column pruning === [[runUninterruptibly]] runUninterruptibly Internal Method [source, scala] \u00b6 runUninterruptibly T : T \u00b6 runUninterruptibly ...FIXME NOTE: runUninterruptibly is used when...FIXME === [[stopConsumer]] stopConsumer Internal Method [source, scala] \u00b6 stopConsumer(): Unit \u00b6 stopConsumer ...FIXME NOTE: stopConsumer is used when...FIXME === [[toString]] Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | _consumer a| [[_consumer]] Kafka's https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/consumer/Consumer.html[Consumer ] ( Consumer[Array[Byte], Array[Byte]] ) < > when KafkaOffsetReader is < >. Used when KafkaOffsetReader : < > < > < > < > < > < > | execContext a| [[execContext]] https://www.scala-lang.org/api/2.12.8/scala/concurrent/ExecutionContextExecutorService.html[scala.concurrent.ExecutionContextExecutorService ] | groupId a| [[groupId]] | kafkaReaderThread a| [[kafkaReaderThread]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ExecutorService.html[java.util.concurrent.ExecutorService ] | maxOffsetFetchAttempts a| [[maxOffsetFetchAttempts]] | nextId a| [[nextId]] Initially 0 | offsetFetchAttemptIntervalMs a| [[offsetFetchAttemptIntervalMs]] |===","title":"KafkaOffsetReader"},{"location":"datasources/kafka/KafkaOffsetReader/#kafkaoffsetreader","text":"KafkaOffsetReader relies on the ConsumerStrategy to < >. KafkaOffsetReader < > with group.id ( ConsumerConfig.GROUP_ID_CONFIG ) configuration explicitly set to < > (i.e. the given < > followed by < >). KafkaOffsetReader is < > when: KafkaRelation is requested to build a distributed data scan with column pruning KafkaSourceProvider is requested to create a KafkaSource , createMicroBatchReader , and createContinuousReader [[options]] .KafkaOffsetReader's Options [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | fetchOffset.numRetries a| [[fetchOffset.numRetries]] Default: 3 | fetchOffset.retryIntervalMs a| [[fetchOffset.retryIntervalMs]] How long to wait before retries Default: 1000 |=== [[kafkaSchema]] KafkaOffsetReader defines the predefined fixed schema . [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.kafka010.KafkaOffsetReader to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaOffsetReader=ALL","title":"KafkaOffsetReader"},{"location":"datasources/kafka/KafkaOffsetReader/#refer-to","text":"=== [[creating-instance]] Creating KafkaOffsetReader Instance KafkaOffsetReader takes the following to be created: [[consumerStrategy]] ConsumerStrategy [[driverKafkaParams]] Kafka parameters (as name-value pairs that are used exclusively to < > [[readerOptions]] Options (as name-value pairs) [[driverGroupIdPrefix]] Prefix of the group ID KafkaOffsetReader initializes the < >. === [[nextGroupId]] nextGroupId Internal Method","title":"Refer to &lt;&gt;."},{"location":"datasources/kafka/KafkaOffsetReader/#source-scala","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetReader/#nextgroupid-string","text":"nextGroupId sets the < > to be the < >, - followed by the < > (i.e. [driverGroupIdPrefix]-[nextId] ). In the end, nextGroupId increments the < > and returns the < >. NOTE: nextGroupId is used exclusively when KafkaOffsetReader is requested for a < >. === [[resetConsumer]] resetConsumer Internal Method","title":"nextGroupId(): String"},{"location":"datasources/kafka/KafkaOffsetReader/#source-scala_1","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetReader/#resetconsumer-unit","text":"resetConsumer ...FIXME NOTE: resetConsumer is used when...FIXME === [[fetchTopicPartitions]] fetchTopicPartitions Method","title":"resetConsumer(): Unit"},{"location":"datasources/kafka/KafkaOffsetReader/#source-scala_2","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetReader/#fetchtopicpartitions-settopicpartition","text":"CAUTION: FIXME fetchTopicPartitions is used when KafkaRelation is requested for getPartitionOffsets . === [[fetchEarliestOffsets]] Fetching Earliest Offsets -- fetchEarliestOffsets Method","title":"fetchTopicPartitions(): Set[TopicPartition]"},{"location":"datasources/kafka/KafkaOffsetReader/#source-scala_3","text":"fetchEarliestOffsets(): Map[TopicPartition, Long] fetchEarliestOffsets(newPartitions: Seq[TopicPartition]): Map[TopicPartition, Long] CAUTION: FIXME NOTE: fetchEarliestOffsets is used when KafkaSource rateLimit and generates a DataFrame for a batch (when new partitions have been assigned). === [[fetchLatestOffsets]] Fetching Latest Offsets -- fetchLatestOffsets Method","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetReader/#source-scala_4","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetReader/#fetchlatestoffsets-maptopicpartition-long","text":"CAUTION: FIXME NOTE: fetchLatestOffsets is used when KafkaSource gets offsets or initialPartitionOffsets is initialized . === [[withRetriesWithoutInterrupt]] withRetriesWithoutInterrupt Internal Method","title":"fetchLatestOffsets(): Map[TopicPartition, Long]"},{"location":"datasources/kafka/KafkaOffsetReader/#source-scala_5","text":"withRetriesWithoutInterrupt( body: => Map[TopicPartition, Long]): Map[TopicPartition, Long] withRetriesWithoutInterrupt ...FIXME NOTE: withRetriesWithoutInterrupt is used when...FIXME === [[fetchSpecificOffsets]] Fetching Offsets for Selected TopicPartitions -- fetchSpecificOffsets Method","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetReader/#source-scala_6","text":"fetchSpecificOffsets( partitionOffsets: Map[TopicPartition, Long], reportDataLoss: String => Unit): KafkaSourceOffset .KafkaOffsetReader's fetchSpecificOffsets image::images/KafkaOffsetReader-fetchSpecificOffsets.png[align=\"center\"] fetchSpecificOffsets requests the < > to poll(0) . fetchSpecificOffsets requests the < > for assigned partitions (using Consumer.assignment() ). fetchSpecificOffsets requests the < > to pause(partitions) . You should see the following DEBUG message in the logs: DEBUG KafkaOffsetReader: Partitions assigned to consumer: [partitions]. Seeking to [partitionOffsets] For every partition offset in the input partitionOffsets , fetchSpecificOffsets requests the < > to: seekToEnd for the latest (aka -1 ) seekToBeginning for the earliest (aka -2 ) seek for other offsets In the end, fetchSpecificOffsets creates a collection of Kafka's TopicPartition and position (using the < >). fetchSpecificOffsets is used when KafkaSource fetches and verifies initial partition offsets . === [[createConsumer]] Creating Kafka Consumer -- createConsumer Internal Method","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetReader/#source-scala_7","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetReader/#createconsumer-consumerarraybyte-arraybyte","text":"createConsumer requests < > to create a Kafka Consumer with < > and < >. NOTE: createConsumer is used when KafkaOffsetReader is < > (and initializes < >) and < > === [[consumer]] Creating Kafka Consumer (Unless Already Available) -- consumer Method","title":"createConsumer(): Consumer[Array[Byte], Array[Byte]]"},{"location":"datasources/kafka/KafkaOffsetReader/#source-scala_8","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetReader/#consumer-consumerarraybyte-arraybyte","text":"consumer gives the cached <<_consumer, Kafka Consumer>> or creates one itself. NOTE: Since consumer method is used (to access the internal <<_consumer, Kafka Consumer>>) in the fetch methods that gives the property of creating a new Kafka Consumer whenever the internal <<_consumer, Kafka Consumer>> reference become null , i.e. as in < >. consumer ...FIXME NOTE: consumer is used when KafkaOffsetReader is requested to < >, < >, < >, and < >. === [[close]] Closing -- close Method","title":"consumer: Consumer[Array[Byte], Array[Byte]]"},{"location":"datasources/kafka/KafkaOffsetReader/#source-scala_9","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetReader/#close-unit","text":"close < > (if the <<_consumer, Kafka Consumer>> is available). close requests the < > to shut down. close is used when: KafkaContinuousReader , KafkaMicroBatchReader , and KafkaSource are requested to stop a streaming reader or source KafkaRelation is requested to build a distributed data scan with column pruning === [[runUninterruptibly]] runUninterruptibly Internal Method","title":"close(): Unit"},{"location":"datasources/kafka/KafkaOffsetReader/#source-scala_10","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetReader/#rununinterruptiblyt-t","text":"runUninterruptibly ...FIXME NOTE: runUninterruptibly is used when...FIXME === [[stopConsumer]] stopConsumer Internal Method","title":"runUninterruptiblyT: T"},{"location":"datasources/kafka/KafkaOffsetReader/#source-scala_11","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetReader/#stopconsumer-unit","text":"stopConsumer ...FIXME NOTE: stopConsumer is used when...FIXME === [[toString]] Textual Representation -- toString Method","title":"stopConsumer(): Unit"},{"location":"datasources/kafka/KafkaOffsetReader/#source-scala_12","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaOffsetReader/#tostring-string","text":"NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | _consumer a| [[_consumer]] Kafka's https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/consumer/Consumer.html[Consumer ] ( Consumer[Array[Byte], Array[Byte]] ) < > when KafkaOffsetReader is < >. Used when KafkaOffsetReader : < > < > < > < > < > < > | execContext a| [[execContext]] https://www.scala-lang.org/api/2.12.8/scala/concurrent/ExecutionContextExecutorService.html[scala.concurrent.ExecutionContextExecutorService ] | groupId a| [[groupId]] | kafkaReaderThread a| [[kafkaReaderThread]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ExecutorService.html[java.util.concurrent.ExecutorService ] | maxOffsetFetchAttempts a| [[maxOffsetFetchAttempts]] | nextId a| [[nextId]] Initially 0 | offsetFetchAttemptIntervalMs a| [[offsetFetchAttemptIntervalMs]] |===","title":"toString: String"},{"location":"datasources/kafka/KafkaRelation/","text":"KafkaRelation \u00b6 [[schema]] KafkaRelation represents a collection of rows with a predefined schema ( BaseRelation ) that supports < > ( TableScan ). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-BaseRelation.html[BaseRelation ] and https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-TableScan.html[TableScan ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. KafkaRelation is < > exclusively when KafkaSourceProvider is requested to create a BaseRelation . [[options]] .KafkaRelation's Options [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | kafkaConsumer.pollTimeoutMs a| [[kafkaConsumer.pollTimeoutMs]][[pollTimeoutMs]] Default: spark.network.timeout configuration if set or 120s |=== [[logging]] [TIP] ==== Enable ALL logging levels for org.apache.spark.sql.kafka010.KafkaRelation to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaRelation=ALL Refer to < >. \u00b6 === [[creating-instance]] Creating KafkaRelation Instance KafkaRelation takes the following when created: [[sqlContext]] SQLContext [[strategy]] ConsumerStrategy [[sourceOptions]] Source options ( Map[String, String] ) [[specifiedKafkaParams]] User-defined Kafka parameters ( Map[String, String] ) [[failOnDataLoss]] failOnDataLoss flag [[startingOffsets]] Starting offsets [[endingOffsets]] Ending offsets === [[getPartitionOffsets]] getPartitionOffsets Internal Method [source, scala] \u00b6 getPartitionOffsets( kafkaReader: KafkaOffsetReader, kafkaOffsets: KafkaOffsetRangeLimit): Map[TopicPartition, Long] CAUTION: FIXME NOTE: getPartitionOffsets is used exclusively when KafkaRelation < >. === [[buildScan]] Building Distributed Data Scan with Column Pruning -- buildScan Method [source, scala] \u00b6 buildScan(): RDD[Row] \u00b6 NOTE: buildScan is part of the https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-TableScan.html[TableScan ] contract to build a distributed data scan with column pruning. buildScan generates a unique group ID of the format spark-kafka-relation-[randomUUID] (to make sure that a streaming query creates a new consumer group). buildScan creates a KafkaOffsetReader with the following: The given < > and the < > Kafka parameters for the driver based on the given < > spark-kafka-relation-[randomUUID]-driver for the driverGroupIdPrefix buildScan uses the KafkaOffsetReader to < > for the starting and ending offsets (based on the given < > and the < >, respectively). buildScan requests the KafkaOffsetReader to close afterwards. buildScan creates offset ranges (that are a collection of KafkaSourceRDDOffsetRanges with a Kafka TopicPartition , beginning and ending offsets and undefined preferred location). buildScan prints out the following INFO message to the logs: Generating RDD of offset ranges: [offsetRanges] buildScan creates a KafkaSourceRDD with the following: Kafka parameters for executors based on the given < > and the unique group ID ( spark-kafka-relation-[randomUUID] ) The offset ranges created < > configuration The given < > flag reuseKafkaConsumer flag off ( false ) buildScan requests the KafkaSourceRDD to map Kafka ConsumerRecords to InternalRows . In the end, buildScan requests the < > to create a DataFrame (with the name kafka and the predefined < >) that is immediately converted to a RDD[InternalRow] . buildScan throws a IllegalStateException when...FIXME different topic partitions for starting offsets topics[[fromTopics]] and ending offsets topics[[untilTopics]] buildScan throws a IllegalStateException when...FIXME [tp] doesn't have a from offset","title":"KafkaRelation"},{"location":"datasources/kafka/KafkaRelation/#kafkarelation","text":"[[schema]] KafkaRelation represents a collection of rows with a predefined schema ( BaseRelation ) that supports < > ( TableScan ). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-BaseRelation.html[BaseRelation ] and https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-TableScan.html[TableScan ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. KafkaRelation is < > exclusively when KafkaSourceProvider is requested to create a BaseRelation . [[options]] .KafkaRelation's Options [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | kafkaConsumer.pollTimeoutMs a| [[kafkaConsumer.pollTimeoutMs]][[pollTimeoutMs]] Default: spark.network.timeout configuration if set or 120s |=== [[logging]] [TIP] ==== Enable ALL logging levels for org.apache.spark.sql.kafka010.KafkaRelation to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaRelation=ALL","title":"KafkaRelation"},{"location":"datasources/kafka/KafkaRelation/#refer-to","text":"=== [[creating-instance]] Creating KafkaRelation Instance KafkaRelation takes the following when created: [[sqlContext]] SQLContext [[strategy]] ConsumerStrategy [[sourceOptions]] Source options ( Map[String, String] ) [[specifiedKafkaParams]] User-defined Kafka parameters ( Map[String, String] ) [[failOnDataLoss]] failOnDataLoss flag [[startingOffsets]] Starting offsets [[endingOffsets]] Ending offsets === [[getPartitionOffsets]] getPartitionOffsets Internal Method","title":"Refer to &lt;&gt;."},{"location":"datasources/kafka/KafkaRelation/#source-scala","text":"getPartitionOffsets( kafkaReader: KafkaOffsetReader, kafkaOffsets: KafkaOffsetRangeLimit): Map[TopicPartition, Long] CAUTION: FIXME NOTE: getPartitionOffsets is used exclusively when KafkaRelation < >. === [[buildScan]] Building Distributed Data Scan with Column Pruning -- buildScan Method","title":"[source, scala]"},{"location":"datasources/kafka/KafkaRelation/#source-scala_1","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaRelation/#buildscan-rddrow","text":"NOTE: buildScan is part of the https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-TableScan.html[TableScan ] contract to build a distributed data scan with column pruning. buildScan generates a unique group ID of the format spark-kafka-relation-[randomUUID] (to make sure that a streaming query creates a new consumer group). buildScan creates a KafkaOffsetReader with the following: The given < > and the < > Kafka parameters for the driver based on the given < > spark-kafka-relation-[randomUUID]-driver for the driverGroupIdPrefix buildScan uses the KafkaOffsetReader to < > for the starting and ending offsets (based on the given < > and the < >, respectively). buildScan requests the KafkaOffsetReader to close afterwards. buildScan creates offset ranges (that are a collection of KafkaSourceRDDOffsetRanges with a Kafka TopicPartition , beginning and ending offsets and undefined preferred location). buildScan prints out the following INFO message to the logs: Generating RDD of offset ranges: [offsetRanges] buildScan creates a KafkaSourceRDD with the following: Kafka parameters for executors based on the given < > and the unique group ID ( spark-kafka-relation-[randomUUID] ) The offset ranges created < > configuration The given < > flag reuseKafkaConsumer flag off ( false ) buildScan requests the KafkaSourceRDD to map Kafka ConsumerRecords to InternalRows . In the end, buildScan requests the < > to create a DataFrame (with the name kafka and the predefined < >) that is immediately converted to a RDD[InternalRow] . buildScan throws a IllegalStateException when...FIXME different topic partitions for starting offsets topics[[fromTopics]] and ending offsets topics[[untilTopics]] buildScan throws a IllegalStateException when...FIXME [tp] doesn't have a from offset","title":"buildScan(): RDD[Row]"},{"location":"datasources/kafka/KafkaScan/","text":"KafkaScan \u00b6 KafkaScan is...FIXME","title":"KafkaScan"},{"location":"datasources/kafka/KafkaScan/#kafkascan","text":"KafkaScan is...FIXME","title":"KafkaScan"},{"location":"datasources/kafka/KafkaSink/","text":"KafkaSink \u00b6 KafkaSink is a streaming sink that KafkaSourceProvider registers as the kafka format. // start spark-shell or a Spark application with spark-sql-kafka-0-10 module // spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT import org.apache.spark.sql.SparkSession val spark: SparkSession = ... spark. readStream. format(\"text\"). load(\"server-logs/*.out\"). as[String]. writeStream. queryName(\"server-logs processor\"). format(\"kafka\"). // <-- uses KafkaSink option(\"topic\", \"topic1\"). option(\"checkpointLocation\", \"/tmp/kafka-sink-checkpoint\"). // <-- mandatory start // in another terminal $ echo hello > server-logs/hello.out // in the terminal with Spark FIXME Creating Instance \u00b6 KafkaSink takes the following when created: [[sqlContext]] SQLContext [[executorKafkaParams]] Kafka parameters (used on executor) as a map of (String, Object) pairs [[topic]] Optional topic name === [[addBatch]] addBatch Method [source, scala] \u00b6 addBatch(batchId: Long, data: DataFrame): Unit \u00b6 Internally, addBatch requests KafkaWriter to write the input data to the < > (if defined) or a topic in < >. addBatch is a part of Sink abstraction.","title":"KafkaSink"},{"location":"datasources/kafka/KafkaSink/#kafkasink","text":"KafkaSink is a streaming sink that KafkaSourceProvider registers as the kafka format. // start spark-shell or a Spark application with spark-sql-kafka-0-10 module // spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT import org.apache.spark.sql.SparkSession val spark: SparkSession = ... spark. readStream. format(\"text\"). load(\"server-logs/*.out\"). as[String]. writeStream. queryName(\"server-logs processor\"). format(\"kafka\"). // <-- uses KafkaSink option(\"topic\", \"topic1\"). option(\"checkpointLocation\", \"/tmp/kafka-sink-checkpoint\"). // <-- mandatory start // in another terminal $ echo hello > server-logs/hello.out // in the terminal with Spark FIXME","title":"KafkaSink"},{"location":"datasources/kafka/KafkaSink/#creating-instance","text":"KafkaSink takes the following when created: [[sqlContext]] SQLContext [[executorKafkaParams]] Kafka parameters (used on executor) as a map of (String, Object) pairs [[topic]] Optional topic name === [[addBatch]] addBatch Method","title":"Creating Instance"},{"location":"datasources/kafka/KafkaSink/#source-scala","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSink/#addbatchbatchid-long-data-dataframe-unit","text":"Internally, addBatch requests KafkaWriter to write the input data to the < > (if defined) or a topic in < >. addBatch is a part of Sink abstraction.","title":"addBatch(batchId: Long, data: DataFrame): Unit"},{"location":"datasources/kafka/KafkaSource/","text":"KafkaSource \u00b6 KafkaSource is a streaming source that loads data from Apache Kafka . Note Kafka topics are checked for new records every trigger and so there is some noticeable delay between when the records have arrived to Kafka topics and when a Spark application processes them. KafkaSource uses the metadata log directory to persist offsets. The directory is the source ID under the sources directory in the checkpointRoot (of the StreamExecution ). Note The checkpointRoot directory is one of the following: checkpointLocation option spark.sql.streaming.checkpointLocation configuration property KafkaSource < > for kafka format (that is registered by KafkaSourceProvider ). [[schema]] KafkaSource uses a predefined (fixed) schema (that cannot be changed ). KafkaSource also supports batch Datasets. Creating Instance \u00b6 KafkaSource takes the following to be created: SQLContext KafkaOffsetReader Parameters of executors (reading from Kafka) Source Options Path of Metadata Log (where KafkaSource persists KafkaSourceOffset offsets in JSON format) Starting offsets (defined using startingOffsets option) failOnDataLoss flag to create KafkaSourceRDD s every trigger and to report an IllegalStateException on data loss . Loading Kafka Records for Streaming Micro-Batch \u00b6 getBatch ( start : Option [ Offset ], end : Offset ): DataFrame getBatch is part of the Source abstraction. getBatch creates a streaming DataFrame with a query plan with LogicalRDD logical operator to scan data from a KafkaSourceRDD . Internally, getBatch initializes < > (unless initialized already). You should see the following INFO message in the logs: GetBatch called with start = [start], end = [end] getBatch requests KafkaSourceOffset for end partition offsets for the input end offset (known as untilPartitionOffsets ). getBatch requests KafkaSourceOffset for start partition offsets for the input start offset (if defined) or uses < > (known as fromPartitionOffsets ). getBatch finds the new partitions (as the difference between the topic partitions in untilPartitionOffsets and fromPartitionOffsets ) and requests < > to fetch their earliest offsets . getBatch < > if the new partitions don't match to what < > fetched. Cannot find earliest offsets of [partitions]. Some data may have been missed You should see the following INFO message in the logs: Partitions added: [newPartitionOffsets] getBatch < > if the new partitions don't have their offsets 0 . Added partition [partition] starts from [offset] instead of 0. Some data may have been missed getBatch < > if the fromPartitionOffsets partitions differ from untilPartitionOffsets partitions. [partitions] are gone. Some data may have been missed You should see the following DEBUG message in the logs: TopicPartitions: [topicPartitions] getBatch < > (sorted by executorId and host of the registered block managers). IMPORTANT: That is when getBatch goes very low-level to allow for cached KafkaConsumers in the executors to be re-used to read the same partition in every batch (aka location preference ). You should see the following DEBUG message in the logs: Sorted executors: [sortedExecutors] getBatch creates a KafkaSourceRDDOffsetRange per TopicPartition . getBatch filters out KafkaSourceRDDOffsetRanges for which until offsets are smaller than from offsets. getBatch < > if they are found. Partition [topicPartition]'s offset was changed from [fromOffset] to [untilOffset], some data may have been missed getBatch creates a KafkaSourceRDD (with < >, < > and reuseKafkaConsumer flag enabled) and maps it to an RDD of InternalRow . IMPORTANT: getBatch creates a KafkaSourceRDD with reuseKafkaConsumer flag enabled. You should see the following INFO message in the logs: GetBatch generating RDD of offset range: [offsetRanges] getBatch sets < > if it was empty (which is when...FIXME) In the end, getBatch creates a streaming DataFrame for the KafkaSourceRDD and the < >. === [[getOffset]] Fetching Offsets (From Metadata Log or Kafka Directly) -- getOffset Method [source, scala] \u00b6 getOffset: Option[Offset] \u00b6 NOTE: getOffset is a part of the ../../Source.md#getOffset[Source Contract]. Internally, getOffset fetches the < > (from the metadata log or Kafka directly). .KafkaSource Initializing initialPartitionOffsets While Fetching Initial Offsets image::images/KafkaSource-initialPartitionOffsets.png[align=\"center\"] NOTE: < > is a lazy value and is initialized the very first time getOffset is called (which is when StreamExecution MicroBatchExecution.md#constructNextBatch-hasNewData[constructs a streaming micro-batch]). [source, scala] \u00b6 scala> spark.version res0: String = 2.3.0-SNAPSHOT // Case 1: Checkpoint directory undefined // initialPartitionOffsets read from Kafka directly val records = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load // Start the streaming query // dump records to the console every 10 seconds import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = records. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // Note the temporary checkpoint directory 17/08/07 11:09:29 INFO StreamExecution: Starting [id = 75dd261d-6b62-40fc-a368-9d95d3cb6f5f, runId = f18a5eb5-ccab-4d9d-8a81-befed41a72bd] with file:///private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-d0055630-24e4-4d9a-8f36-7a12a0f11bc0 to store the query checkpoint. ... INFO KafkaSource: Initial offsets: {\"topic1\":{\"0\":1}} // Stop the streaming query q.stop // Case 2: Checkpoint directory defined // initialPartitionOffsets read from Kafka directly // since the checkpoint directory is not available yet // it will be the next time the query is started val records = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load. select($\"value\" cast \"string\", $\"topic\", $\"partition\", $\"offset\") import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = records. writeStream. format(\"console\"). option(\"truncate\", false). option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // Note the checkpoint directory in use 17/08/07 11:21:25 INFO StreamExecution: Starting [id = b8f59854-61c1-4c2f-931d-62bbaf90ee3b, runId = 70d06a3b-f2b1-4fa8-a518-15df4cf59130] with file:///tmp/checkpoint to store the query checkpoint. ... INFO KafkaSource: Initial offsets: {\"topic1\":{\"0\":1}} ... INFO StreamExecution: Stored offsets for batch 0. Metadata OffsetSeqMetadata(0,1502098526848,Map(spark.sql.shuffle.partitions -> 200, spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider)) // Review the checkpoint location // $ ls -ltr /tmp/checkpoint/offsets // total 8 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:21 0 // $ tail -2 /tmp/checkpoint/offsets/0 | jq // Produce messages to Kafka so the latest offset changes // And more importanly the offset gets stored to checkpoint location Batch: 1 \u00b6 +---------------------------+------+---------+------+ |value |topic |partition|offset| +---------------------------+------+---------+------+ |testing checkpoint location|topic1|0 |2 | +---------------------------+------+---------+------+ // and one more // Note the offset Batch: 2 \u00b6 +------------+------+---------+------+ |value |topic |partition|offset| +------------+------+---------+------+ |another test|topic1|0 |3 | +------------+------+---------+------+ // See what was checkpointed // $ ls -ltr /tmp/checkpoint/offsets // total 24 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:35 0 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:37 1 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:38 2 // $ tail -2 /tmp/checkpoint/offsets/2 | jq // Stop the streaming query q.stop // And start over to see what offset the query starts from // Checkpoint location should have the offsets val q = records. writeStream. format(\"console\"). option(\"truncate\", false). option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // Whoops...console format does not support recovery (!) // Reported as https://issues.apache.org/jira/browse/SPARK-21667 org.apache.spark.sql.AnalysisException: This query does not support recovering from checkpoint location. Delete /tmp/checkpoint/offsets to start over.; at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:222) at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:278) at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:284) ... 61 elided // Change the sink (= output format) to JSON val q = records. writeStream. format(\"json\"). option(\"path\", \"/tmp/json-sink\"). option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory trigger(Trigger.ProcessingTime(10.seconds)). start // Note the checkpoint directory in use 17/08/07 12:09:02 INFO StreamExecution: Starting [id = 02e00924-5f0d-4501-bcb8-80be8a8be385, runId = 5eba2576-dad6-4f95-9031-e72514475edc] with file:///tmp/checkpoint to store the query checkpoint. ... 17/08/07 12:09:02 INFO KafkaSource: GetBatch called with start = Some({\"topic1\":{\"0\":3}}), end = {\"topic1\":{\"0\":4}} 17/08/07 12:09:02 INFO KafkaSource: Partitions added: Map() 17/08/07 12:09:02 DEBUG KafkaSource: TopicPartitions: topic1-0 17/08/07 12:09:02 DEBUG KafkaSource: Sorted executors: 17/08/07 12:09:02 INFO KafkaSource: GetBatch generating RDD of offset range: KafkaSourceRDDOffsetRange(topic1-0,3,4,None) 17/08/07 12:09:03 DEBUG KafkaOffsetReader: Partitions assigned to consumer: [topic1-0]. Seeking to the end. 17/08/07 12:09:03 DEBUG KafkaOffsetReader: Got latest offsets for partition : Map(topic1-0 -> 4) 17/08/07 12:09:03 DEBUG KafkaSource: GetOffset: ArrayBuffer((topic1-0,4)) 17/08/07 12:09:03 DEBUG StreamExecution: getOffset took 122 ms 17/08/07 12:09:03 DEBUG StreamExecution: Resuming at batch 3 with committed offsets {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} and available offsets {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} 17/08/07 12:09:03 DEBUG StreamExecution: Stream running from {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} to {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} getOffset requests < > to fetchLatestOffsets (known later as latest ). NOTE: (Possible performance degradation?) It is possible that getOffset will request the latest offsets from Kafka twice, i.e. while initializing < > (when no metadata log is available and KafkaSource's < > is LatestOffsetRangeLimit ) and always as part of getOffset itself. getOffset then calculates < > based on the < > option. .getOffset's Offset Calculation per maxOffsetsPerTrigger [cols=\"1,1\",options=\"header\",width=\"100%\"] |=== | maxOffsetsPerTrigger | Offsets | Unspecified (i.e. None ) | latest | Defined (but < > is empty) | < > with limit limit, < > as from , until as latest | Defined (and < > contains partitions and offsets) | < > with limit limit, < > as from , until as latest |=== You should see the following DEBUG message in the logs: GetOffset: [offsets] In the end, getOffset creates a KafkaSourceOffset with offsets (as Map[TopicPartition, Long] ). === [[fetchAndVerify]] Fetching and Verifying Specific Offsets -- fetchAndVerify Internal Method [source, scala] \u00b6 fetchAndVerify(specificOffsets: Map[TopicPartition, Long]): KafkaSourceOffset \u00b6 fetchAndVerify requests < > to fetchSpecificOffsets for the given specificOffsets . fetchAndVerify makes sure that the starting offsets in specificOffsets are the same as in Kafka and < > otherwise. startingOffsets for [tp] was [off] but consumer reset to [result(tp)] In the end, fetchAndVerify creates a KafkaSourceOffset (with the result of < >). NOTE: fetchAndVerify is used exclusively when KafkaSource initializes < >. === [[initialPartitionOffsets]] Initial Partition Offsets (of 0 th Batch) -- initialPartitionOffsets Internal Lazy Property [source, scala] \u00b6 initialPartitionOffsets: Map[TopicPartition, Long] \u00b6 initialPartitionOffsets is the initial partition offsets for the batch 0 that were already persisted in the < > or persisted on demand. As the very first step, initialPartitionOffsets creates a custom HDFSMetadataLog (of KafkaSourceOffsets metadata) in the < >. initialPartitionOffsets requests the HDFSMetadataLog for the metadata of the 0 th batch (as KafkaSourceOffset ). If the metadata is available, initialPartitionOffsets requests the metadata for the collection of TopicPartitions and their offsets . If the metadata could not be found, initialPartitionOffsets creates a new KafkaSourceOffset per < >: For EarliestOffsetRangeLimit , initialPartitionOffsets requests the < > to fetchEarliestOffsets For LatestOffsetRangeLimit , initialPartitionOffsets requests the < > to fetchLatestOffsets For SpecificOffsetRangeLimit , initialPartitionOffsets requests the < > to fetchSpecificOffsets (and report a data loss per the < > flag) initialPartitionOffsets requests the custom HDFSMetadataLog to add the offsets to the metadata log (as the metadata of the 0 th batch). initialPartitionOffsets prints out the following INFO message to the logs: Initial offsets: [offsets] Note initialPartitionOffsets is used when KafkaSource is requested for the following: < > < > (when the start offsets are not defined, i.e. before StreamExecution commits the first streaming batch and so nothing is in committedOffsets registry for a KafkaSource data source yet) ==== [[initialPartitionOffsets-HDFSMetadataLog-serialize]] HDFSMetadataLog.serialize [source, scala] \u00b6 serialize( metadata: KafkaSourceOffset, out: OutputStream): Unit serialize requests the OutputStream to write a zero byte (to support Spark 2.1.0 as per SPARK-19517). serialize creates a BufferedWriter over a OutputStreamWriter over the OutputStream (with UTF_8 charset encoding). serialize requests the BufferedWriter to write the v1 version indicator followed by a new line. serialize then requests the KafkaSourceOffset for a JSON-serialized representation and the BufferedWriter to write it out. In the end, serialize requests the BufferedWriter to flush (the underlying stream). serialize is part of the HDFSMetadataLog abstraction. === [[rateLimit]] rateLimit Internal Method [source, scala] \u00b6 rateLimit( limit: Long, from: Map[TopicPartition, Long], until: Map[TopicPartition, Long]): Map[TopicPartition, Long] rateLimit requests < > to fetchEarliestOffsets . CAUTION: FIXME NOTE: rateLimit is used exclusively when KafkaSource < > (when < > option is specified). === [[getSortedExecutorList]] getSortedExecutorList Method CAUTION: FIXME === [[reportDataLoss]] reportDataLoss Internal Method CAUTION: FIXME [NOTE] \u00b6 reportDataLoss is used when KafkaSource does the following: < > < > \u00b6 === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | currentPartitionOffsets | [[currentPartitionOffsets]] Current partition offsets (as Map[TopicPartition, Long] ) Initially NONE and set when KafkaSource is requested to < > or < >. | pollTimeoutMs a| [[pollTimeoutMs]] | sc a| [[sc]] Spark Core's SparkContext (of the < >) Used when: < > (and creating a KafkaSourceRDD ) Initializing the pollTimeoutMs internal property |=== Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.kafka010.KafkaSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaSource=ALL Refer to Logging .","title":"KafkaSource"},{"location":"datasources/kafka/KafkaSource/#kafkasource","text":"KafkaSource is a streaming source that loads data from Apache Kafka . Note Kafka topics are checked for new records every trigger and so there is some noticeable delay between when the records have arrived to Kafka topics and when a Spark application processes them. KafkaSource uses the metadata log directory to persist offsets. The directory is the source ID under the sources directory in the checkpointRoot (of the StreamExecution ). Note The checkpointRoot directory is one of the following: checkpointLocation option spark.sql.streaming.checkpointLocation configuration property KafkaSource < > for kafka format (that is registered by KafkaSourceProvider ). [[schema]] KafkaSource uses a predefined (fixed) schema (that cannot be changed ). KafkaSource also supports batch Datasets.","title":"KafkaSource"},{"location":"datasources/kafka/KafkaSource/#creating-instance","text":"KafkaSource takes the following to be created: SQLContext KafkaOffsetReader Parameters of executors (reading from Kafka) Source Options Path of Metadata Log (where KafkaSource persists KafkaSourceOffset offsets in JSON format) Starting offsets (defined using startingOffsets option) failOnDataLoss flag to create KafkaSourceRDD s every trigger and to report an IllegalStateException on data loss .","title":"Creating Instance"},{"location":"datasources/kafka/KafkaSource/#loading-kafka-records-for-streaming-micro-batch","text":"getBatch ( start : Option [ Offset ], end : Offset ): DataFrame getBatch is part of the Source abstraction. getBatch creates a streaming DataFrame with a query plan with LogicalRDD logical operator to scan data from a KafkaSourceRDD . Internally, getBatch initializes < > (unless initialized already). You should see the following INFO message in the logs: GetBatch called with start = [start], end = [end] getBatch requests KafkaSourceOffset for end partition offsets for the input end offset (known as untilPartitionOffsets ). getBatch requests KafkaSourceOffset for start partition offsets for the input start offset (if defined) or uses < > (known as fromPartitionOffsets ). getBatch finds the new partitions (as the difference between the topic partitions in untilPartitionOffsets and fromPartitionOffsets ) and requests < > to fetch their earliest offsets . getBatch < > if the new partitions don't match to what < > fetched. Cannot find earliest offsets of [partitions]. Some data may have been missed You should see the following INFO message in the logs: Partitions added: [newPartitionOffsets] getBatch < > if the new partitions don't have their offsets 0 . Added partition [partition] starts from [offset] instead of 0. Some data may have been missed getBatch < > if the fromPartitionOffsets partitions differ from untilPartitionOffsets partitions. [partitions] are gone. Some data may have been missed You should see the following DEBUG message in the logs: TopicPartitions: [topicPartitions] getBatch < > (sorted by executorId and host of the registered block managers). IMPORTANT: That is when getBatch goes very low-level to allow for cached KafkaConsumers in the executors to be re-used to read the same partition in every batch (aka location preference ). You should see the following DEBUG message in the logs: Sorted executors: [sortedExecutors] getBatch creates a KafkaSourceRDDOffsetRange per TopicPartition . getBatch filters out KafkaSourceRDDOffsetRanges for which until offsets are smaller than from offsets. getBatch < > if they are found. Partition [topicPartition]'s offset was changed from [fromOffset] to [untilOffset], some data may have been missed getBatch creates a KafkaSourceRDD (with < >, < > and reuseKafkaConsumer flag enabled) and maps it to an RDD of InternalRow . IMPORTANT: getBatch creates a KafkaSourceRDD with reuseKafkaConsumer flag enabled. You should see the following INFO message in the logs: GetBatch generating RDD of offset range: [offsetRanges] getBatch sets < > if it was empty (which is when...FIXME) In the end, getBatch creates a streaming DataFrame for the KafkaSourceRDD and the < >. === [[getOffset]] Fetching Offsets (From Metadata Log or Kafka Directly) -- getOffset Method","title":" Loading Kafka Records for Streaming Micro-Batch"},{"location":"datasources/kafka/KafkaSource/#source-scala","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSource/#getoffset-optionoffset","text":"NOTE: getOffset is a part of the ../../Source.md#getOffset[Source Contract]. Internally, getOffset fetches the < > (from the metadata log or Kafka directly). .KafkaSource Initializing initialPartitionOffsets While Fetching Initial Offsets image::images/KafkaSource-initialPartitionOffsets.png[align=\"center\"] NOTE: < > is a lazy value and is initialized the very first time getOffset is called (which is when StreamExecution MicroBatchExecution.md#constructNextBatch-hasNewData[constructs a streaming micro-batch]).","title":"getOffset: Option[Offset]"},{"location":"datasources/kafka/KafkaSource/#source-scala_1","text":"scala> spark.version res0: String = 2.3.0-SNAPSHOT // Case 1: Checkpoint directory undefined // initialPartitionOffsets read from Kafka directly val records = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load // Start the streaming query // dump records to the console every 10 seconds import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = records. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // Note the temporary checkpoint directory 17/08/07 11:09:29 INFO StreamExecution: Starting [id = 75dd261d-6b62-40fc-a368-9d95d3cb6f5f, runId = f18a5eb5-ccab-4d9d-8a81-befed41a72bd] with file:///private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-d0055630-24e4-4d9a-8f36-7a12a0f11bc0 to store the query checkpoint. ... INFO KafkaSource: Initial offsets: {\"topic1\":{\"0\":1}} // Stop the streaming query q.stop // Case 2: Checkpoint directory defined // initialPartitionOffsets read from Kafka directly // since the checkpoint directory is not available yet // it will be the next time the query is started val records = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load. select($\"value\" cast \"string\", $\"topic\", $\"partition\", $\"offset\") import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = records. writeStream. format(\"console\"). option(\"truncate\", false). option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // Note the checkpoint directory in use 17/08/07 11:21:25 INFO StreamExecution: Starting [id = b8f59854-61c1-4c2f-931d-62bbaf90ee3b, runId = 70d06a3b-f2b1-4fa8-a518-15df4cf59130] with file:///tmp/checkpoint to store the query checkpoint. ... INFO KafkaSource: Initial offsets: {\"topic1\":{\"0\":1}} ... INFO StreamExecution: Stored offsets for batch 0. Metadata OffsetSeqMetadata(0,1502098526848,Map(spark.sql.shuffle.partitions -> 200, spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider)) // Review the checkpoint location // $ ls -ltr /tmp/checkpoint/offsets // total 8 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:21 0 // $ tail -2 /tmp/checkpoint/offsets/0 | jq // Produce messages to Kafka so the latest offset changes // And more importanly the offset gets stored to checkpoint location","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSource/#batch-1","text":"+---------------------------+------+---------+------+ |value |topic |partition|offset| +---------------------------+------+---------+------+ |testing checkpoint location|topic1|0 |2 | +---------------------------+------+---------+------+ // and one more // Note the offset","title":"Batch: 1"},{"location":"datasources/kafka/KafkaSource/#batch-2","text":"+------------+------+---------+------+ |value |topic |partition|offset| +------------+------+---------+------+ |another test|topic1|0 |3 | +------------+------+---------+------+ // See what was checkpointed // $ ls -ltr /tmp/checkpoint/offsets // total 24 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:35 0 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:37 1 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:38 2 // $ tail -2 /tmp/checkpoint/offsets/2 | jq // Stop the streaming query q.stop // And start over to see what offset the query starts from // Checkpoint location should have the offsets val q = records. writeStream. format(\"console\"). option(\"truncate\", false). option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // Whoops...console format does not support recovery (!) // Reported as https://issues.apache.org/jira/browse/SPARK-21667 org.apache.spark.sql.AnalysisException: This query does not support recovering from checkpoint location. Delete /tmp/checkpoint/offsets to start over.; at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:222) at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:278) at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:284) ... 61 elided // Change the sink (= output format) to JSON val q = records. writeStream. format(\"json\"). option(\"path\", \"/tmp/json-sink\"). option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory trigger(Trigger.ProcessingTime(10.seconds)). start // Note the checkpoint directory in use 17/08/07 12:09:02 INFO StreamExecution: Starting [id = 02e00924-5f0d-4501-bcb8-80be8a8be385, runId = 5eba2576-dad6-4f95-9031-e72514475edc] with file:///tmp/checkpoint to store the query checkpoint. ... 17/08/07 12:09:02 INFO KafkaSource: GetBatch called with start = Some({\"topic1\":{\"0\":3}}), end = {\"topic1\":{\"0\":4}} 17/08/07 12:09:02 INFO KafkaSource: Partitions added: Map() 17/08/07 12:09:02 DEBUG KafkaSource: TopicPartitions: topic1-0 17/08/07 12:09:02 DEBUG KafkaSource: Sorted executors: 17/08/07 12:09:02 INFO KafkaSource: GetBatch generating RDD of offset range: KafkaSourceRDDOffsetRange(topic1-0,3,4,None) 17/08/07 12:09:03 DEBUG KafkaOffsetReader: Partitions assigned to consumer: [topic1-0]. Seeking to the end. 17/08/07 12:09:03 DEBUG KafkaOffsetReader: Got latest offsets for partition : Map(topic1-0 -> 4) 17/08/07 12:09:03 DEBUG KafkaSource: GetOffset: ArrayBuffer((topic1-0,4)) 17/08/07 12:09:03 DEBUG StreamExecution: getOffset took 122 ms 17/08/07 12:09:03 DEBUG StreamExecution: Resuming at batch 3 with committed offsets {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} and available offsets {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} 17/08/07 12:09:03 DEBUG StreamExecution: Stream running from {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} to {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} getOffset requests < > to fetchLatestOffsets (known later as latest ). NOTE: (Possible performance degradation?) It is possible that getOffset will request the latest offsets from Kafka twice, i.e. while initializing < > (when no metadata log is available and KafkaSource's < > is LatestOffsetRangeLimit ) and always as part of getOffset itself. getOffset then calculates < > based on the < > option. .getOffset's Offset Calculation per maxOffsetsPerTrigger [cols=\"1,1\",options=\"header\",width=\"100%\"] |=== | maxOffsetsPerTrigger | Offsets | Unspecified (i.e. None ) | latest | Defined (but < > is empty) | < > with limit limit, < > as from , until as latest | Defined (and < > contains partitions and offsets) | < > with limit limit, < > as from , until as latest |=== You should see the following DEBUG message in the logs: GetOffset: [offsets] In the end, getOffset creates a KafkaSourceOffset with offsets (as Map[TopicPartition, Long] ). === [[fetchAndVerify]] Fetching and Verifying Specific Offsets -- fetchAndVerify Internal Method","title":"Batch: 2"},{"location":"datasources/kafka/KafkaSource/#source-scala_2","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSource/#fetchandverifyspecificoffsets-maptopicpartition-long-kafkasourceoffset","text":"fetchAndVerify requests < > to fetchSpecificOffsets for the given specificOffsets . fetchAndVerify makes sure that the starting offsets in specificOffsets are the same as in Kafka and < > otherwise. startingOffsets for [tp] was [off] but consumer reset to [result(tp)] In the end, fetchAndVerify creates a KafkaSourceOffset (with the result of < >). NOTE: fetchAndVerify is used exclusively when KafkaSource initializes < >. === [[initialPartitionOffsets]] Initial Partition Offsets (of 0 th Batch) -- initialPartitionOffsets Internal Lazy Property","title":"fetchAndVerify(specificOffsets: Map[TopicPartition, Long]): KafkaSourceOffset"},{"location":"datasources/kafka/KafkaSource/#source-scala_3","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSource/#initialpartitionoffsets-maptopicpartition-long","text":"initialPartitionOffsets is the initial partition offsets for the batch 0 that were already persisted in the < > or persisted on demand. As the very first step, initialPartitionOffsets creates a custom HDFSMetadataLog (of KafkaSourceOffsets metadata) in the < >. initialPartitionOffsets requests the HDFSMetadataLog for the metadata of the 0 th batch (as KafkaSourceOffset ). If the metadata is available, initialPartitionOffsets requests the metadata for the collection of TopicPartitions and their offsets . If the metadata could not be found, initialPartitionOffsets creates a new KafkaSourceOffset per < >: For EarliestOffsetRangeLimit , initialPartitionOffsets requests the < > to fetchEarliestOffsets For LatestOffsetRangeLimit , initialPartitionOffsets requests the < > to fetchLatestOffsets For SpecificOffsetRangeLimit , initialPartitionOffsets requests the < > to fetchSpecificOffsets (and report a data loss per the < > flag) initialPartitionOffsets requests the custom HDFSMetadataLog to add the offsets to the metadata log (as the metadata of the 0 th batch). initialPartitionOffsets prints out the following INFO message to the logs: Initial offsets: [offsets] Note initialPartitionOffsets is used when KafkaSource is requested for the following: < > < > (when the start offsets are not defined, i.e. before StreamExecution commits the first streaming batch and so nothing is in committedOffsets registry for a KafkaSource data source yet) ==== [[initialPartitionOffsets-HDFSMetadataLog-serialize]] HDFSMetadataLog.serialize","title":"initialPartitionOffsets: Map[TopicPartition, Long]"},{"location":"datasources/kafka/KafkaSource/#source-scala_4","text":"serialize( metadata: KafkaSourceOffset, out: OutputStream): Unit serialize requests the OutputStream to write a zero byte (to support Spark 2.1.0 as per SPARK-19517). serialize creates a BufferedWriter over a OutputStreamWriter over the OutputStream (with UTF_8 charset encoding). serialize requests the BufferedWriter to write the v1 version indicator followed by a new line. serialize then requests the KafkaSourceOffset for a JSON-serialized representation and the BufferedWriter to write it out. In the end, serialize requests the BufferedWriter to flush (the underlying stream). serialize is part of the HDFSMetadataLog abstraction. === [[rateLimit]] rateLimit Internal Method","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSource/#source-scala_5","text":"rateLimit( limit: Long, from: Map[TopicPartition, Long], until: Map[TopicPartition, Long]): Map[TopicPartition, Long] rateLimit requests < > to fetchEarliestOffsets . CAUTION: FIXME NOTE: rateLimit is used exclusively when KafkaSource < > (when < > option is specified). === [[getSortedExecutorList]] getSortedExecutorList Method CAUTION: FIXME === [[reportDataLoss]] reportDataLoss Internal Method CAUTION: FIXME","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSource/#note","text":"reportDataLoss is used when KafkaSource does the following: < >","title":"[NOTE]"},{"location":"datasources/kafka/KafkaSource/#_1","text":"=== [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | currentPartitionOffsets | [[currentPartitionOffsets]] Current partition offsets (as Map[TopicPartition, Long] ) Initially NONE and set when KafkaSource is requested to < > or < >. | pollTimeoutMs a| [[pollTimeoutMs]] | sc a| [[sc]] Spark Core's SparkContext (of the < >) Used when: < > (and creating a KafkaSourceRDD ) Initializing the pollTimeoutMs internal property |===","title":"&lt;&gt;"},{"location":"datasources/kafka/KafkaSource/#logging","text":"Enable ALL logging level for org.apache.spark.sql.kafka010.KafkaSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaSource=ALL Refer to Logging .","title":"Logging"},{"location":"datasources/kafka/KafkaSourceInitialOffsetWriter/","text":"KafkaSourceInitialOffsetWriter \u00b6 KafkaSourceInitialOffsetWriter is a Hadoop DFS-based metadata storage for KafkaSourceOffsets . KafkaSourceInitialOffsetWriter is < > exclusively when KafkaMicroBatchReader is requested to getOrCreateInitialPartitionOffsets . [[VERSION]] KafkaSourceInitialOffsetWriter uses 1 for the version. Creating Instance \u00b6 KafkaSourceInitialOffsetWriter takes the following to be created: [[sparkSession]] SparkSession [[metadataPath]] Path of the metadata log directory === [[deserialize]] Deserializing Metadata (Reading Metadata from Serialized Format) -- deserialize Method [source, scala] \u00b6 deserialize( in: InputStream): KafkaSourceOffset deserialize ...FIXME deserialize is part of the HDFSMetadataLog abstraction.","title":"KafkaSourceInitialOffsetWriter"},{"location":"datasources/kafka/KafkaSourceInitialOffsetWriter/#kafkasourceinitialoffsetwriter","text":"KafkaSourceInitialOffsetWriter is a Hadoop DFS-based metadata storage for KafkaSourceOffsets . KafkaSourceInitialOffsetWriter is < > exclusively when KafkaMicroBatchReader is requested to getOrCreateInitialPartitionOffsets . [[VERSION]] KafkaSourceInitialOffsetWriter uses 1 for the version.","title":"KafkaSourceInitialOffsetWriter"},{"location":"datasources/kafka/KafkaSourceInitialOffsetWriter/#creating-instance","text":"KafkaSourceInitialOffsetWriter takes the following to be created: [[sparkSession]] SparkSession [[metadataPath]] Path of the metadata log directory === [[deserialize]] Deserializing Metadata (Reading Metadata from Serialized Format) -- deserialize Method","title":"Creating Instance"},{"location":"datasources/kafka/KafkaSourceInitialOffsetWriter/#source-scala","text":"deserialize( in: InputStream): KafkaSourceOffset deserialize ...FIXME deserialize is part of the HDFSMetadataLog abstraction.","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSourceOffset/","text":"KafkaSourceOffset \u00b6 KafkaSourceOffset is an Offset for kafka data source . KafkaSourceOffset is < > (directly or indirectly using < >) when: KafkaContinuousReader is requested to setStartOffset , deserializeOffset , and mergeOffsets KafkaMicroBatchReader is requested to getStartOffset , getEndOffset , deserializeOffset , and getOrCreateInitialPartitionOffsets KafkaOffsetReader is requested to fetchSpecificOffsets KafkaSource is requested for the initial partition offsets (of 0 th batch) and getOffset KafkaSourceInitialOffsetWriter is requested to deserialize a KafkaSourceOffset (from an InputStream) KafkaSourceOffset is requested for < > [[creating-instance]][[partitionToOffsets]] KafkaSourceOffset takes a collection of Kafka TopicPartitions with offsets to be created. === [[getPartitionOffsets]] Partition Offsets -- getPartitionOffsets Method [source, scala] \u00b6 getPartitionOffsets( offset: Offset): Map[TopicPartition, Long] getPartitionOffsets takes < > from offset . If offset is KafkaSourceOffset , getPartitionOffsets takes the partitions and offsets straight from it. If however offset is SerializedOffset , getPartitionOffsets deserializes the offsets from JSON. getPartitionOffsets reports an IllegalArgumentException when offset is neither KafkaSourceOffset or SerializedOffset . Invalid conversion from offset of [class] to KafkaSourceOffset getPartitionOffsets is used when: KafkaContinuousReader is requested to planInputPartitions KafkaSource is requested to generate a streaming DataFrame with records from Kafka for a streaming micro-batch === [[json]] JSON-Encoded Offset -- json Method [source, scala] \u00b6 json: String \u00b6 json is part of the Offset abstraction. json ...FIXME === [[apply]] Creating KafkaSourceOffset Instance -- apply Utility Method [source, scala] \u00b6 apply( offsetTuples: (String, Int, Long)*): KafkaSourceOffset // <1> apply( offset: SerializedOffset): KafkaSourceOffset <1> Used in tests only apply ...FIXME apply is used when: KafkaSourceInitialOffsetWriter is requested to deserialize a KafkaSourceOffset (from an InputStream) KafkaSource is requested for the initial partition offsets (of 0 th batch) KafkaSourceOffset is requested to getPartitionOffsets","title":"KafkaSourceOffset"},{"location":"datasources/kafka/KafkaSourceOffset/#kafkasourceoffset","text":"KafkaSourceOffset is an Offset for kafka data source . KafkaSourceOffset is < > (directly or indirectly using < >) when: KafkaContinuousReader is requested to setStartOffset , deserializeOffset , and mergeOffsets KafkaMicroBatchReader is requested to getStartOffset , getEndOffset , deserializeOffset , and getOrCreateInitialPartitionOffsets KafkaOffsetReader is requested to fetchSpecificOffsets KafkaSource is requested for the initial partition offsets (of 0 th batch) and getOffset KafkaSourceInitialOffsetWriter is requested to deserialize a KafkaSourceOffset (from an InputStream) KafkaSourceOffset is requested for < > [[creating-instance]][[partitionToOffsets]] KafkaSourceOffset takes a collection of Kafka TopicPartitions with offsets to be created. === [[getPartitionOffsets]] Partition Offsets -- getPartitionOffsets Method","title":"KafkaSourceOffset"},{"location":"datasources/kafka/KafkaSourceOffset/#source-scala","text":"getPartitionOffsets( offset: Offset): Map[TopicPartition, Long] getPartitionOffsets takes < > from offset . If offset is KafkaSourceOffset , getPartitionOffsets takes the partitions and offsets straight from it. If however offset is SerializedOffset , getPartitionOffsets deserializes the offsets from JSON. getPartitionOffsets reports an IllegalArgumentException when offset is neither KafkaSourceOffset or SerializedOffset . Invalid conversion from offset of [class] to KafkaSourceOffset getPartitionOffsets is used when: KafkaContinuousReader is requested to planInputPartitions KafkaSource is requested to generate a streaming DataFrame with records from Kafka for a streaming micro-batch === [[json]] JSON-Encoded Offset -- json Method","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSourceOffset/#source-scala_1","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSourceOffset/#json-string","text":"json is part of the Offset abstraction. json ...FIXME === [[apply]] Creating KafkaSourceOffset Instance -- apply Utility Method","title":"json: String"},{"location":"datasources/kafka/KafkaSourceOffset/#source-scala_2","text":"apply( offsetTuples: (String, Int, Long)*): KafkaSourceOffset // <1> apply( offset: SerializedOffset): KafkaSourceOffset <1> Used in tests only apply ...FIXME apply is used when: KafkaSourceInitialOffsetWriter is requested to deserialize a KafkaSourceOffset (from an InputStream) KafkaSource is requested for the initial partition offsets (of 0 th batch) KafkaSourceOffset is requested to getPartitionOffsets","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSourceProvider/","text":"KafkaSourceProvider \u00b6 KafkaSourceProvider is a DataSourceRegister that registers kafka data source alias. The Internals of Spark SQL Read up on DataSourceRegister in The Internals of Spark SQL book. KafkaSourceProvider supports micro-batch stream processing (through MicroBatchStream ) and uses a specialized KafkaMicroBatchReader . Properties of Kafka Consumers on Executors \u00b6 ConsumerConfig's Key Value KEY_DESERIALIZER_CLASS_CONFIG ByteArrayDeserializer VALUE_DESERIALIZER_CLASS_CONFIG ByteArrayDeserializer AUTO_OFFSET_RESET_CONFIG none GROUP_ID_CONFIG uniqueGroupId -executor ENABLE_AUTO_COMMIT_CONFIG false RECEIVE_BUFFER_CONFIG 65536 Required Options \u00b6 KafkaSourceProvider requires the following options (that you can set using option method of DataStreamReader or DataStreamWriter ): Exactly one of the following options: subscribe , subscribePattern or assign kafka.bootstrap.servers Tip Refer to Kafka Data Source's Options for the supported configuration options. Creating KafkaTable \u00b6 getTable ( options : CaseInsensitiveStringMap ): KafkaTable getTable creates a KafkaTable with the value of includeheaders option (default: false ). getTable is part of the SimpleTableProvider abstraction (Spark SQL). Creating Streaming Sink \u00b6 createSink ( sqlContext : SQLContext , parameters : Map [ String , String ], partitionColumns : Seq [ String ], outputMode : OutputMode ): Sink createSink creates a KafkaSink for topic option (if defined) and Kafka Producer parameters . createSink is part of the StreamSinkProvider abstraction. Creating Streaming Source \u00b6 createSource ( sqlContext : SQLContext , metadataPath : String , schema : Option [ StructType ], providerName : String , parameters : Map [ String , String ]): Source createSource validates stream options . createSource ...FIXME createSource is part of the StreamSourceProvider abstraction. Validating Options For Batch And Streaming Queries \u00b6 validateGeneralOptions ( parameters : Map [ String , String ]): Unit Note Parameters are case-insensitive, i.e. OptioN and option are equal. validateGeneralOptions makes sure that exactly one topic subscription strategy is used in parameters and can be: subscribe subscribepattern assign validateGeneralOptions reports an IllegalArgumentException when there is no subscription strategy in use or there are more than one strategies used. validateGeneralOptions makes sure that the value of subscription strategies meet the requirements: assign strategy starts with { (the opening curly brace) subscribe strategy has at least one topic (in a comma-separated list of topics) subscribepattern strategy has the pattern defined validateGeneralOptions makes sure that group.id has not been specified and reports an IllegalArgumentException otherwise. Kafka option 'group.id' is not supported as user-specified consumer groups are not used to track offsets. validateGeneralOptions makes sure that auto.offset.reset has not been specified and reports an IllegalArgumentException otherwise. Kafka option 'auto.offset.reset' is not supported. Instead set the source option 'startingoffsets' to 'earliest' or 'latest' to specify where to start. Structured Streaming manages which offsets are consumed internally, rather than relying on the kafkaConsumer to do it. This will ensure that no data is missed when new topics/partitions are dynamically subscribed. Note that 'startingoffsets' only applies when a new Streaming query is started, and that resuming will always pick up from where the query left off. See the docs for more details. validateGeneralOptions makes sure that the following options have not been specified and reports an IllegalArgumentException otherwise: kafka.key.deserializer kafka.value.deserializer kafka.enable.auto.commit kafka.interceptor.classes In the end, validateGeneralOptions makes sure that kafka.bootstrap.servers option was specified and reports an IllegalArgumentException otherwise. Option 'kafka.bootstrap.servers' must be specified for configuring Kafka consumer validateGeneralOptions is used when KafkaSourceProvider validates options for streaming and batch queries. Creating ConsumerStrategy \u00b6 strategy ( caseInsensitiveParams : Map [ String , String ]): ConsumerStrategy strategy converts a key (in caseInsensitiveParams ) to a ConsumerStrategy . Key ConsumerStrategy assign AssignStrategy subscribe SubscribeStrategy subscribepattern SubscribePatternStrategy strategy is used when...FIXME AssignStrategy \u00b6 AssignStrategy with Kafka TopicPartitions strategy uses JsonUtils.partitions method to parse a JSON with topic names and partitions, e.g. {\"topicA\":[0,1],\"topicB\":[0,1]} The topic names and partitions are mapped directly to Kafka's TopicPartition objects. SubscribeStrategy \u00b6 SubscribeStrategy with topic names strategy extracts topic names from a comma-separated string, e.g. topic1,topic2,topic3 SubscribePatternStrategy \u00b6 SubscribePatternStrategy with topic subscription regex pattern (that uses a Java java.util.regex.Pattern for the pattern), e.g. topic\\d Name and Schema of Streaming Source \u00b6 sourceSchema ( sqlContext : SQLContext , schema : Option [ StructType ], providerName : String , parameters : Map [ String , String ]): ( String , StructType ) sourceSchema gives the short name (i.e. kafka ) and the fixed schema . Internally, sourceSchema validates Kafka options and makes sure that the optional input schema is indeed undefined. When the input schema is defined, sourceSchema reports a IllegalArgumentException . Kafka source has a fixed schema and cannot be set with a custom one sourceSchema is part of the StreamSourceProvider abstraction. Validating Kafka Options for Streaming Queries \u00b6 validateStreamOptions ( caseInsensitiveParams : Map [ String , String ]): Unit validateStreamOptions makes sure that endingoffsets option is not used. Otherwise, validateStreamOptions reports a IllegalArgumentException . ending offset not valid in streaming queries validateStreamOptions validates the general options . validateStreamOptions is used when KafkaSourceProvider is requested for the schema for Kafka source and to create a KafkaSource . Converting Configuration Options to KafkaOffsetRangeLimit \u00b6 getKafkaOffsetRangeLimit ( params : Map [ String , String ], offsetOptionKey : String , defaultOffsets : KafkaOffsetRangeLimit ): KafkaOffsetRangeLimit getKafkaOffsetRangeLimit finds the given offsetOptionKey in the params and does the following conversion: latest becomes LatestOffsetRangeLimit earliest becomes EarliestOffsetRangeLimit A JSON-formatted text becomes SpecificOffsetRangeLimit When the given offsetOptionKey is not found, getKafkaOffsetRangeLimit returns the given defaultOffsets getKafkaOffsetRangeLimit is used when: KafkaSourceProvider is requested to createSource , createMicroBatchReader , createContinuousReader , createRelation , and validateBatchOptions Creating Fake BaseRelation \u00b6 createRelation ( sqlContext : SQLContext , parameters : Map [ String , String ]): BaseRelation createRelation ...FIXME createRelation is part of the RelationProvider abstraction (Spark SQL). Validating Configuration Options for Batch Processing \u00b6 validateBatchOptions ( caseInsensitiveParams : Map [ String , String ]): Unit validateBatchOptions ...FIXME validateBatchOptions is used when KafkaSourceProvider is requested to createSource . Configuration Properties \u00b6 failOnDataLoss \u00b6 failOnDataLoss ( caseInsensitiveParams : Map [ String , String ]): Boolean failOnDataLoss looks up the failOnDataLoss configuration property (in the caseInsensitiveParams ) or defaults to true . Utilities \u00b6 kafkaParamsForDriver \u00b6 kafkaParamsForDriver ( specifiedKafkaParams : Map [ String , String ]): Map [ String , Object ] kafkaParamsForDriver ...FIXME kafkaParamsForDriver is used when: KafkaBatch is requested to planInputPartitions KafkaRelation is requested to buildScan KafkaSourceProvider is requested for a streaming source KafkaScan is requested for a MicroBatchStream and ContinuousStream kafkaParamsForExecutors \u00b6 kafkaParamsForExecutors ( specifiedKafkaParams : Map [ String , String ], uniqueGroupId : String ): Map [ String , Object ] kafkaParamsForExecutors sets the Kafka properties for executors. While setting the properties, kafkaParamsForExecutors prints out the following DEBUG message to the logs: executor: Set [key] to [value], earlier value: [value] kafkaParamsForExecutors is used when: KafkaSourceProvider is requested to createSource (for a KafkaSource ), createMicroBatchReader (for a KafkaMicroBatchReader ), and createContinuousReader (for a KafkaContinuousReader ) KafkaRelation is requested to buildScan (for a KafkaSourceRDD ) Kafka Producer Parameters \u00b6 kafkaParamsForProducer ( params : CaseInsensitiveMap [ String ]): ju . Map [ String , Object ] kafkaParamsForProducer ...FIXME kafkaParamsForProducer is used when: KafkaSourceProvider is requested for a streaming sink or relation KafkaTable is requested for a WriteBuilder Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.kafka010.KafkaSourceProvider logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaSourceProvider=ALL Refer to Logging .","title":"KafkaSourceProvider"},{"location":"datasources/kafka/KafkaSourceProvider/#kafkasourceprovider","text":"KafkaSourceProvider is a DataSourceRegister that registers kafka data source alias. The Internals of Spark SQL Read up on DataSourceRegister in The Internals of Spark SQL book. KafkaSourceProvider supports micro-batch stream processing (through MicroBatchStream ) and uses a specialized KafkaMicroBatchReader .","title":"KafkaSourceProvider"},{"location":"datasources/kafka/KafkaSourceProvider/#properties-of-kafka-consumers-on-executors","text":"ConsumerConfig's Key Value KEY_DESERIALIZER_CLASS_CONFIG ByteArrayDeserializer VALUE_DESERIALIZER_CLASS_CONFIG ByteArrayDeserializer AUTO_OFFSET_RESET_CONFIG none GROUP_ID_CONFIG uniqueGroupId -executor ENABLE_AUTO_COMMIT_CONFIG false RECEIVE_BUFFER_CONFIG 65536","title":"Properties of Kafka Consumers on Executors"},{"location":"datasources/kafka/KafkaSourceProvider/#required-options","text":"KafkaSourceProvider requires the following options (that you can set using option method of DataStreamReader or DataStreamWriter ): Exactly one of the following options: subscribe , subscribePattern or assign kafka.bootstrap.servers Tip Refer to Kafka Data Source's Options for the supported configuration options.","title":"Required Options"},{"location":"datasources/kafka/KafkaSourceProvider/#creating-kafkatable","text":"getTable ( options : CaseInsensitiveStringMap ): KafkaTable getTable creates a KafkaTable with the value of includeheaders option (default: false ). getTable is part of the SimpleTableProvider abstraction (Spark SQL).","title":" Creating KafkaTable"},{"location":"datasources/kafka/KafkaSourceProvider/#creating-streaming-sink","text":"createSink ( sqlContext : SQLContext , parameters : Map [ String , String ], partitionColumns : Seq [ String ], outputMode : OutputMode ): Sink createSink creates a KafkaSink for topic option (if defined) and Kafka Producer parameters . createSink is part of the StreamSinkProvider abstraction.","title":" Creating Streaming Sink"},{"location":"datasources/kafka/KafkaSourceProvider/#creating-streaming-source","text":"createSource ( sqlContext : SQLContext , metadataPath : String , schema : Option [ StructType ], providerName : String , parameters : Map [ String , String ]): Source createSource validates stream options . createSource ...FIXME createSource is part of the StreamSourceProvider abstraction.","title":" Creating Streaming Source"},{"location":"datasources/kafka/KafkaSourceProvider/#validating-options-for-batch-and-streaming-queries","text":"validateGeneralOptions ( parameters : Map [ String , String ]): Unit Note Parameters are case-insensitive, i.e. OptioN and option are equal. validateGeneralOptions makes sure that exactly one topic subscription strategy is used in parameters and can be: subscribe subscribepattern assign validateGeneralOptions reports an IllegalArgumentException when there is no subscription strategy in use or there are more than one strategies used. validateGeneralOptions makes sure that the value of subscription strategies meet the requirements: assign strategy starts with { (the opening curly brace) subscribe strategy has at least one topic (in a comma-separated list of topics) subscribepattern strategy has the pattern defined validateGeneralOptions makes sure that group.id has not been specified and reports an IllegalArgumentException otherwise. Kafka option 'group.id' is not supported as user-specified consumer groups are not used to track offsets. validateGeneralOptions makes sure that auto.offset.reset has not been specified and reports an IllegalArgumentException otherwise. Kafka option 'auto.offset.reset' is not supported. Instead set the source option 'startingoffsets' to 'earliest' or 'latest' to specify where to start. Structured Streaming manages which offsets are consumed internally, rather than relying on the kafkaConsumer to do it. This will ensure that no data is missed when new topics/partitions are dynamically subscribed. Note that 'startingoffsets' only applies when a new Streaming query is started, and that resuming will always pick up from where the query left off. See the docs for more details. validateGeneralOptions makes sure that the following options have not been specified and reports an IllegalArgumentException otherwise: kafka.key.deserializer kafka.value.deserializer kafka.enable.auto.commit kafka.interceptor.classes In the end, validateGeneralOptions makes sure that kafka.bootstrap.servers option was specified and reports an IllegalArgumentException otherwise. Option 'kafka.bootstrap.servers' must be specified for configuring Kafka consumer validateGeneralOptions is used when KafkaSourceProvider validates options for streaming and batch queries.","title":" Validating Options For Batch And Streaming Queries"},{"location":"datasources/kafka/KafkaSourceProvider/#creating-consumerstrategy","text":"strategy ( caseInsensitiveParams : Map [ String , String ]): ConsumerStrategy strategy converts a key (in caseInsensitiveParams ) to a ConsumerStrategy . Key ConsumerStrategy assign AssignStrategy subscribe SubscribeStrategy subscribepattern SubscribePatternStrategy strategy is used when...FIXME","title":" Creating ConsumerStrategy"},{"location":"datasources/kafka/KafkaSourceProvider/#assignstrategy","text":"AssignStrategy with Kafka TopicPartitions strategy uses JsonUtils.partitions method to parse a JSON with topic names and partitions, e.g. {\"topicA\":[0,1],\"topicB\":[0,1]} The topic names and partitions are mapped directly to Kafka's TopicPartition objects.","title":" AssignStrategy"},{"location":"datasources/kafka/KafkaSourceProvider/#subscribestrategy","text":"SubscribeStrategy with topic names strategy extracts topic names from a comma-separated string, e.g. topic1,topic2,topic3","title":" SubscribeStrategy"},{"location":"datasources/kafka/KafkaSourceProvider/#subscribepatternstrategy","text":"SubscribePatternStrategy with topic subscription regex pattern (that uses a Java java.util.regex.Pattern for the pattern), e.g. topic\\d","title":" SubscribePatternStrategy"},{"location":"datasources/kafka/KafkaSourceProvider/#name-and-schema-of-streaming-source","text":"sourceSchema ( sqlContext : SQLContext , schema : Option [ StructType ], providerName : String , parameters : Map [ String , String ]): ( String , StructType ) sourceSchema gives the short name (i.e. kafka ) and the fixed schema . Internally, sourceSchema validates Kafka options and makes sure that the optional input schema is indeed undefined. When the input schema is defined, sourceSchema reports a IllegalArgumentException . Kafka source has a fixed schema and cannot be set with a custom one sourceSchema is part of the StreamSourceProvider abstraction.","title":" Name and Schema of Streaming Source"},{"location":"datasources/kafka/KafkaSourceProvider/#validating-kafka-options-for-streaming-queries","text":"validateStreamOptions ( caseInsensitiveParams : Map [ String , String ]): Unit validateStreamOptions makes sure that endingoffsets option is not used. Otherwise, validateStreamOptions reports a IllegalArgumentException . ending offset not valid in streaming queries validateStreamOptions validates the general options . validateStreamOptions is used when KafkaSourceProvider is requested for the schema for Kafka source and to create a KafkaSource .","title":" Validating Kafka Options for Streaming Queries"},{"location":"datasources/kafka/KafkaSourceProvider/#converting-configuration-options-to-kafkaoffsetrangelimit","text":"getKafkaOffsetRangeLimit ( params : Map [ String , String ], offsetOptionKey : String , defaultOffsets : KafkaOffsetRangeLimit ): KafkaOffsetRangeLimit getKafkaOffsetRangeLimit finds the given offsetOptionKey in the params and does the following conversion: latest becomes LatestOffsetRangeLimit earliest becomes EarliestOffsetRangeLimit A JSON-formatted text becomes SpecificOffsetRangeLimit When the given offsetOptionKey is not found, getKafkaOffsetRangeLimit returns the given defaultOffsets getKafkaOffsetRangeLimit is used when: KafkaSourceProvider is requested to createSource , createMicroBatchReader , createContinuousReader , createRelation , and validateBatchOptions","title":" Converting Configuration Options to KafkaOffsetRangeLimit"},{"location":"datasources/kafka/KafkaSourceProvider/#creating-fake-baserelation","text":"createRelation ( sqlContext : SQLContext , parameters : Map [ String , String ]): BaseRelation createRelation ...FIXME createRelation is part of the RelationProvider abstraction (Spark SQL).","title":" Creating Fake BaseRelation"},{"location":"datasources/kafka/KafkaSourceProvider/#validating-configuration-options-for-batch-processing","text":"validateBatchOptions ( caseInsensitiveParams : Map [ String , String ]): Unit validateBatchOptions ...FIXME validateBatchOptions is used when KafkaSourceProvider is requested to createSource .","title":" Validating Configuration Options for Batch Processing"},{"location":"datasources/kafka/KafkaSourceProvider/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"datasources/kafka/KafkaSourceProvider/#failondataloss","text":"failOnDataLoss ( caseInsensitiveParams : Map [ String , String ]): Boolean failOnDataLoss looks up the failOnDataLoss configuration property (in the caseInsensitiveParams ) or defaults to true .","title":" failOnDataLoss"},{"location":"datasources/kafka/KafkaSourceProvider/#utilities","text":"","title":"Utilities"},{"location":"datasources/kafka/KafkaSourceProvider/#kafkaparamsfordriver","text":"kafkaParamsForDriver ( specifiedKafkaParams : Map [ String , String ]): Map [ String , Object ] kafkaParamsForDriver ...FIXME kafkaParamsForDriver is used when: KafkaBatch is requested to planInputPartitions KafkaRelation is requested to buildScan KafkaSourceProvider is requested for a streaming source KafkaScan is requested for a MicroBatchStream and ContinuousStream","title":" kafkaParamsForDriver"},{"location":"datasources/kafka/KafkaSourceProvider/#kafkaparamsforexecutors","text":"kafkaParamsForExecutors ( specifiedKafkaParams : Map [ String , String ], uniqueGroupId : String ): Map [ String , Object ] kafkaParamsForExecutors sets the Kafka properties for executors. While setting the properties, kafkaParamsForExecutors prints out the following DEBUG message to the logs: executor: Set [key] to [value], earlier value: [value] kafkaParamsForExecutors is used when: KafkaSourceProvider is requested to createSource (for a KafkaSource ), createMicroBatchReader (for a KafkaMicroBatchReader ), and createContinuousReader (for a KafkaContinuousReader ) KafkaRelation is requested to buildScan (for a KafkaSourceRDD )","title":" kafkaParamsForExecutors"},{"location":"datasources/kafka/KafkaSourceProvider/#kafka-producer-parameters","text":"kafkaParamsForProducer ( params : CaseInsensitiveMap [ String ]): ju . Map [ String , Object ] kafkaParamsForProducer ...FIXME kafkaParamsForProducer is used when: KafkaSourceProvider is requested for a streaming sink or relation KafkaTable is requested for a WriteBuilder","title":" Kafka Producer Parameters"},{"location":"datasources/kafka/KafkaSourceProvider/#logging","text":"Enable ALL logging level for org.apache.spark.sql.kafka010.KafkaSourceProvider logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaSourceProvider=ALL Refer to Logging .","title":"Logging"},{"location":"datasources/kafka/KafkaSourceRDD/","text":"KafkaSourceRDD \u00b6 KafkaSourceRDD is an RDD of Kafka's ConsumerRecords ( RDD[ConsumerRecord[Array[Byte], Array[Byte]]] ) and no parent RDDs. KafkaSourceRDD is < > when: KafkaRelation is requested to build a distributed data scan with column pruning KafkaSource is requested to generate a streaming DataFrame with records from Kafka for a streaming micro-batch Creating Instance \u00b6 KafkaSourceRDD takes the following when created: [[sc]] SparkContext [[executorKafkaParams]] Collection of key-value settings for executors reading records from Kafka topics [[offsetRanges]] Collection of KafkaSourceRDDOffsetRange offsets [[pollTimeoutMs]] Timeout (in milliseconds) to poll data from Kafka + Used when KafkaSourceRDD < > (for given offsets) and in turn requests the CachedKafkaConsumer to poll for records . [[failOnDataLoss]] Flag to...FIXME [[reuseKafkaConsumer]] Flag to...FIXME === [[getPreferredLocations]] Placement Preferences of Partition (Preferred Locations) -- getPreferredLocations Method [source, scala] \u00b6 getPreferredLocations( split: Partition): Seq[String] NOTE: getPreferredLocations is part of the RDD contract to specify placement preferences. getPreferredLocations converts the given Partition to a KafkaSourceRDDPartition and...FIXME === [[compute]] Computing Partition -- compute Method [source, scala] \u00b6 compute( thePart: Partition, context: TaskContext ): Iterator[ConsumerRecord[Array[Byte], Array[Byte]]] NOTE: compute is part of the RDD contract to compute a given partition. compute uses KafkaDataConsumer utility to acquire a cached KafkaDataConsumer (for a partition). compute < > (based on the offsetRange of the given partition that is assumed a KafkaSourceRDDPartition ). compute returns a NextIterator so that getNext uses the KafkaDataConsumer to get a record . When the beginning and ending offsets (of the offset range) are equal, compute prints out the following INFO message to the logs, requests the KafkaDataConsumer to release and returns an empty iterator. Beginning offset [fromOffset] is the same as ending offset skipping [topic] [partition] compute throws an AssertionError when the beginning offset ( fromOffset ) is after the ending offset ( untilOffset ): [options=\"wrap\"] \u00b6 Beginning offset [fromOffset] is after the ending offset [untilOffset] for topic [topic] partition [partition]. You either provided an invalid fromOffset, or the Kafka topic has been damaged \u00b6 === [[getPartitions]] getPartitions Method [source, scala] \u00b6 getPartitions: Array[Partition] \u00b6 NOTE: getPartitions is part of the RDD contract to...FIXME. getPartitions ...FIXME === [[persist]] Persisting RDD -- persist Method [source, scala] \u00b6 persist: Array[Partition] \u00b6 NOTE: persist is part of the RDD contract to persist an RDD. persist ...FIXME === [[resolveRange]] resolveRange Internal Method [source, scala] \u00b6 resolveRange( consumer: KafkaDataConsumer, range: KafkaSourceRDDOffsetRange ): KafkaSourceRDDOffsetRange resolveRange ...FIXME NOTE: resolveRange is used when...FIXME","title":"KafkaSourceRDD"},{"location":"datasources/kafka/KafkaSourceRDD/#kafkasourcerdd","text":"KafkaSourceRDD is an RDD of Kafka's ConsumerRecords ( RDD[ConsumerRecord[Array[Byte], Array[Byte]]] ) and no parent RDDs. KafkaSourceRDD is < > when: KafkaRelation is requested to build a distributed data scan with column pruning KafkaSource is requested to generate a streaming DataFrame with records from Kafka for a streaming micro-batch","title":"KafkaSourceRDD"},{"location":"datasources/kafka/KafkaSourceRDD/#creating-instance","text":"KafkaSourceRDD takes the following when created: [[sc]] SparkContext [[executorKafkaParams]] Collection of key-value settings for executors reading records from Kafka topics [[offsetRanges]] Collection of KafkaSourceRDDOffsetRange offsets [[pollTimeoutMs]] Timeout (in milliseconds) to poll data from Kafka + Used when KafkaSourceRDD < > (for given offsets) and in turn requests the CachedKafkaConsumer to poll for records . [[failOnDataLoss]] Flag to...FIXME [[reuseKafkaConsumer]] Flag to...FIXME === [[getPreferredLocations]] Placement Preferences of Partition (Preferred Locations) -- getPreferredLocations Method","title":"Creating Instance"},{"location":"datasources/kafka/KafkaSourceRDD/#source-scala","text":"getPreferredLocations( split: Partition): Seq[String] NOTE: getPreferredLocations is part of the RDD contract to specify placement preferences. getPreferredLocations converts the given Partition to a KafkaSourceRDDPartition and...FIXME === [[compute]] Computing Partition -- compute Method","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSourceRDD/#source-scala_1","text":"compute( thePart: Partition, context: TaskContext ): Iterator[ConsumerRecord[Array[Byte], Array[Byte]]] NOTE: compute is part of the RDD contract to compute a given partition. compute uses KafkaDataConsumer utility to acquire a cached KafkaDataConsumer (for a partition). compute < > (based on the offsetRange of the given partition that is assumed a KafkaSourceRDDPartition ). compute returns a NextIterator so that getNext uses the KafkaDataConsumer to get a record . When the beginning and ending offsets (of the offset range) are equal, compute prints out the following INFO message to the logs, requests the KafkaDataConsumer to release and returns an empty iterator. Beginning offset [fromOffset] is the same as ending offset skipping [topic] [partition] compute throws an AssertionError when the beginning offset ( fromOffset ) is after the ending offset ( untilOffset ):","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSourceRDD/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"datasources/kafka/KafkaSourceRDD/#beginning-offset-fromoffset-is-after-the-ending-offset-untiloffset-for-topic-topic-partition-partition-you-either-provided-an-invalid-fromoffset-or-the-kafka-topic-has-been-damaged","text":"=== [[getPartitions]] getPartitions Method","title":"Beginning offset [fromOffset] is after the ending offset [untilOffset] for topic [topic] partition [partition]. You either provided an invalid fromOffset, or the Kafka topic has been damaged"},{"location":"datasources/kafka/KafkaSourceRDD/#source-scala_2","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSourceRDD/#getpartitions-arraypartition","text":"NOTE: getPartitions is part of the RDD contract to...FIXME. getPartitions ...FIXME === [[persist]] Persisting RDD -- persist Method","title":"getPartitions: Array[Partition]"},{"location":"datasources/kafka/KafkaSourceRDD/#source-scala_3","text":"","title":"[source, scala]"},{"location":"datasources/kafka/KafkaSourceRDD/#persist-arraypartition","text":"NOTE: persist is part of the RDD contract to persist an RDD. persist ...FIXME === [[resolveRange]] resolveRange Internal Method","title":"persist: Array[Partition]"},{"location":"datasources/kafka/KafkaSourceRDD/#source-scala_4","text":"resolveRange( consumer: KafkaDataConsumer, range: KafkaSourceRDDOffsetRange ): KafkaSourceRDDOffsetRange resolveRange ...FIXME NOTE: resolveRange is used when...FIXME","title":"[source, scala]"},{"location":"datasources/kafka/KafkaTable/","text":"KafkaTable \u00b6 KafkaTable is...FIXME","title":"KafkaTable"},{"location":"datasources/kafka/KafkaTable/#kafkatable","text":"KafkaTable is...FIXME","title":"KafkaTable"},{"location":"datasources/memory/","text":"Memory Data Source \u00b6 Memory Data Source is made up of the following two base implementations to support the older DataSource API V1 and the modern DataSource API V2: MemoryStreamBase MemorySinkBase Memory data source supports Micro-Batch and Continuous stream processing modes. [cols=\"30,35,35\",options=\"header\",width=\"100%\"] |=== | Stream Processing | Source | Sink | Micro-Batch | MemoryStream | MemorySink | Continuous | ContinuousMemoryStream | MemorySinkV2 |=== [CAUTION] \u00b6 Memory Data Source is not for production use due to design contraints, e.g. infinite in-memory collection of lines read and no fault recovery. MemoryStream is designed primarily for unit tests, tutorials and debugging. \u00b6 === [[memory-sink]] Memory Sink Memory sink requires that a streaming query has a name (defined using DataStreamWriter.queryName or queryName option). Memory sink may optionally define checkpoint location using checkpointLocation option that is used to recover from for Complete output mode only. Memory Sink and CreateViewCommand \u00b6 When a streaming query with memory sink is started , DataStreamWriter uses Dataset.createOrReplaceTempView operator to create or replace a local temporary view with the name of the query (which is required). Examples \u00b6 .Memory Source in Micro-Batch Stream Processing [source, scala] val spark: SparkSession = ??? implicit val ctx = spark.sqlContext import org.apache.spark.sql.execution.streaming.MemoryStream // It uses two implicits: Encoder[Int] and SQLContext val intsIn = MemoryStream[Int] val ints = intsIn.toDF .withColumn(\"t\", current_timestamp()) .withWatermark(\"t\", \"5 minutes\") .groupBy(window($\"t\", \"5 minutes\") as \"window\") .agg(count(\"*\") as \"total\") import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val totalsOver5mins = ints. writeStream. format(\"memory\"). queryName(\"totalsOver5mins\"). outputMode(OutputMode.Append). trigger(Trigger.ProcessingTime(10.seconds)). start val zeroOffset = intsIn.addData(0, 1, 2) totalsOver5mins.processAllAvailable() spark.table(\"totalsOver5mins\").show scala> intsOut.show +-----+ |value| +-----+ | 0| | 1| | 2| +-----+ memoryQuery.stop() \u00b6 .Memory Sink in Micro-Batch Stream Processing [source, scala] val queryName = \"memoryDemo\" val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"memory\") .queryName(queryName) .start // The name of the streaming query is an in-memory table val showAll = sql(s\"select * from $queryName\") scala> showAll.show(truncate = false) +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2019-10-10 15:19:16.431|42 | |2019-10-10 15:19:17.431|43 | +-----------------------+-----+ import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val se = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery import org.apache.spark.sql.execution.streaming.MemorySink val sink = se.sink.asInstanceOf[MemorySink] assert(sink.toString == \"MemorySink\") sink.clear() \u00b6","title":"Memory Data Source"},{"location":"datasources/memory/#memory-data-source","text":"Memory Data Source is made up of the following two base implementations to support the older DataSource API V1 and the modern DataSource API V2: MemoryStreamBase MemorySinkBase Memory data source supports Micro-Batch and Continuous stream processing modes. [cols=\"30,35,35\",options=\"header\",width=\"100%\"] |=== | Stream Processing | Source | Sink | Micro-Batch | MemoryStream | MemorySink | Continuous | ContinuousMemoryStream | MemorySinkV2 |===","title":"Memory Data Source"},{"location":"datasources/memory/#caution","text":"Memory Data Source is not for production use due to design contraints, e.g. infinite in-memory collection of lines read and no fault recovery.","title":"[CAUTION]"},{"location":"datasources/memory/#memorystream-is-designed-primarily-for-unit-tests-tutorials-and-debugging","text":"=== [[memory-sink]] Memory Sink Memory sink requires that a streaming query has a name (defined using DataStreamWriter.queryName or queryName option). Memory sink may optionally define checkpoint location using checkpointLocation option that is used to recover from for Complete output mode only.","title":"MemoryStream is designed primarily for unit tests, tutorials and debugging."},{"location":"datasources/memory/#memory-sink-and-createviewcommand","text":"When a streaming query with memory sink is started , DataStreamWriter uses Dataset.createOrReplaceTempView operator to create or replace a local temporary view with the name of the query (which is required).","title":"Memory Sink and CreateViewCommand"},{"location":"datasources/memory/#examples","text":".Memory Source in Micro-Batch Stream Processing [source, scala] val spark: SparkSession = ??? implicit val ctx = spark.sqlContext import org.apache.spark.sql.execution.streaming.MemoryStream // It uses two implicits: Encoder[Int] and SQLContext val intsIn = MemoryStream[Int] val ints = intsIn.toDF .withColumn(\"t\", current_timestamp()) .withWatermark(\"t\", \"5 minutes\") .groupBy(window($\"t\", \"5 minutes\") as \"window\") .agg(count(\"*\") as \"total\") import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val totalsOver5mins = ints. writeStream. format(\"memory\"). queryName(\"totalsOver5mins\"). outputMode(OutputMode.Append). trigger(Trigger.ProcessingTime(10.seconds)). start val zeroOffset = intsIn.addData(0, 1, 2) totalsOver5mins.processAllAvailable() spark.table(\"totalsOver5mins\").show scala> intsOut.show +-----+ |value| +-----+ | 0| | 1| | 2| +-----+","title":"Examples"},{"location":"datasources/memory/#memoryquerystop","text":".Memory Sink in Micro-Batch Stream Processing [source, scala] val queryName = \"memoryDemo\" val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"memory\") .queryName(queryName) .start // The name of the streaming query is an in-memory table val showAll = sql(s\"select * from $queryName\") scala> showAll.show(truncate = false) +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2019-10-10 15:19:16.431|42 | |2019-10-10 15:19:17.431|43 | +-----------------------+-----+ import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val se = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery import org.apache.spark.sql.execution.streaming.MemorySink val sink = se.sink.asInstanceOf[MemorySink] assert(sink.toString == \"MemorySink\")","title":"memoryQuery.stop()"},{"location":"datasources/memory/#sinkclear","text":"","title":"sink.clear()"},{"location":"datasources/memory/ContinuousMemoryStream/","text":"== [[ContinuousMemoryStream]] ContinuousMemoryStream ContinuousMemoryStream is...FIXME","title":"ContinuousMemoryStream"},{"location":"datasources/memory/MemoryPlan/","text":"MemoryPlan Logical Operator \u00b6 MemoryPlan is a leaf logical operator (i.e. LogicalPlan ) that is used to query the data that has been written into a MemorySink . MemoryPlan is created when starting continuous writing (to a MemorySink ). TIP: See the example in MemoryStream . scala> intsOut.explain(true) == Parsed Logical Plan == SubqueryAlias memstream +- MemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21] == Analyzed Logical Plan == value: int SubqueryAlias memstream +- MemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21] == Optimized Logical Plan == MemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21] == Physical Plan == LocalTableScan [value#21] When executed, MemoryPlan is translated to LocalTableScanExec physical operator (similar to LocalRelation logical operator) in BasicOperators execution planning strategy.","title":"MemoryPlan"},{"location":"datasources/memory/MemoryPlan/#memoryplan-logical-operator","text":"MemoryPlan is a leaf logical operator (i.e. LogicalPlan ) that is used to query the data that has been written into a MemorySink . MemoryPlan is created when starting continuous writing (to a MemorySink ). TIP: See the example in MemoryStream . scala> intsOut.explain(true) == Parsed Logical Plan == SubqueryAlias memstream +- MemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21] == Analyzed Logical Plan == value: int SubqueryAlias memstream +- MemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21] == Optimized Logical Plan == MemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21] == Physical Plan == LocalTableScan [value#21] When executed, MemoryPlan is translated to LocalTableScanExec physical operator (similar to LocalRelation logical operator) in BasicOperators execution planning strategy.","title":"MemoryPlan Logical Operator"},{"location":"datasources/memory/MemorySink/","text":"MemorySink \u00b6 MemorySink is a streaming sink that < >. MemorySink is intended only for testing or demos. MemorySink is used for memory format and requires a query name (by queryName method or queryName option). NOTE: MemorySink was introduced in the https://github.com/apache/spark/pull/12119[pull request for [SPARK-14288][SQL] Memory Sink for streaming]. Use toDebugString to see the batches. Its aim is to allow users to test streaming applications in the Spark shell or other local tests. You can set checkpointLocation using option method or it will be set to spark.sql.streaming.checkpointLocation property. If spark.sql.streaming.checkpointLocation is set, the code uses $location/$queryName directory. Finally, when no spark.sql.streaming.checkpointLocation is set, a temporary directory memory.stream under java.io.tmpdir is used with offsets subdirectory inside. NOTE: The directory is cleaned up at shutdown using ShutdownHookManager.registerShutdownDeleteDir . It creates MemorySink instance based on the schema of the DataFrame it operates on. It creates a new DataFrame using MemoryPlan with MemorySink instance created earlier and registers it as a temporary table (using spark-sql-dataframe.md#registerTempTable[DataFrame.registerTempTable] method). NOTE: At this point you can query the table as if it were a regular non-streaming table using spark-sql-sqlcontext.md#sql[sql] method. A new StreamingQuery.md[StreamingQuery] is started (using StreamingQueryManager.startQuery ) and returned. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.MemorySink logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.MemorySink=ALL Refer to < >. \u00b6 === [[creating-instance]] Creating MemorySink Instance MemorySink takes the following to be created: [[schema]] Output schema [[outputMode]] OutputMode MemorySink initializes the < > internal property. === [[batches]] In-Memory Buffer of Streaming Batches -- batches Internal Property [source, scala] \u00b6 batches: ArrayBuffer[AddedData] \u00b6 batches holds data from streaming batches that have been < > ( written ) to this sink. For Append and Update output modes, batches holds rows from all batches. For Complete output mode, batches holds rows from the last batch only. batches can be cleared ( emptied ) using < >. === [[addBatch]] Adding Batch of Data to Sink -- addBatch Method [source, scala] \u00b6 addBatch( batchId: Long, data: DataFrame): Unit addBatch branches off based on whether the given batchId has already been < > or < >. A batch ID is considered committed when the given batch ID is greater than the < > (if available). addBatch is part of the Sink abstraction. ==== [[addBatch-not-committed]] Batch Not Committed With the batchId not committed, addBatch prints out the following DEBUG message to the logs: Committing batch [batchId] to [this] addBatch collects records from the given data . NOTE: addBatch uses Dataset.collect operator to collect records. For < > and < > output modes, addBatch adds the data (as a AddedData ) to the < > internal registry. For < > output mode, addBatch clears the < > internal registry first before adding the data (as a AddedData ). For any other output mode, addBatch reports an IllegalArgumentException : Output mode [outputMode] is not supported by MemorySink ==== [[addBatch-committed]] Batch Committed With the batchId committed, addBatch simply prints out the following DEBUG message to the logs and returns. Skipping already committed batch: [batchId] === [[clear]] Clearing Up Internal Batch Buffer -- clear Method [source, scala] \u00b6 clear(): Unit \u00b6 clear simply removes ( clears ) all data from the < > internal registry. NOTE: clear is used exclusively in tests.","title":"MemorySink"},{"location":"datasources/memory/MemorySink/#memorysink","text":"MemorySink is a streaming sink that < >. MemorySink is intended only for testing or demos. MemorySink is used for memory format and requires a query name (by queryName method or queryName option). NOTE: MemorySink was introduced in the https://github.com/apache/spark/pull/12119[pull request for [SPARK-14288][SQL] Memory Sink for streaming]. Use toDebugString to see the batches. Its aim is to allow users to test streaming applications in the Spark shell or other local tests. You can set checkpointLocation using option method or it will be set to spark.sql.streaming.checkpointLocation property. If spark.sql.streaming.checkpointLocation is set, the code uses $location/$queryName directory. Finally, when no spark.sql.streaming.checkpointLocation is set, a temporary directory memory.stream under java.io.tmpdir is used with offsets subdirectory inside. NOTE: The directory is cleaned up at shutdown using ShutdownHookManager.registerShutdownDeleteDir . It creates MemorySink instance based on the schema of the DataFrame it operates on. It creates a new DataFrame using MemoryPlan with MemorySink instance created earlier and registers it as a temporary table (using spark-sql-dataframe.md#registerTempTable[DataFrame.registerTempTable] method). NOTE: At this point you can query the table as if it were a regular non-streaming table using spark-sql-sqlcontext.md#sql[sql] method. A new StreamingQuery.md[StreamingQuery] is started (using StreamingQueryManager.startQuery ) and returned. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.MemorySink logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.MemorySink=ALL","title":"MemorySink"},{"location":"datasources/memory/MemorySink/#refer-to","text":"=== [[creating-instance]] Creating MemorySink Instance MemorySink takes the following to be created: [[schema]] Output schema [[outputMode]] OutputMode MemorySink initializes the < > internal property. === [[batches]] In-Memory Buffer of Streaming Batches -- batches Internal Property","title":"Refer to &lt;&gt;."},{"location":"datasources/memory/MemorySink/#source-scala","text":"","title":"[source, scala]"},{"location":"datasources/memory/MemorySink/#batches-arraybufferaddeddata","text":"batches holds data from streaming batches that have been < > ( written ) to this sink. For Append and Update output modes, batches holds rows from all batches. For Complete output mode, batches holds rows from the last batch only. batches can be cleared ( emptied ) using < >. === [[addBatch]] Adding Batch of Data to Sink -- addBatch Method","title":"batches: ArrayBuffer[AddedData]"},{"location":"datasources/memory/MemorySink/#source-scala_1","text":"addBatch( batchId: Long, data: DataFrame): Unit addBatch branches off based on whether the given batchId has already been < > or < >. A batch ID is considered committed when the given batch ID is greater than the < > (if available). addBatch is part of the Sink abstraction. ==== [[addBatch-not-committed]] Batch Not Committed With the batchId not committed, addBatch prints out the following DEBUG message to the logs: Committing batch [batchId] to [this] addBatch collects records from the given data . NOTE: addBatch uses Dataset.collect operator to collect records. For < > and < > output modes, addBatch adds the data (as a AddedData ) to the < > internal registry. For < > output mode, addBatch clears the < > internal registry first before adding the data (as a AddedData ). For any other output mode, addBatch reports an IllegalArgumentException : Output mode [outputMode] is not supported by MemorySink ==== [[addBatch-committed]] Batch Committed With the batchId committed, addBatch simply prints out the following DEBUG message to the logs and returns. Skipping already committed batch: [batchId] === [[clear]] Clearing Up Internal Batch Buffer -- clear Method","title":"[source, scala]"},{"location":"datasources/memory/MemorySink/#source-scala_2","text":"","title":"[source, scala]"},{"location":"datasources/memory/MemorySink/#clear-unit","text":"clear simply removes ( clears ) all data from the < > internal registry. NOTE: clear is used exclusively in tests.","title":"clear(): Unit"},{"location":"datasources/memory/MemorySinkBase/","text":"MemorySinkBase \u00b6 [[contract]] .MemorySinkBase Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | allData a| [[allData]] [source, scala] \u00b6 allData: Seq[Row] \u00b6 | dataSinceBatch a| [[dataSinceBatch]] [source, scala] \u00b6 dataSinceBatch( sinceBatchId: Long): Seq[Row] | latestBatchData a| [[latestBatchData]] [source, scala] \u00b6 latestBatchData: Seq[Row] \u00b6 | latestBatchId a| [[latestBatchId]] [source, scala] \u00b6 latestBatchId: Option[Long] \u00b6 |=== [[implementations]] .MemorySinkBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MemorySinkBase | Description | MemorySink | [[MemorySink]] Streaming sink for Micro-Batch Stream Processing (based on Data Source API V1) | MemorySinkV2 | [[MemorySinkV2]] Streaming sink for Continuous Stream Processing (based on Data Source API V2) |===","title":"MemorySinkBase"},{"location":"datasources/memory/MemorySinkBase/#memorysinkbase","text":"[[contract]] .MemorySinkBase Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | allData a| [[allData]]","title":"MemorySinkBase"},{"location":"datasources/memory/MemorySinkBase/#source-scala","text":"","title":"[source, scala]"},{"location":"datasources/memory/MemorySinkBase/#alldata-seqrow","text":"| dataSinceBatch a| [[dataSinceBatch]]","title":"allData: Seq[Row]"},{"location":"datasources/memory/MemorySinkBase/#source-scala_1","text":"dataSinceBatch( sinceBatchId: Long): Seq[Row] | latestBatchData a| [[latestBatchData]]","title":"[source, scala]"},{"location":"datasources/memory/MemorySinkBase/#source-scala_2","text":"","title":"[source, scala]"},{"location":"datasources/memory/MemorySinkBase/#latestbatchdata-seqrow","text":"| latestBatchId a| [[latestBatchId]]","title":"latestBatchData: Seq[Row]"},{"location":"datasources/memory/MemorySinkBase/#source-scala_3","text":"","title":"[source, scala]"},{"location":"datasources/memory/MemorySinkBase/#latestbatchid-optionlong","text":"|=== [[implementations]] .MemorySinkBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MemorySinkBase | Description | MemorySink | [[MemorySink]] Streaming sink for Micro-Batch Stream Processing (based on Data Source API V1) | MemorySinkV2 | [[MemorySinkV2]] Streaming sink for Continuous Stream Processing (based on Data Source API V2) |===","title":"latestBatchId: Option[Long]"},{"location":"datasources/memory/MemorySinkV2/","text":"MemorySinkV2 \u2014 Writable Streaming Sink for Continuous Stream Processing \u00b6 MemorySinkV2 is a DataSourceV2 for memory data source format in Continuous Stream Processing . Tip Read up on DataSourceV2 in The Internals of Spark SQL online book. MemorySinkV2 is a custom MemorySinkBase .","title":"MemorySinkV2"},{"location":"datasources/memory/MemorySinkV2/#memorysinkv2-writable-streaming-sink-for-continuous-stream-processing","text":"MemorySinkV2 is a DataSourceV2 for memory data source format in Continuous Stream Processing . Tip Read up on DataSourceV2 in The Internals of Spark SQL online book. MemorySinkV2 is a custom MemorySinkBase .","title":"MemorySinkV2 &mdash; Writable Streaming Sink for Continuous Stream Processing"},{"location":"datasources/memory/MemoryStream/","text":"MemoryStream -- Streaming Reader for Micro-Batch Stream Processing \u00b6 MemoryStream is a concrete streaming source of memory data source that supports reading in Micro-Batch Stream Processing . [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.MemoryStream logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.MemoryStream=ALL Refer to < >. \u00b6 === [[creating-instance]] Creating MemoryStream Instance MemoryStream takes the following to be created: [[id]] ID [[sqlContext]] SQLContext MemoryStream initializes the < >. === [[apply]] Creating MemoryStream Instance -- apply Object Factory [source, scala] \u00b6 apply A : Encoder : MemoryStream[A] apply uses an memoryStreamId internal counter to < > with a unique < > and the implicit SQLContext . === [[addData]] Adding Data to Source -- addData Method [source, scala] \u00b6 addData( data: TraversableOnce[A]): Offset addData adds the given data to the < > internal registry. Internally, addData prints out the following DEBUG message to the logs: Adding: [data] In the end, addData increments the < > and adds the data to the < > internal registry. === [[getBatch]] Generating Next Streaming Batch -- getBatch Method getBatch is a part of the Source abstraction. When executed, getBatch uses the internal < > collection to return requested offsets. You should see the following DEBUG message in the logs: DEBUG MemoryStream: MemoryBatch [[startOrdinal], [endOrdinal]]: [newBlocks] === [[logicalPlan]] Logical Plan -- logicalPlan Internal Property [source, scala] \u00b6 logicalPlan: LogicalPlan \u00b6 logicalPlan is part of the MemoryStreamBase abstraction. logicalPlan is simply a StreamingExecutionRelation (for this memory source and the attributes ). MemoryStream uses StreamingExecutionRelation logical plan to build Datasets or DataFrames when requested. scala> val ints = MemoryStream[Int] ints: org.apache.spark.sql.execution.streaming.MemoryStream[Int] = MemoryStream[value#13] scala> ints.toDS.queryExecution.logical.isStreaming res14: Boolean = true scala> ints.toDS.queryExecution.logical res15: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = MemoryStream[value#13] === [[schema]] Schema (schema method) MemoryStream works with the data of the spark-sql-schema.md[schema] as described by the spark-sql-Encoder.md[Encoder] (of the Dataset ). === [[toString]] Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString uses the < > to return the following textual representation: MemoryStream[[output]] === [[planInputPartitions]] Plan Input Partitions -- planInputPartitions Method [source, scala] \u00b6 planInputPartitions(): java.util.List[InputPartition[InternalRow]] \u00b6 NOTE: planInputPartitions is part of the DataSourceReader contract in Spark SQL for the number of InputPartitions to use as RDD partitions (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). planInputPartitions ...FIXME planInputPartitions prints out a DEBUG message to the logs with the < > (with the batches after the < >). planInputPartitions ...FIXME === [[generateDebugString]] generateDebugString Internal Method [source, scala] \u00b6 generateDebugString( rows: Seq[UnsafeRow], startOrdinal: Int, endOrdinal: Int): String generateDebugString resolves and binds the encoder for the data. In the end, generateDebugString returns the following string: MemoryBatch [[startOrdinal], [endOrdinal]]: [rows] NOTE: generateDebugString is used exclusively when MemoryStream is requested to < >. Internal Properties \u00b6 [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | batches a| [[batches]] Batch data ( ListBuffer[Array[UnsafeRow]] ) | currentOffset a| [[currentOffset]] Current offset | lastOffsetCommitted a| [[lastOffsetCommitted]] Last committed offset | output a| [[output]] Output schema ( Seq[Attribute] ) of the logical query plan Used exclusively for < > |===","title":"MemoryStream"},{"location":"datasources/memory/MemoryStream/#memorystream-streaming-reader-for-micro-batch-stream-processing","text":"MemoryStream is a concrete streaming source of memory data source that supports reading in Micro-Batch Stream Processing . [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.MemoryStream logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.MemoryStream=ALL","title":"MemoryStream -- Streaming Reader for Micro-Batch Stream Processing"},{"location":"datasources/memory/MemoryStream/#refer-to","text":"=== [[creating-instance]] Creating MemoryStream Instance MemoryStream takes the following to be created: [[id]] ID [[sqlContext]] SQLContext MemoryStream initializes the < >. === [[apply]] Creating MemoryStream Instance -- apply Object Factory","title":"Refer to &lt;&gt;."},{"location":"datasources/memory/MemoryStream/#source-scala","text":"apply A : Encoder : MemoryStream[A] apply uses an memoryStreamId internal counter to < > with a unique < > and the implicit SQLContext . === [[addData]] Adding Data to Source -- addData Method","title":"[source, scala]"},{"location":"datasources/memory/MemoryStream/#source-scala_1","text":"addData( data: TraversableOnce[A]): Offset addData adds the given data to the < > internal registry. Internally, addData prints out the following DEBUG message to the logs: Adding: [data] In the end, addData increments the < > and adds the data to the < > internal registry. === [[getBatch]] Generating Next Streaming Batch -- getBatch Method getBatch is a part of the Source abstraction. When executed, getBatch uses the internal < > collection to return requested offsets. You should see the following DEBUG message in the logs: DEBUG MemoryStream: MemoryBatch [[startOrdinal], [endOrdinal]]: [newBlocks] === [[logicalPlan]] Logical Plan -- logicalPlan Internal Property","title":"[source, scala]"},{"location":"datasources/memory/MemoryStream/#source-scala_2","text":"","title":"[source, scala]"},{"location":"datasources/memory/MemoryStream/#logicalplan-logicalplan","text":"logicalPlan is part of the MemoryStreamBase abstraction. logicalPlan is simply a StreamingExecutionRelation (for this memory source and the attributes ). MemoryStream uses StreamingExecutionRelation logical plan to build Datasets or DataFrames when requested. scala> val ints = MemoryStream[Int] ints: org.apache.spark.sql.execution.streaming.MemoryStream[Int] = MemoryStream[value#13] scala> ints.toDS.queryExecution.logical.isStreaming res14: Boolean = true scala> ints.toDS.queryExecution.logical res15: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = MemoryStream[value#13] === [[schema]] Schema (schema method) MemoryStream works with the data of the spark-sql-schema.md[schema] as described by the spark-sql-Encoder.md[Encoder] (of the Dataset ). === [[toString]] Textual Representation -- toString Method","title":"logicalPlan: LogicalPlan"},{"location":"datasources/memory/MemoryStream/#source-scala_3","text":"","title":"[source, scala]"},{"location":"datasources/memory/MemoryStream/#tostring-string","text":"NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString uses the < > to return the following textual representation: MemoryStream[[output]] === [[planInputPartitions]] Plan Input Partitions -- planInputPartitions Method","title":"toString: String"},{"location":"datasources/memory/MemoryStream/#source-scala_4","text":"","title":"[source, scala]"},{"location":"datasources/memory/MemoryStream/#planinputpartitions-javautillistinputpartitioninternalrow","text":"NOTE: planInputPartitions is part of the DataSourceReader contract in Spark SQL for the number of InputPartitions to use as RDD partitions (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). planInputPartitions ...FIXME planInputPartitions prints out a DEBUG message to the logs with the < > (with the batches after the < >). planInputPartitions ...FIXME === [[generateDebugString]] generateDebugString Internal Method","title":"planInputPartitions(): java.util.List[InputPartition[InternalRow]]"},{"location":"datasources/memory/MemoryStream/#source-scala_5","text":"generateDebugString( rows: Seq[UnsafeRow], startOrdinal: Int, endOrdinal: Int): String generateDebugString resolves and binds the encoder for the data. In the end, generateDebugString returns the following string: MemoryBatch [[startOrdinal], [endOrdinal]]: [rows] NOTE: generateDebugString is used exclusively when MemoryStream is requested to < >.","title":"[source, scala]"},{"location":"datasources/memory/MemoryStream/#internal-properties","text":"[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | batches a| [[batches]] Batch data ( ListBuffer[Array[UnsafeRow]] ) | currentOffset a| [[currentOffset]] Current offset | lastOffsetCommitted a| [[lastOffsetCommitted]] Last committed offset | output a| [[output]] Output schema ( Seq[Attribute] ) of the logical query plan Used exclusively for < > |===","title":"Internal Properties"},{"location":"datasources/memory/MemoryStreamBase/","text":"MemoryStreamBase -- Base Contract for Memory Sources \u00b6 MemoryStreamBase is the < > of...FIXME [[contract]] .MemoryStreamBase Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | addData a| [[addData]] [source, scala] \u00b6 addData( data: TraversableOnce[A]): Offset | logicalPlan a| [[logicalPlan]] [source, scala] \u00b6 logicalPlan: LogicalPlan \u00b6 |=== [[implementations]] .MemoryStreamBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MemoryStreamBase | Description | ContinuousMemoryStream | [[ContinuousMemoryStream]] | MemoryStream | [[MemoryStream]] MicroBatchReader for Micro-Batch Stream Processing |=== === [[creating-instance]] Creating MemoryStreamBase Instance MemoryStreamBase takes the following to be created: [[sqlContext]] SQLContext NOTE: MemoryStreamBase is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. === [[toDS]] Creating Streaming Dataset -- toDS Method [source, scala] \u00b6 toDS(): Dataset[A] \u00b6 toDS simply creates a Dataset (for the < > and the < >) === [[toDF]] Creating Streaming DataFrame -- toDF Method [source, scala] \u00b6 toDF(): DataFrame \u00b6 toDF simply creates a Dataset of rows (for the < > and the < >) === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | attributes a| [[attributes]] Schema attributes of the < > ( Seq[AttributeReference] ) Used when...FIXME | encoder a| [[encoder]] Spark SQL's ExpressionEncoder for the data Used when...FIXME |===","title":"MemoryStreamBase"},{"location":"datasources/memory/MemoryStreamBase/#memorystreambase-base-contract-for-memory-sources","text":"MemoryStreamBase is the < > of...FIXME [[contract]] .MemoryStreamBase Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | addData a| [[addData]]","title":"MemoryStreamBase -- Base Contract for Memory Sources"},{"location":"datasources/memory/MemoryStreamBase/#source-scala","text":"addData( data: TraversableOnce[A]): Offset | logicalPlan a| [[logicalPlan]]","title":"[source, scala]"},{"location":"datasources/memory/MemoryStreamBase/#source-scala_1","text":"","title":"[source, scala]"},{"location":"datasources/memory/MemoryStreamBase/#logicalplan-logicalplan","text":"|=== [[implementations]] .MemoryStreamBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MemoryStreamBase | Description | ContinuousMemoryStream | [[ContinuousMemoryStream]] | MemoryStream | [[MemoryStream]] MicroBatchReader for Micro-Batch Stream Processing |=== === [[creating-instance]] Creating MemoryStreamBase Instance MemoryStreamBase takes the following to be created: [[sqlContext]] SQLContext NOTE: MemoryStreamBase is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. === [[toDS]] Creating Streaming Dataset -- toDS Method","title":"logicalPlan: LogicalPlan"},{"location":"datasources/memory/MemoryStreamBase/#source-scala_2","text":"","title":"[source, scala]"},{"location":"datasources/memory/MemoryStreamBase/#tods-dataseta","text":"toDS simply creates a Dataset (for the < > and the < >) === [[toDF]] Creating Streaming DataFrame -- toDF Method","title":"toDS(): Dataset[A]"},{"location":"datasources/memory/MemoryStreamBase/#source-scala_3","text":"","title":"[source, scala]"},{"location":"datasources/memory/MemoryStreamBase/#todf-dataframe","text":"toDF simply creates a Dataset of rows (for the < > and the < >) === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | attributes a| [[attributes]] Schema attributes of the < > ( Seq[AttributeReference] ) Used when...FIXME | encoder a| [[encoder]] Spark SQL's ExpressionEncoder for the data Used when...FIXME |===","title":"toDF(): DataFrame"},{"location":"datasources/socket/","text":"Text Socket Data Source \u00b6 Text Socket Data Source comes with the following main abstractions: TextSocketSourceProvider TextSocketSource","title":"Text Socket Data Source"},{"location":"datasources/socket/#text-socket-data-source","text":"Text Socket Data Source comes with the following main abstractions: TextSocketSourceProvider TextSocketSource","title":"Text Socket Data Source"},{"location":"datasources/socket/TextSocketSource/","text":"TextSocketSource \u00b6 TextSocketSource is a streaming source that reads text lines from a socket at the host and port . TextSocketSource uses lines internal in-memory buffer to keep all of the lines that were read from a socket forever. Caution This source is not for production use due to design contraints, e.g. infinite in-memory collection of lines read and no fault recovery. It is designed only for tutorials and debugging. Creating Instance \u00b6 When TextSocketSource is created (see TextSocketSourceProvider ), it gets 4 parameters passed in: host port includeTimestamp flag spark-sql-sqlcontext.md[SQLContext] CAUTION: It appears that the source did not get \"renewed\" to use spark-sql-sparksession.md[SparkSession] instead. It opens a socket at given host and port parameters and reads a buffering character-input stream using the default charset and the default-sized input buffer (of 8192 bytes) line by line. CAUTION: FIXME Review Java's Charset.defaultCharset() It starts a readThread daemon thread (called TextSocketSource(host, port) ) to read lines from the socket. The lines are added to the internal < > buffer. lines Internal Buffer \u00b6 lines : ArrayBuffer [( String , Timestamp )] lines is the internal buffer of all the lines TextSocketSource read from the socket. === [[getOffset]] Maximum Available Offset (getOffset method) TextSocketSource 's offset can either be none or LongOffset of the number of lines in the internal < > buffer. getOffset is a part of the Source abstraction. === [[schema]] Schema (schema method) TextSocketSource supports two spark-sql-schema.md[schemas]: A single value field of String type. value field of StringType type and timestamp field of spark-sql-DataType.md#TimestampType[TimestampType] type of format yyyy-MM-dd HH:mm:ss . Tip Refer to sourceSchema for TextSocketSourceProvider . Stopping TextSocketSource \u00b6 When stopped, TextSocketSource closes the socket connection. Demo \u00b6 import org.apache.spark.sql.SparkSession val spark: SparkSession = SparkSession.builder.getOrCreate() // Connect to localhost:9999 // You can use \"nc -lk 9999\" for demos val textSocket = spark. readStream. format(\"socket\"). option(\"host\", \"localhost\"). option(\"port\", 9999). load import org.apache.spark.sql.Dataset val lines: Dataset[String] = textSocket.as[String].map(_.toUpperCase) val query = lines.writeStream.format(\"console\").start // Start typing the lines in nc session // They will appear UPPERCASE in the terminal ------------------------------------------- Batch: 0 ------------------------------------------- +---------+ | value| +---------+ |UPPERCASE| +---------+ scala> query.explain == Physical Plan == *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true) AS value#21] +- *MapElements <function1>, obj#20: java.lang.String +- *DeserializeToObject value#43.toString, obj#19: java.lang.String +- LocalTableScan [value#43] scala> query.stop","title":"TextSocketSource"},{"location":"datasources/socket/TextSocketSource/#textsocketsource","text":"TextSocketSource is a streaming source that reads text lines from a socket at the host and port . TextSocketSource uses lines internal in-memory buffer to keep all of the lines that were read from a socket forever. Caution This source is not for production use due to design contraints, e.g. infinite in-memory collection of lines read and no fault recovery. It is designed only for tutorials and debugging.","title":"TextSocketSource"},{"location":"datasources/socket/TextSocketSource/#creating-instance","text":"When TextSocketSource is created (see TextSocketSourceProvider ), it gets 4 parameters passed in: host port includeTimestamp flag spark-sql-sqlcontext.md[SQLContext] CAUTION: It appears that the source did not get \"renewed\" to use spark-sql-sparksession.md[SparkSession] instead. It opens a socket at given host and port parameters and reads a buffering character-input stream using the default charset and the default-sized input buffer (of 8192 bytes) line by line. CAUTION: FIXME Review Java's Charset.defaultCharset() It starts a readThread daemon thread (called TextSocketSource(host, port) ) to read lines from the socket. The lines are added to the internal < > buffer.","title":"Creating Instance"},{"location":"datasources/socket/TextSocketSource/#lines-internal-buffer","text":"lines : ArrayBuffer [( String , Timestamp )] lines is the internal buffer of all the lines TextSocketSource read from the socket. === [[getOffset]] Maximum Available Offset (getOffset method) TextSocketSource 's offset can either be none or LongOffset of the number of lines in the internal < > buffer. getOffset is a part of the Source abstraction. === [[schema]] Schema (schema method) TextSocketSource supports two spark-sql-schema.md[schemas]: A single value field of String type. value field of StringType type and timestamp field of spark-sql-DataType.md#TimestampType[TimestampType] type of format yyyy-MM-dd HH:mm:ss . Tip Refer to sourceSchema for TextSocketSourceProvider .","title":" lines Internal Buffer"},{"location":"datasources/socket/TextSocketSource/#stopping-textsocketsource","text":"When stopped, TextSocketSource closes the socket connection.","title":" Stopping TextSocketSource"},{"location":"datasources/socket/TextSocketSource/#demo","text":"import org.apache.spark.sql.SparkSession val spark: SparkSession = SparkSession.builder.getOrCreate() // Connect to localhost:9999 // You can use \"nc -lk 9999\" for demos val textSocket = spark. readStream. format(\"socket\"). option(\"host\", \"localhost\"). option(\"port\", 9999). load import org.apache.spark.sql.Dataset val lines: Dataset[String] = textSocket.as[String].map(_.toUpperCase) val query = lines.writeStream.format(\"console\").start // Start typing the lines in nc session // They will appear UPPERCASE in the terminal ------------------------------------------- Batch: 0 ------------------------------------------- +---------+ | value| +---------+ |UPPERCASE| +---------+ scala> query.explain == Physical Plan == *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true) AS value#21] +- *MapElements <function1>, obj#20: java.lang.String +- *DeserializeToObject value#43.toString, obj#19: java.lang.String +- LocalTableScan [value#43] scala> query.stop","title":"Demo"},{"location":"datasources/socket/TextSocketSourceProvider/","text":"TextSocketSourceProvider \u00b6 TextSocketSourceProvider is a StreamSourceProvider for Text Socket Data Source . TextSocketSourceProvider requires two options (that you can set using option method): host which is the host name. port which is the port number. It must be an integer. TextSocketSourceProvider also supports < > option that is a boolean flag that you can use to include timestamps in the schema. DataSourceRegister \u00b6 TextSocketSourceProvider is a DataSourceRegister with the short name of socket . includeTimestamp Option \u00b6 createSource \u00b6 createSource creates a TextSocketSource (with the host and port ). sourceSchema \u00b6 sourceSchema returns textSocket as the name of the source and the schema that can be one of the two available schemas: SCHEMA_REGULAR (default) which is a schema with a single value field of String type. SCHEMA_TIMESTAMP when <<includeTimestamp, includeTimestamp>> flag option is set. It is not, i.e. false , by default. The schema are value field of StringType type and timestamp field of spark-sql-DataType.md#TimestampType[TimestampType] type of format yyyy-MM-dd HH:mm:ss . TIP: Read about spark-sql-schema.md[schema]. Internally, it starts by printing out the following WARN message to the logs: The socket source should not be used for production applications! It does not support recovery and stores state indefinitely. It then checks whether host and port parameters are defined and if not it throws a AnalysisException : Set a host to read from with option(\"host\", ...).","title":"TextSocketSourceProvider"},{"location":"datasources/socket/TextSocketSourceProvider/#textsocketsourceprovider","text":"TextSocketSourceProvider is a StreamSourceProvider for Text Socket Data Source . TextSocketSourceProvider requires two options (that you can set using option method): host which is the host name. port which is the port number. It must be an integer. TextSocketSourceProvider also supports < > option that is a boolean flag that you can use to include timestamps in the schema.","title":"TextSocketSourceProvider"},{"location":"datasources/socket/TextSocketSourceProvider/#datasourceregister","text":"TextSocketSourceProvider is a DataSourceRegister with the short name of socket .","title":"DataSourceRegister"},{"location":"datasources/socket/TextSocketSourceProvider/#includetimestamp-option","text":"","title":" includeTimestamp Option"},{"location":"datasources/socket/TextSocketSourceProvider/#createsource","text":"createSource creates a TextSocketSource (with the host and port ).","title":" createSource"},{"location":"datasources/socket/TextSocketSourceProvider/#sourceschema","text":"sourceSchema returns textSocket as the name of the source and the schema that can be one of the two available schemas: SCHEMA_REGULAR (default) which is a schema with a single value field of String type. SCHEMA_TIMESTAMP when <<includeTimestamp, includeTimestamp>> flag option is set. It is not, i.e. false , by default. The schema are value field of StringType type and timestamp field of spark-sql-DataType.md#TimestampType[TimestampType] type of format yyyy-MM-dd HH:mm:ss . TIP: Read about spark-sql-schema.md[schema]. Internally, it starts by printing out the following WARN message to the logs: The socket source should not be used for production applications! It does not support recovery and stores state indefinitely. It then checks whether host and port parameters are defined and if not it throws a AnalysisException : Set a host to read from with option(\"host\", ...).","title":" sourceSchema"},{"location":"demo/","text":"Demos \u00b6 The following demos are available: Internals of FlatMapGroupsWithStateExec Physical Operator Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator Exploring Checkpointed State Streaming Watermark with Aggregation in Append Output Mode Streaming Query for Running Counts (Socket Source and Complete Output Mode) Streaming Aggregation with Kafka Data Source groupByKey Streaming Aggregation in Update Mode StateStoreSaveExec with Complete Output Mode StateStoreSaveExec with Update Output Mode Developing Custom Streaming Sink (and Monitoring SQL Queries in web UI) current_timestamp Function For Processing Time in Streaming Queries Using StreamingQueryManager for Query Termination Management Using File Streaming Source Deep Dive into FileStreamSink","title":"Welcome"},{"location":"demo/#demos","text":"The following demos are available: Internals of FlatMapGroupsWithStateExec Physical Operator Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator Exploring Checkpointed State Streaming Watermark with Aggregation in Append Output Mode Streaming Query for Running Counts (Socket Source and Complete Output Mode) Streaming Aggregation with Kafka Data Source groupByKey Streaming Aggregation in Update Mode StateStoreSaveExec with Complete Output Mode StateStoreSaveExec with Update Output Mode Developing Custom Streaming Sink (and Monitoring SQL Queries in web UI) current_timestamp Function For Processing Time in Streaming Queries Using StreamingQueryManager for Query Termination Management Using File Streaming Source Deep Dive into FileStreamSink","title":"Demos"},{"location":"demo/StateStoreSaveExec-Complete/","text":"Demo: StateStoreSaveExec with Complete Output Mode \u00b6 The following example code shows the behaviour of StateStoreSaveExec.md#doExecute-Complete[StateStoreSaveExec in Complete output mode]. [source, scala] \u00b6 // START: Only for easier debugging // The state is then only for one partition // which should make monitoring it easier import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 1) scala> spark.sessionState.conf.numShufflePartitions res1: Int = 1 // END: Only for easier debugging // Read datasets from a Kafka topic // ./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT // Streaming aggregation using groupBy operator is required to have StateStoreSaveExec operator val valuesPerGroup = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load. withColumn(\"tokens\", split('value, \",\")). withColumn(\"group\", 'tokens(0)). withColumn(\"value\", 'tokens(1) cast \"int\"). select(\"group\", \"value\"). groupBy( \"group\"). agg(collect_list(\"value\") as \"values\"). orderBy( \"group\"). agg(collect_list(\"value\") as \"values\"). orderBy( \"group\".asc) // valuesPerGroup is a streaming Dataset with just one source // so it knows nothing about output mode or watermark yet // That's why StatefulOperatorStateInfo is generic // and no batch-specific values are printed out // That will be available after the first streaming batch // Use sq.explain to know the runtime-specific values scala> valuesPerGroup.explain == Physical Plan == *Sort [group#25 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(group#25 ASC NULLS FIRST, 1) +- ObjectHashAggregate(keys=[group#25], functions=[collect_list(value#36, 0, 0)]) +- Exchange hashpartitioning(group#25, 1) +- StateStoreSave [group#25], StatefulOperatorStateInfo( ,899f0fd1-b202-45cd-9ebd-09101ca90fa8,0,0), Append, 0 +- ObjectHashAggregate(keys=[group#25], functions=[merge_collect_list(value#36, 0, 0)]) +- Exchange hashpartitioning(group#25, 1) +- StateStoreRestore [group#25], StatefulOperatorStateInfo( ,899f0fd1-b202-45cd-9ebd-09101ca90fa8,0,0) +- ObjectHashAggregate(keys=[group#25], functions=[merge_collect_list(value#36, 0, 0)]) +- Exchange hashpartitioning(group#25, 1) +- ObjectHashAggregate(keys=[group#25], functions=[partial_collect_list(value#36, 0, 0)]) +- *Project [split(cast(value#1 as string), ,)[0] AS group#25, cast(split(cast(value#1 as string), ,)[1] as int) AS value#36] +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] // Start the query and hence StateStoreSaveExec // Use Complete output mode import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val sq = valuesPerGroup. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Complete). start Batch: 0 \u00b6 +-----+------+ |group|values| +-----+------+ +-----+------+ // there's only 1 stateful operator and hence 0 for the index in stateOperators scala> println(sq.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 0, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 60 } // publish 1 new key-value pair in a single streaming batch // 0,1 Batch: 1 \u00b6 +-----+------+ |group|values| +-----+------+ |0 |[1] | +-----+------+ // it's Complete output mode so numRowsTotal is the number of keys in the state store // no keys were available earlier (it's just started!) and so numRowsUpdated is 0 scala> println(sq.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 1, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 324 } // publish new key and old key in a single streaming batch // new keys // 1,1 // updates to already-stored keys // 0,2 Batch: 2 \u00b6 +-----+------+ |group|values| +-----+------+ |0 |[2, 1]| |1 |[1] | +-----+------+ // it's Complete output mode so numRowsTotal is the number of keys in the state store // no keys were available earlier and so numRowsUpdated is...0?! // Think it's a BUG as it should've been 1 (for the row 0,2) // 8/30 Sent out a question to the Spark user mailing list scala> println(sq.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 2, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 572 } // In the end... sq.stop","title":"StateStoreSaveExec with Complete Output Mode"},{"location":"demo/StateStoreSaveExec-Complete/#demo-statestoresaveexec-with-complete-output-mode","text":"The following example code shows the behaviour of StateStoreSaveExec.md#doExecute-Complete[StateStoreSaveExec in Complete output mode].","title":"Demo: StateStoreSaveExec with Complete Output Mode"},{"location":"demo/StateStoreSaveExec-Complete/#source-scala","text":"// START: Only for easier debugging // The state is then only for one partition // which should make monitoring it easier import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 1) scala> spark.sessionState.conf.numShufflePartitions res1: Int = 1 // END: Only for easier debugging // Read datasets from a Kafka topic // ./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT // Streaming aggregation using groupBy operator is required to have StateStoreSaveExec operator val valuesPerGroup = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load. withColumn(\"tokens\", split('value, \",\")). withColumn(\"group\", 'tokens(0)). withColumn(\"value\", 'tokens(1) cast \"int\"). select(\"group\", \"value\"). groupBy( \"group\"). agg(collect_list(\"value\") as \"values\"). orderBy( \"group\"). agg(collect_list(\"value\") as \"values\"). orderBy( \"group\".asc) // valuesPerGroup is a streaming Dataset with just one source // so it knows nothing about output mode or watermark yet // That's why StatefulOperatorStateInfo is generic // and no batch-specific values are printed out // That will be available after the first streaming batch // Use sq.explain to know the runtime-specific values scala> valuesPerGroup.explain == Physical Plan == *Sort [group#25 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(group#25 ASC NULLS FIRST, 1) +- ObjectHashAggregate(keys=[group#25], functions=[collect_list(value#36, 0, 0)]) +- Exchange hashpartitioning(group#25, 1) +- StateStoreSave [group#25], StatefulOperatorStateInfo( ,899f0fd1-b202-45cd-9ebd-09101ca90fa8,0,0), Append, 0 +- ObjectHashAggregate(keys=[group#25], functions=[merge_collect_list(value#36, 0, 0)]) +- Exchange hashpartitioning(group#25, 1) +- StateStoreRestore [group#25], StatefulOperatorStateInfo( ,899f0fd1-b202-45cd-9ebd-09101ca90fa8,0,0) +- ObjectHashAggregate(keys=[group#25], functions=[merge_collect_list(value#36, 0, 0)]) +- Exchange hashpartitioning(group#25, 1) +- ObjectHashAggregate(keys=[group#25], functions=[partial_collect_list(value#36, 0, 0)]) +- *Project [split(cast(value#1 as string), ,)[0] AS group#25, cast(split(cast(value#1 as string), ,)[1] as int) AS value#36] +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] // Start the query and hence StateStoreSaveExec // Use Complete output mode import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val sq = valuesPerGroup. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Complete). start","title":"[source, scala]"},{"location":"demo/StateStoreSaveExec-Complete/#batch-0","text":"+-----+------+ |group|values| +-----+------+ +-----+------+ // there's only 1 stateful operator and hence 0 for the index in stateOperators scala> println(sq.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 0, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 60 } // publish 1 new key-value pair in a single streaming batch // 0,1","title":"Batch: 0"},{"location":"demo/StateStoreSaveExec-Complete/#batch-1","text":"+-----+------+ |group|values| +-----+------+ |0 |[1] | +-----+------+ // it's Complete output mode so numRowsTotal is the number of keys in the state store // no keys were available earlier (it's just started!) and so numRowsUpdated is 0 scala> println(sq.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 1, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 324 } // publish new key and old key in a single streaming batch // new keys // 1,1 // updates to already-stored keys // 0,2","title":"Batch: 1"},{"location":"demo/StateStoreSaveExec-Complete/#batch-2","text":"+-----+------+ |group|values| +-----+------+ |0 |[2, 1]| |1 |[1] | +-----+------+ // it's Complete output mode so numRowsTotal is the number of keys in the state store // no keys were available earlier and so numRowsUpdated is...0?! // Think it's a BUG as it should've been 1 (for the row 0,2) // 8/30 Sent out a question to the Spark user mailing list scala> println(sq.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 2, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 572 } // In the end... sq.stop","title":"Batch: 2"},{"location":"demo/StateStoreSaveExec-Update/","text":"Demo: StateStoreSaveExec with Update Output Mode \u00b6 CAUTION: FIXME Example of Update with StateStoreSaveExec (and optional watermark)","title":"StateStoreSaveExec with Update Output Mode"},{"location":"demo/StateStoreSaveExec-Update/#demo-statestoresaveexec-with-update-output-mode","text":"CAUTION: FIXME Example of Update with StateStoreSaveExec (and optional watermark)","title":"Demo: StateStoreSaveExec with Update Output Mode"},{"location":"demo/StreamingQueryManager-awaitAnyTermination-resetTerminated/","text":"Demo: Using StreamingQueryManager for Query Termination Management \u00b6 The demo shows how to use StreamingQueryManager (and specifically awaitAnyTermination and resetTerminated ) for query termination management. // Save the code as demo-StreamingQueryManager.scala // Start it using spark-shell // $ ./bin/spark-shell -i demo-StreamingQueryManager.scala // Register a StreamingQueryListener to receive notifications about state changes of streaming queries import org.apache.spark.sql.streaming.StreamingQueryListener val myQueryListener = new StreamingQueryListener { import org.apache.spark.sql.streaming.StreamingQueryListener._ def onQueryTerminated(event: QueryTerminatedEvent): Unit = { println(s\"Query ${event.id} terminated\") } def onQueryStarted(event: QueryStartedEvent): Unit = {} def onQueryProgress(event: QueryProgressEvent): Unit = {} } spark.streams.addListener(myQueryListener) import org.apache.spark.sql.streaming._ import scala.concurrent.duration._ // Start streaming queries // Start the first query val q4s = spark.readStream. format(\"rate\"). load. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(4.seconds)). option(\"truncate\", false). start // Start another query that is slightly slower val q10s = spark.readStream. format(\"rate\"). load. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). option(\"truncate\", false). start // Both queries run concurrently // You should see different outputs in the console // q4s prints out 4 rows every batch and twice as often as q10s // q10s prints out 10 rows every batch /* ------------------------------------------- Batch: 7 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-10-27 13:44:07.462|21 | |2017-10-27 13:44:08.462|22 | |2017-10-27 13:44:09.462|23 | |2017-10-27 13:44:10.462|24 | +-----------------------+-----+ ------------------------------------------- Batch: 8 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-10-27 13:44:11.462|25 | |2017-10-27 13:44:12.462|26 | |2017-10-27 13:44:13.462|27 | |2017-10-27 13:44:14.462|28 | +-----------------------+-----+ ------------------------------------------- Batch: 2 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-10-27 13:44:09.847|6 | |2017-10-27 13:44:10.847|7 | |2017-10-27 13:44:11.847|8 | |2017-10-27 13:44:12.847|9 | |2017-10-27 13:44:13.847|10 | |2017-10-27 13:44:14.847|11 | |2017-10-27 13:44:15.847|12 | |2017-10-27 13:44:16.847|13 | |2017-10-27 13:44:17.847|14 | |2017-10-27 13:44:18.847|15 | +-----------------------+-----+ */ // Stop q4s on a separate thread // as we're about to block the current thread awaiting query termination import java.util.concurrent.Executors import java.util.concurrent.TimeUnit.SECONDS def queryTerminator(query: StreamingQuery) = new Runnable { def run = { println(s\"Stopping streaming query: ${query.id}\") query.stop } } import java.util.concurrent.TimeUnit.SECONDS // Stop the first query after 10 seconds Executors.newSingleThreadScheduledExecutor. scheduleWithFixedDelay(queryTerminator(q4s), 10, 60 * 5, SECONDS) // Stop the other query after 20 seconds Executors.newSingleThreadScheduledExecutor. scheduleWithFixedDelay(queryTerminator(q10s), 20, 60 * 5, SECONDS) // Use StreamingQueryManager to wait for any query termination (either q1 or q2) // the current thread will block indefinitely until either streaming query has finished spark.streams.awaitAnyTermination // You are here only after either streaming query has finished // Executing spark.streams.awaitAnyTermination again would return immediately // You should have received the QueryTerminatedEvent for the query termination // reset the last terminated streaming query spark.streams.resetTerminated // You know at least one query has terminated // Wait for the other query to terminate spark.streams.awaitAnyTermination assert(spark.streams.active.isEmpty) println(\"The demo went all fine. Exiting...\") // leave spark-shell System.exit(0)","title":"Using StreamingQueryManager for Query Termination Management"},{"location":"demo/StreamingQueryManager-awaitAnyTermination-resetTerminated/#demo-using-streamingquerymanager-for-query-termination-management","text":"The demo shows how to use StreamingQueryManager (and specifically awaitAnyTermination and resetTerminated ) for query termination management. // Save the code as demo-StreamingQueryManager.scala // Start it using spark-shell // $ ./bin/spark-shell -i demo-StreamingQueryManager.scala // Register a StreamingQueryListener to receive notifications about state changes of streaming queries import org.apache.spark.sql.streaming.StreamingQueryListener val myQueryListener = new StreamingQueryListener { import org.apache.spark.sql.streaming.StreamingQueryListener._ def onQueryTerminated(event: QueryTerminatedEvent): Unit = { println(s\"Query ${event.id} terminated\") } def onQueryStarted(event: QueryStartedEvent): Unit = {} def onQueryProgress(event: QueryProgressEvent): Unit = {} } spark.streams.addListener(myQueryListener) import org.apache.spark.sql.streaming._ import scala.concurrent.duration._ // Start streaming queries // Start the first query val q4s = spark.readStream. format(\"rate\"). load. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(4.seconds)). option(\"truncate\", false). start // Start another query that is slightly slower val q10s = spark.readStream. format(\"rate\"). load. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). option(\"truncate\", false). start // Both queries run concurrently // You should see different outputs in the console // q4s prints out 4 rows every batch and twice as often as q10s // q10s prints out 10 rows every batch /* ------------------------------------------- Batch: 7 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-10-27 13:44:07.462|21 | |2017-10-27 13:44:08.462|22 | |2017-10-27 13:44:09.462|23 | |2017-10-27 13:44:10.462|24 | +-----------------------+-----+ ------------------------------------------- Batch: 8 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-10-27 13:44:11.462|25 | |2017-10-27 13:44:12.462|26 | |2017-10-27 13:44:13.462|27 | |2017-10-27 13:44:14.462|28 | +-----------------------+-----+ ------------------------------------------- Batch: 2 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-10-27 13:44:09.847|6 | |2017-10-27 13:44:10.847|7 | |2017-10-27 13:44:11.847|8 | |2017-10-27 13:44:12.847|9 | |2017-10-27 13:44:13.847|10 | |2017-10-27 13:44:14.847|11 | |2017-10-27 13:44:15.847|12 | |2017-10-27 13:44:16.847|13 | |2017-10-27 13:44:17.847|14 | |2017-10-27 13:44:18.847|15 | +-----------------------+-----+ */ // Stop q4s on a separate thread // as we're about to block the current thread awaiting query termination import java.util.concurrent.Executors import java.util.concurrent.TimeUnit.SECONDS def queryTerminator(query: StreamingQuery) = new Runnable { def run = { println(s\"Stopping streaming query: ${query.id}\") query.stop } } import java.util.concurrent.TimeUnit.SECONDS // Stop the first query after 10 seconds Executors.newSingleThreadScheduledExecutor. scheduleWithFixedDelay(queryTerminator(q4s), 10, 60 * 5, SECONDS) // Stop the other query after 20 seconds Executors.newSingleThreadScheduledExecutor. scheduleWithFixedDelay(queryTerminator(q10s), 20, 60 * 5, SECONDS) // Use StreamingQueryManager to wait for any query termination (either q1 or q2) // the current thread will block indefinitely until either streaming query has finished spark.streams.awaitAnyTermination // You are here only after either streaming query has finished // Executing spark.streams.awaitAnyTermination again would return immediately // You should have received the QueryTerminatedEvent for the query termination // reset the last terminated streaming query spark.streams.resetTerminated // You know at least one query has terminated // Wait for the other query to terminate spark.streams.awaitAnyTermination assert(spark.streams.active.isEmpty) println(\"The demo went all fine. Exiting...\") // leave spark-shell System.exit(0)","title":"Demo: Using StreamingQueryManager for Query Termination Management"},{"location":"demo/arbitrary-stateful-streaming-aggregation-flatMapGroupsWithState/","text":"Demo: Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator \u00b6 The following demo shows an example of Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState operator. import java.sql.Timestamp type DeviceId = Long case class Signal(timestamp: Timestamp, deviceId: DeviceId, value: Long) // input stream import org.apache.spark.sql.functions._ val signals = spark .readStream .format(\"rate\") .option(\"rowsPerSecond\", 1) .load .withColumn(\"deviceId\", rint(rand() * 10) cast \"int\") // 10 devices randomly assigned to values .withColumn(\"value\", $\"value\" % 10) // randomize the values (just for fun) .as[Signal] // convert to our type (from \"unpleasant\" Row) import org.apache.spark.sql.streaming.GroupState type Key = Int type Count = Long type State = Map[Key, Count] case class EventsCounted(deviceId: DeviceId, count: Long) def countValuesPerDevice( deviceId: Int, signals: Iterator[Signal], state: GroupState[State]): Iterator[EventsCounted] = { val values = signals.toSeq println(s\"Device: $deviceId\") println(s\"Signals (${values.size}):\") values.zipWithIndex.foreach { case (v, idx) => println(s\"$idx. $v\") } println(s\"State: $state\") // update the state with the count of elements for the key val initialState: State = Map(deviceId -> 0) val oldState = state.getOption.getOrElse(initialState) // the name to highlight that the state is for the key only val newValue = oldState(deviceId) + values.size val newState = Map(deviceId -> newValue) state.update(newState) // you must not return as it's already consumed // that leads to a very subtle error where no elements are in an iterator // iterators are one-pass data structures Iterator(EventsCounted(deviceId, newValue)) } // stream processing using flatMapGroupsWithState operator val deviceId: Signal => DeviceId = { case Signal(_, deviceId, _) => deviceId } val signalsByDevice = signals.groupByKey(deviceId) import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode} val signalCounter = signalsByDevice.flatMapGroupsWithState( outputMode = OutputMode.Append, timeoutConf = GroupStateTimeout.NoTimeout)(countValuesPerDevice) import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val sq = signalCounter. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Append). start","title":"Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator"},{"location":"demo/arbitrary-stateful-streaming-aggregation-flatMapGroupsWithState/#demo-arbitrary-stateful-streaming-aggregation-with-keyvaluegroupeddatasetflatmapgroupswithstate-operator","text":"The following demo shows an example of Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState operator. import java.sql.Timestamp type DeviceId = Long case class Signal(timestamp: Timestamp, deviceId: DeviceId, value: Long) // input stream import org.apache.spark.sql.functions._ val signals = spark .readStream .format(\"rate\") .option(\"rowsPerSecond\", 1) .load .withColumn(\"deviceId\", rint(rand() * 10) cast \"int\") // 10 devices randomly assigned to values .withColumn(\"value\", $\"value\" % 10) // randomize the values (just for fun) .as[Signal] // convert to our type (from \"unpleasant\" Row) import org.apache.spark.sql.streaming.GroupState type Key = Int type Count = Long type State = Map[Key, Count] case class EventsCounted(deviceId: DeviceId, count: Long) def countValuesPerDevice( deviceId: Int, signals: Iterator[Signal], state: GroupState[State]): Iterator[EventsCounted] = { val values = signals.toSeq println(s\"Device: $deviceId\") println(s\"Signals (${values.size}):\") values.zipWithIndex.foreach { case (v, idx) => println(s\"$idx. $v\") } println(s\"State: $state\") // update the state with the count of elements for the key val initialState: State = Map(deviceId -> 0) val oldState = state.getOption.getOrElse(initialState) // the name to highlight that the state is for the key only val newValue = oldState(deviceId) + values.size val newState = Map(deviceId -> newValue) state.update(newState) // you must not return as it's already consumed // that leads to a very subtle error where no elements are in an iterator // iterators are one-pass data structures Iterator(EventsCounted(deviceId, newValue)) } // stream processing using flatMapGroupsWithState operator val deviceId: Signal => DeviceId = { case Signal(_, deviceId, _) => deviceId } val signalsByDevice = signals.groupByKey(deviceId) import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode} val signalCounter = signalsByDevice.flatMapGroupsWithState( outputMode = OutputMode.Append, timeoutConf = GroupStateTimeout.NoTimeout)(countValuesPerDevice) import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val sq = signalCounter. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Append). start","title":"Demo: Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator"},{"location":"demo/current_timestamp/","text":"Demo: current_timestamp Function For Processing Time in Streaming Queries \u00b6 The demo shows what happens when you use current_timestamp function in your structured queries. [NOTE] \u00b6 The main motivation was to answer the question https://stackoverflow.com/q/46274593/1305344[How to achieve ingestion time?] in Spark Structured Streaming. You're very welcome to upvote the question and answers at your earliest convenience. Thanks! \u00b6 Quoting the https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html[Apache Flink documentation]: Event time is the time that each individual event occurred on its producing device. This time is typically embedded within the records before they enter Flink and that event timestamp can be extracted from the record. That is exactly how event time is considered in withWatermark operator which you use to describe what column to use for event time. The column could be part of the input dataset or...generated. And that is the moment where my confusion starts. In order to generate the event time column for withWatermark operator you could use current_timestamp or current_date standard functions. [source, scala] \u00b6 // rate format gives event time // but let's generate a brand new column with ours // for demo purposes val values = spark. readStream. format(\"rate\"). load. withColumn(\"current_timestamp\", current_timestamp) scala> values.printSchema root |-- timestamp: timestamp (nullable = true) |-- value: long (nullable = true) |-- current_timestamp: timestamp (nullable = false) Both are special for Spark Structured Streaming as StreamExecution MicroBatchExecution.md#runBatch-triggerLogicalPlan[replaces] their underlying Catalyst expressions, CurrentTimestamp and CurrentDate respectively, with CurrentBatchTimestamp expression and the time of the current batch. [source, scala] \u00b6 import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = values. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). start // note the value of current_timestamp // that corresponds to the batch time Batch: 1 \u00b6 +-----------------------+-----+-------------------+ |timestamp |value|current_timestamp | +-----------------------+-----+-------------------+ |2017-09-18 10:53:31.523|0 |2017-09-18 10:53:40| |2017-09-18 10:53:32.523|1 |2017-09-18 10:53:40| |2017-09-18 10:53:33.523|2 |2017-09-18 10:53:40| |2017-09-18 10:53:34.523|3 |2017-09-18 10:53:40| |2017-09-18 10:53:35.523|4 |2017-09-18 10:53:40| |2017-09-18 10:53:36.523|5 |2017-09-18 10:53:40| |2017-09-18 10:53:37.523|6 |2017-09-18 10:53:40| |2017-09-18 10:53:38.523|7 |2017-09-18 10:53:40| +-----------------------+-----+-------------------+ // Use web UI's SQL tab for the batch (Submitted column) // or sq.recentProgress scala> println(sq.recentProgress(1).timestamp) 2017-09-18T08:53:40.000Z // Note current_batch_timestamp scala> sq.explain(extended = true) == Parsed Logical Plan == 'Project [timestamp#2137, value#2138L, current_batch_timestamp(1505725650005, TimestampType, None) AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true == Analyzed Logical Plan == timestamp: timestamp, value: bigint, current_timestamp: timestamp Project [timestamp#2137, value#2138L, current_batch_timestamp(1505725650005, TimestampType, Some(Europe/Berlin)) AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true == Optimized Logical Plan == Project [timestamp#2137, value#2138L, 1505725650005000 AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true == Physical Plan == *Project [timestamp#2137, value#2138L, 1505725650005000 AS current_timestamp#50] +- Scan ExistingRDD[timestamp#2137,value#2138L] That seems to be closer to processing time than ingestion time given the definition from the https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html[Apache Flink documentation]: Processing time refers to the system time of the machine that is executing the respective operation. Ingestion time is the time that events enter Flink. What do you think?","title":"current_timestamp Function For Processing Time in Streaming Queries"},{"location":"demo/current_timestamp/#demo-current_timestamp-function-for-processing-time-in-streaming-queries","text":"The demo shows what happens when you use current_timestamp function in your structured queries.","title":"Demo: current_timestamp Function For Processing Time in Streaming Queries"},{"location":"demo/current_timestamp/#note","text":"The main motivation was to answer the question https://stackoverflow.com/q/46274593/1305344[How to achieve ingestion time?] in Spark Structured Streaming.","title":"[NOTE]"},{"location":"demo/current_timestamp/#youre-very-welcome-to-upvote-the-question-and-answers-at-your-earliest-convenience-thanks","text":"Quoting the https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html[Apache Flink documentation]: Event time is the time that each individual event occurred on its producing device. This time is typically embedded within the records before they enter Flink and that event timestamp can be extracted from the record. That is exactly how event time is considered in withWatermark operator which you use to describe what column to use for event time. The column could be part of the input dataset or...generated. And that is the moment where my confusion starts. In order to generate the event time column for withWatermark operator you could use current_timestamp or current_date standard functions.","title":"You're very welcome to upvote the question and answers at your earliest convenience. Thanks!"},{"location":"demo/current_timestamp/#source-scala","text":"// rate format gives event time // but let's generate a brand new column with ours // for demo purposes val values = spark. readStream. format(\"rate\"). load. withColumn(\"current_timestamp\", current_timestamp) scala> values.printSchema root |-- timestamp: timestamp (nullable = true) |-- value: long (nullable = true) |-- current_timestamp: timestamp (nullable = false) Both are special for Spark Structured Streaming as StreamExecution MicroBatchExecution.md#runBatch-triggerLogicalPlan[replaces] their underlying Catalyst expressions, CurrentTimestamp and CurrentDate respectively, with CurrentBatchTimestamp expression and the time of the current batch.","title":"[source, scala]"},{"location":"demo/current_timestamp/#source-scala_1","text":"import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = values. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). start // note the value of current_timestamp // that corresponds to the batch time","title":"[source, scala]"},{"location":"demo/current_timestamp/#batch-1","text":"+-----------------------+-----+-------------------+ |timestamp |value|current_timestamp | +-----------------------+-----+-------------------+ |2017-09-18 10:53:31.523|0 |2017-09-18 10:53:40| |2017-09-18 10:53:32.523|1 |2017-09-18 10:53:40| |2017-09-18 10:53:33.523|2 |2017-09-18 10:53:40| |2017-09-18 10:53:34.523|3 |2017-09-18 10:53:40| |2017-09-18 10:53:35.523|4 |2017-09-18 10:53:40| |2017-09-18 10:53:36.523|5 |2017-09-18 10:53:40| |2017-09-18 10:53:37.523|6 |2017-09-18 10:53:40| |2017-09-18 10:53:38.523|7 |2017-09-18 10:53:40| +-----------------------+-----+-------------------+ // Use web UI's SQL tab for the batch (Submitted column) // or sq.recentProgress scala> println(sq.recentProgress(1).timestamp) 2017-09-18T08:53:40.000Z // Note current_batch_timestamp scala> sq.explain(extended = true) == Parsed Logical Plan == 'Project [timestamp#2137, value#2138L, current_batch_timestamp(1505725650005, TimestampType, None) AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true == Analyzed Logical Plan == timestamp: timestamp, value: bigint, current_timestamp: timestamp Project [timestamp#2137, value#2138L, current_batch_timestamp(1505725650005, TimestampType, Some(Europe/Berlin)) AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true == Optimized Logical Plan == Project [timestamp#2137, value#2138L, 1505725650005000 AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true == Physical Plan == *Project [timestamp#2137, value#2138L, 1505725650005000 AS current_timestamp#50] +- Scan ExistingRDD[timestamp#2137,value#2138L] That seems to be closer to processing time than ingestion time given the definition from the https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html[Apache Flink documentation]: Processing time refers to the system time of the machine that is executing the respective operation. Ingestion time is the time that events enter Flink. What do you think?","title":"Batch: 1"},{"location":"demo/custom-sink-webui/","text":"Demo: Developing Custom Streaming Sink (and Monitoring SQL Queries in web UI) \u00b6 The demo shows the steps to develop a custom streaming sink and use it to monitor whether and what SQL queries are executed at runtime (using web UI's SQL tab). [NOTE] \u00b6 The main motivation was to answer the question https://stackoverflow.com/q/46162143/1305344[Why does a single structured query run multiple SQL queries per batch?] that happened to have turned out fairly surprising. You're very welcome to upvote the question and answers at your earliest convenience. Thanks! \u00b6 The steps are as follows: < > < > < > < > < > < > < > Findings (aka surprises ): Custom sinks require that you define a checkpoint location using checkpointLocation option (or spark.sql.streaming.checkpointLocation Spark property). Remove the checkpoint directory (or use a different one every start of a streaming query) to have consistent results. === [[DemoSink]] Creating Custom Sink -- DemoSink [source, scala] \u00b6 package pl.japila.spark.sql.streaming case class DemoSink( sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode) extends Sink { override def addBatch(batchId: Long, data: DataFrame): Unit = { println(s\"addBatch($batchId)\") data.explain() // Why so many lines just to show the input DataFrame? data.sparkSession.createDataFrame( data.sparkSession.sparkContext.parallelize(data.collect()), data.schema) .show(10) } } Save the file under src/main/scala in your project. === [[DemoSinkProvider]] Creating StreamSinkProvider -- DemoSinkProvider [source, scala] \u00b6 package pl.japila.spark.sql.streaming class DemoSinkProvider extends StreamSinkProvider with DataSourceRegister { override def createSink( sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode): Sink = { DemoSink(sqlContext, parameters, partitionColumns, outputMode) } override def shortName(): String = \"demo\" } Save the file under src/main/scala in your project. === [[registering-sink]] Optional Sink Registration using META-INF/services The step is optional, but greatly improve the experience when using the custom sink so you can use it by its name (rather than a fully-qualified class name or using a special class name for the sink provider). Create org.apache.spark.sql.sources.DataSourceRegister in META-INF/services directory with the following content. [source, scala] \u00b6 pl.japila.spark.sql.streaming.DemoSinkProvider \u00b6 Save the file under src/main/resources in your project. === [[build-sbt]] build.sbt Definition If you use my beloved build tool http://www.scala-sbt.org/[sbt ] to manage the project, use the following build.sbt . [source, scala] \u00b6 organization := \"pl.japila.spark\" name := \"spark-structured-streaming-demo-sink\" version := \"0.1\" scalaVersion := \"2.11.11\" libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.2.0\" \u00b6 === [[packaging-sink]] Packaging DemoSink The step depends on what build tool you use to manage the project. Use whatever command you use to create a jar file with the above classes compiled and bundled together. $ sbt package [info] Loading settings from plugins.sbt ... [info] Loading project definition from /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/project [info] Loading settings from build.sbt ... [info] Set current project to spark-structured-streaming-demo-sink (in build file:/Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/) [info] Compiling 1 Scala source to /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/classes ... [info] Done compiling. [info] Packaging /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar ... [info] Done packaging. [success] Total time: 5 s, completed Sep 12, 2017 9:34:19 AM The jar with the sink is /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar . === [[using-sink]] Using DemoSink in Streaming Query The following code reads data from the rate source and simply outputs the result to our custom DemoSink . // Make sure the DemoSink jar is available $ ls /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar // \"Install\" the DemoSink using --jars command-line option $ ./bin/spark-shell --jars /Users/jacek/dev/sandbox/spark-structured-streaming-custom-sink/target/scala-2.11/spark-structured-streaming-custom-sink_2.11-0.1.jar scala> spark.version res0: String = 2.3.0-SNAPSHOT import org.apache.spark.sql.streaming._ import scala.concurrent.duration._ val sq = spark. readStream. format(\"rate\"). load. writeStream. format(\"demo\"). option(\"checkpointLocation\", \"/tmp/demo-checkpoint\"). trigger(Trigger.ProcessingTime(10.seconds)). start // In the end... scala> sq.stop 17/09/12 09:59:28 INFO StreamExecution: Query [id = 03cd78e3-94e2-439c-9c12-cfed0c996812, runId = 6938af91-9806-4404-965a-5ae7525d5d3f] was stopped === [[webui-sql-queries]] Monitoring SQL Queries using web UI's SQL Tab Open http://localhost:4040/SQL/ . You should find that every trigger (aka batch ) results in 3 SQL queries. Why? .web UI's SQL Tab and Completed Queries (3 Queries per Batch) image::images/webui-sql-completed-queries-three-per-batch.png[align=\"center\"] The answer lies in what sources and sink a streaming query uses (and differs per streaming query). In our case, < > collects the rows from the input DataFrame and shows it afterwards. That gives 2 SQL queries (as you can see after executing the following batch queries). [source, scala] \u00b6 // batch non-streaming query val data = (0 to 3).toDF(\"id\") // That gives one SQL query data.collect // That gives one SQL query, too data.show The remaining query (which is the first among the queries) is executed when you load the data. That can be observed easily when you change < > to not \"touch\" the input data (in addBatch ) in any way. [source, scala] \u00b6 override def addBatch(batchId: Long, data: DataFrame): Unit = { println(s\"addBatch($batchId)\") } Re-run the streaming query (using the new DemoSink ) and use web UI's SQL tab to see the queries. You should have just one query per batch (and no Spark jobs given nothing is really done in the sink's addBatch ). .web UI's SQL Tab and Completed Queries (1 Query per Batch) image::images/webui-sql-completed-queries-one-per-batch.png[align=\"center\"]","title":"Developing Custom Streaming Sink (and Monitoring SQL Queries in web UI)"},{"location":"demo/custom-sink-webui/#demo-developing-custom-streaming-sink-and-monitoring-sql-queries-in-web-ui","text":"The demo shows the steps to develop a custom streaming sink and use it to monitor whether and what SQL queries are executed at runtime (using web UI's SQL tab).","title":"Demo: Developing Custom Streaming Sink (and Monitoring SQL Queries in web UI)"},{"location":"demo/custom-sink-webui/#note","text":"The main motivation was to answer the question https://stackoverflow.com/q/46162143/1305344[Why does a single structured query run multiple SQL queries per batch?] that happened to have turned out fairly surprising.","title":"[NOTE]"},{"location":"demo/custom-sink-webui/#youre-very-welcome-to-upvote-the-question-and-answers-at-your-earliest-convenience-thanks","text":"The steps are as follows: < > < > < > < > < > < > < > Findings (aka surprises ): Custom sinks require that you define a checkpoint location using checkpointLocation option (or spark.sql.streaming.checkpointLocation Spark property). Remove the checkpoint directory (or use a different one every start of a streaming query) to have consistent results. === [[DemoSink]] Creating Custom Sink -- DemoSink","title":"You're very welcome to upvote the question and answers at your earliest convenience. Thanks!"},{"location":"demo/custom-sink-webui/#source-scala","text":"package pl.japila.spark.sql.streaming case class DemoSink( sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode) extends Sink { override def addBatch(batchId: Long, data: DataFrame): Unit = { println(s\"addBatch($batchId)\") data.explain() // Why so many lines just to show the input DataFrame? data.sparkSession.createDataFrame( data.sparkSession.sparkContext.parallelize(data.collect()), data.schema) .show(10) } } Save the file under src/main/scala in your project. === [[DemoSinkProvider]] Creating StreamSinkProvider -- DemoSinkProvider","title":"[source, scala]"},{"location":"demo/custom-sink-webui/#source-scala_1","text":"package pl.japila.spark.sql.streaming class DemoSinkProvider extends StreamSinkProvider with DataSourceRegister { override def createSink( sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode): Sink = { DemoSink(sqlContext, parameters, partitionColumns, outputMode) } override def shortName(): String = \"demo\" } Save the file under src/main/scala in your project. === [[registering-sink]] Optional Sink Registration using META-INF/services The step is optional, but greatly improve the experience when using the custom sink so you can use it by its name (rather than a fully-qualified class name or using a special class name for the sink provider). Create org.apache.spark.sql.sources.DataSourceRegister in META-INF/services directory with the following content.","title":"[source, scala]"},{"location":"demo/custom-sink-webui/#source-scala_2","text":"","title":"[source, scala]"},{"location":"demo/custom-sink-webui/#pljapilasparksqlstreamingdemosinkprovider","text":"Save the file under src/main/resources in your project. === [[build-sbt]] build.sbt Definition If you use my beloved build tool http://www.scala-sbt.org/[sbt ] to manage the project, use the following build.sbt .","title":"pl.japila.spark.sql.streaming.DemoSinkProvider"},{"location":"demo/custom-sink-webui/#source-scala_3","text":"organization := \"pl.japila.spark\" name := \"spark-structured-streaming-demo-sink\" version := \"0.1\" scalaVersion := \"2.11.11\"","title":"[source, scala]"},{"location":"demo/custom-sink-webui/#librarydependencies-orgapachespark-spark-sql-220","text":"=== [[packaging-sink]] Packaging DemoSink The step depends on what build tool you use to manage the project. Use whatever command you use to create a jar file with the above classes compiled and bundled together. $ sbt package [info] Loading settings from plugins.sbt ... [info] Loading project definition from /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/project [info] Loading settings from build.sbt ... [info] Set current project to spark-structured-streaming-demo-sink (in build file:/Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/) [info] Compiling 1 Scala source to /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/classes ... [info] Done compiling. [info] Packaging /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar ... [info] Done packaging. [success] Total time: 5 s, completed Sep 12, 2017 9:34:19 AM The jar with the sink is /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar . === [[using-sink]] Using DemoSink in Streaming Query The following code reads data from the rate source and simply outputs the result to our custom DemoSink . // Make sure the DemoSink jar is available $ ls /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar // \"Install\" the DemoSink using --jars command-line option $ ./bin/spark-shell --jars /Users/jacek/dev/sandbox/spark-structured-streaming-custom-sink/target/scala-2.11/spark-structured-streaming-custom-sink_2.11-0.1.jar scala> spark.version res0: String = 2.3.0-SNAPSHOT import org.apache.spark.sql.streaming._ import scala.concurrent.duration._ val sq = spark. readStream. format(\"rate\"). load. writeStream. format(\"demo\"). option(\"checkpointLocation\", \"/tmp/demo-checkpoint\"). trigger(Trigger.ProcessingTime(10.seconds)). start // In the end... scala> sq.stop 17/09/12 09:59:28 INFO StreamExecution: Query [id = 03cd78e3-94e2-439c-9c12-cfed0c996812, runId = 6938af91-9806-4404-965a-5ae7525d5d3f] was stopped === [[webui-sql-queries]] Monitoring SQL Queries using web UI's SQL Tab Open http://localhost:4040/SQL/ . You should find that every trigger (aka batch ) results in 3 SQL queries. Why? .web UI's SQL Tab and Completed Queries (3 Queries per Batch) image::images/webui-sql-completed-queries-three-per-batch.png[align=\"center\"] The answer lies in what sources and sink a streaming query uses (and differs per streaming query). In our case, < > collects the rows from the input DataFrame and shows it afterwards. That gives 2 SQL queries (as you can see after executing the following batch queries).","title":"libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.2.0\""},{"location":"demo/custom-sink-webui/#source-scala_4","text":"// batch non-streaming query val data = (0 to 3).toDF(\"id\") // That gives one SQL query data.collect // That gives one SQL query, too data.show The remaining query (which is the first among the queries) is executed when you load the data. That can be observed easily when you change < > to not \"touch\" the input data (in addBatch ) in any way.","title":"[source, scala]"},{"location":"demo/custom-sink-webui/#source-scala_5","text":"override def addBatch(batchId: Long, data: DataFrame): Unit = { println(s\"addBatch($batchId)\") } Re-run the streaming query (using the new DemoSink ) and use web UI's SQL tab to see the queries. You should have just one query per batch (and no Spark jobs given nothing is really done in the sink's addBatch ). .web UI's SQL Tab and Completed Queries (1 Query per Batch) image::images/webui-sql-completed-queries-one-per-batch.png[align=\"center\"]","title":"[source, scala]"},{"location":"demo/deep-dive-into-filestreamsink/","text":"Demo: Deep Dive into FileStreamSink \u00b6 This demo shows a streaming query that writes out to FileStreamSink . Prerequisites \u00b6 A sample streaming query reads data using socket data source. Start nc . nc -lk 9999 Configure Logging \u00b6 Enable logging for FileStreamSink . Start Streaming Query \u00b6 Use spark-shell for fast interactive prototyping. Describe the source. val lines = spark . readStream . format ( \"socket\" ) . option ( \"host\" , \"localhost\" ) . option ( \"port\" , \"9999\" ) . load Describe the sink and start the streaming query. import org . apache . spark . sql . streaming .{ OutputMode , Trigger } import concurrent . duration . _ val interval = 15 . seconds val trigger = Trigger . ProcessingTime ( interval ) val queryName = s\"micro-batch every $ interval \" val sq = lines . writeStream . format ( \"text\" ) . option ( \"checkpointLocation\" , \"/tmp/checkpointLocation\" ) . trigger ( trigger ) . outputMode ( OutputMode . Append ) // only Append supported . queryName ( queryName ) . start ( path = \"/tmp/socket-file\" ) Use web UI to monitor the query ( http://localhost:4040 ). Deep Dive into Internals \u00b6 import org . apache . spark . sql . streaming . StreamingQuery assert ( sq . isInstanceOf [ StreamingQuery ]) scala> sq.explain(extended = true) == Parsed Logical Plan == StreamingDataSourceV2Relation [value#0], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@aa58ed0, TextSocketV2[host: localhost, port: 9999], -1, 0 == Analyzed Logical Plan == value: string StreamingDataSourceV2Relation [value#0], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@aa58ed0, TextSocketV2[host: localhost, port: 9999], -1, 0 == Optimized Logical Plan == StreamingDataSourceV2Relation [value#0], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@aa58ed0, TextSocketV2[host: localhost, port: 9999], -1, 0 == Physical Plan == *(1) Project [value#0] +- MicroBatchScan[value#0] class org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1 scala> println(sq.lastProgress) { \"id\" : \"f4dc1b6c-6bc7-423a-9bfe-49db2a440bda\", \"runId\" : \"1a05533a-4db0-486d-8c44-7d4a8e49a7bc\", \"name\" : \"micro-batch every 15 seconds\", \"timestamp\" : \"2020-10-17T09:47:30.003Z\", \"batchId\" : 2, \"numInputRows\" : 0, \"inputRowsPerSecond\" : 0.0, \"processedRowsPerSecond\" : 0.0, \"durationMs\" : { \"latestOffset\" : 0, \"triggerExecution\" : 0 }, \"stateOperators\" : [ ], \"sources\" : [ { \"description\" : \"TextSocketV2[host: localhost, port: 9999]\", \"startOffset\" : 0, \"endOffset\" : 0, \"numInputRows\" : 0, \"inputRowsPerSecond\" : 0.0, \"processedRowsPerSecond\" : 0.0 } ], \"sink\" : { \"description\" : \"FileSink[/tmp/socket-file]\", \"numOutputRows\" : -1 } } import org.apache.spark.sql.execution.debug._ scala> sq.debugCodegen() Found 1 WholeStageCodegen subtrees. == Subtree 1 / 1 (maxMethodCodeSize:120; maxConstantPoolSize:100(0.15% used); numInnerClasses:0) == *(1) Project [value#0] +- MicroBatchScan[value#0] class org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1 Generated code: /* 001 */ public Object generate(Object[] references) { /* 002 */ return new GeneratedIteratorForCodegenStage1(references); /* 003 */ } /* 004 */ /* 005 */ // codegenStageId=1 /* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator { /* 007 */ private Object[] references; ... MicroBatchExecution \u00b6 Access MicroBatchExecution . import org . apache . spark . sql . execution . streaming . StreamingQueryWrapper val streamEngine = sq . asInstanceOf [ StreamingQueryWrapper ]. streamingQuery import org . apache . spark . sql . execution . streaming . StreamExecution assert ( streamEngine . isInstanceOf [ StreamExecution ]) import org . apache . spark . sql . execution . streaming . MicroBatchExecution val microBatchEngine = streamEngine . asInstanceOf [ MicroBatchExecution ] IncrementalExecution \u00b6 Access IncrementalExecution . val qe = microBatchEngine . lastExecution import org . apache . spark . sql . execution . streaming . IncrementalExecution assert ( qe . isInstanceOf [ IncrementalExecution ]) assert ( qe != null , \"No physical plan. Waiting for data.\" ) FileStreamSink \u00b6 A streaming query (as a StreamExecution ) is associated with one sink . That's the FileStreamSink in this demo. import org . apache . spark . sql . execution . streaming . FileStreamSink val sink = microBatchEngine . sink . asInstanceOf [ FileStreamSink ] assert ( sink . isInstanceOf [ FileStreamSink ]) scala> println(sink) FileSink[/tmp/socket-file] Since FileStreamSink has already been requested to add at least one batch, requesting it to add 0 th batch again should be skipped. scala> sink.addBatch(batchId = 0, data = spark.range(5).toDF) FileStreamSink: Skipping already committed batch 0 Stop Query \u00b6 spark . streams . active . foreach ( _ . stop )","title":"Deep Dive into FileStreamSink"},{"location":"demo/deep-dive-into-filestreamsink/#demo-deep-dive-into-filestreamsink","text":"This demo shows a streaming query that writes out to FileStreamSink .","title":"Demo: Deep Dive into FileStreamSink"},{"location":"demo/deep-dive-into-filestreamsink/#prerequisites","text":"A sample streaming query reads data using socket data source. Start nc . nc -lk 9999","title":"Prerequisites"},{"location":"demo/deep-dive-into-filestreamsink/#configure-logging","text":"Enable logging for FileStreamSink .","title":"Configure Logging"},{"location":"demo/deep-dive-into-filestreamsink/#start-streaming-query","text":"Use spark-shell for fast interactive prototyping. Describe the source. val lines = spark . readStream . format ( \"socket\" ) . option ( \"host\" , \"localhost\" ) . option ( \"port\" , \"9999\" ) . load Describe the sink and start the streaming query. import org . apache . spark . sql . streaming .{ OutputMode , Trigger } import concurrent . duration . _ val interval = 15 . seconds val trigger = Trigger . ProcessingTime ( interval ) val queryName = s\"micro-batch every $ interval \" val sq = lines . writeStream . format ( \"text\" ) . option ( \"checkpointLocation\" , \"/tmp/checkpointLocation\" ) . trigger ( trigger ) . outputMode ( OutputMode . Append ) // only Append supported . queryName ( queryName ) . start ( path = \"/tmp/socket-file\" ) Use web UI to monitor the query ( http://localhost:4040 ).","title":"Start Streaming Query"},{"location":"demo/deep-dive-into-filestreamsink/#deep-dive-into-internals","text":"import org . apache . spark . sql . streaming . StreamingQuery assert ( sq . isInstanceOf [ StreamingQuery ]) scala> sq.explain(extended = true) == Parsed Logical Plan == StreamingDataSourceV2Relation [value#0], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@aa58ed0, TextSocketV2[host: localhost, port: 9999], -1, 0 == Analyzed Logical Plan == value: string StreamingDataSourceV2Relation [value#0], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@aa58ed0, TextSocketV2[host: localhost, port: 9999], -1, 0 == Optimized Logical Plan == StreamingDataSourceV2Relation [value#0], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@aa58ed0, TextSocketV2[host: localhost, port: 9999], -1, 0 == Physical Plan == *(1) Project [value#0] +- MicroBatchScan[value#0] class org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1 scala> println(sq.lastProgress) { \"id\" : \"f4dc1b6c-6bc7-423a-9bfe-49db2a440bda\", \"runId\" : \"1a05533a-4db0-486d-8c44-7d4a8e49a7bc\", \"name\" : \"micro-batch every 15 seconds\", \"timestamp\" : \"2020-10-17T09:47:30.003Z\", \"batchId\" : 2, \"numInputRows\" : 0, \"inputRowsPerSecond\" : 0.0, \"processedRowsPerSecond\" : 0.0, \"durationMs\" : { \"latestOffset\" : 0, \"triggerExecution\" : 0 }, \"stateOperators\" : [ ], \"sources\" : [ { \"description\" : \"TextSocketV2[host: localhost, port: 9999]\", \"startOffset\" : 0, \"endOffset\" : 0, \"numInputRows\" : 0, \"inputRowsPerSecond\" : 0.0, \"processedRowsPerSecond\" : 0.0 } ], \"sink\" : { \"description\" : \"FileSink[/tmp/socket-file]\", \"numOutputRows\" : -1 } } import org.apache.spark.sql.execution.debug._ scala> sq.debugCodegen() Found 1 WholeStageCodegen subtrees. == Subtree 1 / 1 (maxMethodCodeSize:120; maxConstantPoolSize:100(0.15% used); numInnerClasses:0) == *(1) Project [value#0] +- MicroBatchScan[value#0] class org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1 Generated code: /* 001 */ public Object generate(Object[] references) { /* 002 */ return new GeneratedIteratorForCodegenStage1(references); /* 003 */ } /* 004 */ /* 005 */ // codegenStageId=1 /* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator { /* 007 */ private Object[] references; ...","title":"Deep Dive into Internals"},{"location":"demo/deep-dive-into-filestreamsink/#microbatchexecution","text":"Access MicroBatchExecution . import org . apache . spark . sql . execution . streaming . StreamingQueryWrapper val streamEngine = sq . asInstanceOf [ StreamingQueryWrapper ]. streamingQuery import org . apache . spark . sql . execution . streaming . StreamExecution assert ( streamEngine . isInstanceOf [ StreamExecution ]) import org . apache . spark . sql . execution . streaming . MicroBatchExecution val microBatchEngine = streamEngine . asInstanceOf [ MicroBatchExecution ]","title":"MicroBatchExecution"},{"location":"demo/deep-dive-into-filestreamsink/#incrementalexecution","text":"Access IncrementalExecution . val qe = microBatchEngine . lastExecution import org . apache . spark . sql . execution . streaming . IncrementalExecution assert ( qe . isInstanceOf [ IncrementalExecution ]) assert ( qe != null , \"No physical plan. Waiting for data.\" )","title":"IncrementalExecution"},{"location":"demo/deep-dive-into-filestreamsink/#filestreamsink","text":"A streaming query (as a StreamExecution ) is associated with one sink . That's the FileStreamSink in this demo. import org . apache . spark . sql . execution . streaming . FileStreamSink val sink = microBatchEngine . sink . asInstanceOf [ FileStreamSink ] assert ( sink . isInstanceOf [ FileStreamSink ]) scala> println(sink) FileSink[/tmp/socket-file] Since FileStreamSink has already been requested to add at least one batch, requesting it to add 0 th batch again should be skipped. scala> sink.addBatch(batchId = 0, data = spark.range(5).toDF) FileStreamSink: Skipping already committed batch 0","title":"FileStreamSink"},{"location":"demo/deep-dive-into-filestreamsink/#stop-query","text":"spark . streams . active . foreach ( _ . stop )","title":"Stop Query"},{"location":"demo/exploring-checkpointed-state/","text":"Demo: Exploring Checkpointed State \u00b6 The following demo shows the internals of the checkpointed state of a stateful streaming query . The demo uses the state checkpoint directory that was used in Demo: Streaming Watermark with Aggregation in Append Output Mode . // Change the path to match your configuration val checkpointRootLocation = \"/tmp/checkpoint-watermark_demo/state\" val version = 1L import org.apache.spark.sql.execution.streaming.state.StateStoreId val storeId = StateStoreId( checkpointRootLocation, operatorId = 0, partitionId = 0) // The key and value schemas should match the watermark demo // .groupBy(window($\"time\", windowDuration.toString) as \"sliding_window\") import org.apache.spark.sql.types.{TimestampType, StructField, StructType} val keySchema = StructType( StructField(\"sliding_window\", StructType( StructField(\"start\", TimestampType, nullable = true) :: StructField(\"end\", TimestampType, nullable = true) :: Nil), nullable = false) :: Nil) scala> keySchema.printTreeString root |-- sliding_window: struct (nullable = false) | |-- start: timestamp (nullable = true) | |-- end: timestamp (nullable = true) // .agg(collect_list(\"batch\") as \"batches\", collect_list(\"value\") as \"values\") import org.apache.spark.sql.types.{ArrayType, LongType} val valueSchema = StructType( StructField(\"batches\", ArrayType(LongType, true), true) :: StructField(\"values\", ArrayType(LongType, true), true) :: Nil) scala> valueSchema.printTreeString root |-- batches: array (nullable = true) | |-- element: long (containsNull = true) |-- values: array (nullable = true) | |-- element: long (containsNull = true) val indexOrdinal = None import org.apache.spark.sql.execution.streaming.state.StateStoreConf val storeConf = StateStoreConf(spark.sessionState.conf) val hadoopConf = spark.sessionState.newHadoopConf() import org.apache.spark.sql.execution.streaming.state.StateStoreProvider val provider = StateStoreProvider.createAndInit( storeId, keySchema, valueSchema, indexOrdinal, storeConf, hadoopConf) // You may want to use the following higher-level code instead import java.util.UUID val queryRunId = UUID.randomUUID import org.apache.spark.sql.execution.streaming.state.StateStoreProviderId val storeProviderId = StateStoreProviderId(storeId, queryRunId) import org.apache.spark.sql.execution.streaming.state.StateStore val store = StateStore.get( storeProviderId, keySchema, valueSchema, indexOrdinal, version, storeConf, hadoopConf) import org.apache.spark.sql.execution.streaming.state.UnsafeRowPair def formatRowPair(rowPair: UnsafeRowPair) = { s\"(${rowPair.key.getLong(0)}, ${rowPair.value.getLong(0)})\" } store.iterator.map(formatRowPair).foreach(println) // WIP: Missing value (per window) def formatRowPair(rowPair: UnsafeRowPair) = { val window = rowPair.key.getStruct(0, 2) import scala.concurrent.duration._ val begin = window.getLong(0).millis.toSeconds val end = window.getLong(1).millis.toSeconds val value = rowPair.value.getStruct(0, 4) // input is (time, value, batch) all longs val t = value.getLong(1).millis.toSeconds val v = value.getLong(2) val b = value.getLong(3) s\"(key: [$begin, $end], ($t, $v, $b))\" } store.iterator.map(formatRowPair).foreach(println)","title":"Exploring Checkpointed State"},{"location":"demo/exploring-checkpointed-state/#demo-exploring-checkpointed-state","text":"The following demo shows the internals of the checkpointed state of a stateful streaming query . The demo uses the state checkpoint directory that was used in Demo: Streaming Watermark with Aggregation in Append Output Mode . // Change the path to match your configuration val checkpointRootLocation = \"/tmp/checkpoint-watermark_demo/state\" val version = 1L import org.apache.spark.sql.execution.streaming.state.StateStoreId val storeId = StateStoreId( checkpointRootLocation, operatorId = 0, partitionId = 0) // The key and value schemas should match the watermark demo // .groupBy(window($\"time\", windowDuration.toString) as \"sliding_window\") import org.apache.spark.sql.types.{TimestampType, StructField, StructType} val keySchema = StructType( StructField(\"sliding_window\", StructType( StructField(\"start\", TimestampType, nullable = true) :: StructField(\"end\", TimestampType, nullable = true) :: Nil), nullable = false) :: Nil) scala> keySchema.printTreeString root |-- sliding_window: struct (nullable = false) | |-- start: timestamp (nullable = true) | |-- end: timestamp (nullable = true) // .agg(collect_list(\"batch\") as \"batches\", collect_list(\"value\") as \"values\") import org.apache.spark.sql.types.{ArrayType, LongType} val valueSchema = StructType( StructField(\"batches\", ArrayType(LongType, true), true) :: StructField(\"values\", ArrayType(LongType, true), true) :: Nil) scala> valueSchema.printTreeString root |-- batches: array (nullable = true) | |-- element: long (containsNull = true) |-- values: array (nullable = true) | |-- element: long (containsNull = true) val indexOrdinal = None import org.apache.spark.sql.execution.streaming.state.StateStoreConf val storeConf = StateStoreConf(spark.sessionState.conf) val hadoopConf = spark.sessionState.newHadoopConf() import org.apache.spark.sql.execution.streaming.state.StateStoreProvider val provider = StateStoreProvider.createAndInit( storeId, keySchema, valueSchema, indexOrdinal, storeConf, hadoopConf) // You may want to use the following higher-level code instead import java.util.UUID val queryRunId = UUID.randomUUID import org.apache.spark.sql.execution.streaming.state.StateStoreProviderId val storeProviderId = StateStoreProviderId(storeId, queryRunId) import org.apache.spark.sql.execution.streaming.state.StateStore val store = StateStore.get( storeProviderId, keySchema, valueSchema, indexOrdinal, version, storeConf, hadoopConf) import org.apache.spark.sql.execution.streaming.state.UnsafeRowPair def formatRowPair(rowPair: UnsafeRowPair) = { s\"(${rowPair.key.getLong(0)}, ${rowPair.value.getLong(0)})\" } store.iterator.map(formatRowPair).foreach(println) // WIP: Missing value (per window) def formatRowPair(rowPair: UnsafeRowPair) = { val window = rowPair.key.getStruct(0, 2) import scala.concurrent.duration._ val begin = window.getLong(0).millis.toSeconds val end = window.getLong(1).millis.toSeconds val value = rowPair.value.getStruct(0, 4) // input is (time, value, batch) all longs val t = value.getLong(1).millis.toSeconds val v = value.getLong(2) val b = value.getLong(3) s\"(key: [$begin, $end], ($t, $v, $b))\" } store.iterator.map(formatRowPair).foreach(println)","title":"Demo: Exploring Checkpointed State"},{"location":"demo/groupBy-running-count-complete/","text":"Demo: Streaming Query for Running Counts (Socket Source and Complete Output Mode) \u00b6 The following code shows a streaming aggregation (with Dataset.groupBy operator) in complete output mode that reads text lines from a socket (using socket data source) and outputs running counts of the words. NOTE: The example is \"borrowed\" from http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html[the official documentation of Spark]. Changes and errors are only mine. IMPORTANT: Run nc -lk 9999 first before running the demo. // START: Only for easier debugging // Reduce the number of partitions // The state is then only for one partition // which should make monitoring easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // END: Only for easier debugging val lines = spark .readStream .format(\"socket\") .option(\"host\", \"localhost\") .option(\"port\", 9999) .load scala> lines.printSchema root |-- value: string (nullable = true) import org.apache.spark.sql.functions.explode val words = lines .select(explode(split($\"value\", \"\"\"\\W+\"\"\")) as \"word\") val counts = words.groupBy(\"word\").count scala> counts.printSchema root |-- word: string (nullable = true) |-- count: long (nullable = false) // nc -lk 9999 is supposed to be up at this point val queryName = \"running_counts\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } import org.apache.spark.sql.streaming.OutputMode.Complete val runningCounts = counts .writeStream .format(\"console\") .option(\"checkpointLocation\", checkpointLocation) .outputMode(Complete) .start scala> runningCounts.explain == Physical Plan == WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@205f195c +- *(5) HashAggregate(keys=[word#72], functions=[count(1)]) +- StateStoreSave [word#72], state info [ checkpoint = file:/tmp/checkpoint-running_counts/state, runId = f3b2e642-1790-4a17-ab61-3d894110b063, opId = 0, ver = 0, numPartitions = 1], Complete, 0, 2 +- *(4) HashAggregate(keys=[word#72], functions=[merge_count(1)]) +- StateStoreRestore [word#72], state info [ checkpoint = file:/tmp/checkpoint-running_counts/state, runId = f3b2e642-1790-4a17-ab61-3d894110b063, opId = 0, ver = 0, numPartitions = 1], 2 +- *(3) HashAggregate(keys=[word#72], functions=[merge_count(1)]) +- Exchange hashpartitioning(word#72, 1) +- *(2) HashAggregate(keys=[word#72], functions=[partial_count(1)]) +- Generate explode(split(value#83, \\W+)), false, [word#72] +- *(1) Project [value#83] +- *(1) ScanV2 socket[value#83] (Options: [host=localhost,port=9999]) // Type lines (words) in the terminal with nc // Observe the counts in spark-shell // Use web UI to monitor the state of state (no pun intended) // StateStoreSave and StateStoreRestore operators all have state metrics // Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs // You may also want to check out checkpointed state // in /tmp/checkpoint-running_counts/state/0/0 // Eventually... runningCounts.stop()","title":"Streaming Query for Running Counts (Socket Source and Complete Output Mode)"},{"location":"demo/groupBy-running-count-complete/#demo-streaming-query-for-running-counts-socket-source-and-complete-output-mode","text":"The following code shows a streaming aggregation (with Dataset.groupBy operator) in complete output mode that reads text lines from a socket (using socket data source) and outputs running counts of the words. NOTE: The example is \"borrowed\" from http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html[the official documentation of Spark]. Changes and errors are only mine. IMPORTANT: Run nc -lk 9999 first before running the demo. // START: Only for easier debugging // Reduce the number of partitions // The state is then only for one partition // which should make monitoring easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // END: Only for easier debugging val lines = spark .readStream .format(\"socket\") .option(\"host\", \"localhost\") .option(\"port\", 9999) .load scala> lines.printSchema root |-- value: string (nullable = true) import org.apache.spark.sql.functions.explode val words = lines .select(explode(split($\"value\", \"\"\"\\W+\"\"\")) as \"word\") val counts = words.groupBy(\"word\").count scala> counts.printSchema root |-- word: string (nullable = true) |-- count: long (nullable = false) // nc -lk 9999 is supposed to be up at this point val queryName = \"running_counts\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } import org.apache.spark.sql.streaming.OutputMode.Complete val runningCounts = counts .writeStream .format(\"console\") .option(\"checkpointLocation\", checkpointLocation) .outputMode(Complete) .start scala> runningCounts.explain == Physical Plan == WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@205f195c +- *(5) HashAggregate(keys=[word#72], functions=[count(1)]) +- StateStoreSave [word#72], state info [ checkpoint = file:/tmp/checkpoint-running_counts/state, runId = f3b2e642-1790-4a17-ab61-3d894110b063, opId = 0, ver = 0, numPartitions = 1], Complete, 0, 2 +- *(4) HashAggregate(keys=[word#72], functions=[merge_count(1)]) +- StateStoreRestore [word#72], state info [ checkpoint = file:/tmp/checkpoint-running_counts/state, runId = f3b2e642-1790-4a17-ab61-3d894110b063, opId = 0, ver = 0, numPartitions = 1], 2 +- *(3) HashAggregate(keys=[word#72], functions=[merge_count(1)]) +- Exchange hashpartitioning(word#72, 1) +- *(2) HashAggregate(keys=[word#72], functions=[partial_count(1)]) +- Generate explode(split(value#83, \\W+)), false, [word#72] +- *(1) Project [value#83] +- *(1) ScanV2 socket[value#83] (Options: [host=localhost,port=9999]) // Type lines (words) in the terminal with nc // Observe the counts in spark-shell // Use web UI to monitor the state of state (no pun intended) // StateStoreSave and StateStoreRestore operators all have state metrics // Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs // You may also want to check out checkpointed state // in /tmp/checkpoint-running_counts/state/0/0 // Eventually... runningCounts.stop()","title":"Demo: Streaming Query for Running Counts (Socket Source and Complete Output Mode)"},{"location":"demo/groupByKey-count-Update/","text":"Demo: groupByKey Streaming Aggregation in Update Mode \u00b6 The example shows Dataset.groupByKey streaming operator to count rows in Update output mode. In other words, it is an example of using Dataset.groupByKey with count aggregation function to count customer orders ( T ) per zip code ( K ). package pl.japila.spark.examples import org.apache.spark.sql.SparkSession import org.apache.spark.sql.streaming.{OutputMode, Trigger} object GroupByKeyStreamingApp extends App { val inputTopic = \"GroupByKeyApp-input\" val appName = this.getClass.getSimpleName.replace(\"$\", \"\") val spark = SparkSession.builder .master(\"local[*]\") .appName(appName) .getOrCreate import spark.implicits._ case class Order(id: Long, zipCode: String) // Input (source node) val orders = spark .readStream .format(\"kafka\") .option(\"startingOffsets\", \"latest\") .option(\"subscribe\", inputTopic) .option(\"kafka.bootstrap.servers\", \":9092\") .load .select($\"offset\" as \"id\", $\"value\" as \"zipCode\") // FIXME Use csv, json, avro .as[Order] // Processing logic // groupByKey + count val byZipCode = (o: Order) => o.zipCode val ordersByZipCode = orders.groupByKey(byZipCode) import org.apache.spark.sql.functions.count val typedCountCol = (count(\"zipCode\") as \"count\").as[String] val counts = ordersByZipCode .agg(typedCountCol) .select($\"value\" as \"zip_code\", $\"count\") // Output (sink node) import scala.concurrent.duration._ counts .writeStream .format(\"console\") .outputMode(OutputMode.Update) // FIXME Use Complete .queryName(appName) .trigger(Trigger.ProcessingTime(5.seconds)) .start .awaitTermination() } Credits \u00b6 The example with customer orders and postal codes is borrowed from Apache Beam's Using GroupByKey Programming Guide.","title":"groupByKey Streaming Aggregation in Update Mode"},{"location":"demo/groupByKey-count-Update/#demo-groupbykey-streaming-aggregation-in-update-mode","text":"The example shows Dataset.groupByKey streaming operator to count rows in Update output mode. In other words, it is an example of using Dataset.groupByKey with count aggregation function to count customer orders ( T ) per zip code ( K ). package pl.japila.spark.examples import org.apache.spark.sql.SparkSession import org.apache.spark.sql.streaming.{OutputMode, Trigger} object GroupByKeyStreamingApp extends App { val inputTopic = \"GroupByKeyApp-input\" val appName = this.getClass.getSimpleName.replace(\"$\", \"\") val spark = SparkSession.builder .master(\"local[*]\") .appName(appName) .getOrCreate import spark.implicits._ case class Order(id: Long, zipCode: String) // Input (source node) val orders = spark .readStream .format(\"kafka\") .option(\"startingOffsets\", \"latest\") .option(\"subscribe\", inputTopic) .option(\"kafka.bootstrap.servers\", \":9092\") .load .select($\"offset\" as \"id\", $\"value\" as \"zipCode\") // FIXME Use csv, json, avro .as[Order] // Processing logic // groupByKey + count val byZipCode = (o: Order) => o.zipCode val ordersByZipCode = orders.groupByKey(byZipCode) import org.apache.spark.sql.functions.count val typedCountCol = (count(\"zipCode\") as \"count\").as[String] val counts = ordersByZipCode .agg(typedCountCol) .select($\"value\" as \"zip_code\", $\"count\") // Output (sink node) import scala.concurrent.duration._ counts .writeStream .format(\"console\") .outputMode(OutputMode.Update) // FIXME Use Complete .queryName(appName) .trigger(Trigger.ProcessingTime(5.seconds)) .start .awaitTermination() }","title":"Demo: groupByKey Streaming Aggregation in Update Mode"},{"location":"demo/groupByKey-count-Update/#credits","text":"The example with customer orders and postal codes is borrowed from Apache Beam's Using GroupByKey Programming Guide.","title":"Credits"},{"location":"demo/kafka-data-source/","text":"Demo: Streaming Aggregation with Kafka Data Source \u00b6 The following example code shows a streaming aggregation (with Dataset.groupBy operator) that reads records from Kafka (with Kafka Data Source ). IMPORTANT: Start up Kafka cluster and spark-shell with spark-sql-kafka-0-10 package before running the demo. TIP: You may want to consider copying the following code to append.txt and using :load append.txt command in spark-shell to load it (rather than copying and pasting it). // START: Only for easier debugging // The state is then only for one partition // which should make monitoring easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // END: Only for easier debugging val records = spark .readStream .format(\"kafka\") .option(\"subscribePattern\", \"\"\"topic-\\d{2}\"\"\") // topics with two digits at the end .option(\"kafka.bootstrap.servers\", \":9092\") .load scala> records.printSchema root |-- key: binary (nullable = true) |-- value: binary (nullable = true) |-- topic: string (nullable = true) |-- partition: integer (nullable = true) |-- offset: long (nullable = true) |-- timestamp: timestamp (nullable = true) |-- timestampType: integer (nullable = true) // Since the streaming query uses Append output mode // it has to define a streaming event-time watermark (using Dataset.withWatermark operator) // UnsupportedOperationChecker makes sure that the requirement holds val ids = records .withColumn(\"tokens\", split($\"value\", \",\")) .withColumn(\"seconds\", 'tokens(0) cast \"long\") .withColumn(\"event_time\", to_timestamp(from_unixtime('seconds))) // <-- Event time has to be a timestamp .withColumn(\"id\", 'tokens(1)) .withColumn(\"batch\", 'tokens(2) cast \"int\") .withWatermark(eventTime = \"event_time\", delayThreshold = \"10 seconds\") // <-- define watermark (before groupBy!) .groupBy($\"event_time\") // <-- use event_time for grouping .agg(collect_list(\"batch\") as \"batches\", collect_list(\"id\") as \"ids\") .withColumn(\"event_time\", to_timestamp($\"event_time\")) // <-- convert to human-readable date scala> ids.printSchema root |-- event_time: timestamp (nullable = true) |-- batches: array (nullable = true) | |-- element: integer (containsNull = true) |-- ids: array (nullable = true) | |-- element: string (containsNull = true) assert(ids.isStreaming, \"ids is a streaming query\") // ids knows nothing about the output mode or the current streaming watermark yet // - Output mode is defined on writing side // - streaming watermark is read from rows at runtime // That's why StatefulOperatorStateInfo is generic (and uses the default Append for output mode) // and no batch-specific values are printed out // They will be available right after the first streaming batch // Use explain on a streaming query to know the trigger-specific values scala> ids.explain == Physical Plan == ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[collect_list(batch#141, 0, 0), collect_list(id#129, 0, 0)]) +- StateStoreSave [event_time#118-T10000ms], state info [ checkpoint = <unknown>, runId = a870e6e2-b925-4104-9886-b211c0be1b73, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2 +- ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[merge_collect_list(batch#141, 0, 0), merge_collect_list(id#129, 0, 0)]) +- StateStoreRestore [event_time#118-T10000ms], state info [ checkpoint = <unknown>, runId = a870e6e2-b925-4104-9886-b211c0be1b73, opId = 0, ver = 0, numPartitions = 1], 2 +- ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[merge_collect_list(batch#141, 0, 0), merge_collect_list(id#129, 0, 0)]) +- Exchange hashpartitioning(event_time#118-T10000ms, 1) +- ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[partial_collect_list(batch#141, 0, 0), partial_collect_list(id#129, 0, 0)]) +- EventTimeWatermark event_time#118: timestamp, interval 10 seconds +- *(1) Project [cast(from_unixtime(cast(split(cast(value#8 as string), ,)[0] as bigint), yyyy-MM-dd HH:mm:ss, Some(Europe/Warsaw)) as timestamp) AS event_time#118, split(cast(value#8 as string), ,)[1] AS id#129, cast(split(cast(value#8 as string), ,)[2] as int) AS batch#141] +- StreamingRelation kafka, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13] val queryName = \"ids-kafka\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } // The following make for an easier demo // Kafka cluster is supposed to be up at this point // Make sure that a Kafka topic is available, e.g. topic-00 // Use ./bin/kafka-console-producer.sh --broker-list :9092 --topic topic-00 // And send a record, e.g. 1,1,1 // Define the output mode // and start the query import scala.concurrent.duration._ import org.apache.spark.sql.streaming.OutputMode.Append import org.apache.spark.sql.streaming.Trigger val streamingQuery = ids .writeStream .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", checkpointLocation) .queryName(queryName) .outputMode(Append) .start val lastProgress = streamingQuery.lastProgress scala> :type lastProgress org.apache.spark.sql.streaming.StreamingQueryProgress assert(lastProgress.stateOperators.length == 1, \"There should be one stateful operator\") scala> println(lastProgress.stateOperators.head.prettyJson) { \"numRowsTotal\" : 1, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 742, \"customMetrics\" : { \"loadedMapCacheHitCount\" : 1, \"loadedMapCacheMissCount\" : 1, \"stateOnCurrentVersionSizeBytes\" : 374 } } assert(lastProgress.sources.length == 1, \"There should be one streaming source only\") scala> println(lastProgress.sources.head.prettyJson) { \"description\" : \"KafkaV2[SubscribePattern[topic-\\\\d{2}]]\", \"startOffset\" : { \"topic-00\" : { \"0\" : 1 } }, \"endOffset\" : { \"topic-00\" : { \"0\" : 1 } }, \"numInputRows\" : 0, \"inputRowsPerSecond\" : 0.0, \"processedRowsPerSecond\" : 0.0 } // Eventually... streamingQuery.stop()","title":"Streaming Aggregation with Kafka Data Source"},{"location":"demo/kafka-data-source/#demo-streaming-aggregation-with-kafka-data-source","text":"The following example code shows a streaming aggregation (with Dataset.groupBy operator) that reads records from Kafka (with Kafka Data Source ). IMPORTANT: Start up Kafka cluster and spark-shell with spark-sql-kafka-0-10 package before running the demo. TIP: You may want to consider copying the following code to append.txt and using :load append.txt command in spark-shell to load it (rather than copying and pasting it). // START: Only for easier debugging // The state is then only for one partition // which should make monitoring easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // END: Only for easier debugging val records = spark .readStream .format(\"kafka\") .option(\"subscribePattern\", \"\"\"topic-\\d{2}\"\"\") // topics with two digits at the end .option(\"kafka.bootstrap.servers\", \":9092\") .load scala> records.printSchema root |-- key: binary (nullable = true) |-- value: binary (nullable = true) |-- topic: string (nullable = true) |-- partition: integer (nullable = true) |-- offset: long (nullable = true) |-- timestamp: timestamp (nullable = true) |-- timestampType: integer (nullable = true) // Since the streaming query uses Append output mode // it has to define a streaming event-time watermark (using Dataset.withWatermark operator) // UnsupportedOperationChecker makes sure that the requirement holds val ids = records .withColumn(\"tokens\", split($\"value\", \",\")) .withColumn(\"seconds\", 'tokens(0) cast \"long\") .withColumn(\"event_time\", to_timestamp(from_unixtime('seconds))) // <-- Event time has to be a timestamp .withColumn(\"id\", 'tokens(1)) .withColumn(\"batch\", 'tokens(2) cast \"int\") .withWatermark(eventTime = \"event_time\", delayThreshold = \"10 seconds\") // <-- define watermark (before groupBy!) .groupBy($\"event_time\") // <-- use event_time for grouping .agg(collect_list(\"batch\") as \"batches\", collect_list(\"id\") as \"ids\") .withColumn(\"event_time\", to_timestamp($\"event_time\")) // <-- convert to human-readable date scala> ids.printSchema root |-- event_time: timestamp (nullable = true) |-- batches: array (nullable = true) | |-- element: integer (containsNull = true) |-- ids: array (nullable = true) | |-- element: string (containsNull = true) assert(ids.isStreaming, \"ids is a streaming query\") // ids knows nothing about the output mode or the current streaming watermark yet // - Output mode is defined on writing side // - streaming watermark is read from rows at runtime // That's why StatefulOperatorStateInfo is generic (and uses the default Append for output mode) // and no batch-specific values are printed out // They will be available right after the first streaming batch // Use explain on a streaming query to know the trigger-specific values scala> ids.explain == Physical Plan == ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[collect_list(batch#141, 0, 0), collect_list(id#129, 0, 0)]) +- StateStoreSave [event_time#118-T10000ms], state info [ checkpoint = <unknown>, runId = a870e6e2-b925-4104-9886-b211c0be1b73, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2 +- ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[merge_collect_list(batch#141, 0, 0), merge_collect_list(id#129, 0, 0)]) +- StateStoreRestore [event_time#118-T10000ms], state info [ checkpoint = <unknown>, runId = a870e6e2-b925-4104-9886-b211c0be1b73, opId = 0, ver = 0, numPartitions = 1], 2 +- ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[merge_collect_list(batch#141, 0, 0), merge_collect_list(id#129, 0, 0)]) +- Exchange hashpartitioning(event_time#118-T10000ms, 1) +- ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[partial_collect_list(batch#141, 0, 0), partial_collect_list(id#129, 0, 0)]) +- EventTimeWatermark event_time#118: timestamp, interval 10 seconds +- *(1) Project [cast(from_unixtime(cast(split(cast(value#8 as string), ,)[0] as bigint), yyyy-MM-dd HH:mm:ss, Some(Europe/Warsaw)) as timestamp) AS event_time#118, split(cast(value#8 as string), ,)[1] AS id#129, cast(split(cast(value#8 as string), ,)[2] as int) AS batch#141] +- StreamingRelation kafka, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13] val queryName = \"ids-kafka\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } // The following make for an easier demo // Kafka cluster is supposed to be up at this point // Make sure that a Kafka topic is available, e.g. topic-00 // Use ./bin/kafka-console-producer.sh --broker-list :9092 --topic topic-00 // And send a record, e.g. 1,1,1 // Define the output mode // and start the query import scala.concurrent.duration._ import org.apache.spark.sql.streaming.OutputMode.Append import org.apache.spark.sql.streaming.Trigger val streamingQuery = ids .writeStream .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", checkpointLocation) .queryName(queryName) .outputMode(Append) .start val lastProgress = streamingQuery.lastProgress scala> :type lastProgress org.apache.spark.sql.streaming.StreamingQueryProgress assert(lastProgress.stateOperators.length == 1, \"There should be one stateful operator\") scala> println(lastProgress.stateOperators.head.prettyJson) { \"numRowsTotal\" : 1, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 742, \"customMetrics\" : { \"loadedMapCacheHitCount\" : 1, \"loadedMapCacheMissCount\" : 1, \"stateOnCurrentVersionSizeBytes\" : 374 } } assert(lastProgress.sources.length == 1, \"There should be one streaming source only\") scala> println(lastProgress.sources.head.prettyJson) { \"description\" : \"KafkaV2[SubscribePattern[topic-\\\\d{2}]]\", \"startOffset\" : { \"topic-00\" : { \"0\" : 1 } }, \"endOffset\" : { \"topic-00\" : { \"0\" : 1 } }, \"numInputRows\" : 0, \"inputRowsPerSecond\" : 0.0, \"processedRowsPerSecond\" : 0.0 } // Eventually... streamingQuery.stop()","title":"Demo: Streaming Aggregation with Kafka Data Source"},{"location":"demo/spark-sql-streaming-demo-FlatMapGroupsWithStateExec/","text":"Demo: Internals of FlatMapGroupsWithStateExec Physical Operator \u00b6 The following demo shows the internals of FlatMapGroupsWithStateExec physical operator in a Arbitrary Stateful Streaming Aggregation . // Reduce the number of partitions and hence the state stores // That is supposed to make debugging state checkpointing easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // Define event \"format\" // Use :paste mode in spark-shell import java.sql.Timestamp case class Event(time: Timestamp, value: Long) import scala.concurrent.duration._ object Event { def apply(secs: Long, value: Long): Event = { Event(new Timestamp(secs.seconds.toMillis), value) } } // Using memory data source for full control of the input import org.apache.spark.sql.execution.streaming.MemoryStream implicit val sqlCtx = spark.sqlContext val events = MemoryStream[Event] val values = events.toDS assert(values.isStreaming, \"values must be a streaming Dataset\") values.printSchema /** root |-- time: timestamp (nullable = true) |-- value: long (nullable = false) */ import scala.concurrent.duration._ val delayThreshold = 10.seconds val valuesWatermarked = values .withWatermark(eventTime = \"time\", delayThreshold.toString) // required for EventTimeTimeout // Could use Long directly, but... // Let's use case class to make the demo a bit more advanced case class Count(value: Long) import java.sql.Timestamp import org.apache.spark.sql.streaming.GroupState val keyCounts = (key: Long, values: Iterator[(Timestamp, Long)], state: GroupState[Count]) => { println(s\"\"\">>> keyCounts(key = $key, state = ${state.getOption.getOrElse(\"<empty>\")})\"\"\") println(s\">>> >>> currentProcessingTimeMs: ${state.getCurrentProcessingTimeMs}\") println(s\">>> >>> currentWatermarkMs: ${state.getCurrentWatermarkMs}\") println(s\">>> >>> hasTimedOut: ${state.hasTimedOut}\") val count = Count(values.length) Iterator((key, count)) } import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode} val valuesCounted = valuesWatermarked .as[(Timestamp, Long)] // convert DataFrame to Dataset to make groupByKey easier to write .groupByKey { case (time, value) => value } .flatMapGroupsWithState( OutputMode.Update, timeoutConf = GroupStateTimeout.EventTimeTimeout)(func = keyCounts) .toDF(\"value\", \"count\") valuesCounted.explain /** == Physical Plan == *(2) Project [_1#928L AS value#931L, _2#929 AS count#932] +- *(2) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#928L, if (isnull(assertnotnull(input[0, scala.Tuple2, true])._2)) null else named_struct(value, assertnotnull(assertnotnull(input[0, scala.Tuple2, true])._2).value) AS _2#929] +- FlatMapGroupsWithState $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4117/181063008@d2cdc82, value#923: bigint, newInstance(class scala.Tuple2), [value#923L], [time#915-T10000ms, value#916L], obj#927: scala.Tuple2, state info [ checkpoint = <unknown>, runId = 9af3d00c-fe1f-46a0-8630-4e0d0af88042, opId = 0, ver = 0, numPartitions = 1], class[value[0]: bigint], 2, Update, EventTimeTimeout, 0, 0 +- *(1) Sort [value#923L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(value#923L, 1) +- AppendColumns $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4118/2131767153@3e606b4c, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#923L] +- EventTimeWatermark time#915: timestamp, interval 10 seconds +- StreamingRelation MemoryStream[time#915,value#916L], [time#915, value#916L] */ val queryName = \"FlatMapGroupsWithStateExec_demo\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } import org.apache.spark.sql.streaming.OutputMode.Update val streamingQuery = valuesCounted .writeStream .format(\"memory\") .queryName(queryName) .option(\"checkpointLocation\", checkpointLocation) .outputMode(Update) .start assert(streamingQuery.status.message == \"Waiting for data to arrive\") // Use web UI to monitor the metrics of the streaming query // Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs // You may also want to check out checkpointed state // in /tmp/checkpoint-FlatMapGroupsWithStateExec_demo/state/0/0 val batch = Seq( Event(secs = 1, value = 1), Event(secs = 15, value = 2)) events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 1, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881557237 >>> >>> currentWatermarkMs: 0 >>> >>> hasTimedOut: false >>> keyCounts(key = 2, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881557237 >>> >>> currentWatermarkMs: 0 >>> >>> hasTimedOut: false */ spark.table(queryName).show(truncate = false) /** +-----+-----+ |value|count| +-----+-----+ |1 |[1] | |2 |[1] | +-----+-----+ */ // With at least one execution we can review the execution plan streamingQuery.explain /** == Physical Plan == *(2) Project [_1#928L AS value#931L, _2#929 AS count#932] +- *(2) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#928L, if (isnull(assertnotnull(input[0, scala.Tuple2, true])._2)) null else named_struct(value, assertnotnull(assertnotnull(input[0, scala.Tuple2, true])._2).value) AS _2#929] +- FlatMapGroupsWithState $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4117/181063008@d2cdc82, value#923: bigint, newInstance(class scala.Tuple2), [value#923L], [time#915-T10000ms, value#916L], obj#927: scala.Tuple2, state info [ checkpoint = file:/tmp/checkpoint-FlatMapGroupsWithStateExec_demo/state, runId = 95c3917c-2fd7-45b2-86f6-6c01f0115e1d, opId = 0, ver = 1, numPartitions = 1], class[value[0]: bigint], 2, Update, EventTimeTimeout, 1561881557499, 5000 +- *(1) Sort [value#923L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(value#923L, 1) +- AppendColumns $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4118/2131767153@3e606b4c, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#923L] +- EventTimeWatermark time#915: timestamp, interval 10 seconds +- LocalTableScan <empty>, [time#915, value#916L] */ type Millis = Long def toMillis(datetime: String): Millis = { import java.time.format.DateTimeFormatter import java.time.LocalDateTime import java.time.ZoneOffset LocalDateTime .parse(datetime, DateTimeFormatter.ISO_DATE_TIME) .toInstant(ZoneOffset.UTC) .toEpochMilli } val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkSecs = toMillis(currentWatermark).millis.toSeconds.seconds val expectedWatermarkSecs = 5.seconds assert(currentWatermarkSecs == expectedWatermarkSecs, s\"Current event-time watermark is $currentWatermarkSecs, but should be $expectedWatermarkSecs (maximum event time - delayThreshold ${delayThreshold.toMillis})\") // Let's access the FlatMapGroupsWithStateExec physical operator import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper import org.apache.spark.sql.execution.streaming.StreamExecution val engine: StreamExecution = streamingQuery .asInstanceOf[StreamingQueryWrapper] .streamingQuery import org.apache.spark.sql.execution.streaming.IncrementalExecution val lastMicroBatch: IncrementalExecution = engine.lastExecution // Access executedPlan that is the optimized physical query plan ready for execution // All streaming optimizations have been applied at this point val plan = lastMicroBatch.executedPlan // Find the FlatMapGroupsWithStateExec physical operator import org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec val flatMapOp = plan.collect { case op: FlatMapGroupsWithStateExec => op }.head // Display metrics import org.apache.spark.sql.execution.metric.SQLMetric def formatMetrics(name: String, metric: SQLMetric) = { val desc = metric.name.getOrElse(\"\") val value = metric.value f\"| $name%-30s | $desc%-69s | $value%-10s\" } flatMapOp.metrics.map { case (name, metric) => formatMetrics(name, metric) }.foreach(println) /** | numTotalStateRows | number of total state rows | 0 | stateMemory | memory used by state total (min, med, max) | 390 | loadedMapCacheHitCount | count of cache hit on states cache in provider | 1 | numOutputRows | number of output rows | 0 | stateOnCurrentVersionSizeBytes | estimated size of state only on current version total (min, med, max) | 102 | loadedMapCacheMissCount | count of cache miss on states cache in provider | 0 | commitTimeMs | time to commit changes total (min, med, max) | -2 | allRemovalsTimeMs | total time to remove rows total (min, med, max) | -2 | numUpdatedStateRows | number of updated state rows | 0 | allUpdatesTimeMs | total time to update rows total (min, med, max) | -2 */ val batch = Seq( Event(secs = 1, value = 1), // under the watermark (5000 ms) so it's disregarded Event(secs = 6, value = 3)) // above the watermark so it should be counted events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 3, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881643568 >>> >>> currentWatermarkMs: 5000 >>> >>> hasTimedOut: false */ spark.table(queryName).show(truncate = false) /** +-----+-----+ |value|count| +-----+-----+ |1 |[1] | |2 |[1] | |3 |[1] | +-----+-----+ */ val batch = Seq( Event(secs = 17, value = 3)) // advances the watermark events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 3, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881672887 >>> >>> currentWatermarkMs: 5000 >>> >>> hasTimedOut: false */ val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkSecs = toMillis(currentWatermark).millis.toSeconds.seconds val expectedWatermarkSecs = 7.seconds assert(currentWatermarkSecs == expectedWatermarkSecs, s\"Current event-time watermark is $currentWatermarkSecs, but should be $expectedWatermarkSecs (maximum event time - delayThreshold ${delayThreshold.toMillis})\") spark.table(queryName).show(truncate = false) /** +-----+-----+ |value|count| +-----+-----+ |1 |[1] | |2 |[1] | |3 |[1] | |3 |[1] | +-----+-----+ */ val batch = Seq( Event(secs = 18, value = 3)) // advances the watermark events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 3, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881778165 >>> >>> currentWatermarkMs: 7000 >>> >>> hasTimedOut: false */ // Eventually... streamingQuery.stop()","title":"Internals of FlatMapGroupsWithStateExec Physical Operator"},{"location":"demo/spark-sql-streaming-demo-FlatMapGroupsWithStateExec/#demo-internals-of-flatmapgroupswithstateexec-physical-operator","text":"The following demo shows the internals of FlatMapGroupsWithStateExec physical operator in a Arbitrary Stateful Streaming Aggregation . // Reduce the number of partitions and hence the state stores // That is supposed to make debugging state checkpointing easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // Define event \"format\" // Use :paste mode in spark-shell import java.sql.Timestamp case class Event(time: Timestamp, value: Long) import scala.concurrent.duration._ object Event { def apply(secs: Long, value: Long): Event = { Event(new Timestamp(secs.seconds.toMillis), value) } } // Using memory data source for full control of the input import org.apache.spark.sql.execution.streaming.MemoryStream implicit val sqlCtx = spark.sqlContext val events = MemoryStream[Event] val values = events.toDS assert(values.isStreaming, \"values must be a streaming Dataset\") values.printSchema /** root |-- time: timestamp (nullable = true) |-- value: long (nullable = false) */ import scala.concurrent.duration._ val delayThreshold = 10.seconds val valuesWatermarked = values .withWatermark(eventTime = \"time\", delayThreshold.toString) // required for EventTimeTimeout // Could use Long directly, but... // Let's use case class to make the demo a bit more advanced case class Count(value: Long) import java.sql.Timestamp import org.apache.spark.sql.streaming.GroupState val keyCounts = (key: Long, values: Iterator[(Timestamp, Long)], state: GroupState[Count]) => { println(s\"\"\">>> keyCounts(key = $key, state = ${state.getOption.getOrElse(\"<empty>\")})\"\"\") println(s\">>> >>> currentProcessingTimeMs: ${state.getCurrentProcessingTimeMs}\") println(s\">>> >>> currentWatermarkMs: ${state.getCurrentWatermarkMs}\") println(s\">>> >>> hasTimedOut: ${state.hasTimedOut}\") val count = Count(values.length) Iterator((key, count)) } import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode} val valuesCounted = valuesWatermarked .as[(Timestamp, Long)] // convert DataFrame to Dataset to make groupByKey easier to write .groupByKey { case (time, value) => value } .flatMapGroupsWithState( OutputMode.Update, timeoutConf = GroupStateTimeout.EventTimeTimeout)(func = keyCounts) .toDF(\"value\", \"count\") valuesCounted.explain /** == Physical Plan == *(2) Project [_1#928L AS value#931L, _2#929 AS count#932] +- *(2) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#928L, if (isnull(assertnotnull(input[0, scala.Tuple2, true])._2)) null else named_struct(value, assertnotnull(assertnotnull(input[0, scala.Tuple2, true])._2).value) AS _2#929] +- FlatMapGroupsWithState $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4117/181063008@d2cdc82, value#923: bigint, newInstance(class scala.Tuple2), [value#923L], [time#915-T10000ms, value#916L], obj#927: scala.Tuple2, state info [ checkpoint = <unknown>, runId = 9af3d00c-fe1f-46a0-8630-4e0d0af88042, opId = 0, ver = 0, numPartitions = 1], class[value[0]: bigint], 2, Update, EventTimeTimeout, 0, 0 +- *(1) Sort [value#923L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(value#923L, 1) +- AppendColumns $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4118/2131767153@3e606b4c, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#923L] +- EventTimeWatermark time#915: timestamp, interval 10 seconds +- StreamingRelation MemoryStream[time#915,value#916L], [time#915, value#916L] */ val queryName = \"FlatMapGroupsWithStateExec_demo\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } import org.apache.spark.sql.streaming.OutputMode.Update val streamingQuery = valuesCounted .writeStream .format(\"memory\") .queryName(queryName) .option(\"checkpointLocation\", checkpointLocation) .outputMode(Update) .start assert(streamingQuery.status.message == \"Waiting for data to arrive\") // Use web UI to monitor the metrics of the streaming query // Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs // You may also want to check out checkpointed state // in /tmp/checkpoint-FlatMapGroupsWithStateExec_demo/state/0/0 val batch = Seq( Event(secs = 1, value = 1), Event(secs = 15, value = 2)) events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 1, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881557237 >>> >>> currentWatermarkMs: 0 >>> >>> hasTimedOut: false >>> keyCounts(key = 2, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881557237 >>> >>> currentWatermarkMs: 0 >>> >>> hasTimedOut: false */ spark.table(queryName).show(truncate = false) /** +-----+-----+ |value|count| +-----+-----+ |1 |[1] | |2 |[1] | +-----+-----+ */ // With at least one execution we can review the execution plan streamingQuery.explain /** == Physical Plan == *(2) Project [_1#928L AS value#931L, _2#929 AS count#932] +- *(2) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#928L, if (isnull(assertnotnull(input[0, scala.Tuple2, true])._2)) null else named_struct(value, assertnotnull(assertnotnull(input[0, scala.Tuple2, true])._2).value) AS _2#929] +- FlatMapGroupsWithState $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4117/181063008@d2cdc82, value#923: bigint, newInstance(class scala.Tuple2), [value#923L], [time#915-T10000ms, value#916L], obj#927: scala.Tuple2, state info [ checkpoint = file:/tmp/checkpoint-FlatMapGroupsWithStateExec_demo/state, runId = 95c3917c-2fd7-45b2-86f6-6c01f0115e1d, opId = 0, ver = 1, numPartitions = 1], class[value[0]: bigint], 2, Update, EventTimeTimeout, 1561881557499, 5000 +- *(1) Sort [value#923L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(value#923L, 1) +- AppendColumns $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4118/2131767153@3e606b4c, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#923L] +- EventTimeWatermark time#915: timestamp, interval 10 seconds +- LocalTableScan <empty>, [time#915, value#916L] */ type Millis = Long def toMillis(datetime: String): Millis = { import java.time.format.DateTimeFormatter import java.time.LocalDateTime import java.time.ZoneOffset LocalDateTime .parse(datetime, DateTimeFormatter.ISO_DATE_TIME) .toInstant(ZoneOffset.UTC) .toEpochMilli } val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkSecs = toMillis(currentWatermark).millis.toSeconds.seconds val expectedWatermarkSecs = 5.seconds assert(currentWatermarkSecs == expectedWatermarkSecs, s\"Current event-time watermark is $currentWatermarkSecs, but should be $expectedWatermarkSecs (maximum event time - delayThreshold ${delayThreshold.toMillis})\") // Let's access the FlatMapGroupsWithStateExec physical operator import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper import org.apache.spark.sql.execution.streaming.StreamExecution val engine: StreamExecution = streamingQuery .asInstanceOf[StreamingQueryWrapper] .streamingQuery import org.apache.spark.sql.execution.streaming.IncrementalExecution val lastMicroBatch: IncrementalExecution = engine.lastExecution // Access executedPlan that is the optimized physical query plan ready for execution // All streaming optimizations have been applied at this point val plan = lastMicroBatch.executedPlan // Find the FlatMapGroupsWithStateExec physical operator import org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec val flatMapOp = plan.collect { case op: FlatMapGroupsWithStateExec => op }.head // Display metrics import org.apache.spark.sql.execution.metric.SQLMetric def formatMetrics(name: String, metric: SQLMetric) = { val desc = metric.name.getOrElse(\"\") val value = metric.value f\"| $name%-30s | $desc%-69s | $value%-10s\" } flatMapOp.metrics.map { case (name, metric) => formatMetrics(name, metric) }.foreach(println) /** | numTotalStateRows | number of total state rows | 0 | stateMemory | memory used by state total (min, med, max) | 390 | loadedMapCacheHitCount | count of cache hit on states cache in provider | 1 | numOutputRows | number of output rows | 0 | stateOnCurrentVersionSizeBytes | estimated size of state only on current version total (min, med, max) | 102 | loadedMapCacheMissCount | count of cache miss on states cache in provider | 0 | commitTimeMs | time to commit changes total (min, med, max) | -2 | allRemovalsTimeMs | total time to remove rows total (min, med, max) | -2 | numUpdatedStateRows | number of updated state rows | 0 | allUpdatesTimeMs | total time to update rows total (min, med, max) | -2 */ val batch = Seq( Event(secs = 1, value = 1), // under the watermark (5000 ms) so it's disregarded Event(secs = 6, value = 3)) // above the watermark so it should be counted events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 3, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881643568 >>> >>> currentWatermarkMs: 5000 >>> >>> hasTimedOut: false */ spark.table(queryName).show(truncate = false) /** +-----+-----+ |value|count| +-----+-----+ |1 |[1] | |2 |[1] | |3 |[1] | +-----+-----+ */ val batch = Seq( Event(secs = 17, value = 3)) // advances the watermark events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 3, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881672887 >>> >>> currentWatermarkMs: 5000 >>> >>> hasTimedOut: false */ val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkSecs = toMillis(currentWatermark).millis.toSeconds.seconds val expectedWatermarkSecs = 7.seconds assert(currentWatermarkSecs == expectedWatermarkSecs, s\"Current event-time watermark is $currentWatermarkSecs, but should be $expectedWatermarkSecs (maximum event time - delayThreshold ${delayThreshold.toMillis})\") spark.table(queryName).show(truncate = false) /** +-----+-----+ |value|count| +-----+-----+ |1 |[1] | |2 |[1] | |3 |[1] | |3 |[1] | +-----+-----+ */ val batch = Seq( Event(secs = 18, value = 3)) // advances the watermark events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 3, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881778165 >>> >>> currentWatermarkMs: 7000 >>> >>> hasTimedOut: false */ // Eventually... streamingQuery.stop()","title":"Demo: Internals of FlatMapGroupsWithStateExec Physical Operator"},{"location":"demo/using-file-streaming-source/","text":"Demo: Using File Streaming Source \u00b6 This demo shows a streaming query that reads files using FileStreamSource . Prerequisites \u00b6 Make sure that the source directory is available before starting the query. mkdir /tmp/text-logs Configure Logging \u00b6 Enable logging for FileStreamSource . Start Streaming Query \u00b6 Use spark-shell for fast interactive prototyping. Describe a source to load data from. val lines = spark . readStream . format ( \"text\" ) . option ( \"maxFilesPerTrigger\" , 1 ) . load ( \"/tmp/text-logs\" ) Show the schema. scala> lines.printSchema root |-- value: string (nullable = true) Describe the sink ( console ) and start the streaming query. import org . apache . spark . sql . streaming . Trigger import concurrent . duration . _ val interval = 15 . seconds val trigger = Trigger . ProcessingTime ( interval ) val queryName = s\"one file every micro-batch (every $ interval )\" val sq = lines . writeStream . format ( \"console\" ) . option ( \"checkpointLocation\" , \"/tmp/checkpointLocation\" ) . trigger ( trigger ) . queryName ( queryName ) . start Use web UI to monitor the query ( http://localhost:4040 ). Stop Query \u00b6 spark . streams . active . foreach ( _ . stop )","title":"Using File Streaming Source"},{"location":"demo/using-file-streaming-source/#demo-using-file-streaming-source","text":"This demo shows a streaming query that reads files using FileStreamSource .","title":"Demo: Using File Streaming Source"},{"location":"demo/using-file-streaming-source/#prerequisites","text":"Make sure that the source directory is available before starting the query. mkdir /tmp/text-logs","title":"Prerequisites"},{"location":"demo/using-file-streaming-source/#configure-logging","text":"Enable logging for FileStreamSource .","title":"Configure Logging"},{"location":"demo/using-file-streaming-source/#start-streaming-query","text":"Use spark-shell for fast interactive prototyping. Describe a source to load data from. val lines = spark . readStream . format ( \"text\" ) . option ( \"maxFilesPerTrigger\" , 1 ) . load ( \"/tmp/text-logs\" ) Show the schema. scala> lines.printSchema root |-- value: string (nullable = true) Describe the sink ( console ) and start the streaming query. import org . apache . spark . sql . streaming . Trigger import concurrent . duration . _ val interval = 15 . seconds val trigger = Trigger . ProcessingTime ( interval ) val queryName = s\"one file every micro-batch (every $ interval )\" val sq = lines . writeStream . format ( \"console\" ) . option ( \"checkpointLocation\" , \"/tmp/checkpointLocation\" ) . trigger ( trigger ) . queryName ( queryName ) . start Use web UI to monitor the query ( http://localhost:4040 ).","title":"Start Streaming Query"},{"location":"demo/using-file-streaming-source/#stop-query","text":"spark . streams . active . foreach ( _ . stop )","title":"Stop Query"},{"location":"demo/watermark-aggregation-append/","text":"Demo: Streaming Watermark with Aggregation in Append Output Mode \u00b6 The following demo shows the behaviour and the internals of streaming watermark with a streaming aggregation in Append output mode. The demo also shows the behaviour and the internals of StateStoreSaveExec physical operator in Append output mode. Tip The below code is part of StreamingAggregationAppendMode streaming application. // Reduce the number of partitions and hence the state stores // That is supposed to make debugging state checkpointing easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // Define event \"format\" // Use :paste mode in spark-shell import java.sql.Timestamp case class Event(time: Timestamp, value: Long, batch: Long) import scala.concurrent.duration._ object Event { def apply(secs: Long, value: Long, batch: Long): Event = { Event(new Timestamp(secs.seconds.toMillis), value, batch) } } // Using memory data source for full control of the input import org.apache.spark.sql.execution.streaming.MemoryStream implicit val sqlCtx = spark.sqlContext val events = MemoryStream[Event] val values = events.toDS assert(values.isStreaming, \"values must be a streaming Dataset\") values.printSchema /** root |-- time: timestamp (nullable = true) |-- value: long (nullable = false) |-- batch: long (nullable = false) */ // Streaming aggregation using groupBy operator to demo StateStoreSaveExec operator // Define required watermark for late events for Append output mode import scala.concurrent.duration._ val delayThreshold = 10.seconds val eventTime = \"time\" val valuesWatermarked = values .withWatermark(eventTime, delayThreshold.toString) // defines watermark (before groupBy!) // EventTimeWatermark logical operator is planned as EventTimeWatermarkExec physical operator // Note that as a physical operator EventTimeWatermarkExec shows itself without the Exec suffix valuesWatermarked.explain /** == Physical Plan == EventTimeWatermark time#3: timestamp, interval 10 seconds +- StreamingRelation MemoryStream[time#3,value#4L,batch#5L], [time#3, value#4L, batch#5L] */ val windowDuration = 5.seconds import org.apache.spark.sql.functions.window val countsPer5secWindow = valuesWatermarked .groupBy(window(col(eventTime), windowDuration.toString) as \"sliding_window\") .agg(collect_list(\"batch\") as \"batches\", collect_list(\"value\") as \"values\") countsPer5secWindow.printSchema /** root |-- sliding_window: struct (nullable = false) | |-- start: timestamp (nullable = true) | |-- end: timestamp (nullable = true) |-- batches: array (nullable = true) | |-- element: long (containsNull = true) |-- values: array (nullable = true) | |-- element: long (containsNull = true) */ // valuesPerGroupWindowed is a streaming Dataset with just one source // It knows nothing about output mode or watermark yet // That's why StatefulOperatorStateInfo is generic // and no batch-specific values are printed out // That will be available after the first streaming batch // Use sq.explain to know the runtime-specific values countsPer5secWindow.explain /** == Physical Plan == ObjectHashAggregate(keys=[window#23-T10000ms], functions=[collect_list(batch#5L, 0, 0), collect_list(value#4L, 0, 0)]) +- StateStoreSave [window#23-T10000ms], state info [ checkpoint = <unknown>, runId = 50e62943-fe5d-4a02-8498-7134ecbf5122, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2 +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- StateStoreRestore [window#23-T10000ms], state info [ checkpoint = <unknown>, runId = 50e62943-fe5d-4a02-8498-7134ecbf5122, opId = 0, ver = 0, numPartitions = 1], 2 +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- Exchange hashpartitioning(window#23-T10000ms, 1) +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[partial_collect_list(batch#5L, 0, 0), partial_collect_list(value#4L, 0, 0)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#23-T10000ms, value#4L, batch#5L] +- *(1) Filter isnotnull(time#3-T10000ms) +- EventTimeWatermark time#3: timestamp, interval 10 seconds +- StreamingRelation MemoryStream[time#3,value#4L,batch#5L], [time#3, value#4L, batch#5L] */ val queryName = \"watermark_demo\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } // FIXME Use foreachBatch for batchId and the output Dataset // Start the query and hence StateStoreSaveExec import scala.concurrent.duration._ import org.apache.spark.sql.streaming.OutputMode val streamingQuery = countsPer5secWindow .writeStream .format(\"memory\") .queryName(queryName) .option(\"checkpointLocation\", checkpointLocation) .outputMode(OutputMode.Append) // <-- Use Append output mode .start assert(streamingQuery.status.message == \"Waiting for data to arrive\") type Millis = Long def toMillis(datetime: String): Millis = { import java.time.format.DateTimeFormatter import java.time.LocalDateTime import java.time.ZoneOffset LocalDateTime .parse(datetime, DateTimeFormatter.ISO_DATE_TIME) .toInstant(ZoneOffset.UTC) .toEpochMilli } // Use web UI to monitor the state of state (no pun intended) // StateStoreSave and StateStoreRestore operators all have state metrics // Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs // You may also want to check out checkpointed state // in /tmp/checkpoint-watermark_demo/state/0/0 // The demo is aimed to show the following: // 1. The current watermark // 2. Check out the stats: // - expired state (below the current watermark, goes to output and purged later) // - late state (dropped as if never received and processed) // - saved state rows (above the current watermark) val batch = Seq( Event(1, 1, batch = 1), Event(15, 2, batch = 1)) events.addData(batch) streamingQuery.processAllAvailable() println(streamingQuery.lastProgress.stateOperators(0).prettyJson) /** { \"numRowsTotal\" : 1, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 1102, \"customMetrics\" : { \"loadedMapCacheHitCount\" : 2, \"loadedMapCacheMissCount\" : 0, \"stateOnCurrentVersionSizeBytes\" : 414 } } */ val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 15 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 5.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Saved State Rows // Use the metrics of the StateStoreSave operator // Or simply streamingQuery.lastProgress.stateOperators.head spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | +------------------------------------------+-------+------+ */ // With at least one execution we can review the execution plan streamingQuery.explain /** scala> streamingQuery.explain == Physical Plan == ObjectHashAggregate(keys=[window#18-T10000ms], functions=[collect_list(batch#5L, 0, 0), collect_list(value#4L, 0, 0)]) +- StateStoreSave [window#18-T10000ms], state info [ checkpoint = file:/tmp/checkpoint-watermark_demo/state, runId = 73bb0ede-20f2-400d-8003-aa2fbebdd2e1, opId = 0, ver = 1, numPartitions = 1], Append, 5000, 2 +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- StateStoreRestore [window#18-T10000ms], state info [ checkpoint = file:/tmp/checkpoint-watermark_demo/state, runId = 73bb0ede-20f2-400d-8003-aa2fbebdd2e1, opId = 0, ver = 1, numPartitions = 1], 2 +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- Exchange hashpartitioning(window#18-T10000ms, 1) +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[partial_collect_list(batch#5L, 0, 0), partial_collect_list(value#4L, 0, 0)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#18-T10000ms, value#4L, batch#5L] +- *(1) Filter isnotnull(time#3-T10000ms) +- EventTimeWatermark time#3: timestamp, interval 10 seconds +- LocalTableScan <empty>, [time#3, value#4L, batch#5L] */ import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val engine = streamingQuery .asInstanceOf[StreamingQueryWrapper] .streamingQuery import org.apache.spark.sql.execution.streaming.StreamExecution assert(engine.isInstanceOf[StreamExecution]) val lastMicroBatch = engine.lastExecution import org.apache.spark.sql.execution.streaming.IncrementalExecution assert(lastMicroBatch.isInstanceOf[IncrementalExecution]) // Access executedPlan that is the optimized physical query plan ready for execution // All streaming optimizations have been applied at this point // We just need the EventTimeWatermarkExec physical operator val plan = lastMicroBatch.executedPlan // Let's find the EventTimeWatermarkExec physical operator in the plan // There should be one only import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head // Let's check out the event-time watermark stats // They correspond to the concrete EventTimeWatermarkExec operator for a micro-batch val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ val batch = Seq( Event(1, 1, batch = 2), Event(15, 2, batch = 2), Event(35, 3, batch = 2)) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 35 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 25.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ val batch = Seq( Event(15,1, batch = 3), Event(15,2, batch = 3), Event(20,3, batch = 3), Event(26,4, batch = 3)) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 26 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") // Current event-time watermark should be the same as previously // val expectedWatermarkMs = 25.seconds.toMillis // The current max time is merely 26 so subtracting delayThreshold gives merely 16 assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(26000,15000,19000.0,4) */ val batch = Seq( Event(36, 1, batch = 4)) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 36 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 26.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ val batch = Seq( Event(50, 1, batch = 5) ) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 50 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 40.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| |[1970-01-01 01:00:25, 1970-01-01 01:00:30]|[3] |[4] | |[1970-01-01 01:00:35, 1970-01-01 01:00:40]|[2, 4] |[3, 1]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ // Eventually... streamingQuery.stop()","title":"Streaming Watermark with Aggregation in Append Output Mode"},{"location":"demo/watermark-aggregation-append/#demo-streaming-watermark-with-aggregation-in-append-output-mode","text":"The following demo shows the behaviour and the internals of streaming watermark with a streaming aggregation in Append output mode. The demo also shows the behaviour and the internals of StateStoreSaveExec physical operator in Append output mode. Tip The below code is part of StreamingAggregationAppendMode streaming application. // Reduce the number of partitions and hence the state stores // That is supposed to make debugging state checkpointing easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // Define event \"format\" // Use :paste mode in spark-shell import java.sql.Timestamp case class Event(time: Timestamp, value: Long, batch: Long) import scala.concurrent.duration._ object Event { def apply(secs: Long, value: Long, batch: Long): Event = { Event(new Timestamp(secs.seconds.toMillis), value, batch) } } // Using memory data source for full control of the input import org.apache.spark.sql.execution.streaming.MemoryStream implicit val sqlCtx = spark.sqlContext val events = MemoryStream[Event] val values = events.toDS assert(values.isStreaming, \"values must be a streaming Dataset\") values.printSchema /** root |-- time: timestamp (nullable = true) |-- value: long (nullable = false) |-- batch: long (nullable = false) */ // Streaming aggregation using groupBy operator to demo StateStoreSaveExec operator // Define required watermark for late events for Append output mode import scala.concurrent.duration._ val delayThreshold = 10.seconds val eventTime = \"time\" val valuesWatermarked = values .withWatermark(eventTime, delayThreshold.toString) // defines watermark (before groupBy!) // EventTimeWatermark logical operator is planned as EventTimeWatermarkExec physical operator // Note that as a physical operator EventTimeWatermarkExec shows itself without the Exec suffix valuesWatermarked.explain /** == Physical Plan == EventTimeWatermark time#3: timestamp, interval 10 seconds +- StreamingRelation MemoryStream[time#3,value#4L,batch#5L], [time#3, value#4L, batch#5L] */ val windowDuration = 5.seconds import org.apache.spark.sql.functions.window val countsPer5secWindow = valuesWatermarked .groupBy(window(col(eventTime), windowDuration.toString) as \"sliding_window\") .agg(collect_list(\"batch\") as \"batches\", collect_list(\"value\") as \"values\") countsPer5secWindow.printSchema /** root |-- sliding_window: struct (nullable = false) | |-- start: timestamp (nullable = true) | |-- end: timestamp (nullable = true) |-- batches: array (nullable = true) | |-- element: long (containsNull = true) |-- values: array (nullable = true) | |-- element: long (containsNull = true) */ // valuesPerGroupWindowed is a streaming Dataset with just one source // It knows nothing about output mode or watermark yet // That's why StatefulOperatorStateInfo is generic // and no batch-specific values are printed out // That will be available after the first streaming batch // Use sq.explain to know the runtime-specific values countsPer5secWindow.explain /** == Physical Plan == ObjectHashAggregate(keys=[window#23-T10000ms], functions=[collect_list(batch#5L, 0, 0), collect_list(value#4L, 0, 0)]) +- StateStoreSave [window#23-T10000ms], state info [ checkpoint = <unknown>, runId = 50e62943-fe5d-4a02-8498-7134ecbf5122, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2 +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- StateStoreRestore [window#23-T10000ms], state info [ checkpoint = <unknown>, runId = 50e62943-fe5d-4a02-8498-7134ecbf5122, opId = 0, ver = 0, numPartitions = 1], 2 +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- Exchange hashpartitioning(window#23-T10000ms, 1) +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[partial_collect_list(batch#5L, 0, 0), partial_collect_list(value#4L, 0, 0)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#23-T10000ms, value#4L, batch#5L] +- *(1) Filter isnotnull(time#3-T10000ms) +- EventTimeWatermark time#3: timestamp, interval 10 seconds +- StreamingRelation MemoryStream[time#3,value#4L,batch#5L], [time#3, value#4L, batch#5L] */ val queryName = \"watermark_demo\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } // FIXME Use foreachBatch for batchId and the output Dataset // Start the query and hence StateStoreSaveExec import scala.concurrent.duration._ import org.apache.spark.sql.streaming.OutputMode val streamingQuery = countsPer5secWindow .writeStream .format(\"memory\") .queryName(queryName) .option(\"checkpointLocation\", checkpointLocation) .outputMode(OutputMode.Append) // <-- Use Append output mode .start assert(streamingQuery.status.message == \"Waiting for data to arrive\") type Millis = Long def toMillis(datetime: String): Millis = { import java.time.format.DateTimeFormatter import java.time.LocalDateTime import java.time.ZoneOffset LocalDateTime .parse(datetime, DateTimeFormatter.ISO_DATE_TIME) .toInstant(ZoneOffset.UTC) .toEpochMilli } // Use web UI to monitor the state of state (no pun intended) // StateStoreSave and StateStoreRestore operators all have state metrics // Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs // You may also want to check out checkpointed state // in /tmp/checkpoint-watermark_demo/state/0/0 // The demo is aimed to show the following: // 1. The current watermark // 2. Check out the stats: // - expired state (below the current watermark, goes to output and purged later) // - late state (dropped as if never received and processed) // - saved state rows (above the current watermark) val batch = Seq( Event(1, 1, batch = 1), Event(15, 2, batch = 1)) events.addData(batch) streamingQuery.processAllAvailable() println(streamingQuery.lastProgress.stateOperators(0).prettyJson) /** { \"numRowsTotal\" : 1, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 1102, \"customMetrics\" : { \"loadedMapCacheHitCount\" : 2, \"loadedMapCacheMissCount\" : 0, \"stateOnCurrentVersionSizeBytes\" : 414 } } */ val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 15 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 5.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Saved State Rows // Use the metrics of the StateStoreSave operator // Or simply streamingQuery.lastProgress.stateOperators.head spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | +------------------------------------------+-------+------+ */ // With at least one execution we can review the execution plan streamingQuery.explain /** scala> streamingQuery.explain == Physical Plan == ObjectHashAggregate(keys=[window#18-T10000ms], functions=[collect_list(batch#5L, 0, 0), collect_list(value#4L, 0, 0)]) +- StateStoreSave [window#18-T10000ms], state info [ checkpoint = file:/tmp/checkpoint-watermark_demo/state, runId = 73bb0ede-20f2-400d-8003-aa2fbebdd2e1, opId = 0, ver = 1, numPartitions = 1], Append, 5000, 2 +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- StateStoreRestore [window#18-T10000ms], state info [ checkpoint = file:/tmp/checkpoint-watermark_demo/state, runId = 73bb0ede-20f2-400d-8003-aa2fbebdd2e1, opId = 0, ver = 1, numPartitions = 1], 2 +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- Exchange hashpartitioning(window#18-T10000ms, 1) +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[partial_collect_list(batch#5L, 0, 0), partial_collect_list(value#4L, 0, 0)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#18-T10000ms, value#4L, batch#5L] +- *(1) Filter isnotnull(time#3-T10000ms) +- EventTimeWatermark time#3: timestamp, interval 10 seconds +- LocalTableScan <empty>, [time#3, value#4L, batch#5L] */ import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val engine = streamingQuery .asInstanceOf[StreamingQueryWrapper] .streamingQuery import org.apache.spark.sql.execution.streaming.StreamExecution assert(engine.isInstanceOf[StreamExecution]) val lastMicroBatch = engine.lastExecution import org.apache.spark.sql.execution.streaming.IncrementalExecution assert(lastMicroBatch.isInstanceOf[IncrementalExecution]) // Access executedPlan that is the optimized physical query plan ready for execution // All streaming optimizations have been applied at this point // We just need the EventTimeWatermarkExec physical operator val plan = lastMicroBatch.executedPlan // Let's find the EventTimeWatermarkExec physical operator in the plan // There should be one only import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head // Let's check out the event-time watermark stats // They correspond to the concrete EventTimeWatermarkExec operator for a micro-batch val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ val batch = Seq( Event(1, 1, batch = 2), Event(15, 2, batch = 2), Event(35, 3, batch = 2)) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 35 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 25.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ val batch = Seq( Event(15,1, batch = 3), Event(15,2, batch = 3), Event(20,3, batch = 3), Event(26,4, batch = 3)) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 26 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") // Current event-time watermark should be the same as previously // val expectedWatermarkMs = 25.seconds.toMillis // The current max time is merely 26 so subtracting delayThreshold gives merely 16 assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(26000,15000,19000.0,4) */ val batch = Seq( Event(36, 1, batch = 4)) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 36 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 26.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ val batch = Seq( Event(50, 1, batch = 5) ) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 50 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 40.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| |[1970-01-01 01:00:25, 1970-01-01 01:00:30]|[3] |[4] | |[1970-01-01 01:00:35, 1970-01-01 01:00:40]|[2, 4] |[3, 1]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ // Eventually... streamingQuery.stop()","title":"Demo: Streaming Watermark with Aggregation in Append Output Mode"},{"location":"logical-operators/ContinuousExecutionRelation/","text":"ContinuousExecutionRelation Leaf Logical Operator \u00b6 ContinuousExecutionRelation is a MultiInstanceRelation ( Spark SQL ) leaf logical operator. Tip Learn more about Leaf Logical Operators in The Internals of Spark SQL book. Creating Instance \u00b6 ContinuousExecutionRelation takes the following to be created: [[source]] ContinuousReadSupport source [[extraOptions]] Options ( Map[String, String] ) [[output]] Output attributes ( Seq[Attribute] ) [[session]] SparkSession ( Spark SQL ) ContinuousExecutionRelation is created (to represent StreamingRelationV2 with ContinuousReadSupport data source) when ContinuousExecution is created (and requested for the logical plan ).","title":"ContinuousExecutionRelation"},{"location":"logical-operators/ContinuousExecutionRelation/#continuousexecutionrelation-leaf-logical-operator","text":"ContinuousExecutionRelation is a MultiInstanceRelation ( Spark SQL ) leaf logical operator. Tip Learn more about Leaf Logical Operators in The Internals of Spark SQL book.","title":"ContinuousExecutionRelation Leaf Logical Operator"},{"location":"logical-operators/ContinuousExecutionRelation/#creating-instance","text":"ContinuousExecutionRelation takes the following to be created: [[source]] ContinuousReadSupport source [[extraOptions]] Options ( Map[String, String] ) [[output]] Output attributes ( Seq[Attribute] ) [[session]] SparkSession ( Spark SQL ) ContinuousExecutionRelation is created (to represent StreamingRelationV2 with ContinuousReadSupport data source) when ContinuousExecution is created (and requested for the logical plan ).","title":"Creating Instance"},{"location":"logical-operators/Deduplicate/","text":"Deduplicate Unary Logical Operator \u00b6 Deduplicate is a unary logical operator that represents dropDuplicates operator. Deduplicate has < > flag enabled for streaming Datasets. val uniqueRates = spark. readStream. format(\"rate\"). load. dropDuplicates(\"value\") // <-- creates Deduplicate logical operator // Note the streaming flag scala> println(uniqueRates.queryExecution.logical.numberedTreeString) 00 Deduplicate [value#33L], true // <-- streaming flag enabled 01 +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4785f176,rate,List(),None,List(),None,Map(),None), rate, [timestamp#32, value#33L] CAUTION: FIXME Example with duplicates across batches to show that Deduplicate keeps state and withWatermark operator should also be used to limit how much is stored (to not cause OOM) Note UnsupportedOperationChecker ensures that dropDuplicates operator is not used after aggregation on streaming Datasets. The following code is not supported in Structured Streaming and results in an AnalysisException . val counts = spark. readStream. format(\"rate\"). load. groupBy(window($\"timestamp\", \"5 seconds\") as \"group\"). agg(count(\"value\") as \"value_count\"). dropDuplicates // <-- after groupBy import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val sq = counts. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Complete). start org.apache.spark.sql.AnalysisException: dropDuplicates is not supported after aggregation on a streaming DataFrame/Dataset;; Note Deduplicate logical operator is translated ( planned ) to: StreamingDeduplicateExec physical operator in StreamingDeduplicationStrategy execution planning strategy for streaming Datasets (aka streaming plans ) Aggregate physical operator in ReplaceDeduplicateWithAggregate execution planning strategy for non-streaming/batch Datasets ( batch plans ) [[output]] The output schema of Deduplicate is exactly the < >'s output schema. Creating Instance \u00b6 Deduplicate takes the following when created: [[keys]] Attributes for keys [[child]] Child logical operator (i.e. LogicalPlan ) [[streaming]] Flag whether the logical operator is for streaming (enabled) or batch (disabled) mode","title":"Deduplicate"},{"location":"logical-operators/Deduplicate/#deduplicate-unary-logical-operator","text":"Deduplicate is a unary logical operator that represents dropDuplicates operator. Deduplicate has < > flag enabled for streaming Datasets. val uniqueRates = spark. readStream. format(\"rate\"). load. dropDuplicates(\"value\") // <-- creates Deduplicate logical operator // Note the streaming flag scala> println(uniqueRates.queryExecution.logical.numberedTreeString) 00 Deduplicate [value#33L], true // <-- streaming flag enabled 01 +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4785f176,rate,List(),None,List(),None,Map(),None), rate, [timestamp#32, value#33L] CAUTION: FIXME Example with duplicates across batches to show that Deduplicate keeps state and withWatermark operator should also be used to limit how much is stored (to not cause OOM) Note UnsupportedOperationChecker ensures that dropDuplicates operator is not used after aggregation on streaming Datasets. The following code is not supported in Structured Streaming and results in an AnalysisException . val counts = spark. readStream. format(\"rate\"). load. groupBy(window($\"timestamp\", \"5 seconds\") as \"group\"). agg(count(\"value\") as \"value_count\"). dropDuplicates // <-- after groupBy import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val sq = counts. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Complete). start org.apache.spark.sql.AnalysisException: dropDuplicates is not supported after aggregation on a streaming DataFrame/Dataset;; Note Deduplicate logical operator is translated ( planned ) to: StreamingDeduplicateExec physical operator in StreamingDeduplicationStrategy execution planning strategy for streaming Datasets (aka streaming plans ) Aggregate physical operator in ReplaceDeduplicateWithAggregate execution planning strategy for non-streaming/batch Datasets ( batch plans ) [[output]] The output schema of Deduplicate is exactly the < >'s output schema.","title":"Deduplicate Unary Logical Operator"},{"location":"logical-operators/Deduplicate/#creating-instance","text":"Deduplicate takes the following when created: [[keys]] Attributes for keys [[child]] Child logical operator (i.e. LogicalPlan ) [[streaming]] Flag whether the logical operator is for streaming (enabled) or batch (disabled) mode","title":"Creating Instance"},{"location":"logical-operators/EventTimeWatermark/","text":"EventTimeWatermark Unary Logical Operator \u00b6 EventTimeWatermark is a unary logical operator that is < > to represent Dataset.withWatermark operator in a logical query plan of a streaming query. Note A unary logical operator ( UnaryNode ) is a logical operator with a single < > logical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan.html[UnaryNode ] (and logical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. When requested for the < >, EventTimeWatermark logical operator goes over the output attributes of the < > logical operator to find the matching attribute based on the < > attribute and updates it to include spark.watermarkDelayMs metadata key with the < > interval (< >). EventTimeWatermark is resolved ( planned ) to EventTimeWatermarkExec physical operator in StatefulAggregationStrategy execution planning strategy. [NOTE] \u00b6 EliminateEventTimeWatermark logical optimization rule (i.e. Rule[LogicalPlan] ) removes EventTimeWatermark logical operator from a logical plan if the < > logical operator is not streaming, i.e. when Dataset.withWatermark operator is used on a batch query. val logs = spark. read. // <-- batch non-streaming query that makes `EliminateEventTimeWatermark` rule applicable format(\"text\"). load(\"logs\") // logs is a batch Dataset assert(!logs.isStreaming) val q = logs. withWatermark(eventTime = \"timestamp\", delayThreshold = \"30 seconds\") // <-- creates EventTimeWatermark scala> println(q.queryExecution.logical.numberedTreeString) // <-- no EventTimeWatermark as it was removed immediately 00 Relation[value#0] text \u00b6 === [[creating-instance]] Creating EventTimeWatermark Instance EventTimeWatermark takes the following to be created: [[eventTime]] Watermark column ( Attribute ) [[delay]] Watermark delay ( CalendarInterval ) [[child]] Child logical operator ( LogicalPlan ) === [[output]] Output Schema -- output Property [source, scala] \u00b6 output: Seq[Attribute] \u00b6 NOTE: output is part of the QueryPlan Contract to describe the attributes of (the schema of) the output. output finds < > column in the output schema of the < > logical operator and updates the Metadata of the column with < > key and the milliseconds for the delay. output removes < > key from the other columns. [source, scala] \u00b6 // FIXME How to access/show the eventTime column with the metadata updated to include spark.watermarkDelayMs? import org.apache.spark.sql.catalyst.plans.logical.EventTimeWatermark val etw = q.queryExecution.logical.asInstanceOf[EventTimeWatermark] scala> etw.output.toStructType.printTreeString root |-- timestamp: timestamp (nullable = true) |-- value: long (nullable = true) === [[watermarkDelayMs]][[delayKey]] Watermark Metadata (Marker) -- spark.watermarkDelayMs Metadata Key spark.watermarkDelayMs metadata key is used to mark one of the < > as the watermark attribute ( eventTime watermark ). === [[getDelayMs]] Converting Human-Friendly CalendarInterval to Milliseconds -- getDelayMs Object Method [source, scala] \u00b6 getDelayMs( delay: CalendarInterval): Long getDelayMs ...FIXME NOTE: getDelayMs is used when...FIXME","title":"EventTimeWatermark"},{"location":"logical-operators/EventTimeWatermark/#eventtimewatermark-unary-logical-operator","text":"EventTimeWatermark is a unary logical operator that is < > to represent Dataset.withWatermark operator in a logical query plan of a streaming query. Note A unary logical operator ( UnaryNode ) is a logical operator with a single < > logical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan.html[UnaryNode ] (and logical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. When requested for the < >, EventTimeWatermark logical operator goes over the output attributes of the < > logical operator to find the matching attribute based on the < > attribute and updates it to include spark.watermarkDelayMs metadata key with the < > interval (< >). EventTimeWatermark is resolved ( planned ) to EventTimeWatermarkExec physical operator in StatefulAggregationStrategy execution planning strategy.","title":"EventTimeWatermark Unary Logical Operator"},{"location":"logical-operators/EventTimeWatermark/#note","text":"EliminateEventTimeWatermark logical optimization rule (i.e. Rule[LogicalPlan] ) removes EventTimeWatermark logical operator from a logical plan if the < > logical operator is not streaming, i.e. when Dataset.withWatermark operator is used on a batch query.","title":"[NOTE]"},{"location":"logical-operators/EventTimeWatermark/#val-logs-spark-read-batch-non-streaming-query-that-makes-eliminateeventtimewatermark-rule-applicable-formattext-loadlogs-logs-is-a-batch-dataset-assertlogsisstreaming-val-q-logs-withwatermarkeventtime-timestamp-delaythreshold-30-seconds-creates-eventtimewatermark-scala-printlnqqueryexecutionlogicalnumberedtreestring-no-eventtimewatermark-as-it-was-removed-immediately-00-relationvalue0-text","text":"=== [[creating-instance]] Creating EventTimeWatermark Instance EventTimeWatermark takes the following to be created: [[eventTime]] Watermark column ( Attribute ) [[delay]] Watermark delay ( CalendarInterval ) [[child]] Child logical operator ( LogicalPlan ) === [[output]] Output Schema -- output Property","title":"val logs = spark.\n  read. // &lt;-- batch non-streaming query that makes `EliminateEventTimeWatermark` rule applicable\n  format(&quot;text&quot;).\n  load(&quot;logs&quot;)\n\n// logs is a batch Dataset\nassert(!logs.isStreaming)\n\nval q = logs.\n  withWatermark(eventTime = &quot;timestamp&quot;, delayThreshold = &quot;30 seconds&quot;) // &lt;-- creates EventTimeWatermark\nscala&gt; println(q.queryExecution.logical.numberedTreeString) // &lt;-- no EventTimeWatermark as it was removed immediately\n00 Relation[value#0] text\n"},{"location":"logical-operators/EventTimeWatermark/#source-scala","text":"","title":"[source, scala]"},{"location":"logical-operators/EventTimeWatermark/#output-seqattribute","text":"NOTE: output is part of the QueryPlan Contract to describe the attributes of (the schema of) the output. output finds < > column in the output schema of the < > logical operator and updates the Metadata of the column with < > key and the milliseconds for the delay. output removes < > key from the other columns.","title":"output: Seq[Attribute]"},{"location":"logical-operators/EventTimeWatermark/#source-scala_1","text":"// FIXME How to access/show the eventTime column with the metadata updated to include spark.watermarkDelayMs? import org.apache.spark.sql.catalyst.plans.logical.EventTimeWatermark val etw = q.queryExecution.logical.asInstanceOf[EventTimeWatermark] scala> etw.output.toStructType.printTreeString root |-- timestamp: timestamp (nullable = true) |-- value: long (nullable = true) === [[watermarkDelayMs]][[delayKey]] Watermark Metadata (Marker) -- spark.watermarkDelayMs Metadata Key spark.watermarkDelayMs metadata key is used to mark one of the < > as the watermark attribute ( eventTime watermark ). === [[getDelayMs]] Converting Human-Friendly CalendarInterval to Milliseconds -- getDelayMs Object Method","title":"[source, scala]"},{"location":"logical-operators/EventTimeWatermark/#source-scala_2","text":"getDelayMs( delay: CalendarInterval): Long getDelayMs ...FIXME NOTE: getDelayMs is used when...FIXME","title":"[source, scala]"},{"location":"logical-operators/FlatMapGroupsWithState/","text":"FlatMapGroupsWithState Unary Logical Operator \u00b6 FlatMapGroupsWithState is a unary logical operator that represents the following operators in a logical query plan of a streaming query: KeyValueGroupedDataset.mapGroupsWithState KeyValueGroupedDataset.flatMapGroupsWithState Note A unary logical operator ( UnaryNode ) is a logical operator with a single < > logical operator. Read up on UnaryNode (and logical operators in general) in The Internals of Spark SQL online book. Execution Planning \u00b6 FlatMapGroupsWithState is resolved ( planned ) to: FlatMapGroupsWithStateExec unary physical operator for streaming datasets (in FlatMapGroupsWithStateStrategy execution planning strategy) MapGroupsExec physical operator for batch datasets (in BasicOperators execution planning strategy) Creating Instance \u00b6 FlatMapGroupsWithState takes the following to be created: State function ( (Any, Iterator[Any], LogicalGroupState[Any]) => Iterator[Any] ) Catalyst Expression for keys Catalyst Expression for values Grouping Attributes Data Attributes Output Object Attribute State ExpressionEncoder OutputMode isMapGroupsWithState flag (default: false ) GroupStateTimeout Child logical operator FlatMapGroupsWithState is created (using apply factory method) for KeyValueGroupedDataset.mapGroupsWithState and KeyValueGroupedDataset.flatMapGroupsWithState operators. Creating SerializeFromObject with FlatMapGroupsWithState \u00b6 apply [ K : Encoder , V : Encoder , S : Encoder , U : Encoder ]( func : ( Any , Iterator [ Any ], LogicalGroupState [ Any ]) => Iterator [ Any ], groupingAttributes : Seq [ Attribute ], dataAttributes : Seq [ Attribute ], outputMode : OutputMode , isMapGroupsWithState : Boolean , timeout : GroupStateTimeout , child : LogicalPlan ): LogicalPlan apply creates a SerializeFromObject logical operator with a FlatMapGroupsWithState as its child logical operator. Internally, apply creates SerializeFromObject object consumer (aka unary logical operator) with FlatMapGroupsWithState logical plan. Internally, apply finds ExpressionEncoder for the type S and creates a FlatMapGroupsWithState with UnresolvedDeserializer for the types K and V . In the end, apply creates a SerializeFromObject object consumer with the FlatMapGroupsWithState . apply is used for KeyValueGroupedDataset.mapGroupsWithState and KeyValueGroupedDataset.flatMapGroupsWithState operators.","title":"FlatMapGroupsWithState"},{"location":"logical-operators/FlatMapGroupsWithState/#flatmapgroupswithstate-unary-logical-operator","text":"FlatMapGroupsWithState is a unary logical operator that represents the following operators in a logical query plan of a streaming query: KeyValueGroupedDataset.mapGroupsWithState KeyValueGroupedDataset.flatMapGroupsWithState Note A unary logical operator ( UnaryNode ) is a logical operator with a single < > logical operator. Read up on UnaryNode (and logical operators in general) in The Internals of Spark SQL online book.","title":"FlatMapGroupsWithState Unary Logical Operator"},{"location":"logical-operators/FlatMapGroupsWithState/#execution-planning","text":"FlatMapGroupsWithState is resolved ( planned ) to: FlatMapGroupsWithStateExec unary physical operator for streaming datasets (in FlatMapGroupsWithStateStrategy execution planning strategy) MapGroupsExec physical operator for batch datasets (in BasicOperators execution planning strategy)","title":"Execution Planning"},{"location":"logical-operators/FlatMapGroupsWithState/#creating-instance","text":"FlatMapGroupsWithState takes the following to be created: State function ( (Any, Iterator[Any], LogicalGroupState[Any]) => Iterator[Any] ) Catalyst Expression for keys Catalyst Expression for values Grouping Attributes Data Attributes Output Object Attribute State ExpressionEncoder OutputMode isMapGroupsWithState flag (default: false ) GroupStateTimeout Child logical operator FlatMapGroupsWithState is created (using apply factory method) for KeyValueGroupedDataset.mapGroupsWithState and KeyValueGroupedDataset.flatMapGroupsWithState operators.","title":"Creating Instance"},{"location":"logical-operators/FlatMapGroupsWithState/#creating-serializefromobject-with-flatmapgroupswithstate","text":"apply [ K : Encoder , V : Encoder , S : Encoder , U : Encoder ]( func : ( Any , Iterator [ Any ], LogicalGroupState [ Any ]) => Iterator [ Any ], groupingAttributes : Seq [ Attribute ], dataAttributes : Seq [ Attribute ], outputMode : OutputMode , isMapGroupsWithState : Boolean , timeout : GroupStateTimeout , child : LogicalPlan ): LogicalPlan apply creates a SerializeFromObject logical operator with a FlatMapGroupsWithState as its child logical operator. Internally, apply creates SerializeFromObject object consumer (aka unary logical operator) with FlatMapGroupsWithState logical plan. Internally, apply finds ExpressionEncoder for the type S and creates a FlatMapGroupsWithState with UnresolvedDeserializer for the types K and V . In the end, apply creates a SerializeFromObject object consumer with the FlatMapGroupsWithState . apply is used for KeyValueGroupedDataset.mapGroupsWithState and KeyValueGroupedDataset.flatMapGroupsWithState operators.","title":" Creating SerializeFromObject with FlatMapGroupsWithState"},{"location":"logical-operators/StreamingDataSourceV2Relation/","text":"StreamingDataSourceV2Relation Logical Operator \u00b6 StreamingDataSourceV2Relation is a leaf logical operator that represents StreamingRelationV2 logical operator (with tables with a SupportsRead and MICRO_BATCH_READ or CONTINUOUS_READ capabilities) at execution time. Tip Learn more about Leaf Logical Operators , SupportsRead and Table Capabilities in The Internals of Spark SQL online book. Creating Instance \u00b6 StreamingDataSourceV2Relation takes the following to be created: Output Attributes ( Spark SQL ) Scan ( Spark SQL ) SparkDataStream Start Offset (default: undefined) End Offset (default: undefined) StreamingDataSourceV2Relation is created when: MicroBatchExecution stream execution engine is requested for an analyzed logical query plan (for StreamingRelationV2 with a SupportsRead table with MICRO_BATCH_READ capability) ContinuousExecution stream execution engine is requested for an analyzed logical query plan (for StreamingRelationV2 with a SupportsRead table with CONTINUOUS_READ capability) Computing Stats \u00b6 computeStats (): Statistics For Scans with SupportsReportStatistics , computeStats requests the scan to estimateStatistics . Tip Learn more about Scan and SupportsReportStatistics in The Internals of Spark SQL online book. For other types of scans, computeStats simply assumes the default size and no row count. computeStats is part of the LeafNode abstraction.","title":"StreamingDataSourceV2Relation"},{"location":"logical-operators/StreamingDataSourceV2Relation/#streamingdatasourcev2relation-logical-operator","text":"StreamingDataSourceV2Relation is a leaf logical operator that represents StreamingRelationV2 logical operator (with tables with a SupportsRead and MICRO_BATCH_READ or CONTINUOUS_READ capabilities) at execution time. Tip Learn more about Leaf Logical Operators , SupportsRead and Table Capabilities in The Internals of Spark SQL online book.","title":"StreamingDataSourceV2Relation Logical Operator"},{"location":"logical-operators/StreamingDataSourceV2Relation/#creating-instance","text":"StreamingDataSourceV2Relation takes the following to be created: Output Attributes ( Spark SQL ) Scan ( Spark SQL ) SparkDataStream Start Offset (default: undefined) End Offset (default: undefined) StreamingDataSourceV2Relation is created when: MicroBatchExecution stream execution engine is requested for an analyzed logical query plan (for StreamingRelationV2 with a SupportsRead table with MICRO_BATCH_READ capability) ContinuousExecution stream execution engine is requested for an analyzed logical query plan (for StreamingRelationV2 with a SupportsRead table with CONTINUOUS_READ capability)","title":"Creating Instance"},{"location":"logical-operators/StreamingDataSourceV2Relation/#computing-stats","text":"computeStats (): Statistics For Scans with SupportsReportStatistics , computeStats requests the scan to estimateStatistics . Tip Learn more about Scan and SupportsReportStatistics in The Internals of Spark SQL online book. For other types of scans, computeStats simply assumes the default size and no row count. computeStats is part of the LeafNode abstraction.","title":" Computing Stats"},{"location":"logical-operators/StreamingExecutionRelation/","text":"StreamingExecutionRelation Leaf Logical Operator \u00b6 StreamingExecutionRelation is a leaf logical operator ( Spark SQL ) that represents a streaming source in the logical query plan of a streaming query. The main use of StreamingExecutionRelation logical operator is to be a \"placeholder\" in a logical query plan that will be replaced with the real relation (with new data that has arrived since the last batch) or an empty LocalRelation when StreamExecution is requested to transforming logical plan to include the Sources and MicroBatchReaders with new data . Note Right after StreamExecution has started running streaming batches it initializes the streaming sources by transforming the analyzed logical plan of the streaming query so that every StreamingRelation logical operator is replaced by the corresponding StreamingExecutionRelation . Note StreamingExecutionRelation is also resolved ( planned ) to a StreamingRelationExec physical operator in StreamingRelationStrategy execution planning strategy only when explaining a streaming Dataset . Creating Instance \u00b6 StreamingExecutionRelation takes the following to be created: BaseStreamingSource Output Attributes ( Seq[Attribute] ) SparkSession StreamingExecutionRelation is created when: MicroBatchExecution stream execution engine is requested for the analyzed logical query plan (for every StreamingRelation )","title":"StreamingExecutionRelation"},{"location":"logical-operators/StreamingExecutionRelation/#streamingexecutionrelation-leaf-logical-operator","text":"StreamingExecutionRelation is a leaf logical operator ( Spark SQL ) that represents a streaming source in the logical query plan of a streaming query. The main use of StreamingExecutionRelation logical operator is to be a \"placeholder\" in a logical query plan that will be replaced with the real relation (with new data that has arrived since the last batch) or an empty LocalRelation when StreamExecution is requested to transforming logical plan to include the Sources and MicroBatchReaders with new data . Note Right after StreamExecution has started running streaming batches it initializes the streaming sources by transforming the analyzed logical plan of the streaming query so that every StreamingRelation logical operator is replaced by the corresponding StreamingExecutionRelation . Note StreamingExecutionRelation is also resolved ( planned ) to a StreamingRelationExec physical operator in StreamingRelationStrategy execution planning strategy only when explaining a streaming Dataset .","title":"StreamingExecutionRelation Leaf Logical Operator"},{"location":"logical-operators/StreamingExecutionRelation/#creating-instance","text":"StreamingExecutionRelation takes the following to be created: BaseStreamingSource Output Attributes ( Seq[Attribute] ) SparkSession StreamingExecutionRelation is created when: MicroBatchExecution stream execution engine is requested for the analyzed logical query plan (for every StreamingRelation )","title":"Creating Instance"},{"location":"logical-operators/StreamingRelation/","text":"StreamingRelation Leaf Logical Operator \u00b6 StreamingRelation is a leaf logical operator ( Spark SQL ) that represents a streaming source in a logical plan. StreamingRelation is resolved ( planned ) to a StreamingExecutionRelation (right after StreamExecution starts running batches ). Creating Instance \u00b6 StreamingRelation takes the following to be created: DataSource Short Name of the Streaming Source Output Attributes ( Seq[Attribute] ) StreamingRelation is created when: DataStreamReader is requested to load data from a streaming source and creates a streaming query. Creating StreamingRelation for DataSource \u00b6 apply ( dataSource : DataSource ): StreamingRelation apply creates a StreamingRelation for the given DataSource . apply is used when: DataStreamReader is requested for a streaming query isStreaming \u00b6 isStreaming : Boolean isStreaming is part of the LogicalPlan ( Spark SQL ) abstraction. isStreaming flag is always true . import org . apache . spark . sql . execution . streaming . StreamingRelation val relation = rate . queryExecution . logical . asInstanceOf [ StreamingRelation ] assert ( relation . isStreaming ) Text Respresentation \u00b6 toString : String toString gives the source name . Demo \u00b6 val rate = spark . readStream . format ( \"rate\" ). load ( \"hello\" ) scala> println(rate.queryExecution.logical.numberedTreeString) 00 StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@69ab1abc, rate, Map(path -> hello), [timestamp#0, value#1L]","title":"StreamingRelation"},{"location":"logical-operators/StreamingRelation/#streamingrelation-leaf-logical-operator","text":"StreamingRelation is a leaf logical operator ( Spark SQL ) that represents a streaming source in a logical plan. StreamingRelation is resolved ( planned ) to a StreamingExecutionRelation (right after StreamExecution starts running batches ).","title":"StreamingRelation Leaf Logical Operator"},{"location":"logical-operators/StreamingRelation/#creating-instance","text":"StreamingRelation takes the following to be created: DataSource Short Name of the Streaming Source Output Attributes ( Seq[Attribute] ) StreamingRelation is created when: DataStreamReader is requested to load data from a streaming source and creates a streaming query.","title":"Creating Instance"},{"location":"logical-operators/StreamingRelation/#creating-streamingrelation-for-datasource","text":"apply ( dataSource : DataSource ): StreamingRelation apply creates a StreamingRelation for the given DataSource . apply is used when: DataStreamReader is requested for a streaming query","title":" Creating StreamingRelation for DataSource"},{"location":"logical-operators/StreamingRelation/#isstreaming","text":"isStreaming : Boolean isStreaming is part of the LogicalPlan ( Spark SQL ) abstraction. isStreaming flag is always true . import org . apache . spark . sql . execution . streaming . StreamingRelation val relation = rate . queryExecution . logical . asInstanceOf [ StreamingRelation ] assert ( relation . isStreaming )","title":" isStreaming"},{"location":"logical-operators/StreamingRelation/#text-respresentation","text":"toString : String toString gives the source name .","title":" Text Respresentation"},{"location":"logical-operators/StreamingRelation/#demo","text":"val rate = spark . readStream . format ( \"rate\" ). load ( \"hello\" ) scala> println(rate.queryExecution.logical.numberedTreeString) 00 StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@69ab1abc, rate, Map(path -> hello), [timestamp#0, value#1L]","title":"Demo"},{"location":"logical-operators/StreamingRelationV2/","text":"StreamingRelationV2 Leaf Logical Operator \u00b6 StreamingRelationV2 is a leaf logical operator that represents SupportsRead streaming tables (with MICRO_BATCH_READ or CONTINUOUS_READ capabilities) in a logical plan of a streaming query. Tip Learn more about Leaf Logical Operators , SupportsRead and Table Capabilities in The Internals of Spark SQL online book. Creating Instance \u00b6 StreamingRelationV2 takes the following to be created: TableProvider ( Spark SQL ) Source Name Table ( Spark SQL ) Extra Options Output Attributes ( Spark SQL ) StreamingRelation SparkSession ( Spark SQL ) StreamingRelationV2 is created when: DataStreamReader is reqested to load data (for a SupportsRead table with MICRO_BATCH_READ or CONTINUOUS_READ capabilities) MemoryStreamBase is requested for a logical query plan Logical Resolution \u00b6 StreamingRelationV2 is resolved to the following leaf logical operators: StreamingDataSourceV2Relation or StreamingExecutionRelation when MicroBatchExecution stream execution engine is requested for an analyzed logical plan StreamingDataSourceV2Relation when ContinuousExecution stream execution engine is created (and initializes an analyzed logical plan )","title":"StreamingRelationV2"},{"location":"logical-operators/StreamingRelationV2/#streamingrelationv2-leaf-logical-operator","text":"StreamingRelationV2 is a leaf logical operator that represents SupportsRead streaming tables (with MICRO_BATCH_READ or CONTINUOUS_READ capabilities) in a logical plan of a streaming query. Tip Learn more about Leaf Logical Operators , SupportsRead and Table Capabilities in The Internals of Spark SQL online book.","title":"StreamingRelationV2 Leaf Logical Operator"},{"location":"logical-operators/StreamingRelationV2/#creating-instance","text":"StreamingRelationV2 takes the following to be created: TableProvider ( Spark SQL ) Source Name Table ( Spark SQL ) Extra Options Output Attributes ( Spark SQL ) StreamingRelation SparkSession ( Spark SQL ) StreamingRelationV2 is created when: DataStreamReader is reqested to load data (for a SupportsRead table with MICRO_BATCH_READ or CONTINUOUS_READ capabilities) MemoryStreamBase is requested for a logical query plan","title":"Creating Instance"},{"location":"logical-operators/StreamingRelationV2/#logical-resolution","text":"StreamingRelationV2 is resolved to the following leaf logical operators: StreamingDataSourceV2Relation or StreamingExecutionRelation when MicroBatchExecution stream execution engine is requested for an analyzed logical plan StreamingDataSourceV2Relation when ContinuousExecution stream execution engine is created (and initializes an analyzed logical plan )","title":"Logical Resolution"},{"location":"logical-operators/WriteToContinuousDataSource/","text":"WriteToContinuousDataSource Unary Logical Operator \u00b6 WriteToContinuousDataSource is a unary logical operator ( LogicalPlan ) that is created when ContinuousExecution is requested to run a streaming query in continuous mode (to create an IncrementalExecution ). WriteToContinuousDataSource is planned ( translated ) to a WriteToContinuousDataSourceExec unary physical operator (when DataSourceV2Strategy execution planning strategy is requested to plan a logical query). [[output]] WriteToContinuousDataSource uses empty output schema (which is exactly to say that no output is expected whatsoever). Creating Instance \u00b6 WriteToContinuousDataSource takes the following to be created: [[query]] Child logical operator ( LogicalPlan )","title":"WriteToContinuousDataSource"},{"location":"logical-operators/WriteToContinuousDataSource/#writetocontinuousdatasource-unary-logical-operator","text":"WriteToContinuousDataSource is a unary logical operator ( LogicalPlan ) that is created when ContinuousExecution is requested to run a streaming query in continuous mode (to create an IncrementalExecution ). WriteToContinuousDataSource is planned ( translated ) to a WriteToContinuousDataSourceExec unary physical operator (when DataSourceV2Strategy execution planning strategy is requested to plan a logical query). [[output]] WriteToContinuousDataSource uses empty output schema (which is exactly to say that no output is expected whatsoever).","title":"WriteToContinuousDataSource Unary Logical Operator"},{"location":"logical-operators/WriteToContinuousDataSource/#creating-instance","text":"WriteToContinuousDataSource takes the following to be created: [[query]] Child logical operator ( LogicalPlan )","title":"Creating Instance"},{"location":"micro-batch-execution/","text":"Micro-Batch Stream Processing \u00b6 Micro-Batch Stream Processing is a stream processing model in Spark Structured Streaming that is used for streaming queries with Trigger.Once and Trigger.ProcessingTime triggers. Micro-batch stream processing uses MicroBatchExecution stream execution engine. Micro-batch stream processing supports MicroBatchStream data sources. Micro-batch stream processing is often referred to as Structured Streaming V1 . Execution Phases \u00b6 When MicroBatchExecution stream processing engine is requested to run an activated streaming query , the query execution goes through the following execution phases every trigger ( micro-batch ): triggerExecution getOffset for Source s or setOffsetRange for MicroBatchReader s getEndOffset walCommit getBatch queryPlanning addBatch Execution phases with execution times are available using StreamingQueryProgress under durationMs . scala> :type sq org.apache.spark.sql.streaming.StreamingQuery sq.lastProgress.durationMs.get(\"walCommit\") Tip Enable INFO logging level for StreamExecution logger to be notified about durations. 17/08/11 09:04:17 INFO StreamExecution: Streaming query made progress: { \"id\" : \"ec8f8228-90f6-4e1f-8ad2-80222affed63\", \"runId\" : \"f605c134-cfb0-4378-88c1-159d8a7c232e\", \"name\" : \"rates-to-console\", \"timestamp\" : \"2017-08-11T07:04:17.373Z\", \"batchId\" : 0, \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0, \"durationMs\" : { // <-- Durations (in millis) \"addBatch\" : 38, \"getBatch\" : 1, \"getOffset\" : 0, \"queryPlanning\" : 1, \"triggerExecution\" : 62, \"walCommit\" : 19 }, Monitoring \u00b6 MicroBatchExecution posts events to announce when a streaming query is started and stopped as well as after every micro-batch. StreamingQueryListener interface can be used to intercept the events and act accordingly. After triggerExecution phase MicroBatchExecution is requested to finish up a streaming batch (trigger) and generate a StreamingQueryProgress (with execution statistics). MicroBatchExecution prints out the following DEBUG message to the logs: Execution stats: [executionStats] MicroBatchExecution posts a QueryProgressEvent with the StreamingQueryProgress and prints out the following INFO message to the logs: Streaming query made progress: [newProgress] Demo \u00b6 import org . apache . spark . sql . streaming . Trigger import scala . concurrent . duration . _ val sq = spark . readStream . format ( \"rate\" ) . load . writeStream . format ( \"console\" ) . option ( \"truncate\" , false ) . trigger ( Trigger . ProcessingTime ( 1 . minute )) // <-- Uses MicroBatchExecution for execution . queryName ( \"rate2console\" ) . start assert ( sq . isActive ) scala> sq.explain == Physical Plan == WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@678e6267 +- *(1) Project [timestamp#54, value#55L] +- *(1) ScanV2 rate[timestamp#54, value#55L] sq . stop","title":"Micro-Batch Stream Processing"},{"location":"micro-batch-execution/#micro-batch-stream-processing","text":"Micro-Batch Stream Processing is a stream processing model in Spark Structured Streaming that is used for streaming queries with Trigger.Once and Trigger.ProcessingTime triggers. Micro-batch stream processing uses MicroBatchExecution stream execution engine. Micro-batch stream processing supports MicroBatchStream data sources. Micro-batch stream processing is often referred to as Structured Streaming V1 .","title":"Micro-Batch Stream Processing"},{"location":"micro-batch-execution/#execution-phases","text":"When MicroBatchExecution stream processing engine is requested to run an activated streaming query , the query execution goes through the following execution phases every trigger ( micro-batch ): triggerExecution getOffset for Source s or setOffsetRange for MicroBatchReader s getEndOffset walCommit getBatch queryPlanning addBatch Execution phases with execution times are available using StreamingQueryProgress under durationMs . scala> :type sq org.apache.spark.sql.streaming.StreamingQuery sq.lastProgress.durationMs.get(\"walCommit\") Tip Enable INFO logging level for StreamExecution logger to be notified about durations. 17/08/11 09:04:17 INFO StreamExecution: Streaming query made progress: { \"id\" : \"ec8f8228-90f6-4e1f-8ad2-80222affed63\", \"runId\" : \"f605c134-cfb0-4378-88c1-159d8a7c232e\", \"name\" : \"rates-to-console\", \"timestamp\" : \"2017-08-11T07:04:17.373Z\", \"batchId\" : 0, \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0, \"durationMs\" : { // <-- Durations (in millis) \"addBatch\" : 38, \"getBatch\" : 1, \"getOffset\" : 0, \"queryPlanning\" : 1, \"triggerExecution\" : 62, \"walCommit\" : 19 },","title":"Execution Phases"},{"location":"micro-batch-execution/#monitoring","text":"MicroBatchExecution posts events to announce when a streaming query is started and stopped as well as after every micro-batch. StreamingQueryListener interface can be used to intercept the events and act accordingly. After triggerExecution phase MicroBatchExecution is requested to finish up a streaming batch (trigger) and generate a StreamingQueryProgress (with execution statistics). MicroBatchExecution prints out the following DEBUG message to the logs: Execution stats: [executionStats] MicroBatchExecution posts a QueryProgressEvent with the StreamingQueryProgress and prints out the following INFO message to the logs: Streaming query made progress: [newProgress]","title":"Monitoring"},{"location":"micro-batch-execution/#demo","text":"import org . apache . spark . sql . streaming . Trigger import scala . concurrent . duration . _ val sq = spark . readStream . format ( \"rate\" ) . load . writeStream . format ( \"console\" ) . option ( \"truncate\" , false ) . trigger ( Trigger . ProcessingTime ( 1 . minute )) // <-- Uses MicroBatchExecution for execution . queryName ( \"rate2console\" ) . start assert ( sq . isActive ) scala> sq.explain == Physical Plan == WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@678e6267 +- *(1) Project [timestamp#54, value#55L] +- *(1) ScanV2 rate[timestamp#54, value#55L] sq . stop","title":"Demo"},{"location":"micro-batch-execution/MicroBatchExecution/","text":"MicroBatchExecution \u00b6 MicroBatchExecution is a stream execution engine for Micro-Batch Stream Processing . Creating Instance \u00b6 MicroBatchExecution takes the following to be created: SparkSession ( Spark SQL ) Name of the streaming query Path of the Checkpoint Directory Analyzed logical query plan of the streaming query ( Spark SQL ) Table Sink ( Spark SQL ) Trigger Clock OutputMode Extra Options ( Map[String, String] ) deleteCheckpointOnStop flag to control whether to delete the checkpoint directory on stop MicroBatchExecution is created when: StreamingQueryManager is requested to create a streaming query (when DataStreamWriter is requested to start an execution of the streaming query ) with the following: All sink s All trigger s but ContinuousTrigger Once created, MicroBatchExecution is requested to run an activated streaming query . Initializing Query Progress for New Trigger \u00b6 startTrigger (): Unit startTrigger is part of the ProgressReporter abstraction. startTrigger ...FIXME Streaming Sources Registry \u00b6 sources : Seq [ SparkDataStream ] MicroBatchExecution uses...FIXME Streaming sources and readers (of the StreamingExecutionRelations of the analyzed logical query plan of the streaming query) Default: (empty) sources is part of the ProgressReporter abstraction. Initialized when MicroBatchExecution is requested for the transformed logical query plan Used when: Populating start offsets (for the available and committed offsets) Constructing or skipping next streaming micro-batch (and persisting offsets to write-ahead log) TriggerExecutor \u00b6 triggerExecutor : TriggerExecutor MicroBatchExecution uses a TriggerExecutor that is how micro-batches are executed at regular intervals. triggerExecutor is initialized based on the given Trigger (given when creating the MicroBatchExecution ): ProcessingTimeExecutor for Trigger.ProcessingTime OneTimeExecutor for OneTimeTrigger (aka Trigger.Once trigger) triggerExecutor throws an IllegalStateException when the Trigger is not one of the built-in implementations . Unknown type of trigger: [trigger] triggerExecutor is used when: StreamExecution is requested to run an activated streaming query (at regular intervals) Running Activated Streaming Query \u00b6 runActivatedStream ( sparkSessionForStream : SparkSession ): Unit runActivatedStream simply requests the TriggerExecutor to execute micro-batches using the batch runner (until MicroBatchExecution is terminated due to a query stop or a failure). runActivatedStream is part of StreamExecution abstraction. TriggerExecutor's Batch Runner \u00b6 The batch runner (of the TriggerExecutor ) is executed as long as the MicroBatchExecution is active . Note trigger and batch are considered equivalent and used interchangeably. The batch runner initializes query progress for the new trigger (aka startTrigger ). The batch runner starts triggerExecution execution phase that is made up of the following steps: Populating start offsets from checkpoint before the first \"zero\" batch (at every start or restart) Constructing or skipping the next streaming micro-batch Running the streaming micro-batch At the start or restart ( resume ) of a streaming query (when the current batch ID is uninitialized and -1 ), the batch runner populates start offsets from checkpoint and then prints out the following INFO message to the logs (using the committedOffsets internal registry): Stream started from [committedOffsets] The batch runner sets the human-readable description for any Spark job submitted (that streaming sources may submit to get new data) as the batch description . The batch runner constructs the next streaming micro-batch (when the isCurrentBatchConstructed internal flag is off). The batch runner records trigger offsets (with the committed and available offsets). The batch runner updates the current StreamingQueryStatus with the isNewDataAvailable for isDataAvailable property. With the isCurrentBatchConstructed flag enabled, the batch runner updates the status message to one of the following (per isNewDataAvailable ) and runs the streaming micro-batch . Processing new data No new data but cleaning up state With the isCurrentBatchConstructed flag disabled ( false ), the batch runner simply updates the status message to the following: Waiting for data to arrive [[runActivatedStream-triggerExecution-finishTrigger]] The batch runner finalizes query progress for the trigger (with a flag that indicates whether the current batch had new data). With the isCurrentBatchConstructed flag enabled ( true ), the batch runner increments the currentBatchId and turns the isCurrentBatchConstructed flag off ( false ). With the isCurrentBatchConstructed flag disabled ( false ), the batch runner simply sleeps (as long as configured using the spark.sql.streaming.pollingDelay configuration property). In the end, the batch runner updates the status message to the following status and returns whether the MicroBatchExecution is active or not. Waiting for next trigger Populating Start Offsets From Checkpoint (Resuming from Checkpoint) \u00b6 populateStartOffsets ( sparkSessionToRunBatches : SparkSession ): Unit populateStartOffsets requests the Offset Write-Ahead Log for the latest committed batch id with metadata (i.e. OffsetSeq ). Note The batch id could not be available in the write-ahead log when a streaming query started with a new log or no batch was persisted ( added ) to the log before. populateStartOffsets branches off based on whether the latest committed batch was available or not . populateStartOffsets is used when MicroBatchExecution is requested to run an activated streaming query ( before the first \"zero\" micro-batch ). Latest Committed Batch Available \u00b6 When the latest committed batch id with the metadata was available in the Offset Write-Ahead Log , populateStartOffsets (re)initializes the internal state as follows: Sets the current batch ID to the latest committed batch ID found Turns the isCurrentBatchConstructed internal flag on ( true ) Sets the available offsets to the offsets (from the metadata) When the latest batch ID found is greater than 0 , populateStartOffsets requests the Offset Write-Ahead Log for the second latest batch ID with metadata or throws an IllegalStateException if not found. batch [latestBatchId - 1] doesn't exist populateStartOffsets sets the committed offsets to the second latest committed offsets. populateStartOffsets updates the offset metadata. CAUTION: FIXME Describe me populateStartOffsets requests the Offset Commit Log for the latest committed batch id with metadata . CAUTION: FIXME Describe me When the latest committed batch id with metadata was found which is exactly the latest batch ID (found in the Offset Commit Log ), populateStartOffsets ...FIXME When the latest committed batch id with metadata was found, but it is not exactly the second latest batch ID (found in the Offset Commit Log ), populateStartOffsets prints out the following WARN message to the logs: Batch completion log latest batch id is [latestCommittedBatchId], which is not trailing batchid [latestBatchId] by one When no commit log present in the Offset Commit Log , populateStartOffsets prints out the following INFO message to the logs: no commit log present In the end, populateStartOffsets prints out the following DEBUG message to the logs: Resuming at batch [currentBatchId] with committed offsets [committedOffsets] and available offsets [availableOffsets] No Latest Committed Batch \u00b6 When the latest committed batch id with the metadata could not be found in the Offset Write-Ahead Log , it is assumed that the streaming query is started for the very first time (or the checkpoint location has changed). populateStartOffsets prints out the following INFO message to the logs: Starting new streaming query. [[populateStartOffsets-currentBatchId-0]] populateStartOffsets sets the current batch ID to 0 and creates a new WatermarkTracker . Constructing Or Skipping Next Streaming Micro-Batch \u00b6 constructNextBatch ( noDataBatchesEnabled : Boolean ): Boolean constructNextBatch is used when MicroBatchExecution is requested to run the activated streaming query . Note constructNextBatch is only executed when the isCurrentBatchConstructed internal flag is enabled ( true ). constructNextBatch performs the following steps: Requesting the latest offsets from every streaming source (of the streaming query) Updating availableOffsets StreamProgress with the latest available offsets Updating batch metadata with the current event-time watermark and batch timestamp Checking whether to construct the next micro-batch or not (skip it) In the end, constructNextBatch returns whether the next streaming micro-batch was constructed or skipped . Requesting Latest Offsets from Streaming Sources (getOffset, setOffsetRange and getEndOffset Phases) \u00b6 constructNextBatch firstly requests every streaming source for the latest offsets. Note constructNextBatch checks out the latest offset in every streaming data source sequentially, i.e. one data source at a time. For every streaming source (Data Source API V1), constructNextBatch updates the status message to the following: Getting offsets from [source] getOffset Phase \u00b6 In getOffset time-tracking section , constructNextBatch requests the Source for the latest offset . For every MicroBatchReader (Data Source API V2), constructNextBatch updates the status message to the following: Getting offsets from [source] setOffsetRange Phase \u00b6 In setOffsetRange time-tracking section , constructNextBatch finds the available offsets of the source (in the available offset internal registry) and, if found, requests the MicroBatchReader to deserialize the offset (from JSON format ). constructNextBatch requests the MicroBatchReader to set the desired offset range . getEndOffset Phase \u00b6 In getEndOffset time-tracking section , constructNextBatch requests the MicroBatchReader for the end offset . Updating availableOffsets StreamProgress with Latest Available Offsets \u00b6 constructNextBatch updates the availableOffsets StreamProgress with the latest reported offsets. Updating Batch Metadata with Current Event-Time Watermark and Batch Timestamp \u00b6 constructNextBatch updates the batch metadata with the current event-time watermark (from the WatermarkTracker ) and the batch timestamp. Checking Whether to Construct Next Micro-Batch or Not (Skip It) \u00b6 constructNextBatch checks whether or not the next streaming micro-batch should be constructed ( lastExecutionRequiresAnotherBatch ). constructNextBatch uses the last IncrementalExecution if the last execution requires another micro-batch (using the batch metadata ) and the given noDataBatchesEnabled flag is enabled ( true ). constructNextBatch also checks out whether new data is available (based on available and committed offsets) . Note shouldConstructNextBatch local flag is enabled ( true ) when there is new data available (based on offsets) or the last execution requires another micro-batch (and the given noDataBatchesEnabled flag is enabled). constructNextBatch prints out the following TRACE message to the logs: noDataBatchesEnabled = [noDataBatchesEnabled], lastExecutionRequiresAnotherBatch = [lastExecutionRequiresAnotherBatch], isNewDataAvailable = [isNewDataAvailable], shouldConstructNextBatch = [shouldConstructNextBatch] constructNextBatch branches off per whether to constructs or skip the next batch (per shouldConstructNextBatch flag in the above TRACE message). Constructing Next Micro-Batch \u00b6 With the shouldConstructNextBatch flag enabled ( true ), constructNextBatch updates the status message to the following: Writing offsets to log [[constructNextBatch-walCommit]] In walCommit time-tracking section , constructNextBatch requests the availableOffsets StreamProgress to convert to OffsetSeq (with the BaseStreamingSources and the current batch metadata (event-time watermark and timestamp) ) that is in turn added to the write-ahead log for the current batch ID . constructNextBatch prints out the following INFO message to the logs: Committed offsets for batch [currentBatchId]. Metadata [offsetSeqMetadata] Fixme ( if (currentBatchId != 0) ... ) Fixme ( if (minLogEntriesToMaintain < currentBatchId) ... ) constructNextBatch turns the noNewData internal flag off ( false ). In case of a failure while adding the available offsets to the write-ahead log , constructNextBatch throws an AssertionError : Concurrent update to the log. Multiple streaming jobs detected for [currentBatchId] Skipping Next Micro-Batch \u00b6 With the shouldConstructNextBatch flag disabled ( false ), constructNextBatch turns the noNewData flag on ( true ) and wakes up ( notifies ) all threads waiting for the awaitProgressLockCondition lock. Running Single Streaming Micro-Batch \u00b6 runBatch ( sparkSessionToRunBatch : SparkSession ): Unit runBatch prints out the following DEBUG message to the logs (with the current batch ID ): Running batch [currentBatchId] runBatch then performs the following steps (aka phases ): getBatch Phase -- Creating Logical Query Plans For Unprocessed Data From Sources and MicroBatchReaders Transforming Logical Plan to Include Sources and MicroBatchReaders with New Data Transforming CurrentTimestamp and CurrentDate Expressions (Per Batch Metadata) Adapting Transformed Logical Plan to Sink with StreamWriteSupport Setting Local Properties queryPlanning Phase -- Creating and Preparing IncrementalExecution for Execution nextBatch Phase -- Creating DataFrame (with IncrementalExecution for New Data) addBatch Phase -- Adding DataFrame With New Data to Sink Updating Watermark and Committing Offsets to Offset Commit Log In the end, runBatch prints out the following DEBUG message to the logs (with the current batch ID ): Completed batch [currentBatchId] Note runBatch is used exclusively when MicroBatchExecution is requested to run an activated streaming query (and there is new data to process). getBatch Phase -- Creating Logical Query Plans For Unprocessed Data From Sources and MicroBatchReaders \u00b6 In getBatch time-tracking section , runBatch goes over the available offsets and processes every Source and MicroBatchReader (associated with the available offsets) to create logical query plans ( newData ) for data processing (per offset ranges). Note runBatch requests sources and readers for data per offset range sequentially, one by one. getBatch Phase and Sources \u00b6 For a Source (with the available offset s different from the committedOffsets registry), runBatch does the following: Requests the committedOffsets for the committed offsets for the Source (if available) Requests the Source for a dataframe for the offset range (the current and available offsets) runBatch prints out the following DEBUG message to the logs. Retrieving data from [source]: [current] -> [available] In the end, runBatch returns the Source and the logical plan of the streaming dataset (for the offset range). In case the Source returns a dataframe that is not streaming, runBatch throws an AssertionError : DataFrame returned by getBatch from [source] did not have isStreaming=true\\n[logicalQueryPlan] getBatch Phase and MicroBatchReaders \u00b6 For a MicroBatchReader (with the available offset s different from the committedOffsets registry), runBatch does the following: Requests the committedOffsets for the committed offsets for the MicroBatchReader (if available) Requests the MicroBatchReader to deserialize the committed offsets (if available) Requests the MicroBatchReader to deserialize the available offsets (only for SerializedOffset s) Requests the MicroBatchReader to set the offset range (the current and available offsets) runBatch prints out the following DEBUG message to the logs. Retrieving data from [reader]: [current] -> [availableV2] runBatch looks up the DataSourceV2 and the options for the MicroBatchReader (in the readerToDataSourceMap internal registry). In the end, runBatch requests the MicroBatchReader for the read schema and creates a StreamingDataSourceV2Relation logical operator (with the read schema, the DataSourceV2 , options, and the MicroBatchReader ). Transforming Logical Plan to Include Sources and MicroBatchReaders with New Data \u00b6 runBatch transforms the analyzed logical plan to include Sources and MicroBatchReaders with new data ( newBatchesPlan with logical plans to process data that has arrived since the last batch). For every StreamingExecutionRelation , runBatch tries to find the corresponding logical plan for processing new data. If the logical plan is found, runBatch makes the plan a child operator of Project (with Aliases ) logical operator and replaces the StreamingExecutionRelation . Otherwise, if not found, runBatch simply creates an empty streaming LocalRelation (for scanning data from an empty local collection). In case the number of columns in dataframes with new data and StreamingExecutionRelation 's do not match, runBatch throws an AssertionError : Invalid batch: [output] != [dataPlan.output] Transforming CurrentTimestamp and CurrentDate Expressions (Per Batch Metadata) \u00b6 runBatch replaces all CurrentTimestamp and CurrentDate expressions in the transformed logical plan (with new data) with the current batch timestamp (based on the batch metadata ). Note CurrentTimestamp and CurrentDate expressions correspond to current_timestamp and current_date standard function, respectively. Adapting Transformed Logical Plan to Sink with StreamWriteSupport \u00b6 runBatch ...FIXME For a Sink (Data Source API V1), runBatch changes nothing. For any other BaseStreamingSink type, runBatch simply throws an IllegalArgumentException : unknown sink type for [sink] Setting Local Properties \u00b6 runBatch sets the local properties. Local Property Value streaming.sql.batchId currentBatchId __is_continuous_processing false queryPlanning Phase -- Creating and Preparing IncrementalExecution for Execution \u00b6 In queryPlanning time-tracking section , runBatch creates a new IncrementalExecution with the following: Transformed logical plan Output mode state checkpoint directory Run ID Batch ID Batch Metadata (Event-Time Watermark and Timestamp) In the end (of the queryPlanning phase), runBatch requests the IncrementalExecution to prepare the transformed logical plan for execution (i.e. execute the executedPlan query execution phase). Tip Read up on the executedPlan query execution phase in https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-QueryExecution.html[The Internals of Spark SQL]. nextBatch Phase \u2014 Creating DataFrame (with IncrementalExecution for New Data) \u00b6 runBatch creates a new DataFrame with the new IncrementalExecution . The DataFrame represents the result of executing the current micro-batch of the streaming query. addBatch Phase \u2014 Adding DataFrame With New Data to Sink \u00b6 In addBatch time-tracking section , runBatch adds the DataFrame with new data to the BaseStreamingSink . For a Sink (Data Source API V1), runBatch simply requests the Sink to add the DataFrame (with the batch ID ). runBatch uses SQLExecution.withNewExecutionId to execute and track all the Spark jobs under one execution id (so it is reported as one single multi-job execution, e.g. in web UI). Note SQLExecution.withNewExecutionId posts a SparkListenerSQLExecutionStart event before execution and a SparkListenerSQLExecutionEnd event right afterwards. Tip Register SparkListener to get notified about the SQL execution events ( SparkListenerSQLExecutionStart and SparkListenerSQLExecutionEnd ). Updating Watermark and Committing Offsets to Offset Commit Log \u00b6 runBatch requests the WatermarkTracker to update event-time watermark (with the executedPlan of the IncrementalExecution ). runBatch requests the Offset Commit Log to persisting metadata of the streaming micro-batch (with the current batch ID and event-time watermark of the WatermarkTracker ). In the end, runBatch adds the available offsets to the committed offsets (and updates the offset s of every source with new data in the current micro-batch). Stopping Stream Processing (Execution of Streaming Query) \u00b6 stop (): Unit stop sets the state to TERMINATED . When the stream execution thread is alive, stop requests the current SparkContext to cancelJobGroup identified by the runId and waits for this thread to die. Just to make sure that there are no more streaming jobs, stop requests the current SparkContext to cancelJobGroup identified by the runId again. In the end, stop prints out the following INFO message to the logs: Query [prettyIdString] was stopped stop is part of the StreamingQuery abstraction. Checking Whether New Data Is Available \u00b6 isNewDataAvailable : Boolean isNewDataAvailable returns whether or not there are streaming sources (in the available offsets ) for which committed offsets are different from the available offsets or not available (committed) at all. isNewDataAvailable is true when there is at least one such streaming source. isNewDataAvailable is used when: MicroBatchExecution is requested to run an activated streaming query and construct the next streaming micro-batch Analyzed Logical Plan \u00b6 logicalPlan : LogicalPlan logicalPlan is part of the StreamExecution abstraction. logicalPlan resolves ( replaces ) StreamingRelation , StreamingRelationV2 logical operators to StreamingExecutionRelation logical operators. logicalPlan uses the transformed logical plan to set the uniqueSources and sources internal registries to be the BaseStreamingSources of all the StreamingExecutionRelations unique and not, respectively. Lazy Value logicalPlan is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards. Internally, logicalPlan transforms the analyzed logical plan . For every StreamingRelation logical operator, logicalPlan tries to replace it with the StreamingExecutionRelation that was used earlier for the same StreamingRelation (if used multiple times in the plan) or creates a new one. While creating a new StreamingExecutionRelation , logicalPlan requests the DataSource to create a streaming Source with the metadata path as sources/uniqueID directory in the checkpoint root directory . logicalPlan prints out the following INFO message to the logs: Using Source [source] from DataSourceV1 named '[sourceName]' [dataSourceV1] For every StreamingRelationV2 logical operator with a MicroBatchStream data source (which is not on the list of spark.sql.streaming.disabledV2MicroBatchReaders ), logicalPlan tries to replace it with the StreamingExecutionRelation that was used earlier for the same StreamingRelationV2 (if used multiple times in the plan) or creates a new one. While creating a new StreamingExecutionRelation , logicalPlan requests the MicroBatchStream to create a MicroBatchStream with the metadata path as sources/uniqueID directory in the checkpoint root directory . logicalPlan prints out the following INFO message to the logs: Using MicroBatchReader [reader] from DataSourceV2 named '[sourceName]' [dataSourceV2] For every other StreamingRelationV2 leaf logical operator, logicalPlan tries to replace it with the StreamingExecutionRelation that was used earlier for the same StreamingRelationV2 (if used multiple times in the plan) or creates a new one. While creating a new StreamingExecutionRelation , logicalPlan requests the StreamingRelation for the underlying DataSource that is in turn requested to create a streaming Source with the metadata path as sources/uniqueID directory in the checkpoint root directory . logicalPlan prints out the following INFO message to the logs: Using Source [source] from DataSourceV2 named '[sourceName]' [dataSourceV2] logicalPlan requests the transformed analyzed logical plan for all StreamingExecutionRelations that are then requested for BaseStreamingSources , and saves them as the sources internal registry. In the end, logicalPlan sets the uniqueSources internal registry to be the unique BaseStreamingSources above. logicalPlan throws an AssertionError when not executed on the stream execution thread . logicalPlan must be initialized in QueryExecutionThread but the current thread was [currentThread] streaming.sql.batchId Local Property \u00b6 MicroBatchExecution defines streaming.sql.batchId as the name of the local property to be the current batch or epoch IDs (that Spark tasks can use at execution time). streaming.sql.batchId is used when: MicroBatchExecution is requested to run a single streaming micro-batch (and sets the property to be the current batch ID) DataWritingSparkTask is requested to run (and needs an epoch ID) WatermarkTracker \u00b6 WatermarkTracker that is created when MicroBatchExecution is requested to populate start offsets (when requested to run an activated streaming query ) isCurrentBatchConstructed Flag \u00b6 isCurrentBatchConstructed : Boolean MicroBatchExecution uses isCurrentBatchConstructed internal flag to control whether or not to run a streaming micro-batch . Default: false When false , changed to whatever constructing the next streaming micro-batch gives back when running activated streaming query Disabled ( false ) after running a streaming micro-batch (when enabled after constructing the next streaming micro-batch ) Enabled ( true ) when populating start offsets (when running an activated streaming query ) and re-starting a streaming query from a checkpoint (using the Offset Write-Ahead Log ) Disabled ( false ) when populating start offsets (when running an activated streaming query ) and re-starting a streaming query from a checkpoint when the latest offset checkpointed (written) to the offset write-ahead log has been successfully processed and committed to the Offset Commit Log Demo \u00b6 import org.apache.spark.sql.streaming.Trigger val query = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") // <-- not a StreamWriteSupport sink .option(\"truncate\", false) .trigger(Trigger.Once) // <-- Gives MicroBatchExecution .queryName(\"rate2console\") .start // The following gives access to the internals // And to MicroBatchExecution import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val engine = query.asInstanceOf[StreamingQueryWrapper].streamingQuery import org.apache.spark.sql.execution.streaming.StreamExecution assert(engine.isInstanceOf[StreamExecution]) import org.apache.spark.sql.execution.streaming.MicroBatchExecution val microBatchEngine = engine.asInstanceOf[MicroBatchExecution] assert(microBatchEngine.trigger == Trigger.Once) Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.execution.streaming.MicroBatchExecution logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.MicroBatchExecution=ALL Refer to Logging .","title":"MicroBatchExecution"},{"location":"micro-batch-execution/MicroBatchExecution/#microbatchexecution","text":"MicroBatchExecution is a stream execution engine for Micro-Batch Stream Processing .","title":"MicroBatchExecution"},{"location":"micro-batch-execution/MicroBatchExecution/#creating-instance","text":"MicroBatchExecution takes the following to be created: SparkSession ( Spark SQL ) Name of the streaming query Path of the Checkpoint Directory Analyzed logical query plan of the streaming query ( Spark SQL ) Table Sink ( Spark SQL ) Trigger Clock OutputMode Extra Options ( Map[String, String] ) deleteCheckpointOnStop flag to control whether to delete the checkpoint directory on stop MicroBatchExecution is created when: StreamingQueryManager is requested to create a streaming query (when DataStreamWriter is requested to start an execution of the streaming query ) with the following: All sink s All trigger s but ContinuousTrigger Once created, MicroBatchExecution is requested to run an activated streaming query .","title":"Creating Instance"},{"location":"micro-batch-execution/MicroBatchExecution/#initializing-query-progress-for-new-trigger","text":"startTrigger (): Unit startTrigger is part of the ProgressReporter abstraction. startTrigger ...FIXME","title":" Initializing Query Progress for New Trigger"},{"location":"micro-batch-execution/MicroBatchExecution/#streaming-sources-registry","text":"sources : Seq [ SparkDataStream ] MicroBatchExecution uses...FIXME Streaming sources and readers (of the StreamingExecutionRelations of the analyzed logical query plan of the streaming query) Default: (empty) sources is part of the ProgressReporter abstraction. Initialized when MicroBatchExecution is requested for the transformed logical query plan Used when: Populating start offsets (for the available and committed offsets) Constructing or skipping next streaming micro-batch (and persisting offsets to write-ahead log)","title":" Streaming Sources Registry"},{"location":"micro-batch-execution/MicroBatchExecution/#triggerexecutor","text":"triggerExecutor : TriggerExecutor MicroBatchExecution uses a TriggerExecutor that is how micro-batches are executed at regular intervals. triggerExecutor is initialized based on the given Trigger (given when creating the MicroBatchExecution ): ProcessingTimeExecutor for Trigger.ProcessingTime OneTimeExecutor for OneTimeTrigger (aka Trigger.Once trigger) triggerExecutor throws an IllegalStateException when the Trigger is not one of the built-in implementations . Unknown type of trigger: [trigger] triggerExecutor is used when: StreamExecution is requested to run an activated streaming query (at regular intervals)","title":" TriggerExecutor"},{"location":"micro-batch-execution/MicroBatchExecution/#running-activated-streaming-query","text":"runActivatedStream ( sparkSessionForStream : SparkSession ): Unit runActivatedStream simply requests the TriggerExecutor to execute micro-batches using the batch runner (until MicroBatchExecution is terminated due to a query stop or a failure). runActivatedStream is part of StreamExecution abstraction.","title":" Running Activated Streaming Query"},{"location":"micro-batch-execution/MicroBatchExecution/#triggerexecutors-batch-runner","text":"The batch runner (of the TriggerExecutor ) is executed as long as the MicroBatchExecution is active . Note trigger and batch are considered equivalent and used interchangeably. The batch runner initializes query progress for the new trigger (aka startTrigger ). The batch runner starts triggerExecution execution phase that is made up of the following steps: Populating start offsets from checkpoint before the first \"zero\" batch (at every start or restart) Constructing or skipping the next streaming micro-batch Running the streaming micro-batch At the start or restart ( resume ) of a streaming query (when the current batch ID is uninitialized and -1 ), the batch runner populates start offsets from checkpoint and then prints out the following INFO message to the logs (using the committedOffsets internal registry): Stream started from [committedOffsets] The batch runner sets the human-readable description for any Spark job submitted (that streaming sources may submit to get new data) as the batch description . The batch runner constructs the next streaming micro-batch (when the isCurrentBatchConstructed internal flag is off). The batch runner records trigger offsets (with the committed and available offsets). The batch runner updates the current StreamingQueryStatus with the isNewDataAvailable for isDataAvailable property. With the isCurrentBatchConstructed flag enabled, the batch runner updates the status message to one of the following (per isNewDataAvailable ) and runs the streaming micro-batch . Processing new data No new data but cleaning up state With the isCurrentBatchConstructed flag disabled ( false ), the batch runner simply updates the status message to the following: Waiting for data to arrive [[runActivatedStream-triggerExecution-finishTrigger]] The batch runner finalizes query progress for the trigger (with a flag that indicates whether the current batch had new data). With the isCurrentBatchConstructed flag enabled ( true ), the batch runner increments the currentBatchId and turns the isCurrentBatchConstructed flag off ( false ). With the isCurrentBatchConstructed flag disabled ( false ), the batch runner simply sleeps (as long as configured using the spark.sql.streaming.pollingDelay configuration property). In the end, the batch runner updates the status message to the following status and returns whether the MicroBatchExecution is active or not. Waiting for next trigger","title":" TriggerExecutor's Batch Runner"},{"location":"micro-batch-execution/MicroBatchExecution/#populating-start-offsets-from-checkpoint-resuming-from-checkpoint","text":"populateStartOffsets ( sparkSessionToRunBatches : SparkSession ): Unit populateStartOffsets requests the Offset Write-Ahead Log for the latest committed batch id with metadata (i.e. OffsetSeq ). Note The batch id could not be available in the write-ahead log when a streaming query started with a new log or no batch was persisted ( added ) to the log before. populateStartOffsets branches off based on whether the latest committed batch was available or not . populateStartOffsets is used when MicroBatchExecution is requested to run an activated streaming query ( before the first \"zero\" micro-batch ).","title":" Populating Start Offsets From Checkpoint (Resuming from Checkpoint)"},{"location":"micro-batch-execution/MicroBatchExecution/#latest-committed-batch-available","text":"When the latest committed batch id with the metadata was available in the Offset Write-Ahead Log , populateStartOffsets (re)initializes the internal state as follows: Sets the current batch ID to the latest committed batch ID found Turns the isCurrentBatchConstructed internal flag on ( true ) Sets the available offsets to the offsets (from the metadata) When the latest batch ID found is greater than 0 , populateStartOffsets requests the Offset Write-Ahead Log for the second latest batch ID with metadata or throws an IllegalStateException if not found. batch [latestBatchId - 1] doesn't exist populateStartOffsets sets the committed offsets to the second latest committed offsets. populateStartOffsets updates the offset metadata. CAUTION: FIXME Describe me populateStartOffsets requests the Offset Commit Log for the latest committed batch id with metadata . CAUTION: FIXME Describe me When the latest committed batch id with metadata was found which is exactly the latest batch ID (found in the Offset Commit Log ), populateStartOffsets ...FIXME When the latest committed batch id with metadata was found, but it is not exactly the second latest batch ID (found in the Offset Commit Log ), populateStartOffsets prints out the following WARN message to the logs: Batch completion log latest batch id is [latestCommittedBatchId], which is not trailing batchid [latestBatchId] by one When no commit log present in the Offset Commit Log , populateStartOffsets prints out the following INFO message to the logs: no commit log present In the end, populateStartOffsets prints out the following DEBUG message to the logs: Resuming at batch [currentBatchId] with committed offsets [committedOffsets] and available offsets [availableOffsets]","title":" Latest Committed Batch Available"},{"location":"micro-batch-execution/MicroBatchExecution/#no-latest-committed-batch","text":"When the latest committed batch id with the metadata could not be found in the Offset Write-Ahead Log , it is assumed that the streaming query is started for the very first time (or the checkpoint location has changed). populateStartOffsets prints out the following INFO message to the logs: Starting new streaming query. [[populateStartOffsets-currentBatchId-0]] populateStartOffsets sets the current batch ID to 0 and creates a new WatermarkTracker .","title":" No Latest Committed Batch"},{"location":"micro-batch-execution/MicroBatchExecution/#constructing-or-skipping-next-streaming-micro-batch","text":"constructNextBatch ( noDataBatchesEnabled : Boolean ): Boolean constructNextBatch is used when MicroBatchExecution is requested to run the activated streaming query . Note constructNextBatch is only executed when the isCurrentBatchConstructed internal flag is enabled ( true ). constructNextBatch performs the following steps: Requesting the latest offsets from every streaming source (of the streaming query) Updating availableOffsets StreamProgress with the latest available offsets Updating batch metadata with the current event-time watermark and batch timestamp Checking whether to construct the next micro-batch or not (skip it) In the end, constructNextBatch returns whether the next streaming micro-batch was constructed or skipped .","title":" Constructing Or Skipping Next Streaming Micro-Batch"},{"location":"micro-batch-execution/MicroBatchExecution/#requesting-latest-offsets-from-streaming-sources-getoffset-setoffsetrange-and-getendoffset-phases","text":"constructNextBatch firstly requests every streaming source for the latest offsets. Note constructNextBatch checks out the latest offset in every streaming data source sequentially, i.e. one data source at a time. For every streaming source (Data Source API V1), constructNextBatch updates the status message to the following: Getting offsets from [source]","title":" Requesting Latest Offsets from Streaming Sources (getOffset, setOffsetRange and getEndOffset Phases)"},{"location":"micro-batch-execution/MicroBatchExecution/#getoffset-phase","text":"In getOffset time-tracking section , constructNextBatch requests the Source for the latest offset . For every MicroBatchReader (Data Source API V2), constructNextBatch updates the status message to the following: Getting offsets from [source]","title":" getOffset Phase"},{"location":"micro-batch-execution/MicroBatchExecution/#setoffsetrange-phase","text":"In setOffsetRange time-tracking section , constructNextBatch finds the available offsets of the source (in the available offset internal registry) and, if found, requests the MicroBatchReader to deserialize the offset (from JSON format ). constructNextBatch requests the MicroBatchReader to set the desired offset range .","title":" setOffsetRange Phase"},{"location":"micro-batch-execution/MicroBatchExecution/#getendoffset-phase","text":"In getEndOffset time-tracking section , constructNextBatch requests the MicroBatchReader for the end offset .","title":" getEndOffset Phase"},{"location":"micro-batch-execution/MicroBatchExecution/#updating-availableoffsets-streamprogress-with-latest-available-offsets","text":"constructNextBatch updates the availableOffsets StreamProgress with the latest reported offsets.","title":" Updating availableOffsets StreamProgress with Latest Available Offsets"},{"location":"micro-batch-execution/MicroBatchExecution/#updating-batch-metadata-with-current-event-time-watermark-and-batch-timestamp","text":"constructNextBatch updates the batch metadata with the current event-time watermark (from the WatermarkTracker ) and the batch timestamp.","title":" Updating Batch Metadata with Current Event-Time Watermark and Batch Timestamp"},{"location":"micro-batch-execution/MicroBatchExecution/#checking-whether-to-construct-next-micro-batch-or-not-skip-it","text":"constructNextBatch checks whether or not the next streaming micro-batch should be constructed ( lastExecutionRequiresAnotherBatch ). constructNextBatch uses the last IncrementalExecution if the last execution requires another micro-batch (using the batch metadata ) and the given noDataBatchesEnabled flag is enabled ( true ). constructNextBatch also checks out whether new data is available (based on available and committed offsets) . Note shouldConstructNextBatch local flag is enabled ( true ) when there is new data available (based on offsets) or the last execution requires another micro-batch (and the given noDataBatchesEnabled flag is enabled). constructNextBatch prints out the following TRACE message to the logs: noDataBatchesEnabled = [noDataBatchesEnabled], lastExecutionRequiresAnotherBatch = [lastExecutionRequiresAnotherBatch], isNewDataAvailable = [isNewDataAvailable], shouldConstructNextBatch = [shouldConstructNextBatch] constructNextBatch branches off per whether to constructs or skip the next batch (per shouldConstructNextBatch flag in the above TRACE message).","title":" Checking Whether to Construct Next Micro-Batch or Not (Skip It)"},{"location":"micro-batch-execution/MicroBatchExecution/#constructing-next-micro-batch","text":"With the shouldConstructNextBatch flag enabled ( true ), constructNextBatch updates the status message to the following: Writing offsets to log [[constructNextBatch-walCommit]] In walCommit time-tracking section , constructNextBatch requests the availableOffsets StreamProgress to convert to OffsetSeq (with the BaseStreamingSources and the current batch metadata (event-time watermark and timestamp) ) that is in turn added to the write-ahead log for the current batch ID . constructNextBatch prints out the following INFO message to the logs: Committed offsets for batch [currentBatchId]. Metadata [offsetSeqMetadata] Fixme ( if (currentBatchId != 0) ... ) Fixme ( if (minLogEntriesToMaintain < currentBatchId) ... ) constructNextBatch turns the noNewData internal flag off ( false ). In case of a failure while adding the available offsets to the write-ahead log , constructNextBatch throws an AssertionError : Concurrent update to the log. Multiple streaming jobs detected for [currentBatchId]","title":" Constructing Next Micro-Batch"},{"location":"micro-batch-execution/MicroBatchExecution/#skipping-next-micro-batch","text":"With the shouldConstructNextBatch flag disabled ( false ), constructNextBatch turns the noNewData flag on ( true ) and wakes up ( notifies ) all threads waiting for the awaitProgressLockCondition lock.","title":" Skipping Next Micro-Batch"},{"location":"micro-batch-execution/MicroBatchExecution/#running-single-streaming-micro-batch","text":"runBatch ( sparkSessionToRunBatch : SparkSession ): Unit runBatch prints out the following DEBUG message to the logs (with the current batch ID ): Running batch [currentBatchId] runBatch then performs the following steps (aka phases ): getBatch Phase -- Creating Logical Query Plans For Unprocessed Data From Sources and MicroBatchReaders Transforming Logical Plan to Include Sources and MicroBatchReaders with New Data Transforming CurrentTimestamp and CurrentDate Expressions (Per Batch Metadata) Adapting Transformed Logical Plan to Sink with StreamWriteSupport Setting Local Properties queryPlanning Phase -- Creating and Preparing IncrementalExecution for Execution nextBatch Phase -- Creating DataFrame (with IncrementalExecution for New Data) addBatch Phase -- Adding DataFrame With New Data to Sink Updating Watermark and Committing Offsets to Offset Commit Log In the end, runBatch prints out the following DEBUG message to the logs (with the current batch ID ): Completed batch [currentBatchId] Note runBatch is used exclusively when MicroBatchExecution is requested to run an activated streaming query (and there is new data to process).","title":" Running Single Streaming Micro-Batch"},{"location":"micro-batch-execution/MicroBatchExecution/#getbatch-phase-creating-logical-query-plans-for-unprocessed-data-from-sources-and-microbatchreaders","text":"In getBatch time-tracking section , runBatch goes over the available offsets and processes every Source and MicroBatchReader (associated with the available offsets) to create logical query plans ( newData ) for data processing (per offset ranges). Note runBatch requests sources and readers for data per offset range sequentially, one by one.","title":" getBatch Phase -- Creating Logical Query Plans For Unprocessed Data From Sources and MicroBatchReaders"},{"location":"micro-batch-execution/MicroBatchExecution/#getbatch-phase-and-sources","text":"For a Source (with the available offset s different from the committedOffsets registry), runBatch does the following: Requests the committedOffsets for the committed offsets for the Source (if available) Requests the Source for a dataframe for the offset range (the current and available offsets) runBatch prints out the following DEBUG message to the logs. Retrieving data from [source]: [current] -> [available] In the end, runBatch returns the Source and the logical plan of the streaming dataset (for the offset range). In case the Source returns a dataframe that is not streaming, runBatch throws an AssertionError : DataFrame returned by getBatch from [source] did not have isStreaming=true\\n[logicalQueryPlan]","title":" getBatch Phase and Sources"},{"location":"micro-batch-execution/MicroBatchExecution/#getbatch-phase-and-microbatchreaders","text":"For a MicroBatchReader (with the available offset s different from the committedOffsets registry), runBatch does the following: Requests the committedOffsets for the committed offsets for the MicroBatchReader (if available) Requests the MicroBatchReader to deserialize the committed offsets (if available) Requests the MicroBatchReader to deserialize the available offsets (only for SerializedOffset s) Requests the MicroBatchReader to set the offset range (the current and available offsets) runBatch prints out the following DEBUG message to the logs. Retrieving data from [reader]: [current] -> [availableV2] runBatch looks up the DataSourceV2 and the options for the MicroBatchReader (in the readerToDataSourceMap internal registry). In the end, runBatch requests the MicroBatchReader for the read schema and creates a StreamingDataSourceV2Relation logical operator (with the read schema, the DataSourceV2 , options, and the MicroBatchReader ).","title":" getBatch Phase and MicroBatchReaders"},{"location":"micro-batch-execution/MicroBatchExecution/#transforming-logical-plan-to-include-sources-and-microbatchreaders-with-new-data","text":"runBatch transforms the analyzed logical plan to include Sources and MicroBatchReaders with new data ( newBatchesPlan with logical plans to process data that has arrived since the last batch). For every StreamingExecutionRelation , runBatch tries to find the corresponding logical plan for processing new data. If the logical plan is found, runBatch makes the plan a child operator of Project (with Aliases ) logical operator and replaces the StreamingExecutionRelation . Otherwise, if not found, runBatch simply creates an empty streaming LocalRelation (for scanning data from an empty local collection). In case the number of columns in dataframes with new data and StreamingExecutionRelation 's do not match, runBatch throws an AssertionError : Invalid batch: [output] != [dataPlan.output]","title":" Transforming Logical Plan to Include Sources and MicroBatchReaders with New Data"},{"location":"micro-batch-execution/MicroBatchExecution/#transforming-currenttimestamp-and-currentdate-expressions-per-batch-metadata","text":"runBatch replaces all CurrentTimestamp and CurrentDate expressions in the transformed logical plan (with new data) with the current batch timestamp (based on the batch metadata ). Note CurrentTimestamp and CurrentDate expressions correspond to current_timestamp and current_date standard function, respectively.","title":" Transforming CurrentTimestamp and CurrentDate Expressions (Per Batch Metadata)"},{"location":"micro-batch-execution/MicroBatchExecution/#adapting-transformed-logical-plan-to-sink-with-streamwritesupport","text":"runBatch ...FIXME For a Sink (Data Source API V1), runBatch changes nothing. For any other BaseStreamingSink type, runBatch simply throws an IllegalArgumentException : unknown sink type for [sink]","title":" Adapting Transformed Logical Plan to Sink with StreamWriteSupport"},{"location":"micro-batch-execution/MicroBatchExecution/#setting-local-properties","text":"runBatch sets the local properties. Local Property Value streaming.sql.batchId currentBatchId __is_continuous_processing false","title":" Setting Local Properties"},{"location":"micro-batch-execution/MicroBatchExecution/#queryplanning-phase-creating-and-preparing-incrementalexecution-for-execution","text":"In queryPlanning time-tracking section , runBatch creates a new IncrementalExecution with the following: Transformed logical plan Output mode state checkpoint directory Run ID Batch ID Batch Metadata (Event-Time Watermark and Timestamp) In the end (of the queryPlanning phase), runBatch requests the IncrementalExecution to prepare the transformed logical plan for execution (i.e. execute the executedPlan query execution phase). Tip Read up on the executedPlan query execution phase in https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-QueryExecution.html[The Internals of Spark SQL].","title":" queryPlanning Phase -- Creating and Preparing IncrementalExecution for Execution"},{"location":"micro-batch-execution/MicroBatchExecution/#nextbatch-phase-creating-dataframe-with-incrementalexecution-for-new-data","text":"runBatch creates a new DataFrame with the new IncrementalExecution . The DataFrame represents the result of executing the current micro-batch of the streaming query.","title":" nextBatch Phase &mdash; Creating DataFrame (with IncrementalExecution for New Data)"},{"location":"micro-batch-execution/MicroBatchExecution/#addbatch-phase-adding-dataframe-with-new-data-to-sink","text":"In addBatch time-tracking section , runBatch adds the DataFrame with new data to the BaseStreamingSink . For a Sink (Data Source API V1), runBatch simply requests the Sink to add the DataFrame (with the batch ID ). runBatch uses SQLExecution.withNewExecutionId to execute and track all the Spark jobs under one execution id (so it is reported as one single multi-job execution, e.g. in web UI). Note SQLExecution.withNewExecutionId posts a SparkListenerSQLExecutionStart event before execution and a SparkListenerSQLExecutionEnd event right afterwards. Tip Register SparkListener to get notified about the SQL execution events ( SparkListenerSQLExecutionStart and SparkListenerSQLExecutionEnd ).","title":" addBatch Phase &mdash; Adding DataFrame With New Data to Sink"},{"location":"micro-batch-execution/MicroBatchExecution/#updating-watermark-and-committing-offsets-to-offset-commit-log","text":"runBatch requests the WatermarkTracker to update event-time watermark (with the executedPlan of the IncrementalExecution ). runBatch requests the Offset Commit Log to persisting metadata of the streaming micro-batch (with the current batch ID and event-time watermark of the WatermarkTracker ). In the end, runBatch adds the available offsets to the committed offsets (and updates the offset s of every source with new data in the current micro-batch).","title":" Updating Watermark and Committing Offsets to Offset Commit Log"},{"location":"micro-batch-execution/MicroBatchExecution/#stopping-stream-processing-execution-of-streaming-query","text":"stop (): Unit stop sets the state to TERMINATED . When the stream execution thread is alive, stop requests the current SparkContext to cancelJobGroup identified by the runId and waits for this thread to die. Just to make sure that there are no more streaming jobs, stop requests the current SparkContext to cancelJobGroup identified by the runId again. In the end, stop prints out the following INFO message to the logs: Query [prettyIdString] was stopped stop is part of the StreamingQuery abstraction.","title":" Stopping Stream Processing (Execution of Streaming Query)"},{"location":"micro-batch-execution/MicroBatchExecution/#checking-whether-new-data-is-available","text":"isNewDataAvailable : Boolean isNewDataAvailable returns whether or not there are streaming sources (in the available offsets ) for which committed offsets are different from the available offsets or not available (committed) at all. isNewDataAvailable is true when there is at least one such streaming source. isNewDataAvailable is used when: MicroBatchExecution is requested to run an activated streaming query and construct the next streaming micro-batch","title":" Checking Whether New Data Is Available"},{"location":"micro-batch-execution/MicroBatchExecution/#analyzed-logical-plan","text":"logicalPlan : LogicalPlan logicalPlan is part of the StreamExecution abstraction. logicalPlan resolves ( replaces ) StreamingRelation , StreamingRelationV2 logical operators to StreamingExecutionRelation logical operators. logicalPlan uses the transformed logical plan to set the uniqueSources and sources internal registries to be the BaseStreamingSources of all the StreamingExecutionRelations unique and not, respectively. Lazy Value logicalPlan is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards. Internally, logicalPlan transforms the analyzed logical plan . For every StreamingRelation logical operator, logicalPlan tries to replace it with the StreamingExecutionRelation that was used earlier for the same StreamingRelation (if used multiple times in the plan) or creates a new one. While creating a new StreamingExecutionRelation , logicalPlan requests the DataSource to create a streaming Source with the metadata path as sources/uniqueID directory in the checkpoint root directory . logicalPlan prints out the following INFO message to the logs: Using Source [source] from DataSourceV1 named '[sourceName]' [dataSourceV1] For every StreamingRelationV2 logical operator with a MicroBatchStream data source (which is not on the list of spark.sql.streaming.disabledV2MicroBatchReaders ), logicalPlan tries to replace it with the StreamingExecutionRelation that was used earlier for the same StreamingRelationV2 (if used multiple times in the plan) or creates a new one. While creating a new StreamingExecutionRelation , logicalPlan requests the MicroBatchStream to create a MicroBatchStream with the metadata path as sources/uniqueID directory in the checkpoint root directory . logicalPlan prints out the following INFO message to the logs: Using MicroBatchReader [reader] from DataSourceV2 named '[sourceName]' [dataSourceV2] For every other StreamingRelationV2 leaf logical operator, logicalPlan tries to replace it with the StreamingExecutionRelation that was used earlier for the same StreamingRelationV2 (if used multiple times in the plan) or creates a new one. While creating a new StreamingExecutionRelation , logicalPlan requests the StreamingRelation for the underlying DataSource that is in turn requested to create a streaming Source with the metadata path as sources/uniqueID directory in the checkpoint root directory . logicalPlan prints out the following INFO message to the logs: Using Source [source] from DataSourceV2 named '[sourceName]' [dataSourceV2] logicalPlan requests the transformed analyzed logical plan for all StreamingExecutionRelations that are then requested for BaseStreamingSources , and saves them as the sources internal registry. In the end, logicalPlan sets the uniqueSources internal registry to be the unique BaseStreamingSources above. logicalPlan throws an AssertionError when not executed on the stream execution thread . logicalPlan must be initialized in QueryExecutionThread but the current thread was [currentThread]","title":" Analyzed Logical Plan"},{"location":"micro-batch-execution/MicroBatchExecution/#streamingsqlbatchid-local-property","text":"MicroBatchExecution defines streaming.sql.batchId as the name of the local property to be the current batch or epoch IDs (that Spark tasks can use at execution time). streaming.sql.batchId is used when: MicroBatchExecution is requested to run a single streaming micro-batch (and sets the property to be the current batch ID) DataWritingSparkTask is requested to run (and needs an epoch ID)","title":" streaming.sql.batchId Local Property"},{"location":"micro-batch-execution/MicroBatchExecution/#watermarktracker","text":"WatermarkTracker that is created when MicroBatchExecution is requested to populate start offsets (when requested to run an activated streaming query )","title":" WatermarkTracker"},{"location":"micro-batch-execution/MicroBatchExecution/#iscurrentbatchconstructed-flag","text":"isCurrentBatchConstructed : Boolean MicroBatchExecution uses isCurrentBatchConstructed internal flag to control whether or not to run a streaming micro-batch . Default: false When false , changed to whatever constructing the next streaming micro-batch gives back when running activated streaming query Disabled ( false ) after running a streaming micro-batch (when enabled after constructing the next streaming micro-batch ) Enabled ( true ) when populating start offsets (when running an activated streaming query ) and re-starting a streaming query from a checkpoint (using the Offset Write-Ahead Log ) Disabled ( false ) when populating start offsets (when running an activated streaming query ) and re-starting a streaming query from a checkpoint when the latest offset checkpointed (written) to the offset write-ahead log has been successfully processed and committed to the Offset Commit Log","title":" isCurrentBatchConstructed Flag"},{"location":"micro-batch-execution/MicroBatchExecution/#demo","text":"import org.apache.spark.sql.streaming.Trigger val query = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") // <-- not a StreamWriteSupport sink .option(\"truncate\", false) .trigger(Trigger.Once) // <-- Gives MicroBatchExecution .queryName(\"rate2console\") .start // The following gives access to the internals // And to MicroBatchExecution import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val engine = query.asInstanceOf[StreamingQueryWrapper].streamingQuery import org.apache.spark.sql.execution.streaming.StreamExecution assert(engine.isInstanceOf[StreamExecution]) import org.apache.spark.sql.execution.streaming.MicroBatchExecution val microBatchEngine = engine.asInstanceOf[MicroBatchExecution] assert(microBatchEngine.trigger == Trigger.Once)","title":"Demo"},{"location":"micro-batch-execution/MicroBatchExecution/#logging","text":"Enable ALL logging level for org.apache.spark.sql.execution.streaming.MicroBatchExecution logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.MicroBatchExecution=ALL Refer to Logging .","title":"Logging"},{"location":"micro-batch-execution/MicroBatchReader/","text":"MicroBatchReader -- Data Source Readers in Micro-Batch Stream Processing (Data Source API V2) \u00b6 MicroBatchReader is the extension of Spark SQL's DataSourceReader abstraction for data source readers in Micro-Batch Stream Processing . MicroBatchReader is part of the novel Data Source API V2 in Spark SQL. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-data-source-api-v2.html[Data Source API V2] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. [[contract]] .MicroBatchReader Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | commit a| [[commit]] [source, java] \u00b6 void commit(Offset end) \u00b6 Used when...FIXME | deserializeOffset a| [[deserializeOffset]] [source, java] \u00b6 Offset deserializeOffset(String json) \u00b6 Deserializes offset (from JSON format) Used when...FIXME | getEndOffset a| [[getEndOffset]] [source, java] \u00b6 Offset getEndOffset() \u00b6 End offset of this reader Used when...FIXME | getStartOffset a| [[getStartOffset]] [source, java] \u00b6 Offset getStartOffset() \u00b6 Start (beginning) offset of this reader Used when...FIXME | setOffsetRange a| [[setOffsetRange]] [source, java] \u00b6 void setOffsetRange( Optional start, Optional end) Sets the desired offset range for input partitions created from this reader (for data scan) Used when...FIXME |=== [[implementations]] .MicroBatchReaders [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | MicroBatchReader | Description | KafkaMicroBatchReader | [[KafkaMicroBatchReader]] | MemoryStream | [[MemoryStream]] | RateStreamMicroBatchReader | [[RateStreamMicroBatchReader]] | TextSocketMicroBatchReader | [[TextSocketMicroBatchReader]] |===","title":"MicroBatchReader"},{"location":"micro-batch-execution/MicroBatchReader/#microbatchreader-data-source-readers-in-micro-batch-stream-processing-data-source-api-v2","text":"MicroBatchReader is the extension of Spark SQL's DataSourceReader abstraction for data source readers in Micro-Batch Stream Processing . MicroBatchReader is part of the novel Data Source API V2 in Spark SQL. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-data-source-api-v2.html[Data Source API V2] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. [[contract]] .MicroBatchReader Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | commit a| [[commit]]","title":"MicroBatchReader -- Data Source Readers in Micro-Batch Stream Processing (Data Source API V2)"},{"location":"micro-batch-execution/MicroBatchReader/#source-java","text":"","title":"[source, java]"},{"location":"micro-batch-execution/MicroBatchReader/#void-commitoffset-end","text":"Used when...FIXME | deserializeOffset a| [[deserializeOffset]]","title":"void commit(Offset end)"},{"location":"micro-batch-execution/MicroBatchReader/#source-java_1","text":"","title":"[source, java]"},{"location":"micro-batch-execution/MicroBatchReader/#offset-deserializeoffsetstring-json","text":"Deserializes offset (from JSON format) Used when...FIXME | getEndOffset a| [[getEndOffset]]","title":"Offset deserializeOffset(String json)"},{"location":"micro-batch-execution/MicroBatchReader/#source-java_2","text":"","title":"[source, java]"},{"location":"micro-batch-execution/MicroBatchReader/#offset-getendoffset","text":"End offset of this reader Used when...FIXME | getStartOffset a| [[getStartOffset]]","title":"Offset getEndOffset()"},{"location":"micro-batch-execution/MicroBatchReader/#source-java_3","text":"","title":"[source, java]"},{"location":"micro-batch-execution/MicroBatchReader/#offset-getstartoffset","text":"Start (beginning) offset of this reader Used when...FIXME | setOffsetRange a| [[setOffsetRange]]","title":"Offset getStartOffset()"},{"location":"micro-batch-execution/MicroBatchReader/#source-java_4","text":"void setOffsetRange( Optional start, Optional end) Sets the desired offset range for input partitions created from this reader (for data scan) Used when...FIXME |=== [[implementations]] .MicroBatchReaders [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | MicroBatchReader | Description | KafkaMicroBatchReader | [[KafkaMicroBatchReader]] | MemoryStream | [[MemoryStream]] | RateStreamMicroBatchReader | [[RateStreamMicroBatchReader]] | TextSocketMicroBatchReader | [[TextSocketMicroBatchReader]] |===","title":"[source, java]"},{"location":"micro-batch-execution/MicroBatchWriter/","text":"MicroBatchWriter \u2014 Data Source Writer in Micro-Batch Stream Processing (Data Source API V2) \u00b6 [[batchId]][[writer]][[creating-instance]][[commit]][[abort]] MicroBatchWriter is a DataSourceWriter (Spark SQL) that uses the given batch ID as the epoch when requested to commit, abort and create a WriterFactory for a given StreamWriter in Micro-Batch Stream Processing . TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceWriter.html[DataSourceWriter ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. MicroBatchWriter is part of the novel Data Source API V2 in Spark SQL. MicroBatchWriter is < > exclusively when MicroBatchExecution is requested to < >.","title":"MicroBatchWriter"},{"location":"micro-batch-execution/MicroBatchWriter/#microbatchwriter-data-source-writer-in-micro-batch-stream-processing-data-source-api-v2","text":"[[batchId]][[writer]][[creating-instance]][[commit]][[abort]] MicroBatchWriter is a DataSourceWriter (Spark SQL) that uses the given batch ID as the epoch when requested to commit, abort and create a WriterFactory for a given StreamWriter in Micro-Batch Stream Processing . TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceWriter.html[DataSourceWriter ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. MicroBatchWriter is part of the novel Data Source API V2 in Spark SQL. MicroBatchWriter is < > exclusively when MicroBatchExecution is requested to < >.","title":"MicroBatchWriter &mdash; Data Source Writer in Micro-Batch Stream Processing (Data Source API V2)"},{"location":"monitoring/ExecutionStats/","text":"== [[ExecutionStats]] ExecutionStats ExecutionStats is...FIXME","title":"ExecutionStats"},{"location":"monitoring/MetricsReporter/","text":"MetricsReporter \u00b6 MetricsReporter is a Metrics Source for streaming queries . MetricsReporter uses the last StreamingQueryProgress (of the StreamExecution ) if available or simply defaults to a \"zero\" value. Creating Instance \u00b6 MetricsReporter takes the following to be created: StreamExecution Source Name MetricsReporter is created for stream execution engines . Gauges \u00b6 inputRate-total \u00b6 Reports inputRowsPerSecond (across all streaming sources) processingRate-total \u00b6 Reports processedRowsPerSecond (across all streaming sources) latency \u00b6 Reports triggerExecution duration of the last StreamingQueryProgress eventTime-watermark \u00b6 Reports watermark of the last StreamingQueryProgress Format: yyyy-MM-dd'T'HH:mm:ss.SSS'Z' states-rowsTotal \u00b6 Reports the total of numRowsTotal of all StateOperatorProgress es of the last StreamingQueryProgress states-usedBytes \u00b6 Reports the total of memoryUsedBytes of all StateOperatorProgress es of the last StreamingQueryProgress","title":"MetricsReporter"},{"location":"monitoring/MetricsReporter/#metricsreporter","text":"MetricsReporter is a Metrics Source for streaming queries . MetricsReporter uses the last StreamingQueryProgress (of the StreamExecution ) if available or simply defaults to a \"zero\" value.","title":"MetricsReporter"},{"location":"monitoring/MetricsReporter/#creating-instance","text":"MetricsReporter takes the following to be created: StreamExecution Source Name MetricsReporter is created for stream execution engines .","title":"Creating Instance"},{"location":"monitoring/MetricsReporter/#gauges","text":"","title":"Gauges"},{"location":"monitoring/MetricsReporter/#inputrate-total","text":"Reports inputRowsPerSecond (across all streaming sources)","title":"inputRate-total"},{"location":"monitoring/MetricsReporter/#processingrate-total","text":"Reports processedRowsPerSecond (across all streaming sources)","title":"processingRate-total"},{"location":"monitoring/MetricsReporter/#latency","text":"Reports triggerExecution duration of the last StreamingQueryProgress","title":"latency"},{"location":"monitoring/MetricsReporter/#eventtime-watermark","text":"Reports watermark of the last StreamingQueryProgress Format: yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","title":"eventTime-watermark"},{"location":"monitoring/MetricsReporter/#states-rowstotal","text":"Reports the total of numRowsTotal of all StateOperatorProgress es of the last StreamingQueryProgress","title":"states-rowsTotal"},{"location":"monitoring/MetricsReporter/#states-usedbytes","text":"Reports the total of memoryUsedBytes of all StateOperatorProgress es of the last StreamingQueryProgress","title":"states-usedBytes"},{"location":"monitoring/ProgressReporter/","text":"ProgressReporter \u00b6 ProgressReporter is an abstraction of execution progress reporters that report statistics of execution of a streaming query. Contract \u00b6 currentBatchId \u00b6 currentBatchId : Long ID of the streaming batch Used when: MicroBatchExecution is requested to plan a query for the batch (while running a batch ) ContinuousExecution is requested to plan a query for the epoch (while running continuously ) ProgressReporter is requested for a new StreamingQueryProgress (while finishing a trigger ) other usage id \u00b6 id : UUID Universally unique identifier (UUID) of the streaming query (that remains unchanged between restarts) lastExecution \u00b6 lastExecution : QueryExecution IncrementalExecution of the streaming execution round (a batch or an epoch) IncrementalExecution is created and executed in the queryPlanning phase of MicroBatchExecution and ContinuousExecution stream execution engines. logicalPlan \u00b6 logicalPlan : LogicalPlan Logical query plan of the streaming query Important The most interesting usage of the LogicalPlan is when stream execution engines replace ( transform ) input StreamingExecutionRelation and StreamingDataSourceV2Relation operators with (operators with) data or LocalRelation (to represent no data at a source). Used when ProgressReporter is requested for the following: extract statistics from the most recent query execution (to add watermark metric for streaming watermark ) extractSourceToNumInputRows name \u00b6 name : String Name of the streaming query newData \u00b6 newData : Map [ SparkDataStream , LogicalPlan ] SparkDataStream s (from all data sources ) with the more recent unprocessed input data (as LogicalPlan ) Used exclusively for MicroBatchExecution (when requested to run a single micro-batch ) Used when ProgressReporter is requested to extractSourceToNumInputRows offsetSeqMetadata \u00b6 offsetSeqMetadata : OffsetSeqMetadata OffsetSeqMetadata (with the current micro-batch event-time watermark and timestamp ) postEvent \u00b6 postEvent ( event : StreamingQueryListener . Event ): Unit Posts StreamingQueryListener.Event Used when: ProgressReporter is requested to update progress (and posts a QueryProgressEvent ) StreamExecution is requested to run stream processing (and posts a QueryStartedEvent at the beginning and a QueryTerminatedEvent after a query has been stopped) runId \u00b6 runId : UUID Universally unique identifier (UUID) of a single run of the streaming query (that changes every restart) sink \u00b6 sink : Table The one and only Table of the streaming query sinkCommitProgress \u00b6 sinkCommitProgress : Option [ StreamWriterCommitProgress ] StreamWriterCommitProgress with number of output rows: None when MicroBatchExecution stream execution engine is requested to populateStartOffsets Assigned a StreamWriterCommitProgress when MicroBatchExecution stream execution engine is about to complete running a micro-batch Used when finishTrigger (and updating progress ) sources \u00b6 sources : Seq [ SparkDataStream ] sparkSession \u00b6 sparkSession : SparkSession SparkSession of the streaming query Tip Find out more on SparkSession in The Internals of Spark SQL online book. triggerClock \u00b6 triggerClock : Clock Clock of the streaming query Implementations \u00b6 StreamExecution spark.sql.streaming.noDataProgressEventInterval \u00b6 ProgressReporter uses the spark.sql.streaming.noDataProgressEventInterval configuration property to control how long to wait between two progress events when there is no data (default: 10000L ) when finishing a trigger . Demo \u00b6 import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sampleQuery = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .option(\"truncate\", false) .trigger(Trigger.ProcessingTime(10.seconds)) .start // Using public API import org.apache.spark.sql.streaming.SourceProgress scala> sampleQuery. | lastProgress. | sources. | map { case sp: SourceProgress => | s\"source = ${sp.description} => endOffset = ${sp.endOffset}\" }. | foreach(println) source = RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8] => endOffset = 663 scala> println(sampleQuery.lastProgress.sources(0)) res40: org.apache.spark.sql.streaming.SourceProgress = { \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\", \"startOffset\" : 333, \"endOffset\" : 343, \"numInputRows\" : 10, \"inputRowsPerSecond\" : 0.9998000399920015, \"processedRowsPerSecond\" : 200.0 } // With a hack import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val offsets = sampleQuery. asInstanceOf[StreamingQueryWrapper]. streamingQuery. availableOffsets. map { case (source, offset) => s\"source = $source => offset = $offset\" } scala> offsets.foreach(println) source = RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8] => offset = 293 StreamingQueryProgress Queue \u00b6 progressBuffer : Queue [ StreamingQueryProgress ] progressBuffer is a scala.collection.mutable.Queue of StreamingQueryProgress es. progressBuffer has a new StreamingQueryProgress added when ProgressReporter is requested to update progress of a streaming query . The oldest StreamingQueryProgress is removed ( dequeued ) above spark.sql.streaming.numRecentProgressUpdates threshold. progressBuffer is used when ProgressReporter is requested for the last and the recent StreamingQueryProgresses . Current StreamingQueryStatus \u00b6 status : StreamingQueryStatus status is the current StreamingQueryStatus . status is used when StreamingQueryWrapper is requested for the current status of a streaming query . Updating Progress of Streaming Query \u00b6 updateProgress ( newProgress : StreamingQueryProgress ): Unit updateProgress records the input newProgress and posts a QueryProgressEvent event. updateProgress adds the input newProgress to progressBuffer . updateProgress removes elements from progressBuffer if their number is or exceeds the value of spark.sql.streaming.numRecentProgressUpdates configuration property. updateProgress posts a QueryProgressEvent (with the input newProgress ). updateProgress prints out the following INFO message to the logs: Streaming query made progress: [newProgress] updateProgress is used when ProgressReporter is requested to finish up a trigger . Initializing Query Progress for New Trigger \u00b6 startTrigger (): Unit startTrigger prints out the following DEBUG message to the logs: Starting Trigger Calculation .startTrigger's Internal Registry Changes For New Trigger [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Registry | New Value | < > | < > | < > | Requests the < > for the current timestamp (in millis) | < > | Enables ( true ) the isTriggerActive flag of the < > | < > | null | < > | null | < > | Clears the < > |=== startTrigger is used when: MicroBatchExecution stream execution engine is requested to run an activated streaming query (at the beginning of every trigger ) ContinuousExecution stream execution engine is requested to run an activated streaming query (at the beginning of every trigger) StreamExecution starts running batches (as part of TriggerExecutor executing a batch runner). Finishing Up Streaming Batch (Trigger) \u00b6 finishTrigger ( hasNewData : Boolean ): Unit finishTrigger sets currentTriggerEndTimestamp to the current time (using triggerClock ). finishTrigger < >. finishTrigger calculates the processing time (in seconds) as the difference between the < > and < > timestamps. finishTrigger calculates the input time (in seconds) as the difference between the start time of the < > and < > triggers. .ProgressReporter's finishTrigger and Timestamps image::images/ProgressReporter-finishTrigger-timestamps.png[align=\"center\"] finishTrigger prints out the following DEBUG message to the logs: Execution stats: [executionStats] finishTrigger creates a < > (aka source statistics) for < >. finishTrigger creates a < > (aka sink statistics) for the < >. finishTrigger creates a StreamingQueryProgress . If there was any data (using the input hasNewData flag), finishTrigger resets < > (i.e. becomes the minimum possible time) and < >. Otherwise, when no data was available (using the input hasNewData flag), finishTrigger < > only when < > passed. In the end, finishTrigger disables isTriggerActive flag of < > (i.e. sets it to false ). NOTE: finishTrigger is used exclusively when MicroBatchExecution is requested to < > (after < > at the end of a streaming batch). Time-Tracking Section (Recording Execution Time) \u00b6 reportTimeTaken [ T ]( triggerDetailKey : String )( body : => T ): T reportTimeTaken measures the time to execute body and records it in the currentDurationsMs internal registry under triggerDetailKey key. If the triggerDetailKey key was recorded already, the current execution time is added. In the end, reportTimeTaken prints out the following DEBUG message to the logs and returns the result of executing body . [triggerDetailKey] took [time] ms reportTimeTaken is used when stream execution engines are requested to execute the following phases (that appear as triggerDetailKey in the DEBUG message in the logs): MicroBatchExecution triggerExecution getOffset setOffsetRange getEndOffset walCommit getBatch queryPlanning addBatch ContinuousExecution queryPlanning runContinuous Updating Status Message \u00b6 updateStatusMessage ( message : String ): Unit updateStatusMessage simply updates the message in the StreamingQueryStatus internal registry. updateStatusMessage is used when: StreamExecution is requested to run stream processing MicroBatchExecution is requested to run an activated streaming query or construct the next streaming micro-batch Generating Execution Statistics \u00b6 extractExecutionStats ( hasNewData : Boolean ): ExecutionStats extractExecutionStats generates an ExecutionStats of the < > of the streaming query. Internally, extractExecutionStats generate watermark metric (using the event-time watermark of the < >) if there is a EventTimeWatermark unary logical operator in the < > of the streaming query. extractExecutionStats extractStateOperatorMetrics . extractExecutionStats extractSourceToNumInputRows . extractExecutionStats finds the EventTimeWatermarkExec unary physical operator (with non-zero EventTimeStats ) and generates max , min , and avg statistics. In the end, extractExecutionStats creates a ExecutionStats with the execution statistics. If the input hasNewData flag is turned off ( false ), extractExecutionStats returns an ExecutionStats with no input rows and event-time statistics (that require data to be processed to have any sense). NOTE: extractExecutionStats is used exclusively when ProgressReporter is requested to < >. Generating StateStoreWriter Metrics (StateOperatorProgress) \u00b6 extractStateOperatorMetrics ( hasNewData : Boolean ): Seq [ StateOperatorProgress ] extractStateOperatorMetrics requests the < > for the optimized execution plan ( executedPlan ) and finds all StateStoreWriter physical operators and requests them for StateOperatorProgress . extractStateOperatorMetrics clears ( zeros ) the numRowsUpdated metric for the given hasNewData turned off ( false ). extractStateOperatorMetrics returns an empty collection for the < > uninitialized ( null ). extractStateOperatorMetrics is used when ProgressReporter is requested to generate execution statistics . Recording Trigger Offsets (StreamProgress) \u00b6 recordTriggerOffsets ( from : StreamProgress , to : StreamProgress ): Unit recordTriggerOffsets simply sets ( records ) the < > and < > internal registries to the json representations of the from and to StreamProgresses . recordTriggerOffsets is used when: MicroBatchExecution is requested to < > ContinuousExecution is requested to < > Last StreamingQueryProgress \u00b6 lastProgress : StreamingQueryProgress The last StreamingQueryProgress currentDurationsMs \u00b6 scala.collection.mutable.HashMap of action names (aka triggerDetailKey ) and their cumulative times (in milliseconds). Starts empty when ProgressReporter sets the state for a new batch with new entries added or updated when reporting execution time (of an action). currentDurationsMs is available as durationMs of a streaming query. scala> :type q org.apache.spark.sql.streaming.StreamingQuery scala> query.lastProgress.durationMs res1: java.util.Map[String,Long] = {triggerExecution=60, queryPlanning=1, getBatch=5, getOffset=0, addBatch=30, walCommit=23} scala> println(q.lastProgress) { \"id\" : \"03fc78fc-fe19-408c-a1ae-812d0e28fcee\", \"runId\" : \"8c247071-afba-40e5-aad2-0e6f45f22488\", \"name\" : null, \"timestamp\" : \"2017-08-14T20:30:00.004Z\", \"batchId\" : 1, \"numInputRows\" : 432, \"inputRowsPerSecond\" : 0.9993568953312452, \"processedRowsPerSecond\" : 1380.1916932907347, \"durationMs\" : { \"addBatch\" : 237, \"getBatch\" : 26, \"getOffset\" : 0, \"queryPlanning\" : 1, \"triggerExecution\" : 313, \"walCommit\" : 45 }, \"stateOperators\" : [ ], \"sources\" : [ { \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\", \"startOffset\" : 0, \"endOffset\" : 432, \"numInputRows\" : 432, \"inputRowsPerSecond\" : 0.9993568953312452, \"processedRowsPerSecond\" : 1380.1916932907347 } ], \"sink\" : { \"description\" : \"ConsoleSink[numRows=20, truncate=true]\" } } Internal Properties \u00b6 currentTriggerEndTimestamp \u00b6 Timestamp of when the current batch/trigger has ended Default: -1L currentTriggerStartOffsets \u00b6 currentTriggerStartOffsets : Map [ BaseStreamingSource , String ] Start offsets (in JSON format ) per streaming source Used exclusively when < > (for a SourceProgress ) Reset ( null ) when < > Initialized when < > currentTriggerStartTimestamp \u00b6 Timestamp of when the current batch/trigger has started Default: -1L lastTriggerStartTimestamp \u00b6 Timestamp of when the last batch/trigger started Default: -1L Logging \u00b6 Configure logging of the concrete stream execution progress reporters to see what happens inside: ContinuousExecution MicroBatchExecution","title":"ProgressReporter"},{"location":"monitoring/ProgressReporter/#progressreporter","text":"ProgressReporter is an abstraction of execution progress reporters that report statistics of execution of a streaming query.","title":"ProgressReporter"},{"location":"monitoring/ProgressReporter/#contract","text":"","title":"Contract"},{"location":"monitoring/ProgressReporter/#currentbatchid","text":"currentBatchId : Long ID of the streaming batch Used when: MicroBatchExecution is requested to plan a query for the batch (while running a batch ) ContinuousExecution is requested to plan a query for the epoch (while running continuously ) ProgressReporter is requested for a new StreamingQueryProgress (while finishing a trigger ) other usage","title":" currentBatchId"},{"location":"monitoring/ProgressReporter/#id","text":"id : UUID Universally unique identifier (UUID) of the streaming query (that remains unchanged between restarts)","title":" id"},{"location":"monitoring/ProgressReporter/#lastexecution","text":"lastExecution : QueryExecution IncrementalExecution of the streaming execution round (a batch or an epoch) IncrementalExecution is created and executed in the queryPlanning phase of MicroBatchExecution and ContinuousExecution stream execution engines.","title":" lastExecution"},{"location":"monitoring/ProgressReporter/#logicalplan","text":"logicalPlan : LogicalPlan Logical query plan of the streaming query Important The most interesting usage of the LogicalPlan is when stream execution engines replace ( transform ) input StreamingExecutionRelation and StreamingDataSourceV2Relation operators with (operators with) data or LocalRelation (to represent no data at a source). Used when ProgressReporter is requested for the following: extract statistics from the most recent query execution (to add watermark metric for streaming watermark ) extractSourceToNumInputRows","title":" logicalPlan"},{"location":"monitoring/ProgressReporter/#name","text":"name : String Name of the streaming query","title":" name"},{"location":"monitoring/ProgressReporter/#newdata","text":"newData : Map [ SparkDataStream , LogicalPlan ] SparkDataStream s (from all data sources ) with the more recent unprocessed input data (as LogicalPlan ) Used exclusively for MicroBatchExecution (when requested to run a single micro-batch ) Used when ProgressReporter is requested to extractSourceToNumInputRows","title":" newData"},{"location":"monitoring/ProgressReporter/#offsetseqmetadata","text":"offsetSeqMetadata : OffsetSeqMetadata OffsetSeqMetadata (with the current micro-batch event-time watermark and timestamp )","title":" offsetSeqMetadata"},{"location":"monitoring/ProgressReporter/#postevent","text":"postEvent ( event : StreamingQueryListener . Event ): Unit Posts StreamingQueryListener.Event Used when: ProgressReporter is requested to update progress (and posts a QueryProgressEvent ) StreamExecution is requested to run stream processing (and posts a QueryStartedEvent at the beginning and a QueryTerminatedEvent after a query has been stopped)","title":" postEvent"},{"location":"monitoring/ProgressReporter/#runid","text":"runId : UUID Universally unique identifier (UUID) of a single run of the streaming query (that changes every restart)","title":" runId"},{"location":"monitoring/ProgressReporter/#sink","text":"sink : Table The one and only Table of the streaming query","title":" sink"},{"location":"monitoring/ProgressReporter/#sinkcommitprogress","text":"sinkCommitProgress : Option [ StreamWriterCommitProgress ] StreamWriterCommitProgress with number of output rows: None when MicroBatchExecution stream execution engine is requested to populateStartOffsets Assigned a StreamWriterCommitProgress when MicroBatchExecution stream execution engine is about to complete running a micro-batch Used when finishTrigger (and updating progress )","title":" sinkCommitProgress"},{"location":"monitoring/ProgressReporter/#sources","text":"sources : Seq [ SparkDataStream ]","title":" sources"},{"location":"monitoring/ProgressReporter/#sparksession","text":"sparkSession : SparkSession SparkSession of the streaming query Tip Find out more on SparkSession in The Internals of Spark SQL online book.","title":" sparkSession"},{"location":"monitoring/ProgressReporter/#triggerclock","text":"triggerClock : Clock Clock of the streaming query","title":" triggerClock"},{"location":"monitoring/ProgressReporter/#implementations","text":"StreamExecution","title":"Implementations"},{"location":"monitoring/ProgressReporter/#sparksqlstreamingnodataprogresseventinterval","text":"ProgressReporter uses the spark.sql.streaming.noDataProgressEventInterval configuration property to control how long to wait between two progress events when there is no data (default: 10000L ) when finishing a trigger .","title":" spark.sql.streaming.noDataProgressEventInterval"},{"location":"monitoring/ProgressReporter/#demo","text":"import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sampleQuery = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .option(\"truncate\", false) .trigger(Trigger.ProcessingTime(10.seconds)) .start // Using public API import org.apache.spark.sql.streaming.SourceProgress scala> sampleQuery. | lastProgress. | sources. | map { case sp: SourceProgress => | s\"source = ${sp.description} => endOffset = ${sp.endOffset}\" }. | foreach(println) source = RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8] => endOffset = 663 scala> println(sampleQuery.lastProgress.sources(0)) res40: org.apache.spark.sql.streaming.SourceProgress = { \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\", \"startOffset\" : 333, \"endOffset\" : 343, \"numInputRows\" : 10, \"inputRowsPerSecond\" : 0.9998000399920015, \"processedRowsPerSecond\" : 200.0 } // With a hack import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val offsets = sampleQuery. asInstanceOf[StreamingQueryWrapper]. streamingQuery. availableOffsets. map { case (source, offset) => s\"source = $source => offset = $offset\" } scala> offsets.foreach(println) source = RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8] => offset = 293","title":"Demo"},{"location":"monitoring/ProgressReporter/#streamingqueryprogress-queue","text":"progressBuffer : Queue [ StreamingQueryProgress ] progressBuffer is a scala.collection.mutable.Queue of StreamingQueryProgress es. progressBuffer has a new StreamingQueryProgress added when ProgressReporter is requested to update progress of a streaming query . The oldest StreamingQueryProgress is removed ( dequeued ) above spark.sql.streaming.numRecentProgressUpdates threshold. progressBuffer is used when ProgressReporter is requested for the last and the recent StreamingQueryProgresses .","title":" StreamingQueryProgress Queue"},{"location":"monitoring/ProgressReporter/#current-streamingquerystatus","text":"status : StreamingQueryStatus status is the current StreamingQueryStatus . status is used when StreamingQueryWrapper is requested for the current status of a streaming query .","title":" Current StreamingQueryStatus"},{"location":"monitoring/ProgressReporter/#updating-progress-of-streaming-query","text":"updateProgress ( newProgress : StreamingQueryProgress ): Unit updateProgress records the input newProgress and posts a QueryProgressEvent event. updateProgress adds the input newProgress to progressBuffer . updateProgress removes elements from progressBuffer if their number is or exceeds the value of spark.sql.streaming.numRecentProgressUpdates configuration property. updateProgress posts a QueryProgressEvent (with the input newProgress ). updateProgress prints out the following INFO message to the logs: Streaming query made progress: [newProgress] updateProgress is used when ProgressReporter is requested to finish up a trigger .","title":" Updating Progress of Streaming Query"},{"location":"monitoring/ProgressReporter/#initializing-query-progress-for-new-trigger","text":"startTrigger (): Unit startTrigger prints out the following DEBUG message to the logs: Starting Trigger Calculation .startTrigger's Internal Registry Changes For New Trigger [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Registry | New Value | < > | < > | < > | Requests the < > for the current timestamp (in millis) | < > | Enables ( true ) the isTriggerActive flag of the < > | < > | null | < > | null | < > | Clears the < > |=== startTrigger is used when: MicroBatchExecution stream execution engine is requested to run an activated streaming query (at the beginning of every trigger ) ContinuousExecution stream execution engine is requested to run an activated streaming query (at the beginning of every trigger) StreamExecution starts running batches (as part of TriggerExecutor executing a batch runner).","title":" Initializing Query Progress for New Trigger"},{"location":"monitoring/ProgressReporter/#finishing-up-streaming-batch-trigger","text":"finishTrigger ( hasNewData : Boolean ): Unit finishTrigger sets currentTriggerEndTimestamp to the current time (using triggerClock ). finishTrigger < >. finishTrigger calculates the processing time (in seconds) as the difference between the < > and < > timestamps. finishTrigger calculates the input time (in seconds) as the difference between the start time of the < > and < > triggers. .ProgressReporter's finishTrigger and Timestamps image::images/ProgressReporter-finishTrigger-timestamps.png[align=\"center\"] finishTrigger prints out the following DEBUG message to the logs: Execution stats: [executionStats] finishTrigger creates a < > (aka source statistics) for < >. finishTrigger creates a < > (aka sink statistics) for the < >. finishTrigger creates a StreamingQueryProgress . If there was any data (using the input hasNewData flag), finishTrigger resets < > (i.e. becomes the minimum possible time) and < >. Otherwise, when no data was available (using the input hasNewData flag), finishTrigger < > only when < > passed. In the end, finishTrigger disables isTriggerActive flag of < > (i.e. sets it to false ). NOTE: finishTrigger is used exclusively when MicroBatchExecution is requested to < > (after < > at the end of a streaming batch).","title":" Finishing Up Streaming Batch (Trigger)"},{"location":"monitoring/ProgressReporter/#time-tracking-section-recording-execution-time","text":"reportTimeTaken [ T ]( triggerDetailKey : String )( body : => T ): T reportTimeTaken measures the time to execute body and records it in the currentDurationsMs internal registry under triggerDetailKey key. If the triggerDetailKey key was recorded already, the current execution time is added. In the end, reportTimeTaken prints out the following DEBUG message to the logs and returns the result of executing body . [triggerDetailKey] took [time] ms reportTimeTaken is used when stream execution engines are requested to execute the following phases (that appear as triggerDetailKey in the DEBUG message in the logs): MicroBatchExecution triggerExecution getOffset setOffsetRange getEndOffset walCommit getBatch queryPlanning addBatch ContinuousExecution queryPlanning runContinuous","title":" Time-Tracking Section (Recording Execution Time)"},{"location":"monitoring/ProgressReporter/#updating-status-message","text":"updateStatusMessage ( message : String ): Unit updateStatusMessage simply updates the message in the StreamingQueryStatus internal registry. updateStatusMessage is used when: StreamExecution is requested to run stream processing MicroBatchExecution is requested to run an activated streaming query or construct the next streaming micro-batch","title":" Updating Status Message"},{"location":"monitoring/ProgressReporter/#generating-execution-statistics","text":"extractExecutionStats ( hasNewData : Boolean ): ExecutionStats extractExecutionStats generates an ExecutionStats of the < > of the streaming query. Internally, extractExecutionStats generate watermark metric (using the event-time watermark of the < >) if there is a EventTimeWatermark unary logical operator in the < > of the streaming query. extractExecutionStats extractStateOperatorMetrics . extractExecutionStats extractSourceToNumInputRows . extractExecutionStats finds the EventTimeWatermarkExec unary physical operator (with non-zero EventTimeStats ) and generates max , min , and avg statistics. In the end, extractExecutionStats creates a ExecutionStats with the execution statistics. If the input hasNewData flag is turned off ( false ), extractExecutionStats returns an ExecutionStats with no input rows and event-time statistics (that require data to be processed to have any sense). NOTE: extractExecutionStats is used exclusively when ProgressReporter is requested to < >.","title":" Generating Execution Statistics"},{"location":"monitoring/ProgressReporter/#generating-statestorewriter-metrics-stateoperatorprogress","text":"extractStateOperatorMetrics ( hasNewData : Boolean ): Seq [ StateOperatorProgress ] extractStateOperatorMetrics requests the < > for the optimized execution plan ( executedPlan ) and finds all StateStoreWriter physical operators and requests them for StateOperatorProgress . extractStateOperatorMetrics clears ( zeros ) the numRowsUpdated metric for the given hasNewData turned off ( false ). extractStateOperatorMetrics returns an empty collection for the < > uninitialized ( null ). extractStateOperatorMetrics is used when ProgressReporter is requested to generate execution statistics .","title":" Generating StateStoreWriter Metrics (StateOperatorProgress)"},{"location":"monitoring/ProgressReporter/#recording-trigger-offsets-streamprogress","text":"recordTriggerOffsets ( from : StreamProgress , to : StreamProgress ): Unit recordTriggerOffsets simply sets ( records ) the < > and < > internal registries to the json representations of the from and to StreamProgresses . recordTriggerOffsets is used when: MicroBatchExecution is requested to < > ContinuousExecution is requested to < >","title":" Recording Trigger Offsets (StreamProgress)"},{"location":"monitoring/ProgressReporter/#last-streamingqueryprogress","text":"lastProgress : StreamingQueryProgress The last StreamingQueryProgress","title":" Last StreamingQueryProgress"},{"location":"monitoring/ProgressReporter/#currentdurationsms","text":"scala.collection.mutable.HashMap of action names (aka triggerDetailKey ) and their cumulative times (in milliseconds). Starts empty when ProgressReporter sets the state for a new batch with new entries added or updated when reporting execution time (of an action). currentDurationsMs is available as durationMs of a streaming query. scala> :type q org.apache.spark.sql.streaming.StreamingQuery scala> query.lastProgress.durationMs res1: java.util.Map[String,Long] = {triggerExecution=60, queryPlanning=1, getBatch=5, getOffset=0, addBatch=30, walCommit=23} scala> println(q.lastProgress) { \"id\" : \"03fc78fc-fe19-408c-a1ae-812d0e28fcee\", \"runId\" : \"8c247071-afba-40e5-aad2-0e6f45f22488\", \"name\" : null, \"timestamp\" : \"2017-08-14T20:30:00.004Z\", \"batchId\" : 1, \"numInputRows\" : 432, \"inputRowsPerSecond\" : 0.9993568953312452, \"processedRowsPerSecond\" : 1380.1916932907347, \"durationMs\" : { \"addBatch\" : 237, \"getBatch\" : 26, \"getOffset\" : 0, \"queryPlanning\" : 1, \"triggerExecution\" : 313, \"walCommit\" : 45 }, \"stateOperators\" : [ ], \"sources\" : [ { \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\", \"startOffset\" : 0, \"endOffset\" : 432, \"numInputRows\" : 432, \"inputRowsPerSecond\" : 0.9993568953312452, \"processedRowsPerSecond\" : 1380.1916932907347 } ], \"sink\" : { \"description\" : \"ConsoleSink[numRows=20, truncate=true]\" } }","title":"currentDurationsMs"},{"location":"monitoring/ProgressReporter/#internal-properties","text":"","title":"Internal Properties"},{"location":"monitoring/ProgressReporter/#currenttriggerendtimestamp","text":"Timestamp of when the current batch/trigger has ended Default: -1L","title":"currentTriggerEndTimestamp"},{"location":"monitoring/ProgressReporter/#currenttriggerstartoffsets","text":"currentTriggerStartOffsets : Map [ BaseStreamingSource , String ] Start offsets (in JSON format ) per streaming source Used exclusively when < > (for a SourceProgress ) Reset ( null ) when < > Initialized when < >","title":"currentTriggerStartOffsets"},{"location":"monitoring/ProgressReporter/#currenttriggerstarttimestamp","text":"Timestamp of when the current batch/trigger has started Default: -1L","title":"currentTriggerStartTimestamp"},{"location":"monitoring/ProgressReporter/#lasttriggerstarttimestamp","text":"Timestamp of when the last batch/trigger started Default: -1L","title":"lastTriggerStartTimestamp"},{"location":"monitoring/ProgressReporter/#logging","text":"Configure logging of the concrete stream execution progress reporters to see what happens inside: ContinuousExecution MicroBatchExecution","title":"Logging"},{"location":"monitoring/SinkProgress/","text":"SinkProgress \u00b6 SinkProgress is...FIXME","title":"SinkProgress"},{"location":"monitoring/SinkProgress/#sinkprogress","text":"SinkProgress is...FIXME","title":"SinkProgress"},{"location":"monitoring/SourceProgress/","text":"SourceProgress \u00b6 SourceProgress is...FIXME","title":"SourceProgress"},{"location":"monitoring/SourceProgress/#sourceprogress","text":"SourceProgress is...FIXME","title":"SourceProgress"},{"location":"monitoring/StateOperatorProgress/","text":"StateOperatorProgress \u00b6 StateOperatorProgress is information about updates made to stateful operators in a StreamingQuery during a trigger: numRowsTotal numRowsUpdated memoryUsedBytes Custom Metrics (default: empty) StateOperatorProgress is created when StateStoreWriter is requested to getProgress .","title":"StateOperatorProgress"},{"location":"monitoring/StateOperatorProgress/#stateoperatorprogress","text":"StateOperatorProgress is information about updates made to stateful operators in a StreamingQuery during a trigger: numRowsTotal numRowsUpdated memoryUsedBytes Custom Metrics (default: empty) StateOperatorProgress is created when StateStoreWriter is requested to getProgress .","title":"StateOperatorProgress"},{"location":"monitoring/StreamingQueryListener/","text":"StreamingQueryListener \u2014 Intercepting Life Cycle Events of Streaming Queries \u00b6 StreamingQueryListener is an abstraction of listeners to be notified about the life cycle events of all the streaming queries in a Spark Structured Streaming application: Query started Query made progress Query terminated StreamingQueryListener is used internally by StreamingQueryListenerBus to post a streaming event to all registered StreamingQueryListeners . StreamingQueryListener can be used by Spark developers to intercept events in Spark Structured Streaming applications. Contract \u00b6 onQueryProgress \u00b6 onQueryProgress ( event : QueryProgressEvent ): Unit Informs that MicroBatchExecution has finished triggerExecution phase (the end of a streaming batch) onQueryStarted \u00b6 onQueryStarted ( event : QueryStartedEvent ): Unit Informs that DataStreamWriter was requested to start execution of the streaming query (on the stream execution thread ) Note onQueryStarted is used internally to unblock the starting thread of StreamExecution . onQueryTerminated \u00b6 onQueryTerminated ( event : QueryTerminatedEvent ): Unit Informs that a streaming query was < > or terminated due to an error Lifecycle Events \u00b6 StreamingQueryListener is informed about the life cycle events when StreamingQueryListenerBus is requested to doPostEvent . QueryStartedEvent \u00b6 id runId name Intercepted by onQueryStarted Posted when StreamExecution is requested to run stream processing (when DataStreamWriter is requested to start execution of the streaming query on the stream execution thread ) QueryProgressEvent \u00b6 StreamingQueryProgress Intercepted by onQueryProgress Posted when ProgressReporter is requested to update progress of a streaming query (after MicroBatchExecution has finished triggerExecution phase at the end of a streaming batch) QueryTerminatedEvent \u00b6 id runId exception if terminated due to an error Intercepted by onQueryTerminated Posted when StreamExecution is requested to run stream processing (and the streaming query was stopped or terminated due to an error) Registering StreamingQueryListener \u00b6 StreamingQueryListener can be registered using StreamingQueryManager.addListener method. val queryListener: StreamingQueryListener = ... spark.streams.addListener(queryListener) Deregistering StreamingQueryListener \u00b6 StreamingQueryListener can be deregistered using StreamingQueryManager.removeListener method. val queryListener : StreamingQueryListener = ... spark . streams . removeListener ( queryListener )","title":"StreamingQueryListener"},{"location":"monitoring/StreamingQueryListener/#streamingquerylistener-intercepting-life-cycle-events-of-streaming-queries","text":"StreamingQueryListener is an abstraction of listeners to be notified about the life cycle events of all the streaming queries in a Spark Structured Streaming application: Query started Query made progress Query terminated StreamingQueryListener is used internally by StreamingQueryListenerBus to post a streaming event to all registered StreamingQueryListeners . StreamingQueryListener can be used by Spark developers to intercept events in Spark Structured Streaming applications.","title":"StreamingQueryListener &mdash; Intercepting Life Cycle Events of Streaming Queries"},{"location":"monitoring/StreamingQueryListener/#contract","text":"","title":"Contract"},{"location":"monitoring/StreamingQueryListener/#onqueryprogress","text":"onQueryProgress ( event : QueryProgressEvent ): Unit Informs that MicroBatchExecution has finished triggerExecution phase (the end of a streaming batch)","title":" onQueryProgress"},{"location":"monitoring/StreamingQueryListener/#onquerystarted","text":"onQueryStarted ( event : QueryStartedEvent ): Unit Informs that DataStreamWriter was requested to start execution of the streaming query (on the stream execution thread ) Note onQueryStarted is used internally to unblock the starting thread of StreamExecution .","title":" onQueryStarted"},{"location":"monitoring/StreamingQueryListener/#onqueryterminated","text":"onQueryTerminated ( event : QueryTerminatedEvent ): Unit Informs that a streaming query was < > or terminated due to an error","title":" onQueryTerminated"},{"location":"monitoring/StreamingQueryListener/#lifecycle-events","text":"StreamingQueryListener is informed about the life cycle events when StreamingQueryListenerBus is requested to doPostEvent .","title":" Lifecycle Events"},{"location":"monitoring/StreamingQueryListener/#querystartedevent","text":"id runId name Intercepted by onQueryStarted Posted when StreamExecution is requested to run stream processing (when DataStreamWriter is requested to start execution of the streaming query on the stream execution thread )","title":" QueryStartedEvent"},{"location":"monitoring/StreamingQueryListener/#queryprogressevent","text":"StreamingQueryProgress Intercepted by onQueryProgress Posted when ProgressReporter is requested to update progress of a streaming query (after MicroBatchExecution has finished triggerExecution phase at the end of a streaming batch)","title":" QueryProgressEvent"},{"location":"monitoring/StreamingQueryListener/#queryterminatedevent","text":"id runId exception if terminated due to an error Intercepted by onQueryTerminated Posted when StreamExecution is requested to run stream processing (and the streaming query was stopped or terminated due to an error)","title":" QueryTerminatedEvent"},{"location":"monitoring/StreamingQueryListener/#registering-streamingquerylistener","text":"StreamingQueryListener can be registered using StreamingQueryManager.addListener method. val queryListener: StreamingQueryListener = ... spark.streams.addListener(queryListener)","title":"Registering StreamingQueryListener"},{"location":"monitoring/StreamingQueryListener/#deregistering-streamingquerylistener","text":"StreamingQueryListener can be deregistered using StreamingQueryManager.removeListener method. val queryListener : StreamingQueryListener = ... spark . streams . removeListener ( queryListener )","title":"Deregistering StreamingQueryListener"},{"location":"monitoring/StreamingQueryProgress/","text":"StreamingQueryProgress \u00b6 StreamingQueryProgress is information about a single micro-batch ( progress ) of a StreamingQuery : Unique identifier Unique identifier of a query execution Name Time when a trigger has started (in ISO8601 format) Unique ID of a micro-batch Batch Duration Durations of the internal phases (in ms) Statistics of the event time as seen in a batch StateOperatorProgress for every stateful operator SourceProgress for every streaming source SinkProgress Observed Metrics StreamingQueryProgress is created when StreamExecution is requested to finish a trigger . Last and Recent Progresses \u00b6 Use lastProgress property of a StreamingQuery to access the most recent StreamingQueryProgress update. val sq : StreamingQuery = ... sq . lastProgress Use recentProgress property of a StreamingQuery to access the most recent StreamingQueryProgress updates. val sq : StreamingQuery = ... sq . recentProgress StreamingQueryListener \u00b6 Use StreamingQueryListener to be notified about StreamingQueryProgress updates while a streaming query is executed.","title":"StreamingQueryProgress"},{"location":"monitoring/StreamingQueryProgress/#streamingqueryprogress","text":"StreamingQueryProgress is information about a single micro-batch ( progress ) of a StreamingQuery : Unique identifier Unique identifier of a query execution Name Time when a trigger has started (in ISO8601 format) Unique ID of a micro-batch Batch Duration Durations of the internal phases (in ms) Statistics of the event time as seen in a batch StateOperatorProgress for every stateful operator SourceProgress for every streaming source SinkProgress Observed Metrics StreamingQueryProgress is created when StreamExecution is requested to finish a trigger .","title":"StreamingQueryProgress"},{"location":"monitoring/StreamingQueryProgress/#last-and-recent-progresses","text":"Use lastProgress property of a StreamingQuery to access the most recent StreamingQueryProgress update. val sq : StreamingQuery = ... sq . lastProgress Use recentProgress property of a StreamingQuery to access the most recent StreamingQueryProgress updates. val sq : StreamingQuery = ... sq . recentProgress","title":"Last and Recent Progresses"},{"location":"monitoring/StreamingQueryProgress/#streamingquerylistener","text":"Use StreamingQueryListener to be notified about StreamingQueryProgress updates while a streaming query is executed.","title":"StreamingQueryListener"},{"location":"monitoring/StreamingQueryStatus/","text":"StreamingQueryStatus \u00b6 StreamingQueryStatus is...FIXME","title":"StreamingQueryStatus"},{"location":"monitoring/StreamingQueryStatus/#streamingquerystatus","text":"StreamingQueryStatus is...FIXME","title":"StreamingQueryStatus"},{"location":"operators/","text":"Streaming Operators \u2014 High-Level Declarative Streaming Dataset API \u00b6 Dataset API defines a set of operators that are used in Spark Structured Streaming and together constitute the High-Level Declarative Streaming Dataset API .","title":"Streaming Operators"},{"location":"operators/#streaming-operators-high-level-declarative-streaming-dataset-api","text":"Dataset API defines a set of operators that are used in Spark Structured Streaming and together constitute the High-Level Declarative Streaming Dataset API .","title":"Streaming Operators &mdash; High-Level Declarative Streaming Dataset API"},{"location":"operators/crossJoin/","text":"crossJoin Operator \u2014 Streaming Join \u00b6 crossJoin ( right : Dataset [ _ ]): DataFrame crossJoin operator...FIXME","title":"crossJoin"},{"location":"operators/crossJoin/#crossjoin-operator-streaming-join","text":"crossJoin ( right : Dataset [ _ ]): DataFrame crossJoin operator...FIXME","title":"crossJoin Operator &mdash; Streaming Join"},{"location":"operators/dropDuplicates/","text":"dropDuplicates Operator \u2014 Streaming Deduplication \u00b6 dropDuplicates (): Dataset [ T ] dropDuplicates ( colNames : Seq [ String ]): Dataset [ T ] dropDuplicates ( col1 : String , cols : String * ): Dataset [ T ] dropDuplicates operator drops duplicate records (given a subset of columns) Note For a streaming Dataset, dropDuplicates will keep all data across triggers as intermediate state to drop duplicates rows. You can use withWatermark operator to limit how late the duplicate data can be and system will accordingly limit the state. In addition, too late data older than watermark will be dropped to avoid any possibility of duplicates. Demo \u00b6 // Start a streaming query // Using old-fashioned MemoryStream (with the deprecated SQLContext) import org.apache.spark.sql.execution.streaming.MemoryStream import org.apache.spark.sql.SQLContext implicit val sqlContext: SQLContext = spark.sqlContext val source = MemoryStream[(Int, Int)] val ids = source.toDS.toDF(\"time\", \"id\"). withColumn(\"time\", $\"time\" cast \"timestamp\"). // <-- convert time column from Int to Timestamp dropDuplicates(\"id\"). withColumn(\"time\", $\"time\" cast \"long\") // <-- convert time column back from Timestamp to Int // Conversions are only for display purposes // Internally we need timestamps for watermark to work // Displaying timestamps could be too much for such a simple task scala> println(ids.queryExecution.analyzed.numberedTreeString) 00 Project [cast(time#10 as bigint) AS time#15L, id#6] 01 +- Deduplicate [id#6], true 02 +- Project [cast(time#5 as timestamp) AS time#10, id#6] 03 +- Project [_1#2 AS time#5, _2#3 AS id#6] 04 +- StreamingExecutionRelation MemoryStream[_1#2,_2#3], [_1#2, _2#3] import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = ids. writeStream. format(\"memory\"). queryName(\"dups\"). outputMode(OutputMode.Append). trigger(Trigger.ProcessingTime(30.seconds)). option(\"checkpointLocation\", \"checkpoint-dir\"). // <-- use checkpointing to save state between restarts start // Publish duplicate records source.addData(1 -> 1) source.addData(2 -> 1) source.addData(3 -> 1) q.processAllAvailable() // Check out how dropDuplicates removes duplicates // --> per single streaming batch (easy) scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 1| 1| +----+---+ source.addData(4 -> 1) source.addData(5 -> 2) // --> across streaming batches (harder) scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 1| 1| | 5| 2| +----+---+ // Check out the internal state scala> println(q.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 2, \"numRowsUpdated\" : 1, \"memoryUsedBytes\" : 17751 } // You could use web UI's SQL tab instead // Use Details for Query source.addData(6 -> 2) scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 1| 1| | 5| 2| +----+---+ // Check out the internal state scala> println(q.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 2, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 17751 } // Restart the streaming query q.stop val q = ids. writeStream. format(\"memory\"). queryName(\"dups\"). outputMode(OutputMode.Complete). // <-- memory sink supports checkpointing for Complete output mode only trigger(Trigger.ProcessingTime(30.seconds)). option(\"checkpointLocation\", \"checkpoint-dir\"). // <-- use checkpointing to save state between restarts start // Doh! MemorySink is fine, but Complete is only available with a streaming aggregation // Answer it if you know why --> https://stackoverflow.com/q/45756997/1305344 // It's a high time to work on https://issues.apache.org/jira/browse/SPARK-21667 // to understand the low-level details (and the reason, it seems) // Disabling operation checks and starting over // ./bin/spark-shell -c spark.sql.streaming.unsupportedOperationCheck=false // it works now --> no exception! scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ +----+---+ source.addData(0 -> 1) // wait till the batch is triggered scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 0| 1| +----+---+ source.addData(1 -> 1) source.addData(2 -> 1) // wait till the batch is triggered scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ +----+---+ // What?! No rows?! It doesn't look as if it worked fine :( // Use groupBy to pass the requirement of having streaming aggregation for Complete output mode val counts = ids.groupBy(\"id\").agg(first($\"time\") as \"first_time\") scala> counts.explain == Physical Plan == *HashAggregate(keys=[id#246], functions=[first(time#255L, false)]) +- StateStoreSave [id#246], StatefulOperatorStateInfo(<unknown>,3585583b-42d7-4547-8d62-255581c48275,0,0), Append, 0 +- *HashAggregate(keys=[id#246], functions=[merge_first(time#255L, false)]) +- StateStoreRestore [id#246], StatefulOperatorStateInfo(<unknown>,3585583b-42d7-4547-8d62-255581c48275,0,0) +- *HashAggregate(keys=[id#246], functions=[merge_first(time#255L, false)]) +- *HashAggregate(keys=[id#246], functions=[partial_first(time#255L, false)]) +- *Project [cast(time#250 as bigint) AS time#255L, id#246] +- StreamingDeduplicate [id#246], StatefulOperatorStateInfo(<unknown>,3585583b-42d7-4547-8d62-255581c48275,1,0), 0 +- Exchange hashpartitioning(id#246, 200) +- *Project [cast(_1#242 as timestamp) AS time#250, _2#243 AS id#246] +- StreamingRelation MemoryStream[_1#242,_2#243], [_1#242, _2#243] val q = counts. writeStream. format(\"memory\"). queryName(\"dups\"). outputMode(OutputMode.Complete). // <-- memory sink supports checkpointing for Complete output mode only trigger(Trigger.ProcessingTime(30.seconds)). option(\"checkpointLocation\", \"checkpoint-dir\"). // <-- use checkpointing to save state between restarts start source.addData(0 -> 1) source.addData(1 -> 1) // wait till the batch is triggered scala> spark.table(\"dups\").show +---+----------+ | id|first_time| +---+----------+ | 1| 0| +---+----------+ // Publish duplicates // Check out how dropDuplicates removes duplicates // Stop the streaming query // Specify event time watermark to remove old duplicates","title":"dropDuplicates"},{"location":"operators/dropDuplicates/#dropduplicates-operator-streaming-deduplication","text":"dropDuplicates (): Dataset [ T ] dropDuplicates ( colNames : Seq [ String ]): Dataset [ T ] dropDuplicates ( col1 : String , cols : String * ): Dataset [ T ] dropDuplicates operator drops duplicate records (given a subset of columns) Note For a streaming Dataset, dropDuplicates will keep all data across triggers as intermediate state to drop duplicates rows. You can use withWatermark operator to limit how late the duplicate data can be and system will accordingly limit the state. In addition, too late data older than watermark will be dropped to avoid any possibility of duplicates.","title":"dropDuplicates Operator &mdash; Streaming Deduplication"},{"location":"operators/dropDuplicates/#demo","text":"// Start a streaming query // Using old-fashioned MemoryStream (with the deprecated SQLContext) import org.apache.spark.sql.execution.streaming.MemoryStream import org.apache.spark.sql.SQLContext implicit val sqlContext: SQLContext = spark.sqlContext val source = MemoryStream[(Int, Int)] val ids = source.toDS.toDF(\"time\", \"id\"). withColumn(\"time\", $\"time\" cast \"timestamp\"). // <-- convert time column from Int to Timestamp dropDuplicates(\"id\"). withColumn(\"time\", $\"time\" cast \"long\") // <-- convert time column back from Timestamp to Int // Conversions are only for display purposes // Internally we need timestamps for watermark to work // Displaying timestamps could be too much for such a simple task scala> println(ids.queryExecution.analyzed.numberedTreeString) 00 Project [cast(time#10 as bigint) AS time#15L, id#6] 01 +- Deduplicate [id#6], true 02 +- Project [cast(time#5 as timestamp) AS time#10, id#6] 03 +- Project [_1#2 AS time#5, _2#3 AS id#6] 04 +- StreamingExecutionRelation MemoryStream[_1#2,_2#3], [_1#2, _2#3] import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = ids. writeStream. format(\"memory\"). queryName(\"dups\"). outputMode(OutputMode.Append). trigger(Trigger.ProcessingTime(30.seconds)). option(\"checkpointLocation\", \"checkpoint-dir\"). // <-- use checkpointing to save state between restarts start // Publish duplicate records source.addData(1 -> 1) source.addData(2 -> 1) source.addData(3 -> 1) q.processAllAvailable() // Check out how dropDuplicates removes duplicates // --> per single streaming batch (easy) scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 1| 1| +----+---+ source.addData(4 -> 1) source.addData(5 -> 2) // --> across streaming batches (harder) scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 1| 1| | 5| 2| +----+---+ // Check out the internal state scala> println(q.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 2, \"numRowsUpdated\" : 1, \"memoryUsedBytes\" : 17751 } // You could use web UI's SQL tab instead // Use Details for Query source.addData(6 -> 2) scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 1| 1| | 5| 2| +----+---+ // Check out the internal state scala> println(q.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 2, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 17751 } // Restart the streaming query q.stop val q = ids. writeStream. format(\"memory\"). queryName(\"dups\"). outputMode(OutputMode.Complete). // <-- memory sink supports checkpointing for Complete output mode only trigger(Trigger.ProcessingTime(30.seconds)). option(\"checkpointLocation\", \"checkpoint-dir\"). // <-- use checkpointing to save state between restarts start // Doh! MemorySink is fine, but Complete is only available with a streaming aggregation // Answer it if you know why --> https://stackoverflow.com/q/45756997/1305344 // It's a high time to work on https://issues.apache.org/jira/browse/SPARK-21667 // to understand the low-level details (and the reason, it seems) // Disabling operation checks and starting over // ./bin/spark-shell -c spark.sql.streaming.unsupportedOperationCheck=false // it works now --> no exception! scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ +----+---+ source.addData(0 -> 1) // wait till the batch is triggered scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 0| 1| +----+---+ source.addData(1 -> 1) source.addData(2 -> 1) // wait till the batch is triggered scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ +----+---+ // What?! No rows?! It doesn't look as if it worked fine :( // Use groupBy to pass the requirement of having streaming aggregation for Complete output mode val counts = ids.groupBy(\"id\").agg(first($\"time\") as \"first_time\") scala> counts.explain == Physical Plan == *HashAggregate(keys=[id#246], functions=[first(time#255L, false)]) +- StateStoreSave [id#246], StatefulOperatorStateInfo(<unknown>,3585583b-42d7-4547-8d62-255581c48275,0,0), Append, 0 +- *HashAggregate(keys=[id#246], functions=[merge_first(time#255L, false)]) +- StateStoreRestore [id#246], StatefulOperatorStateInfo(<unknown>,3585583b-42d7-4547-8d62-255581c48275,0,0) +- *HashAggregate(keys=[id#246], functions=[merge_first(time#255L, false)]) +- *HashAggregate(keys=[id#246], functions=[partial_first(time#255L, false)]) +- *Project [cast(time#250 as bigint) AS time#255L, id#246] +- StreamingDeduplicate [id#246], StatefulOperatorStateInfo(<unknown>,3585583b-42d7-4547-8d62-255581c48275,1,0), 0 +- Exchange hashpartitioning(id#246, 200) +- *Project [cast(_1#242 as timestamp) AS time#250, _2#243 AS id#246] +- StreamingRelation MemoryStream[_1#242,_2#243], [_1#242, _2#243] val q = counts. writeStream. format(\"memory\"). queryName(\"dups\"). outputMode(OutputMode.Complete). // <-- memory sink supports checkpointing for Complete output mode only trigger(Trigger.ProcessingTime(30.seconds)). option(\"checkpointLocation\", \"checkpoint-dir\"). // <-- use checkpointing to save state between restarts start source.addData(0 -> 1) source.addData(1 -> 1) // wait till the batch is triggered scala> spark.table(\"dups\").show +---+----------+ | id|first_time| +---+----------+ | 1| 0| +---+----------+ // Publish duplicates // Check out how dropDuplicates removes duplicates // Stop the streaming query // Specify event time watermark to remove old duplicates","title":"Demo"},{"location":"operators/explain/","text":"Dataset.explain Operator \u2014 Explaining Streaming Queries \u00b6 explain (): Unit // <1> explain ( extended : Boolean ): Unit <1> Calls explain with extended flag disabled Dataset.explain operator explains query plans, i.e. prints the logical and (with extended flag enabled) physical query plans to the console. Internally, explain creates a ExplainCommand runnable command with the logical plan and extended flag. explain then executes the plan with ExplainCommand runnable command and collects the results that are printed out to the standard output. [NOTE] \u00b6 explain uses SparkSession to access the current SessionState to execute the plan. [source, scala] \u00b6 import org.apache.spark.sql.execution.command.ExplainCommand val explain = ExplainCommand(...) spark.sessionState.executePlan(explain) ==== For streaming Datasets, ExplainCommand command simply creates a IncrementalExecution for the SparkSession and the logical plan. NOTE: For the purpose of explain , IncrementalExecution is created with the output mode Append , checkpoint location <unknown> , run id a random number, current batch id 0 and offset metadata empty. They do not really matter when explaining the load-part of a streaming query. Demo \u00b6 val records = spark. readStream. format(\"rate\"). load scala> records.explain == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L] scala> records.explain(extended = true) == Parsed Logical Plan == StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] == Analyzed Logical Plan == timestamp: timestamp, value: bigint StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] == Optimized Logical Plan == StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L]","title":"explain"},{"location":"operators/explain/#datasetexplain-operator-explaining-streaming-queries","text":"explain (): Unit // <1> explain ( extended : Boolean ): Unit <1> Calls explain with extended flag disabled Dataset.explain operator explains query plans, i.e. prints the logical and (with extended flag enabled) physical query plans to the console. Internally, explain creates a ExplainCommand runnable command with the logical plan and extended flag. explain then executes the plan with ExplainCommand runnable command and collects the results that are printed out to the standard output.","title":"Dataset.explain Operator &mdash; Explaining Streaming Queries"},{"location":"operators/explain/#note","text":"explain uses SparkSession to access the current SessionState to execute the plan.","title":"[NOTE]"},{"location":"operators/explain/#source-scala","text":"import org.apache.spark.sql.execution.command.ExplainCommand val explain = ExplainCommand(...) spark.sessionState.executePlan(explain) ==== For streaming Datasets, ExplainCommand command simply creates a IncrementalExecution for the SparkSession and the logical plan. NOTE: For the purpose of explain , IncrementalExecution is created with the output mode Append , checkpoint location <unknown> , run id a random number, current batch id 0 and offset metadata empty. They do not really matter when explaining the load-part of a streaming query.","title":"[source, scala]"},{"location":"operators/explain/#demo","text":"val records = spark. readStream. format(\"rate\"). load scala> records.explain == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L] scala> records.explain(extended = true) == Parsed Logical Plan == StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] == Analyzed Logical Plan == timestamp: timestamp, value: bigint StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] == Optimized Logical Plan == StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L]","title":"Demo"},{"location":"operators/flatMapGroupsWithState/","text":"flatMapGroupsWithState \u00b6 flatMapGroupsWithState is...FIXME","title":"flatMapGroupsWithState"},{"location":"operators/flatMapGroupsWithState/#flatmapgroupswithstate","text":"flatMapGroupsWithState is...FIXME","title":"flatMapGroupsWithState"},{"location":"operators/groupBy/","text":"groupBy Operator \u2014 Streaming Aggregation \u00b6 groupBy ( cols : Column * ): RelationalGroupedDataset groupBy ( col1 : String , cols : String * ): RelationalGroupedDataset groupBy operator aggregates rows by zero, one or more columns. Demo \u00b6 val fromTopic1 = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load // extract event time et al // time,key,value /* 2017-08-23T00:00:00.002Z,1,now 2017-08-23T00:05:00.002Z,1,5 mins later 2017-08-23T00:09:00.002Z,1,9 mins later 2017-08-23T00:11:00.002Z,1,11 mins later 2017-08-23T01:00:00.002Z,1,1 hour later // late event = watermark should be (1 hour - 10 minutes) already 2017-08-23T00:49:59.002Z,1,==> SHOULD NOT BE INCLUDED in aggregation as too late <== CAUTION: FIXME SHOULD NOT BE INCLUDED is included contrary to my understanding?! */ val timedValues = fromTopic1. select('value cast \"string\"). withColumn(\"tokens\", split('value, \",\")). withColumn(\"time\", to_timestamp('tokens(0))). withColumn(\"key\", 'tokens(1) cast \"int\"). withColumn(\"value\", 'tokens(2)). select(\"time\", \"key\", \"value\") // aggregation with watermark val counts = timedValues. withWatermark(\"time\", \"10 minutes\"). groupBy(\"key\"). agg(collect_list('value) as \"values\", collect_list('time) as \"times\") // Note that StatefulOperatorStateInfo is mostly generic // since no batch-specific values are currently available // only after the first streaming batch scala> counts.explain == Physical Plan == ObjectHashAggregate(keys=[key#27], functions=[collect_list(value#33, 0, 0), collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreSave [key#27], StatefulOperatorStateInfo(<unknown>,25149816-1f14-4901-af13-896286a26d42,0,0), Append, 0 +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreRestore [key#27], StatefulOperatorStateInfo(<unknown>,25149816-1f14-4901-af13-896286a26d42,0,0) +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- ObjectHashAggregate(keys=[key#27], functions=[partial_collect_list(value#33, 0, 0), partial_collect_list(time#22-T600000ms, 0, 0)]) +- EventTimeWatermark time#22: timestamp, interval 10 minutes +- *Project [cast(split(cast(value#1 as string), ,)[0] as timestamp) AS time#22, cast(split(cast(value#1 as string), ,)[1] as int) AS key#27, split(cast(value#1 as string), ,)[2] AS value#33] +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] import org.apache.spark.sql.streaming._ import scala.concurrent.duration._ val sq = counts.writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(30.seconds)). outputMode(OutputMode.Update). // <-- only Update or Complete acceptable because of groupBy aggregation start // After StreamingQuery was started, // the physical plan is complete (with batch-specific values) scala> sq.explain == Physical Plan == ObjectHashAggregate(keys=[key#27], functions=[collect_list(value#33, 0, 0), collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreSave [key#27], StatefulOperatorStateInfo(file:/private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-635d6519-b6ca-4686-9b6b-5db0e83cfd51/state,855cec1c-25dc-4a86-ae54-c6cdd4ed02ec,0,0), Update, 0 +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreRestore [key#27], StatefulOperatorStateInfo(file:/private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-635d6519-b6ca-4686-9b6b-5db0e83cfd51/state,855cec1c-25dc-4a86-ae54-c6cdd4ed02ec,0,0) +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- ObjectHashAggregate(keys=[key#27], functions=[partial_collect_list(value#33, 0, 0), partial_collect_list(time#22-T600000ms, 0, 0)]) +- EventTimeWatermark time#22: timestamp, interval 10 minutes +- *Project [cast(split(cast(value#76 as string), ,)[0] as timestamp) AS time#22, cast(split(cast(value#76 as string), ,)[1] as int) AS key#27, split(cast(value#76 as string), ,)[2] AS value#33] +- Scan ExistingRDD[key#75,value#76,topic#77,partition#78,offset#79L,timestamp#80,timestampType#81]","title":"groupBy"},{"location":"operators/groupBy/#groupby-operator-streaming-aggregation","text":"groupBy ( cols : Column * ): RelationalGroupedDataset groupBy ( col1 : String , cols : String * ): RelationalGroupedDataset groupBy operator aggregates rows by zero, one or more columns.","title":"groupBy Operator &mdash; Streaming Aggregation"},{"location":"operators/groupBy/#demo","text":"val fromTopic1 = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load // extract event time et al // time,key,value /* 2017-08-23T00:00:00.002Z,1,now 2017-08-23T00:05:00.002Z,1,5 mins later 2017-08-23T00:09:00.002Z,1,9 mins later 2017-08-23T00:11:00.002Z,1,11 mins later 2017-08-23T01:00:00.002Z,1,1 hour later // late event = watermark should be (1 hour - 10 minutes) already 2017-08-23T00:49:59.002Z,1,==> SHOULD NOT BE INCLUDED in aggregation as too late <== CAUTION: FIXME SHOULD NOT BE INCLUDED is included contrary to my understanding?! */ val timedValues = fromTopic1. select('value cast \"string\"). withColumn(\"tokens\", split('value, \",\")). withColumn(\"time\", to_timestamp('tokens(0))). withColumn(\"key\", 'tokens(1) cast \"int\"). withColumn(\"value\", 'tokens(2)). select(\"time\", \"key\", \"value\") // aggregation with watermark val counts = timedValues. withWatermark(\"time\", \"10 minutes\"). groupBy(\"key\"). agg(collect_list('value) as \"values\", collect_list('time) as \"times\") // Note that StatefulOperatorStateInfo is mostly generic // since no batch-specific values are currently available // only after the first streaming batch scala> counts.explain == Physical Plan == ObjectHashAggregate(keys=[key#27], functions=[collect_list(value#33, 0, 0), collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreSave [key#27], StatefulOperatorStateInfo(<unknown>,25149816-1f14-4901-af13-896286a26d42,0,0), Append, 0 +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreRestore [key#27], StatefulOperatorStateInfo(<unknown>,25149816-1f14-4901-af13-896286a26d42,0,0) +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- ObjectHashAggregate(keys=[key#27], functions=[partial_collect_list(value#33, 0, 0), partial_collect_list(time#22-T600000ms, 0, 0)]) +- EventTimeWatermark time#22: timestamp, interval 10 minutes +- *Project [cast(split(cast(value#1 as string), ,)[0] as timestamp) AS time#22, cast(split(cast(value#1 as string), ,)[1] as int) AS key#27, split(cast(value#1 as string), ,)[2] AS value#33] +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] import org.apache.spark.sql.streaming._ import scala.concurrent.duration._ val sq = counts.writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(30.seconds)). outputMode(OutputMode.Update). // <-- only Update or Complete acceptable because of groupBy aggregation start // After StreamingQuery was started, // the physical plan is complete (with batch-specific values) scala> sq.explain == Physical Plan == ObjectHashAggregate(keys=[key#27], functions=[collect_list(value#33, 0, 0), collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreSave [key#27], StatefulOperatorStateInfo(file:/private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-635d6519-b6ca-4686-9b6b-5db0e83cfd51/state,855cec1c-25dc-4a86-ae54-c6cdd4ed02ec,0,0), Update, 0 +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreRestore [key#27], StatefulOperatorStateInfo(file:/private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-635d6519-b6ca-4686-9b6b-5db0e83cfd51/state,855cec1c-25dc-4a86-ae54-c6cdd4ed02ec,0,0) +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- ObjectHashAggregate(keys=[key#27], functions=[partial_collect_list(value#33, 0, 0), partial_collect_list(time#22-T600000ms, 0, 0)]) +- EventTimeWatermark time#22: timestamp, interval 10 minutes +- *Project [cast(split(cast(value#76 as string), ,)[0] as timestamp) AS time#22, cast(split(cast(value#76 as string), ,)[1] as int) AS key#27, split(cast(value#76 as string), ,)[2] AS value#33] +- Scan ExistingRDD[key#75,value#76,topic#77,partition#78,offset#79L,timestamp#80,timestampType#81]","title":"Demo"},{"location":"operators/groupByKey/","text":"groupByKey Operator \u2014 Streaming Aggregation \u00b6 groupByKey ( func : T => K ): KeyValueGroupedDataset [ K , T ] groupByKey operator aggregates rows by a typed grouping function for Arbitrary Stateful Streaming Aggregation . groupByKey creates a KeyValueGroupedDataset (with keys of type K and rows of type T ) to apply aggregation functions over groups of rows (of type T ) by key (of type K ) per the given func key-generating function. Note The type of the input argument of func is the type of rows in the Dataset (i.e. Dataset[T] ). groupByKey simply applies the func function to every row (of type T ) and associates it with a logical group per key (of type K ). func : T => K Internally, groupByKey creates a structured query with the AppendColumns unary logical operator (with the given func and the analyzed logical plan of the target Dataset that groupByKey was executed on) and creates a new QueryExecution . In the end, groupByKey creates a KeyValueGroupedDataset with the following: Encoders for K keys and T rows The new QueryExecution (with the AppendColumns unary logical operator) The output schema of the analyzed logical plan The new columns of the AppendColumns logical operator (i.e. the attributes of the key) scala> :type sq org.apache.spark.sql.Dataset[Long] val baseCode = 'A'.toInt val byUpperChar = (n: java.lang.Long) => (n % 3 + baseCode).toString val kvs = sq.groupByKey(byUpperChar) scala> :type kvs org.apache.spark.sql.KeyValueGroupedDataset[String,Long] // Peeking under the surface of KeyValueGroupedDataset import org.apache.spark.sql.catalyst.plans.logical.AppendColumns val appendColumnsOp = kvs.queryExecution.analyzed.collect { case ac: AppendColumns => ac }.head scala> println(appendColumnsOp.newColumns) List(value#7) Demo: Aggregating Orders Per Zip Code \u00b6 Go to Demo: groupByKey Streaming Aggregation in Update Mode . Demo: Aggregating Metrics Per Device \u00b6 The following example code shows how to apply groupByKey operator to a structured stream of timestamped values of different devices. // input stream import java.sql.Timestamp val signals = spark. readStream. format(\"rate\"). option(\"rowsPerSecond\", 1). load. withColumn(\"value\", $\"value\" % 10) // <-- randomize the values (just for fun) withColumn(\"deviceId\", lit(util.Random.nextInt(10))). // <-- 10 devices randomly assigned to values as[(Timestamp, Long, Int)] // <-- convert to a \"better\" type (from \"unpleasant\" Row) // stream processing using groupByKey operator // groupByKey(func: ((Timestamp, Long, Int)) => K): KeyValueGroupedDataset[K, (Timestamp, Long, Int)] // K becomes Int which is a device id val deviceId: ((Timestamp, Long, Int)) => Int = { case (_, _, deviceId) => deviceId } scala> val signalsByDevice = signals.groupByKey(deviceId) signalsByDevice: org.apache.spark.sql.KeyValueGroupedDataset[Int,(java.sql.Timestamp, Long, Int)] = org.apache.spark.sql.KeyValueGroupedDataset@19d40bc6","title":"groupByKey"},{"location":"operators/groupByKey/#groupbykey-operator-streaming-aggregation","text":"groupByKey ( func : T => K ): KeyValueGroupedDataset [ K , T ] groupByKey operator aggregates rows by a typed grouping function for Arbitrary Stateful Streaming Aggregation . groupByKey creates a KeyValueGroupedDataset (with keys of type K and rows of type T ) to apply aggregation functions over groups of rows (of type T ) by key (of type K ) per the given func key-generating function. Note The type of the input argument of func is the type of rows in the Dataset (i.e. Dataset[T] ). groupByKey simply applies the func function to every row (of type T ) and associates it with a logical group per key (of type K ). func : T => K Internally, groupByKey creates a structured query with the AppendColumns unary logical operator (with the given func and the analyzed logical plan of the target Dataset that groupByKey was executed on) and creates a new QueryExecution . In the end, groupByKey creates a KeyValueGroupedDataset with the following: Encoders for K keys and T rows The new QueryExecution (with the AppendColumns unary logical operator) The output schema of the analyzed logical plan The new columns of the AppendColumns logical operator (i.e. the attributes of the key) scala> :type sq org.apache.spark.sql.Dataset[Long] val baseCode = 'A'.toInt val byUpperChar = (n: java.lang.Long) => (n % 3 + baseCode).toString val kvs = sq.groupByKey(byUpperChar) scala> :type kvs org.apache.spark.sql.KeyValueGroupedDataset[String,Long] // Peeking under the surface of KeyValueGroupedDataset import org.apache.spark.sql.catalyst.plans.logical.AppendColumns val appendColumnsOp = kvs.queryExecution.analyzed.collect { case ac: AppendColumns => ac }.head scala> println(appendColumnsOp.newColumns) List(value#7)","title":"groupByKey Operator &mdash; Streaming Aggregation"},{"location":"operators/groupByKey/#demo-aggregating-orders-per-zip-code","text":"Go to Demo: groupByKey Streaming Aggregation in Update Mode .","title":"Demo: Aggregating Orders Per Zip Code"},{"location":"operators/groupByKey/#demo-aggregating-metrics-per-device","text":"The following example code shows how to apply groupByKey operator to a structured stream of timestamped values of different devices. // input stream import java.sql.Timestamp val signals = spark. readStream. format(\"rate\"). option(\"rowsPerSecond\", 1). load. withColumn(\"value\", $\"value\" % 10) // <-- randomize the values (just for fun) withColumn(\"deviceId\", lit(util.Random.nextInt(10))). // <-- 10 devices randomly assigned to values as[(Timestamp, Long, Int)] // <-- convert to a \"better\" type (from \"unpleasant\" Row) // stream processing using groupByKey operator // groupByKey(func: ((Timestamp, Long, Int)) => K): KeyValueGroupedDataset[K, (Timestamp, Long, Int)] // K becomes Int which is a device id val deviceId: ((Timestamp, Long, Int)) => Int = { case (_, _, deviceId) => deviceId } scala> val signalsByDevice = signals.groupByKey(deviceId) signalsByDevice: org.apache.spark.sql.KeyValueGroupedDataset[Int,(java.sql.Timestamp, Long, Int)] = org.apache.spark.sql.KeyValueGroupedDataset@19d40bc6","title":"Demo: Aggregating Metrics Per Device"},{"location":"operators/join/","text":"join Operator \u2014 Streaming Join \u00b6 join ( right : Dataset [ _ ]): DataFrame join ( right : Dataset [ _ ], joinExprs : Column ): DataFrame join ( right : Dataset [ _ ], joinExprs : Column , joinType : String ): DataFrame join ( right : Dataset [ _ ], usingColumns : Seq [ String ]): DataFrame join ( right : Dataset [ _ ], usingColumns : Seq [ String ], joinType : String ): DataFrame join ( right : Dataset [ _ ], usingColumn : String ): DataFrame Streaming Join","title":"join"},{"location":"operators/join/#join-operator-streaming-join","text":"join ( right : Dataset [ _ ]): DataFrame join ( right : Dataset [ _ ], joinExprs : Column ): DataFrame join ( right : Dataset [ _ ], joinExprs : Column , joinType : String ): DataFrame join ( right : Dataset [ _ ], usingColumns : Seq [ String ]): DataFrame join ( right : Dataset [ _ ], usingColumns : Seq [ String ], joinType : String ): DataFrame join ( right : Dataset [ _ ], usingColumn : String ): DataFrame Streaming Join","title":"join Operator &mdash; Streaming Join"},{"location":"operators/joinWith/","text":"joinWith Operator \u2014 Streaming Join \u00b6 joinWith [ U ]( other : Dataset [ U ], condition : Column ): Dataset [( T , U )] joinWith [ U ]( other : Dataset [ U ], condition : Column , joinType : String ): Dataset [( T , U )] Streaming Join","title":"joinWith"},{"location":"operators/joinWith/#joinwith-operator-streaming-join","text":"joinWith [ U ]( other : Dataset [ U ], condition : Column ): Dataset [( T , U )] joinWith [ U ]( other : Dataset [ U ], condition : Column , joinType : String ): Dataset [( T , U )] Streaming Join","title":"joinWith Operator &mdash; Streaming Join"},{"location":"operators/withWatermark/","text":"withWatermark Operator \u2014 Event-Time Watermark \u00b6 withWatermark ( eventTime : String , delayThreshold : String ): Dataset [ T ] withWatermark specifies a streaming watermark (on the given eventTime column with a delay threshold). withWatermark specifies the eventTime column for event time watermark and delayThreshold for event lateness . eventTime specifies the column to use for watermark and can be either part of Dataset from the source or custom-generated using current_time or current_timestamp functions. Note Watermark tracks a point in time before which it is assumed no more late events are supposed to arrive (and if they have, the late events are considered really late and simply dropped). Note Spark Structured Streaming uses watermark for the following: To know when a given time window aggregation (using groupBy operator with window standard function) can be finalized and thus emitted when using output modes that do not allow updates, like Append output mode. To minimize the amount of state that we need to keep for ongoing aggregations, e.g. mapGroupsWithState (for implicit state management), flatMapGroupsWithState (for user-defined state management) and dropDuplicates operators. The current watermark is computed by looking at the maximum eventTime seen across all of the partitions in a query minus a user-specified delayThreshold . Due to the cost of coordinating this value across partitions, the actual watermark used is only guaranteed to be at least delayThreshold behind the actual event time. Note In some cases Spark may still process records that arrive more than delayThreshold late.","title":"withWatermark"},{"location":"operators/withWatermark/#withwatermark-operator-event-time-watermark","text":"withWatermark ( eventTime : String , delayThreshold : String ): Dataset [ T ] withWatermark specifies a streaming watermark (on the given eventTime column with a delay threshold). withWatermark specifies the eventTime column for event time watermark and delayThreshold for event lateness . eventTime specifies the column to use for watermark and can be either part of Dataset from the source or custom-generated using current_time or current_timestamp functions. Note Watermark tracks a point in time before which it is assumed no more late events are supposed to arrive (and if they have, the late events are considered really late and simply dropped). Note Spark Structured Streaming uses watermark for the following: To know when a given time window aggregation (using groupBy operator with window standard function) can be finalized and thus emitted when using output modes that do not allow updates, like Append output mode. To minimize the amount of state that we need to keep for ongoing aggregations, e.g. mapGroupsWithState (for implicit state management), flatMapGroupsWithState (for user-defined state management) and dropDuplicates operators. The current watermark is computed by looking at the maximum eventTime seen across all of the partitions in a query minus a user-specified delayThreshold . Due to the cost of coordinating this value across partitions, the actual watermark used is only guaranteed to be at least delayThreshold behind the actual event time. Note In some cases Spark may still process records that arrive more than delayThreshold late.","title":"withWatermark Operator &mdash; Event-Time Watermark"},{"location":"operators/writeStream/","text":"writeStream Operator \u00b6 writeStream : DataStreamWriter [ T ] writeStream creates a DataStreamWriter for persisting the result of a streaming query to an external data system","title":"writeStream"},{"location":"operators/writeStream/#writestream-operator","text":"writeStream : DataStreamWriter [ T ] writeStream creates a DataStreamWriter for persisting the result of a streaming query to an external data system","title":"writeStream Operator"},{"location":"physical-operators/EventTimeWatermarkExec/","text":"EventTimeWatermarkExec Unary Physical Operator \u00b6 EventTimeWatermarkExec is a unary physical operator that represents EventTimeWatermark logical operator at execution time. Tip A unary physical operator ( UnaryExecNode ) is a physical operator with a single child physical operator. Learn more about Unary Physical Operators (and physical operators in general) in The Internals of Spark SQL online book. EventTimeWatermarkExec operator is used to extract ( project ) the values of the event-time watermark column and add them all to the EventTimeStatsAccum accumulator (and produce a EventTimeStats ). Creating Instance \u00b6 EventTimeWatermarkExec takes the following to be created: Catalyst Attribute for event time ( Spark SQL ) Delay Interval ( Spark SQL ) Child Physical Operator ( Spark SQL ) When created, EventTimeWatermarkExec registers the EventTimeStatsAccum accumulator (with the current SparkContext ). EventTimeWatermarkExec is created when StatefulAggregationStrategy execution planning strategy is executed (requested to plan a EventTimeWatermark logical operator for execution). EventTimeStats Accumulator \u00b6 eventTimeStats : EventTimeStatsAccum EventTimeWatermarkExec creates an EventTimeStatsAccum accumulator when created . When executed , EventTimeWatermarkExec uses the EventTimeStatsAccum to extract and accumulate eventTime values (as Long s) from every row in a streaming batch. Note Since the execution (data processing) happens on Spark executors, the only way to establish communication between the tasks (on the executors) and the driver is to use accumulator facility. Learn more about Accumulators in The Internals of Apache Spark online book. eventTimeStats is registered (with the current SparkContext ) when EventTimeWatermarkExec is created . eventTimeStats uses no name ( unnamed accumulator ). eventTimeStats is used to transfer the statistics (maximum, minimum, average and update count) of the long values in the event-time watermark column to be used for the following: ProgressReporter is requested for the most recent execution statistics (for max , min , avg , and watermark event-time watermark statistics) WatermarkTracker is requested to updateWatermark Executing Physical Operator \u00b6 doExecute (): RDD [ InternalRow ] doExecute is part of the SparkPlan ( Spark SQL ) abstraction. doExecute executes the child physical operator and maps over the partitions (using RDD.mapPartitions ). doExecute creates an unsafe projection (per partition) for the column with the event time in the output schema of the child physical operator. The unsafe projection is to extract event times from the (stream of) internal rows of the child physical operator. For every row in a partition, doExecute requests the eventTimeStats accumulator to accumulate the event time . Note The event time value is in seconds (not millis as the value is divided by 1000 ). Output Attributes \u00b6 output : Seq [ Attribute ] output is part of the QueryPlan ( Spark SQL ) abstraction. output requests the child physical operator for the output attributes to find the event time attribute and any other column with metadata that contains spark.watermarkDelayMs key. For the event time attribute, output updates the metadata to include the delay interval for the spark.watermarkDelayMs key. For any other column (not the event time attribute ) with the spark.watermarkDelayMs key, output removes the key from the attribute metadata. Demo \u00b6 Check out Demo: Streaming Watermark with Aggregation in Append Output Mode to deep dive into the internals of Streaming Watermark .","title":"EventTimeWatermarkExec"},{"location":"physical-operators/EventTimeWatermarkExec/#eventtimewatermarkexec-unary-physical-operator","text":"EventTimeWatermarkExec is a unary physical operator that represents EventTimeWatermark logical operator at execution time. Tip A unary physical operator ( UnaryExecNode ) is a physical operator with a single child physical operator. Learn more about Unary Physical Operators (and physical operators in general) in The Internals of Spark SQL online book. EventTimeWatermarkExec operator is used to extract ( project ) the values of the event-time watermark column and add them all to the EventTimeStatsAccum accumulator (and produce a EventTimeStats ).","title":"EventTimeWatermarkExec Unary Physical Operator"},{"location":"physical-operators/EventTimeWatermarkExec/#creating-instance","text":"EventTimeWatermarkExec takes the following to be created: Catalyst Attribute for event time ( Spark SQL ) Delay Interval ( Spark SQL ) Child Physical Operator ( Spark SQL ) When created, EventTimeWatermarkExec registers the EventTimeStatsAccum accumulator (with the current SparkContext ). EventTimeWatermarkExec is created when StatefulAggregationStrategy execution planning strategy is executed (requested to plan a EventTimeWatermark logical operator for execution).","title":"Creating Instance"},{"location":"physical-operators/EventTimeWatermarkExec/#eventtimestats-accumulator","text":"eventTimeStats : EventTimeStatsAccum EventTimeWatermarkExec creates an EventTimeStatsAccum accumulator when created . When executed , EventTimeWatermarkExec uses the EventTimeStatsAccum to extract and accumulate eventTime values (as Long s) from every row in a streaming batch. Note Since the execution (data processing) happens on Spark executors, the only way to establish communication between the tasks (on the executors) and the driver is to use accumulator facility. Learn more about Accumulators in The Internals of Apache Spark online book. eventTimeStats is registered (with the current SparkContext ) when EventTimeWatermarkExec is created . eventTimeStats uses no name ( unnamed accumulator ). eventTimeStats is used to transfer the statistics (maximum, minimum, average and update count) of the long values in the event-time watermark column to be used for the following: ProgressReporter is requested for the most recent execution statistics (for max , min , avg , and watermark event-time watermark statistics) WatermarkTracker is requested to updateWatermark","title":" EventTimeStats Accumulator"},{"location":"physical-operators/EventTimeWatermarkExec/#executing-physical-operator","text":"doExecute (): RDD [ InternalRow ] doExecute is part of the SparkPlan ( Spark SQL ) abstraction. doExecute executes the child physical operator and maps over the partitions (using RDD.mapPartitions ). doExecute creates an unsafe projection (per partition) for the column with the event time in the output schema of the child physical operator. The unsafe projection is to extract event times from the (stream of) internal rows of the child physical operator. For every row in a partition, doExecute requests the eventTimeStats accumulator to accumulate the event time . Note The event time value is in seconds (not millis as the value is divided by 1000 ).","title":" Executing Physical Operator"},{"location":"physical-operators/EventTimeWatermarkExec/#output-attributes","text":"output : Seq [ Attribute ] output is part of the QueryPlan ( Spark SQL ) abstraction. output requests the child physical operator for the output attributes to find the event time attribute and any other column with metadata that contains spark.watermarkDelayMs key. For the event time attribute, output updates the metadata to include the delay interval for the spark.watermarkDelayMs key. For any other column (not the event time attribute ) with the spark.watermarkDelayMs key, output removes the key from the attribute metadata.","title":" Output Attributes"},{"location":"physical-operators/EventTimeWatermarkExec/#demo","text":"Check out Demo: Streaming Watermark with Aggregation in Append Output Mode to deep dive into the internals of Streaming Watermark .","title":"Demo"},{"location":"physical-operators/FlatMapGroupsWithStateExec/","text":"FlatMapGroupsWithStateExec Unary Physical Operator \u00b6 FlatMapGroupsWithStateExec is a unary physical operator that represents FlatMapGroupsWithState logical operator at execution time. Note A unary physical operator ( UnaryExecNode ) is a physical operator with a single child physical operator. Read up on UnaryExecNode (and physical operators in general) in The Internals of Spark SQL online book. FlatMapGroupsWithStateExec is an ObjectProducerExec physical operator and so produces a single output object . Tip Read up on ObjectProducerExec physical operator in The Internals of Spark SQL online book. Tip Check out Demo: Internals of FlatMapGroupsWithStateExec Physical Operator . Note FlatMapGroupsWithStateExec is given an OutputMode when created, but it does not seem to be used at all. Check out the question What's the purpose of OutputMode in flatMapGroupsWithState? How/where is it used? on StackOverflow. Creating Instance \u00b6 FlatMapGroupsWithStateExec takes the following to be created: User-defined state function that is applied to every group (of type (Any, Iterator[Any], LogicalGroupState[Any]) => Iterator[Any] ) Deserializer expression for keys Deserializer expression for values Grouping attributes (as used for grouping in KeyValueGroupedDataset for mapGroupsWithState or flatMapGroupsWithState operators) Data attributes Output object attribute (that is the reference to the single object field this operator outputs) Optional StatefulOperatorStateInfo State encoder ( ExpressionEncoder[Any] ) State format version OutputMode GroupStateTimeout Optional Batch Processing Time Optional Event-Time Watermark Child physical operator FlatMapGroupsWithStateExec is created when FlatMapGroupsWithStateStrategy execution planning strategy is executed (and plans a FlatMapGroupsWithState logical operator for execution). Executing Physical Operator \u00b6 doExecute (): RDD [ InternalRow ] doExecute first initializes the metrics (which happens on the driver). doExecute then requests the child physical operator to execute (and generate an RDD[InternalRow] ). doExecute uses StateStoreOps to create a StateStoreRDD with a storeUpdateFunction that does the following (for a partition): Creates an InputProcessor for a given StateStore (only when the GroupStateTimeout is EventTimeTimeout ) Filters out late data based on the event-time watermark , i.e. rows from a given Iterator[InternalRow] that are older than the event-time watermark are excluded from the steps that follow Requests the InputProcessor to create an iterator of a new data processed from the (possibly filtered) iterator Requests the InputProcessor to create an iterator of a timed-out state data Creates an iterator by concatenating the above iterators (with the new data processed first) In the end, creates a CompletionIterator that executes a completion function ( completionFunction ) after it has successfully iterated through all the elements (i.e. when a client has consumed all the rows). The completion method requests the given StateStore to commit changes followed by setting the store-specific metrics doExecute is part of Spark SQL's SparkPlan abstraction. Performance Metrics \u00b6 FlatMapGroupsWithStateExec uses the performance metrics of StateStoreWriter . StateStoreWriter \u00b6 FlatMapGroupsWithStateExec is a stateful physical operator that can write to a state store (and MicroBatchExecution requests whether to run another batch or not based on the GroupStateTimeout ). FlatMapGroupsWithStateExec uses the GroupStateTimeout (and possibly the updated metadata ) when asked whether to run another batch or not (when MicroBatchExecution is requested to construct the next streaming micro-batch when requested to run the activated streaming query ). Streaming Event-Time Watermark Support \u00b6 FlatMapGroupsWithStateExec is a physical operator that supports streaming event-time watermark . FlatMapGroupsWithStateExec is given the optional event time watermark when created. The event-time watermark is initially undefined ( None ) when planned for execution (in FlatMapGroupsWithStateStrategy execution planning strategy). Note FlatMapGroupsWithStateStrategy converts FlatMapGroupsWithState unary logical operator to FlatMapGroupsWithStateExec physical operator with undefined StatefulOperatorStateInfo , batchTimestampMs , and eventTimeWatermark . The event-time watermark (with the StatefulOperatorStateInfo and the batchTimestampMs ) is only defined to the current event-time watermark of the given OffsetSeqMetadata when IncrementalExecution query execution pipeline is requested to apply the state preparation rule (as part of the preparations rules). Note The preparations rules are executed (applied to a physical query plan) at the executedPlan phase of Structured Query Execution Pipeline to generate an optimized physical query plan ready for execution). Read up on Structured Query Execution Pipeline in The Internals of Spark SQL online book. IncrementalExecution is used as the lastExecution of the available streaming query execution engines . It is created in the queryPlanning phase (of the MicroBatchExecution and ContinuousExecution execution engines) based on the current OffsetSeqMetadata . Note The optional event-time watermark can only be defined when the state preparation rule is executed which is at the executedPlan phase of Structured Query Execution Pipeline which is also part of the queryPlanning phase. StateManager \u00b6 stateManager : StateManager While being created, FlatMapGroupsWithStateExec creates a StateManager (with the state encoder and the isTimeoutEnabled flag). A StateManager is created per state format version that is given while creating a FlatMapGroupsWithStateExec (to choose between the available implementations ). The state format version is controlled by spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion internal configuration property. The StateManager is used exclusively when FlatMapGroupsWithStateExec physical operator is executed for the following: State schema (for the value schema of a StateStoreRDD ) State data for a key in a StateStore while processing new data All state data (for all keys) in a StateStore while processing timed-out state data Removing the state for a key from a StateStore when all rows have been processed Persisting the state for a key in a StateStore when all rows have been processed keyExpressions Method \u00b6 keyExpressions : Seq [ Attribute ] keyExpressions simply returns the grouping attributes . keyExpressions is part of the WatermarkSupport abstraction. Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not \u00b6 shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ): Boolean shouldRunAnotherBatch uses the GroupStateTimeout as follows: With EventTimeTimeout , shouldRunAnotherBatch is true only when the event-time watermark is defined and is older (below) the event-time watermark of the given OffsetSeqMetadata With NoTimeout (and other GroupStateTimeouts if there were any), shouldRunAnotherBatch is always false With ProcessingTimeTimeout , shouldRunAnotherBatch is always true shouldRunAnotherBatch is part of the StateStoreWriter abstraction. Internal Properties \u00b6 isTimeoutEnabled Flag \u00b6 Flag that says whether the GroupStateTimeout is not NoTimeout Used when: FlatMapGroupsWithStateExec is created (and creates the internal StateManager ) InputProcessor is requested to processTimedOutState watermarkPresent Flag \u00b6 Flag that says whether the child physical operator has a watermark attribute (among the output attributes). Used when InputProcessor is requested to callFunctionAndUpdateState Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec=ALL Refer to Logging .","title":"FlatMapGroupsWithStateExec"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#flatmapgroupswithstateexec-unary-physical-operator","text":"FlatMapGroupsWithStateExec is a unary physical operator that represents FlatMapGroupsWithState logical operator at execution time. Note A unary physical operator ( UnaryExecNode ) is a physical operator with a single child physical operator. Read up on UnaryExecNode (and physical operators in general) in The Internals of Spark SQL online book. FlatMapGroupsWithStateExec is an ObjectProducerExec physical operator and so produces a single output object . Tip Read up on ObjectProducerExec physical operator in The Internals of Spark SQL online book. Tip Check out Demo: Internals of FlatMapGroupsWithStateExec Physical Operator . Note FlatMapGroupsWithStateExec is given an OutputMode when created, but it does not seem to be used at all. Check out the question What's the purpose of OutputMode in flatMapGroupsWithState? How/where is it used? on StackOverflow.","title":"FlatMapGroupsWithStateExec Unary Physical Operator"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#creating-instance","text":"FlatMapGroupsWithStateExec takes the following to be created: User-defined state function that is applied to every group (of type (Any, Iterator[Any], LogicalGroupState[Any]) => Iterator[Any] ) Deserializer expression for keys Deserializer expression for values Grouping attributes (as used for grouping in KeyValueGroupedDataset for mapGroupsWithState or flatMapGroupsWithState operators) Data attributes Output object attribute (that is the reference to the single object field this operator outputs) Optional StatefulOperatorStateInfo State encoder ( ExpressionEncoder[Any] ) State format version OutputMode GroupStateTimeout Optional Batch Processing Time Optional Event-Time Watermark Child physical operator FlatMapGroupsWithStateExec is created when FlatMapGroupsWithStateStrategy execution planning strategy is executed (and plans a FlatMapGroupsWithState logical operator for execution).","title":"Creating Instance"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#executing-physical-operator","text":"doExecute (): RDD [ InternalRow ] doExecute first initializes the metrics (which happens on the driver). doExecute then requests the child physical operator to execute (and generate an RDD[InternalRow] ). doExecute uses StateStoreOps to create a StateStoreRDD with a storeUpdateFunction that does the following (for a partition): Creates an InputProcessor for a given StateStore (only when the GroupStateTimeout is EventTimeTimeout ) Filters out late data based on the event-time watermark , i.e. rows from a given Iterator[InternalRow] that are older than the event-time watermark are excluded from the steps that follow Requests the InputProcessor to create an iterator of a new data processed from the (possibly filtered) iterator Requests the InputProcessor to create an iterator of a timed-out state data Creates an iterator by concatenating the above iterators (with the new data processed first) In the end, creates a CompletionIterator that executes a completion function ( completionFunction ) after it has successfully iterated through all the elements (i.e. when a client has consumed all the rows). The completion method requests the given StateStore to commit changes followed by setting the store-specific metrics doExecute is part of Spark SQL's SparkPlan abstraction.","title":" Executing Physical Operator"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#performance-metrics","text":"FlatMapGroupsWithStateExec uses the performance metrics of StateStoreWriter .","title":" Performance Metrics"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#statestorewriter","text":"FlatMapGroupsWithStateExec is a stateful physical operator that can write to a state store (and MicroBatchExecution requests whether to run another batch or not based on the GroupStateTimeout ). FlatMapGroupsWithStateExec uses the GroupStateTimeout (and possibly the updated metadata ) when asked whether to run another batch or not (when MicroBatchExecution is requested to construct the next streaming micro-batch when requested to run the activated streaming query ).","title":" StateStoreWriter"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#streaming-event-time-watermark-support","text":"FlatMapGroupsWithStateExec is a physical operator that supports streaming event-time watermark . FlatMapGroupsWithStateExec is given the optional event time watermark when created. The event-time watermark is initially undefined ( None ) when planned for execution (in FlatMapGroupsWithStateStrategy execution planning strategy). Note FlatMapGroupsWithStateStrategy converts FlatMapGroupsWithState unary logical operator to FlatMapGroupsWithStateExec physical operator with undefined StatefulOperatorStateInfo , batchTimestampMs , and eventTimeWatermark . The event-time watermark (with the StatefulOperatorStateInfo and the batchTimestampMs ) is only defined to the current event-time watermark of the given OffsetSeqMetadata when IncrementalExecution query execution pipeline is requested to apply the state preparation rule (as part of the preparations rules). Note The preparations rules are executed (applied to a physical query plan) at the executedPlan phase of Structured Query Execution Pipeline to generate an optimized physical query plan ready for execution). Read up on Structured Query Execution Pipeline in The Internals of Spark SQL online book. IncrementalExecution is used as the lastExecution of the available streaming query execution engines . It is created in the queryPlanning phase (of the MicroBatchExecution and ContinuousExecution execution engines) based on the current OffsetSeqMetadata . Note The optional event-time watermark can only be defined when the state preparation rule is executed which is at the executedPlan phase of Structured Query Execution Pipeline which is also part of the queryPlanning phase.","title":" Streaming Event-Time Watermark Support"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#statemanager","text":"stateManager : StateManager While being created, FlatMapGroupsWithStateExec creates a StateManager (with the state encoder and the isTimeoutEnabled flag). A StateManager is created per state format version that is given while creating a FlatMapGroupsWithStateExec (to choose between the available implementations ). The state format version is controlled by spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion internal configuration property. The StateManager is used exclusively when FlatMapGroupsWithStateExec physical operator is executed for the following: State schema (for the value schema of a StateStoreRDD ) State data for a key in a StateStore while processing new data All state data (for all keys) in a StateStore while processing timed-out state data Removing the state for a key from a StateStore when all rows have been processed Persisting the state for a key in a StateStore when all rows have been processed","title":" StateManager"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#keyexpressions-method","text":"keyExpressions : Seq [ Attribute ] keyExpressions simply returns the grouping attributes . keyExpressions is part of the WatermarkSupport abstraction.","title":" keyExpressions Method"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#checking-out-whether-last-batch-execution-requires-another-non-data-batch-or-not","text":"shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ): Boolean shouldRunAnotherBatch uses the GroupStateTimeout as follows: With EventTimeTimeout , shouldRunAnotherBatch is true only when the event-time watermark is defined and is older (below) the event-time watermark of the given OffsetSeqMetadata With NoTimeout (and other GroupStateTimeouts if there were any), shouldRunAnotherBatch is always false With ProcessingTimeTimeout , shouldRunAnotherBatch is always true shouldRunAnotherBatch is part of the StateStoreWriter abstraction.","title":" Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#internal-properties","text":"","title":"Internal Properties"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#istimeoutenabled-flag","text":"Flag that says whether the GroupStateTimeout is not NoTimeout Used when: FlatMapGroupsWithStateExec is created (and creates the internal StateManager ) InputProcessor is requested to processTimedOutState","title":" isTimeoutEnabled Flag"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#watermarkpresent-flag","text":"Flag that says whether the child physical operator has a watermark attribute (among the output attributes). Used when InputProcessor is requested to callFunctionAndUpdateState","title":" watermarkPresent Flag"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#logging","text":"Enable ALL logging level for org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec=ALL Refer to Logging .","title":"Logging"},{"location":"physical-operators/MicroBatchScanExec/","text":"MicroBatchScanExec Physical Operator \u00b6 MicroBatchScanExec is...FIXME","title":"MicroBatchScanExec"},{"location":"physical-operators/MicroBatchScanExec/#microbatchscanexec-physical-operator","text":"MicroBatchScanExec is...FIXME","title":"MicroBatchScanExec Physical Operator"},{"location":"physical-operators/StateStoreReader/","text":"== [[StateStoreReader]] StateStoreReader StateStoreReader is...FIXME","title":"StateStoreReader"},{"location":"physical-operators/StateStoreRestoreExec/","text":"StateStoreRestoreExec Unary Physical Operator \u00b6 StateStoreRestoreExec is a unary physical operator that restores (reads) a streaming state from a state store (for the keys from the < > physical operator). [NOTE] \u00b6 A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode ] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. \u00b6 StateStoreRestoreExec is < > exclusively when StatefulAggregationStrategy execution planning strategy is requested to plan a streaming aggregation for execution ( Aggregate logical operators in the logical plan of a streaming query). The optional < > is initially undefined (i.e. when StateStoreRestoreExec is < >). StateStoreRestoreExec is updated to hold the streaming batch-specific execution property when IncrementalExecution prepares a streaming physical plan for execution (and state preparation rule is executed when StreamExecution MicroBatchExecution.md#runBatch-queryPlanning[plans a streaming query] for a streaming batch). When < >, StateStoreRestoreExec executes the < > physical operator and creates a StateStoreRDD to map over partitions with storeUpdateFunction that restores the state for the keys in the input rows if available. [[output]] The output schema of StateStoreRestoreExec is exactly the < >'s output schema. [[outputPartitioning]] The output partitioning of StateStoreRestoreExec is exactly the < >'s output partitioning. === [[metrics]] Performance Metrics (SQLMetrics) [cols=\"1m,1,3\",options=\"header\",width=\"100%\"] |=== | Key | Name (in UI) | Description | numOutputRows | number of output rows | [[numOutputRows]] The number of input rows from the < > physical operator (for which StateStoreRestoreExec tried to find the state) |=== .StateStoreRestoreExec in web UI (Details for Query) image::images/StateStoreRestoreExec-webui-query-details.png[align=\"center\"] Creating Instance \u00b6 StateStoreRestoreExec takes the following to be created: [[keyExpressions]] Key expressions (Catalyst attributes for the grouping keys) [[stateInfo]] Optional StatefulOperatorStateInfo (default: None ) [[stateFormatVersion]] Version of the state format (based on the spark.sql.streaming.aggregation.stateFormatVersion configuration property) [[child]] Child physical operator ( SparkPlan ) === [[stateManager]] StateStoreRestoreExec and StreamingAggregationStateManager -- stateManager Property [source, scala] \u00b6 stateManager: StreamingAggregationStateManager \u00b6 stateManager is a StreamingAggregationStateManager that is created together with StateStoreRestoreExec . The StreamingAggregationStateManager is created for the < >, the output schema of the < > physical operator and the < >. The StreamingAggregationStateManager is used when StateStoreRestoreExec is requested to < > for the following: Schema of the values in a state store Extracting the columns for the key from the input row Looking up the value of a key from a state store Executing Physical Operator (Generating RDD[InternalRow]) \u00b6 doExecute (): RDD [ InternalRow ] doExecute is part of the SparkPlan abstraction. Internally, doExecute executes < > physical operator and creates a StateStoreRDD with storeUpdateFunction that does the following per < > operator's RDD partition: Generates an unsafe projection to access the key field (using < > and the output schema of < > operator). For every input row (as InternalRow ) Extracts the key from the row (using the unsafe projection above) Gets the saved state in StateStore for the key if available (it might not be if the key appeared in the input the first time) Increments < > metric (that in the end is the number of rows from the < > operator) Generates collection made up of the current row and possibly the state for the key if available NOTE: The number of rows from StateStoreRestoreExec is the number of rows from the < > operator with additional rows for the saved state. NOTE: There is no way in StateStoreRestoreExec to find out how many rows had associated state available in a state store. You would have to use the corresponding StateStoreSaveExec operator's StateStoreSaveExec.md#metrics[metrics] (most likely number of total state rows but that could depend on the output mode).","title":"StateStoreRestoreExec"},{"location":"physical-operators/StateStoreRestoreExec/#statestorerestoreexec-unary-physical-operator","text":"StateStoreRestoreExec is a unary physical operator that restores (reads) a streaming state from a state store (for the keys from the < > physical operator).","title":"StateStoreRestoreExec Unary Physical Operator"},{"location":"physical-operators/StateStoreRestoreExec/#note","text":"A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator.","title":"[NOTE]"},{"location":"physical-operators/StateStoreRestoreExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-sparkplanhtmlunaryexecnode-and-physical-operators-in-general-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-book","text":"StateStoreRestoreExec is < > exclusively when StatefulAggregationStrategy execution planning strategy is requested to plan a streaming aggregation for execution ( Aggregate logical operators in the logical plan of a streaming query). The optional < > is initially undefined (i.e. when StateStoreRestoreExec is < >). StateStoreRestoreExec is updated to hold the streaming batch-specific execution property when IncrementalExecution prepares a streaming physical plan for execution (and state preparation rule is executed when StreamExecution MicroBatchExecution.md#runBatch-queryPlanning[plans a streaming query] for a streaming batch). When < >, StateStoreRestoreExec executes the < > physical operator and creates a StateStoreRDD to map over partitions with storeUpdateFunction that restores the state for the keys in the input rows if available. [[output]] The output schema of StateStoreRestoreExec is exactly the < >'s output schema. [[outputPartitioning]] The output partitioning of StateStoreRestoreExec is exactly the < >'s output partitioning. === [[metrics]] Performance Metrics (SQLMetrics) [cols=\"1m,1,3\",options=\"header\",width=\"100%\"] |=== | Key | Name (in UI) | Description | numOutputRows | number of output rows | [[numOutputRows]] The number of input rows from the < > physical operator (for which StateStoreRestoreExec tried to find the state) |=== .StateStoreRestoreExec in web UI (Details for Query) image::images/StateStoreRestoreExec-webui-query-details.png[align=\"center\"]","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book."},{"location":"physical-operators/StateStoreRestoreExec/#creating-instance","text":"StateStoreRestoreExec takes the following to be created: [[keyExpressions]] Key expressions (Catalyst attributes for the grouping keys) [[stateInfo]] Optional StatefulOperatorStateInfo (default: None ) [[stateFormatVersion]] Version of the state format (based on the spark.sql.streaming.aggregation.stateFormatVersion configuration property) [[child]] Child physical operator ( SparkPlan ) === [[stateManager]] StateStoreRestoreExec and StreamingAggregationStateManager -- stateManager Property","title":"Creating Instance"},{"location":"physical-operators/StateStoreRestoreExec/#source-scala","text":"","title":"[source, scala]"},{"location":"physical-operators/StateStoreRestoreExec/#statemanager-streamingaggregationstatemanager","text":"stateManager is a StreamingAggregationStateManager that is created together with StateStoreRestoreExec . The StreamingAggregationStateManager is created for the < >, the output schema of the < > physical operator and the < >. The StreamingAggregationStateManager is used when StateStoreRestoreExec is requested to < > for the following: Schema of the values in a state store Extracting the columns for the key from the input row Looking up the value of a key from a state store","title":"stateManager: StreamingAggregationStateManager"},{"location":"physical-operators/StateStoreRestoreExec/#executing-physical-operator-generating-rddinternalrow","text":"doExecute (): RDD [ InternalRow ] doExecute is part of the SparkPlan abstraction. Internally, doExecute executes < > physical operator and creates a StateStoreRDD with storeUpdateFunction that does the following per < > operator's RDD partition: Generates an unsafe projection to access the key field (using < > and the output schema of < > operator). For every input row (as InternalRow ) Extracts the key from the row (using the unsafe projection above) Gets the saved state in StateStore for the key if available (it might not be if the key appeared in the input the first time) Increments < > metric (that in the end is the number of rows from the < > operator) Generates collection made up of the current row and possibly the state for the key if available NOTE: The number of rows from StateStoreRestoreExec is the number of rows from the < > operator with additional rows for the saved state. NOTE: There is no way in StateStoreRestoreExec to find out how many rows had associated state available in a state store. You would have to use the corresponding StateStoreSaveExec operator's StateStoreSaveExec.md#metrics[metrics] (most likely number of total state rows but that could depend on the output mode).","title":" Executing Physical Operator (Generating RDD[InternalRow])"},{"location":"physical-operators/StateStoreSaveExec/","text":"StateStoreSaveExec Unary Physical Operator \u00b6 StateStoreSaveExec is a unary physical operator that saves a streaming state to a state store with support for streaming watermark . Note A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode ] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. StateStoreSaveExec is < > when StatefulAggregationStrategy execution planning strategy is requested to plan a streaming aggregation for execution ( Aggregate logical operators in the logical plan of a streaming query). The optional properties, i.e. the < >, the < >, and the < >, are initially undefined when StateStoreSaveExec is < >. StateStoreSaveExec is updated to hold execution-specific configuration when IncrementalExecution is requested to prepare the logical plan (of a streaming query) for execution (when the state preparation rule is executed). Note Unlike StateStoreRestoreExec operator, StateStoreSaveExec takes output mode and event time watermark when created . When < >, StateStoreSaveExec creates a StateStoreRDD to map over partitions with storeUpdateFunction that manages the StateStore . Note The number of partitions of StateStoreRDD (and hence the number of Spark tasks) is what was defined for the < > physical plan. There will be that many StateStores as there are partitions in StateStoreRDD . NOTE: StateStoreSaveExec < > differently per output mode. When < >, StateStoreSaveExec executes the < > physical operator and creates a StateStoreRDD (with storeUpdateFunction specific to the output mode). [[output]] The output schema of StateStoreSaveExec is exactly the < >'s output schema. [[outputPartitioning]] The output partitioning of StateStoreSaveExec is exactly the < >'s output partitioning. [[stateManager]] StateStoreRestoreExec uses a StreamingAggregationStateManager (that is created for the keyExpressions , the output of the child physical operator and the stateFormatVersion ). [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.StateStoreSaveExec to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.StateStoreSaveExec=ALL Refer to < >. \u00b6 === [[metrics]] Performance Metrics (SQLMetrics) StateStoreSaveExec uses the performance metrics as other stateful physical operators that write to a state store . The following table shows how the performance metrics are computed (and so their exact meaning). [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Name (in web UI) | Description | total time to update rows a| [[allUpdatesTimeMs]] Time taken to read the input rows and store them in a state store (possibly filtering out expired rows per watermarkPredicateForData predicate) The number of rows stored is the < > metric For < > output mode, the time taken to filter out expired rows (per the required watermarkPredicateForData predicate) and the < > to store rows in a state store For < > output mode, the time taken to go over all the input rows and request the < > to store rows in a state store For < > output mode, the time taken to filter out expired rows (per the optional watermarkPredicateForData predicate) and the < > to store rows in a state store | total time to remove rows a| [[allRemovalsTimeMs]] For < > output mode, the time taken for the < > to remove all expired entries from a state store (per watermarkPredicateForKeys predicate) that is the total time of iterating over all entries in the state store (the number of entries removed from a state store is the difference between the number of output rows of the child operator and the number of total state rows metric) For < > output mode, always 0 For < > output mode, the time taken for the < > to remove all expired entries from a state store (per watermarkPredicateForKeys predicate) | time to commit changes a| [[commitTimeMs]] Time taken for the StreamingAggregationStateManager to commit changes to a state store | number of output rows a| [[numOutputRows]] For < > output mode, the metric does not seem to be used For < > output mode, the number of rows in a StateStore (i.e. all values in a StateStore in the < > that should be equivalent to the < > metric) For < > output mode, the number of rows that the < > was requested to store in a state store (that did not expire per the optional watermarkPredicateForData predicate) that is equivalent to the < > metric) | number of total state rows a| [[numTotalStateRows]] Number of entries in a state store at the very end of < > (aka numTotalStateRows ) Corresponds to numRowsTotal attribute in stateOperators in StreamingQueryProgress (and is available as sq.lastProgress.stateOperators for an operator). | number of updated state rows a| [[numUpdatedStateRows]] Number of the entries that were stored as updates in a state store in a trigger and for the keys in the result rows of the upstream physical operator (aka numUpdatedStateRows ) For < > output mode, the number of input rows that have not expired yet (per the required watermarkPredicateForData predicate) and that the < > was requested to store in a state store (the time taken is the < > metric) For < > output mode, the number of input rows (which should be exactly the number of output rows from the < >) For < > output mode, the number of rows that the < > was requested to store in a state store (that did not expire per the optional watermarkPredicateForData predicate) that is equivalent to the < > metric) Corresponds to numRowsUpdated attribute in stateOperators in StreamingQueryProgress (and is available as sq.lastProgress.stateOperators for an operator). | memory used by state a| [[stateMemory]] Estimated memory used by a StateStore (aka stateMemory ) after StateStoreSaveExec finished < > (per the StateStoreMetrics of the StateStore ) |=== Creating Instance \u00b6 StateStoreSaveExec takes the following to be created: [[keyExpressions]] Key expressions (Catalyst attributes for the grouping keys) [[stateInfo]] Execution-specific StatefulOperatorStateInfo (default: None ) [[outputMode]] Execution-specific OutputMode (default: None ) [[eventTimeWatermark]] Event-time watermark (default: None ) [[stateFormatVersion]] Version of the state format (based on the spark.sql.streaming.aggregation.stateFormatVersion configuration property) [[child]] Child physical operator ( SparkPlan ) Executing Physical Operator \u00b6 doExecute (): RDD [ InternalRow ] doExecute is part of the SparkPlan abstraction (Spark SQL). Internally, doExecute initializes metrics . NOTE: doExecute requires that the optional < > is at this point defined (that should have happened when IncrementalExecution had prepared a streaming aggregation for execution ). doExecute executes < > physical operator and creates a StateStoreRDD with storeUpdateFunction that: Generates an unsafe projection to access the key field (using < > and the output schema of < >). Branches off per < >: < >, < > and < >. doExecute throws an UnsupportedOperationException when executed with an invalid < >: Invalid output mode: [outputMode] ==== [[doExecute-Append]] Append Output Mode NOTE: Append is the default output mode when not specified explicitly. NOTE: Append output mode requires that a streaming query defines event-time watermark (e.g. using withWatermark operator) on the event-time column that is used in aggregation (directly or using window standard function). For Append output mode, doExecute does the following: Finds late (aggregate) rows from < > physical operator (that have expired per watermark ) Stores the late rows in the state store and increments the < > metric Gets all the added (late) rows from the state store Creates an iterator that removes the late rows from the state store when requested the next row and in the end commits the state updates TIP: Refer to < > for an example of StateStoreSaveExec with Append output mode. CAUTION: FIXME When is \"Filtering state store on:\" printed out? Uses watermarkPredicateForData predicate to exclude matching rows and (like in Complete output mode) stores all the remaining rows in StateStore . (like in < > output mode) While storing the rows, increments < > metric (for every row) and records the total time in < > metric. Takes all the rows from StateStore and returns a NextIterator that: In getNext , finds the first row that matches watermarkPredicateForKeys predicate, removes it from StateStore , and returns it back. + If no row was found, getNext also marks the iterator as finished. In close , records the time to iterate over all the rows in < > metric, commits the updates to StateStore followed by recording the time in < > metric and recording StateStore metrics . Complete Output Mode \u00b6 For Complete output mode, doExecute does the following: Takes all UnsafeRow rows (from the parent iterator) Stores the rows by key in the state store eagerly (i.e. all rows that are available in the parent iterator before proceeding) Commits the state updates In the end, reads the key-row pairs from the state store and passes the rows along (i.e. to the following physical operator) The number of keys stored in the state store is recorded in < > metric. NOTE: In Complete output mode the < > metric is exactly the < > metric. TIP: Refer to < > for an example of StateStoreSaveExec with Complete output mode. Stores all rows (as UnsafeRow ) in StateStore . While storing the rows, increments < > metric (for every row) and records the total time in < > metric. Records 0 in < > metric. Commits the state updates to StateStore and records the time in < > metric. Records StateStore metrics In the end, takes all the rows stored in StateStore and increments numOutputRows metric. Update Output Mode \u00b6 For Update output mode, doExecute returns an iterator that filters out late aggregate rows (per watermark if defined) and stores the \"young\" rows in the state store (one by one, i.e. every next ). With no more rows available, that removes the late rows from the state store (all at once) and commits the state updates . TIP: Refer to < > for an example of StateStoreSaveExec with Update output mode. doExecute returns Iterator of rows that uses watermarkPredicateForData predicate to filter out late rows. In hasNext , when rows are no longer available: Records the total time to iterate over all the rows in < > metric. removeKeysOlderThanWatermark and records the time in < > metric. Commits the updates to StateStore and records the time in < > metric. Records StateStore metrics In next , stores a row in StateStore and increments numOutputRows and numUpdatedStateRows metrics. === [[shouldRunAnotherBatch]] Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not -- shouldRunAnotherBatch Method shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ): Boolean shouldRunAnotherBatch is positive ( true ) when all of the following are met: < > is either Append or Update < > is defined and is older (below) the current event-time watermark (of the given OffsetSeqMetadata ) Otherwise, shouldRunAnotherBatch is negative ( false ). shouldRunAnotherBatch is part of the StateStoreWriter abstraction.","title":"StateStoreSaveExec"},{"location":"physical-operators/StateStoreSaveExec/#statestoresaveexec-unary-physical-operator","text":"StateStoreSaveExec is a unary physical operator that saves a streaming state to a state store with support for streaming watermark . Note A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode ] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. StateStoreSaveExec is < > when StatefulAggregationStrategy execution planning strategy is requested to plan a streaming aggregation for execution ( Aggregate logical operators in the logical plan of a streaming query). The optional properties, i.e. the < >, the < >, and the < >, are initially undefined when StateStoreSaveExec is < >. StateStoreSaveExec is updated to hold execution-specific configuration when IncrementalExecution is requested to prepare the logical plan (of a streaming query) for execution (when the state preparation rule is executed). Note Unlike StateStoreRestoreExec operator, StateStoreSaveExec takes output mode and event time watermark when created . When < >, StateStoreSaveExec creates a StateStoreRDD to map over partitions with storeUpdateFunction that manages the StateStore . Note The number of partitions of StateStoreRDD (and hence the number of Spark tasks) is what was defined for the < > physical plan. There will be that many StateStores as there are partitions in StateStoreRDD . NOTE: StateStoreSaveExec < > differently per output mode. When < >, StateStoreSaveExec executes the < > physical operator and creates a StateStoreRDD (with storeUpdateFunction specific to the output mode). [[output]] The output schema of StateStoreSaveExec is exactly the < >'s output schema. [[outputPartitioning]] The output partitioning of StateStoreSaveExec is exactly the < >'s output partitioning. [[stateManager]] StateStoreRestoreExec uses a StreamingAggregationStateManager (that is created for the keyExpressions , the output of the child physical operator and the stateFormatVersion ). [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.StateStoreSaveExec to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.StateStoreSaveExec=ALL","title":"StateStoreSaveExec Unary Physical Operator"},{"location":"physical-operators/StateStoreSaveExec/#refer-to","text":"=== [[metrics]] Performance Metrics (SQLMetrics) StateStoreSaveExec uses the performance metrics as other stateful physical operators that write to a state store . The following table shows how the performance metrics are computed (and so their exact meaning). [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Name (in web UI) | Description | total time to update rows a| [[allUpdatesTimeMs]] Time taken to read the input rows and store them in a state store (possibly filtering out expired rows per watermarkPredicateForData predicate) The number of rows stored is the < > metric For < > output mode, the time taken to filter out expired rows (per the required watermarkPredicateForData predicate) and the < > to store rows in a state store For < > output mode, the time taken to go over all the input rows and request the < > to store rows in a state store For < > output mode, the time taken to filter out expired rows (per the optional watermarkPredicateForData predicate) and the < > to store rows in a state store | total time to remove rows a| [[allRemovalsTimeMs]] For < > output mode, the time taken for the < > to remove all expired entries from a state store (per watermarkPredicateForKeys predicate) that is the total time of iterating over all entries in the state store (the number of entries removed from a state store is the difference between the number of output rows of the child operator and the number of total state rows metric) For < > output mode, always 0 For < > output mode, the time taken for the < > to remove all expired entries from a state store (per watermarkPredicateForKeys predicate) | time to commit changes a| [[commitTimeMs]] Time taken for the StreamingAggregationStateManager to commit changes to a state store | number of output rows a| [[numOutputRows]] For < > output mode, the metric does not seem to be used For < > output mode, the number of rows in a StateStore (i.e. all values in a StateStore in the < > that should be equivalent to the < > metric) For < > output mode, the number of rows that the < > was requested to store in a state store (that did not expire per the optional watermarkPredicateForData predicate) that is equivalent to the < > metric) | number of total state rows a| [[numTotalStateRows]] Number of entries in a state store at the very end of < > (aka numTotalStateRows ) Corresponds to numRowsTotal attribute in stateOperators in StreamingQueryProgress (and is available as sq.lastProgress.stateOperators for an operator). | number of updated state rows a| [[numUpdatedStateRows]] Number of the entries that were stored as updates in a state store in a trigger and for the keys in the result rows of the upstream physical operator (aka numUpdatedStateRows ) For < > output mode, the number of input rows that have not expired yet (per the required watermarkPredicateForData predicate) and that the < > was requested to store in a state store (the time taken is the < > metric) For < > output mode, the number of input rows (which should be exactly the number of output rows from the < >) For < > output mode, the number of rows that the < > was requested to store in a state store (that did not expire per the optional watermarkPredicateForData predicate) that is equivalent to the < > metric) Corresponds to numRowsUpdated attribute in stateOperators in StreamingQueryProgress (and is available as sq.lastProgress.stateOperators for an operator). | memory used by state a| [[stateMemory]] Estimated memory used by a StateStore (aka stateMemory ) after StateStoreSaveExec finished < > (per the StateStoreMetrics of the StateStore ) |===","title":"Refer to &lt;&gt;."},{"location":"physical-operators/StateStoreSaveExec/#creating-instance","text":"StateStoreSaveExec takes the following to be created: [[keyExpressions]] Key expressions (Catalyst attributes for the grouping keys) [[stateInfo]] Execution-specific StatefulOperatorStateInfo (default: None ) [[outputMode]] Execution-specific OutputMode (default: None ) [[eventTimeWatermark]] Event-time watermark (default: None ) [[stateFormatVersion]] Version of the state format (based on the spark.sql.streaming.aggregation.stateFormatVersion configuration property) [[child]] Child physical operator ( SparkPlan )","title":"Creating Instance"},{"location":"physical-operators/StateStoreSaveExec/#executing-physical-operator","text":"doExecute (): RDD [ InternalRow ] doExecute is part of the SparkPlan abstraction (Spark SQL). Internally, doExecute initializes metrics . NOTE: doExecute requires that the optional < > is at this point defined (that should have happened when IncrementalExecution had prepared a streaming aggregation for execution ). doExecute executes < > physical operator and creates a StateStoreRDD with storeUpdateFunction that: Generates an unsafe projection to access the key field (using < > and the output schema of < >). Branches off per < >: < >, < > and < >. doExecute throws an UnsupportedOperationException when executed with an invalid < >: Invalid output mode: [outputMode] ==== [[doExecute-Append]] Append Output Mode NOTE: Append is the default output mode when not specified explicitly. NOTE: Append output mode requires that a streaming query defines event-time watermark (e.g. using withWatermark operator) on the event-time column that is used in aggregation (directly or using window standard function). For Append output mode, doExecute does the following: Finds late (aggregate) rows from < > physical operator (that have expired per watermark ) Stores the late rows in the state store and increments the < > metric Gets all the added (late) rows from the state store Creates an iterator that removes the late rows from the state store when requested the next row and in the end commits the state updates TIP: Refer to < > for an example of StateStoreSaveExec with Append output mode. CAUTION: FIXME When is \"Filtering state store on:\" printed out? Uses watermarkPredicateForData predicate to exclude matching rows and (like in Complete output mode) stores all the remaining rows in StateStore . (like in < > output mode) While storing the rows, increments < > metric (for every row) and records the total time in < > metric. Takes all the rows from StateStore and returns a NextIterator that: In getNext , finds the first row that matches watermarkPredicateForKeys predicate, removes it from StateStore , and returns it back. + If no row was found, getNext also marks the iterator as finished. In close , records the time to iterate over all the rows in < > metric, commits the updates to StateStore followed by recording the time in < > metric and recording StateStore metrics .","title":" Executing Physical Operator"},{"location":"physical-operators/StateStoreSaveExec/#complete-output-mode","text":"For Complete output mode, doExecute does the following: Takes all UnsafeRow rows (from the parent iterator) Stores the rows by key in the state store eagerly (i.e. all rows that are available in the parent iterator before proceeding) Commits the state updates In the end, reads the key-row pairs from the state store and passes the rows along (i.e. to the following physical operator) The number of keys stored in the state store is recorded in < > metric. NOTE: In Complete output mode the < > metric is exactly the < > metric. TIP: Refer to < > for an example of StateStoreSaveExec with Complete output mode. Stores all rows (as UnsafeRow ) in StateStore . While storing the rows, increments < > metric (for every row) and records the total time in < > metric. Records 0 in < > metric. Commits the state updates to StateStore and records the time in < > metric. Records StateStore metrics In the end, takes all the rows stored in StateStore and increments numOutputRows metric.","title":" Complete Output Mode"},{"location":"physical-operators/StateStoreSaveExec/#update-output-mode","text":"For Update output mode, doExecute returns an iterator that filters out late aggregate rows (per watermark if defined) and stores the \"young\" rows in the state store (one by one, i.e. every next ). With no more rows available, that removes the late rows from the state store (all at once) and commits the state updates . TIP: Refer to < > for an example of StateStoreSaveExec with Update output mode. doExecute returns Iterator of rows that uses watermarkPredicateForData predicate to filter out late rows. In hasNext , when rows are no longer available: Records the total time to iterate over all the rows in < > metric. removeKeysOlderThanWatermark and records the time in < > metric. Commits the updates to StateStore and records the time in < > metric. Records StateStore metrics In next , stores a row in StateStore and increments numOutputRows and numUpdatedStateRows metrics. === [[shouldRunAnotherBatch]] Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not -- shouldRunAnotherBatch Method shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ): Boolean shouldRunAnotherBatch is positive ( true ) when all of the following are met: < > is either Append or Update < > is defined and is older (below) the current event-time watermark (of the given OffsetSeqMetadata ) Otherwise, shouldRunAnotherBatch is negative ( false ). shouldRunAnotherBatch is part of the StateStoreWriter abstraction.","title":" Update Output Mode"},{"location":"physical-operators/StateStoreWriter/","text":"StateStoreWriter Physical Operators \u00b6 StateStoreWriter is an extension of the StatefulOperator abstraction for stateful physical operators that write to a state store and collect the write metrics for execution progress reporting . Implementations \u00b6 FlatMapGroupsWithStateExec StateStoreSaveExec StreamingDeduplicateExec StreamingGlobalLimitExec StreamingSymmetricHashJoinExec Performance Metrics \u00b6 ID Name numOutputRows number of output rows numTotalStateRows number of total state rows numUpdatedStateRows number of updated state rows allUpdatesTimeMs time to update allRemovalsTimeMs time to remove commitTimeMs time to commit changes stateMemory memory used by state Setting StateStore-Specific Metrics for Stateful Physical Operator \u00b6 setStoreMetrics ( store : StateStore ): Unit setStoreMetrics requests the specified StateStore for the metrics and records the following metrics of a physical operator: numTotalStateRows as the number of keys stateMemory as the memory used (in bytes) setStoreMetrics records the custom metrics . setStoreMetrics is used when the following physical operators are executed: FlatMapGroupsWithStateExec StateStoreSaveExec StreamingDeduplicateExec StreamingGlobalLimitExec StateOperatorProgress \u00b6 getProgress (): StateOperatorProgress getProgress ...FIXME getProgress is used when ProgressReporter is requested to extractStateOperatorMetrics (when MicroBatchExecution is requested to run the activated streaming query ). Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not \u00b6 shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ): Boolean shouldRunAnotherBatch is negative ( false ) by default (to indicate that another non-data batch is not required given the OffsetSeqMetadata with the event-time watermark and the batch timestamp). shouldRunAnotherBatch is used when IncrementalExecution is requested to check out whether the last batch execution requires another batch (when MicroBatchExecution is requested to run the activated streaming query ). stateStoreCustomMetrics Internal Method \u00b6 stateStoreCustomMetrics : Map [ String , SQLMetric ] stateStoreCustomMetrics ...FIXME stateStoreCustomMetrics is used when StateStoreWriter is requested for the metrics and getProgress .","title":"StateStoreWriter"},{"location":"physical-operators/StateStoreWriter/#statestorewriter-physical-operators","text":"StateStoreWriter is an extension of the StatefulOperator abstraction for stateful physical operators that write to a state store and collect the write metrics for execution progress reporting .","title":"StateStoreWriter Physical Operators"},{"location":"physical-operators/StateStoreWriter/#implementations","text":"FlatMapGroupsWithStateExec StateStoreSaveExec StreamingDeduplicateExec StreamingGlobalLimitExec StreamingSymmetricHashJoinExec","title":"Implementations"},{"location":"physical-operators/StateStoreWriter/#performance-metrics","text":"ID Name numOutputRows number of output rows numTotalStateRows number of total state rows numUpdatedStateRows number of updated state rows allUpdatesTimeMs time to update allRemovalsTimeMs time to remove commitTimeMs time to commit changes stateMemory memory used by state","title":" Performance Metrics"},{"location":"physical-operators/StateStoreWriter/#setting-statestore-specific-metrics-for-stateful-physical-operator","text":"setStoreMetrics ( store : StateStore ): Unit setStoreMetrics requests the specified StateStore for the metrics and records the following metrics of a physical operator: numTotalStateRows as the number of keys stateMemory as the memory used (in bytes) setStoreMetrics records the custom metrics . setStoreMetrics is used when the following physical operators are executed: FlatMapGroupsWithStateExec StateStoreSaveExec StreamingDeduplicateExec StreamingGlobalLimitExec","title":" Setting StateStore-Specific Metrics for Stateful Physical Operator"},{"location":"physical-operators/StateStoreWriter/#stateoperatorprogress","text":"getProgress (): StateOperatorProgress getProgress ...FIXME getProgress is used when ProgressReporter is requested to extractStateOperatorMetrics (when MicroBatchExecution is requested to run the activated streaming query ).","title":" StateOperatorProgress"},{"location":"physical-operators/StateStoreWriter/#checking-out-whether-last-batch-execution-requires-another-non-data-batch-or-not","text":"shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ): Boolean shouldRunAnotherBatch is negative ( false ) by default (to indicate that another non-data batch is not required given the OffsetSeqMetadata with the event-time watermark and the batch timestamp). shouldRunAnotherBatch is used when IncrementalExecution is requested to check out whether the last batch execution requires another batch (when MicroBatchExecution is requested to run the activated streaming query ).","title":" Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not"},{"location":"physical-operators/StateStoreWriter/#statestorecustommetrics-internal-method","text":"stateStoreCustomMetrics : Map [ String , SQLMetric ] stateStoreCustomMetrics ...FIXME stateStoreCustomMetrics is used when StateStoreWriter is requested for the metrics and getProgress .","title":" stateStoreCustomMetrics Internal Method"},{"location":"physical-operators/StatefulOperator/","text":"StatefulOperator Physical Operators \u00b6 StatefulOperator is the < > of < > that < > or < > state (described by < >). [[contract]] .StatefulOperator Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | stateInfo a| [[stateInfo]] [source, scala] \u00b6 stateInfo: Option[StatefulOperatorStateInfo] \u00b6 The StatefulOperatorStateInfo of the physical operator |=== [[extensions]] .StatefulOperators (Direct Implementations) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | StatefulOperator | Description | < > | [[StateStoreReader]] | StateStoreWriter | [[StateStoreWriter]] Physical operator that writes to a state store and collects the write metrics for execution progress reporting |===","title":"StatefulOperator"},{"location":"physical-operators/StatefulOperator/#statefuloperator-physical-operators","text":"StatefulOperator is the < > of < > that < > or < > state (described by < >). [[contract]] .StatefulOperator Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | stateInfo a| [[stateInfo]]","title":"StatefulOperator Physical Operators"},{"location":"physical-operators/StatefulOperator/#source-scala","text":"","title":"[source, scala]"},{"location":"physical-operators/StatefulOperator/#stateinfo-optionstatefuloperatorstateinfo","text":"The StatefulOperatorStateInfo of the physical operator |=== [[extensions]] .StatefulOperators (Direct Implementations) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | StatefulOperator | Description | < > | [[StateStoreReader]] | StateStoreWriter | [[StateStoreWriter]] Physical operator that writes to a state store and collects the write metrics for execution progress reporting |===","title":"stateInfo: Option[StatefulOperatorStateInfo]"},{"location":"physical-operators/StreamingDeduplicateExec/","text":"StreamingDeduplicateExec Unary Physical Operator \u00b6 StreamingDeduplicateExec is a unary physical operator that writes state to StateStore with support for streaming watermark . [NOTE] \u00b6 A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode ] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. \u00b6 StreamingDeduplicateExec is < > exclusively when StreamingDeduplicationStrategy execution planning strategy is executed (to plan Deduplicate unary logical operators). val uniqueValues = spark. readStream. format(\"rate\"). load. dropDuplicates(\"value\") // <-- creates Deduplicate logical operator scala> println(uniqueValues.queryExecution.logical.numberedTreeString) 00 Deduplicate [value#214L], true 01 +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4785f176,rate,List(),None,List(),None,Map(),None), rate, [timestamp#213, value#214L] scala> uniqueValues.explain == Physical Plan == StreamingDeduplicate [value#214L], StatefulOperatorStateInfo(<unknown>,5a65879c-67bc-4e77-b417-6100db6a52a2,0,0), 0 +- Exchange hashpartitioning(value#214L, 200) +- StreamingRelation rate, [timestamp#213, value#214L] // Start the query and hence StreamingDeduplicateExec import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val sq = uniqueValues. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // sorting not supported for non-aggregate queries // and so values are unsorted ------------------------------------------- Batch: 0 ------------------------------------------- +---------+-----+ |timestamp|value| +---------+-----+ +---------+-----+ ------------------------------------------- Batch: 1 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-07-25 22:12:03.018|0 | |2017-07-25 22:12:08.018|5 | |2017-07-25 22:12:04.018|1 | |2017-07-25 22:12:06.018|3 | |2017-07-25 22:12:05.018|2 | |2017-07-25 22:12:07.018|4 | +-----------------------+-----+ ------------------------------------------- Batch: 2 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-07-25 22:12:10.018|7 | |2017-07-25 22:12:09.018|6 | |2017-07-25 22:12:12.018|9 | |2017-07-25 22:12:13.018|10 | |2017-07-25 22:12:15.018|12 | |2017-07-25 22:12:11.018|8 | |2017-07-25 22:12:14.018|11 | |2017-07-25 22:12:16.018|13 | |2017-07-25 22:12:17.018|14 | |2017-07-25 22:12:18.018|15 | +-----------------------+-----+ // Eventually... sq.stop [[metrics]] StreamingDeduplicateExec uses the performance metrics of StateStoreWriter . .StreamingDeduplicateExec in web UI (Details for Query) image::images/StreamingDeduplicateExec-webui-query-details.png[align=\"center\"] [[output]] The output schema of StreamingDeduplicateExec is exactly the < >'s output schema. [[outputPartitioning]] The output partitioning of StreamingDeduplicateExec is exactly the < >'s output partitioning. [source, scala] \u00b6 /** // Start spark-shell with debugging and Kafka support SPARK_SUBMIT_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\" \\ ./bin/spark-shell \\ --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT */ // Reading val topic1 = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). option(\"startingOffsets\", \"earliest\"). load // Processing with deduplication // Don't use watermark // The following won't work due to https://issues.apache.org/jira/browse/SPARK-21546 /** val records = topic1. withColumn(\"eventtime\", 'timestamp). // \u2190 just to put the right name given the purpose withWatermark(eventTime = \"eventtime\", delayThreshold = \"30 seconds\"). // \u2190 use the renamed eventtime column dropDuplicates(\"value\"). // dropDuplicates will use watermark // only when eventTime column exists // include the watermark column => internal design leak? select('key cast \"string\", 'value cast \"string\", 'eventtime). as[(String, String, java.sql.Timestamp)] */ val records = topic1. dropDuplicates(\"value\"). select('key cast \"string\", 'value cast \"string\"). as[(String, String)] scala> records.explain == Physical Plan == *Project [cast(key#0 as string) AS key#249, cast(value#1 as string) AS value#250] +- StreamingDeduplicate [value#1], StatefulOperatorStateInfo( ,68198b93-6184-49ae-8098-006c32cc6192,0,0), 0 +- Exchange hashpartitioning(value#1, 200) +- *Project [key#0, value#1] +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] // Writing import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val sq = records. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). queryName(\"from-kafka-topic1-to-console\"). outputMode(OutputMode.Update). start // Eventually... sq.stop [TIP] \u00b6 Enable INFO logging level for org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec=INFO Refer to spark-sql-streaming-spark-logging.md[Logging]. \u00b6 === [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method [source, scala] \u00b6 doExecute(): RDD[InternalRow] \u00b6 NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. RDD[InternalRow] ). Internally, doExecute initializes metrics . doExecute executes < > physical operator and creates a StateStoreRDD with storeUpdateFunction that: Generates an unsafe projection to access the key field (using < > and the output schema of < >). Filters out rows from Iterator[InternalRow] that match watermarkPredicateForData (when defined and < > is EventTimeTimeout ) For every row (as InternalRow ) Extracts the key from the row (using the unsafe projection above) Gets the saved state in StateStore for the key (when there was a state for the key in the row) Filters out (aka drops ) the row (when there was no state for the key in the row) Stores a new (and empty) state for the key and increments < > and < > metrics. In the end, storeUpdateFunction creates a CompletionIterator that executes a completion function (aka completionFunction ) after it has successfully iterated through all the elements (i.e. when a client has consumed all the rows). + The completion function does the following: Updates < > metric (that is the total time to execute storeUpdateFunction ) Updates < > metric with the time taken to remove keys older than the watermark from the StateStore Updates < > metric with the time taken to commit the changes to the StateStore Sets StateStore-specific metrics Creating Instance \u00b6 StreamingDeduplicateExec takes the following when created: [[keyExpressions]] Duplicate keys (as used in dropDuplicates operator) [[child]] Child physical operator ( SparkPlan ) [[stateInfo]] StatefulOperatorStateInfo [[eventTimeWatermark]] Event-time watermark Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not \u00b6 shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ): Boolean shouldRunAnotherBatch ...FIXME shouldRunAnotherBatch is part of the StateStoreWriter abstraction.","title":"StreamingDeduplicateExec"},{"location":"physical-operators/StreamingDeduplicateExec/#streamingdeduplicateexec-unary-physical-operator","text":"StreamingDeduplicateExec is a unary physical operator that writes state to StateStore with support for streaming watermark .","title":"StreamingDeduplicateExec Unary Physical Operator"},{"location":"physical-operators/StreamingDeduplicateExec/#note","text":"A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator.","title":"[NOTE]"},{"location":"physical-operators/StreamingDeduplicateExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-sparkplanhtmlunaryexecnode-and-physical-operators-in-general-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-book","text":"StreamingDeduplicateExec is < > exclusively when StreamingDeduplicationStrategy execution planning strategy is executed (to plan Deduplicate unary logical operators). val uniqueValues = spark. readStream. format(\"rate\"). load. dropDuplicates(\"value\") // <-- creates Deduplicate logical operator scala> println(uniqueValues.queryExecution.logical.numberedTreeString) 00 Deduplicate [value#214L], true 01 +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4785f176,rate,List(),None,List(),None,Map(),None), rate, [timestamp#213, value#214L] scala> uniqueValues.explain == Physical Plan == StreamingDeduplicate [value#214L], StatefulOperatorStateInfo(<unknown>,5a65879c-67bc-4e77-b417-6100db6a52a2,0,0), 0 +- Exchange hashpartitioning(value#214L, 200) +- StreamingRelation rate, [timestamp#213, value#214L] // Start the query and hence StreamingDeduplicateExec import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val sq = uniqueValues. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // sorting not supported for non-aggregate queries // and so values are unsorted ------------------------------------------- Batch: 0 ------------------------------------------- +---------+-----+ |timestamp|value| +---------+-----+ +---------+-----+ ------------------------------------------- Batch: 1 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-07-25 22:12:03.018|0 | |2017-07-25 22:12:08.018|5 | |2017-07-25 22:12:04.018|1 | |2017-07-25 22:12:06.018|3 | |2017-07-25 22:12:05.018|2 | |2017-07-25 22:12:07.018|4 | +-----------------------+-----+ ------------------------------------------- Batch: 2 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-07-25 22:12:10.018|7 | |2017-07-25 22:12:09.018|6 | |2017-07-25 22:12:12.018|9 | |2017-07-25 22:12:13.018|10 | |2017-07-25 22:12:15.018|12 | |2017-07-25 22:12:11.018|8 | |2017-07-25 22:12:14.018|11 | |2017-07-25 22:12:16.018|13 | |2017-07-25 22:12:17.018|14 | |2017-07-25 22:12:18.018|15 | +-----------------------+-----+ // Eventually... sq.stop [[metrics]] StreamingDeduplicateExec uses the performance metrics of StateStoreWriter . .StreamingDeduplicateExec in web UI (Details for Query) image::images/StreamingDeduplicateExec-webui-query-details.png[align=\"center\"] [[output]] The output schema of StreamingDeduplicateExec is exactly the < >'s output schema. [[outputPartitioning]] The output partitioning of StreamingDeduplicateExec is exactly the < >'s output partitioning.","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book."},{"location":"physical-operators/StreamingDeduplicateExec/#source-scala","text":"/** // Start spark-shell with debugging and Kafka support SPARK_SUBMIT_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\" \\ ./bin/spark-shell \\ --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT */ // Reading val topic1 = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). option(\"startingOffsets\", \"earliest\"). load // Processing with deduplication // Don't use watermark // The following won't work due to https://issues.apache.org/jira/browse/SPARK-21546 /** val records = topic1. withColumn(\"eventtime\", 'timestamp). // \u2190 just to put the right name given the purpose withWatermark(eventTime = \"eventtime\", delayThreshold = \"30 seconds\"). // \u2190 use the renamed eventtime column dropDuplicates(\"value\"). // dropDuplicates will use watermark // only when eventTime column exists // include the watermark column => internal design leak? select('key cast \"string\", 'value cast \"string\", 'eventtime). as[(String, String, java.sql.Timestamp)] */ val records = topic1. dropDuplicates(\"value\"). select('key cast \"string\", 'value cast \"string\"). as[(String, String)] scala> records.explain == Physical Plan == *Project [cast(key#0 as string) AS key#249, cast(value#1 as string) AS value#250] +- StreamingDeduplicate [value#1], StatefulOperatorStateInfo( ,68198b93-6184-49ae-8098-006c32cc6192,0,0), 0 +- Exchange hashpartitioning(value#1, 200) +- *Project [key#0, value#1] +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] // Writing import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val sq = records. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). queryName(\"from-kafka-topic1-to-console\"). outputMode(OutputMode.Update). start // Eventually... sq.stop","title":"[source, scala]"},{"location":"physical-operators/StreamingDeduplicateExec/#tip","text":"Enable INFO logging level for org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec=INFO","title":"[TIP]"},{"location":"physical-operators/StreamingDeduplicateExec/#refer-to-spark-sql-streaming-spark-loggingmdlogging","text":"=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method","title":"Refer to spark-sql-streaming-spark-logging.md[Logging]."},{"location":"physical-operators/StreamingDeduplicateExec/#source-scala_1","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingDeduplicateExec/#doexecute-rddinternalrow","text":"NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. RDD[InternalRow] ). Internally, doExecute initializes metrics . doExecute executes < > physical operator and creates a StateStoreRDD with storeUpdateFunction that: Generates an unsafe projection to access the key field (using < > and the output schema of < >). Filters out rows from Iterator[InternalRow] that match watermarkPredicateForData (when defined and < > is EventTimeTimeout ) For every row (as InternalRow ) Extracts the key from the row (using the unsafe projection above) Gets the saved state in StateStore for the key (when there was a state for the key in the row) Filters out (aka drops ) the row (when there was no state for the key in the row) Stores a new (and empty) state for the key and increments < > and < > metrics. In the end, storeUpdateFunction creates a CompletionIterator that executes a completion function (aka completionFunction ) after it has successfully iterated through all the elements (i.e. when a client has consumed all the rows). + The completion function does the following: Updates < > metric (that is the total time to execute storeUpdateFunction ) Updates < > metric with the time taken to remove keys older than the watermark from the StateStore Updates < > metric with the time taken to commit the changes to the StateStore Sets StateStore-specific metrics","title":"doExecute(): RDD[InternalRow]"},{"location":"physical-operators/StreamingDeduplicateExec/#creating-instance","text":"StreamingDeduplicateExec takes the following when created: [[keyExpressions]] Duplicate keys (as used in dropDuplicates operator) [[child]] Child physical operator ( SparkPlan ) [[stateInfo]] StatefulOperatorStateInfo [[eventTimeWatermark]] Event-time watermark","title":"Creating Instance"},{"location":"physical-operators/StreamingDeduplicateExec/#checking-out-whether-last-batch-execution-requires-another-non-data-batch-or-not","text":"shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ): Boolean shouldRunAnotherBatch ...FIXME shouldRunAnotherBatch is part of the StateStoreWriter abstraction.","title":" Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not"},{"location":"physical-operators/StreamingGlobalLimitExec/","text":"StreamingGlobalLimitExec Unary Physical Operator \u00b6 StreamingGlobalLimitExec is a unary physical operator that represents a Limit logical operator of a streaming query at execution time. [NOTE] \u00b6 A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode ] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. \u00b6 [NOTE] \u00b6 Limit logical operator represents Dataset.limit operator in a logical query plan. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-Limit.html[Limit Logical Operator] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. \u00b6 StreamingGlobalLimitExec is a < >. StreamingGlobalLimitExec supports < > output mode only. The optional properties, i.e. the < > and the < >, are initially undefined when StreamingGlobalLimitExec is < >. StreamingGlobalLimitExec is updated to hold execution-specific configuration when IncrementalExecution is requested to prepare the logical plan (of a streaming query) for execution (when the state preparation rule is executed). Creating Instance \u00b6 StreamingGlobalLimitExec takes the following to be created: [[streamLimit]] Streaming Limit [[child]] Child physical operator ( SparkPlan ) [[stateInfo]] StatefulOperatorStateInfo (default: None ) [[outputMode]] OutputMode (default: None ) StreamingGlobalLimitExec is created when StreamingGlobalLimitStrategy execution planning strategy is requested to plan a Limit logical operator (in the logical plan of a streaming query) for execution. === [[StateStoreWriter]] StreamingGlobalLimitExec as StateStoreWriter StreamingGlobalLimitExec is a stateful physical operator that can write to a state store . === [[metrics]] Performance Metrics StreamingGlobalLimitExec uses the performance metrics of the parent StateStoreWriter . === [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method [source, scala] \u00b6 doExecute(): RDD[InternalRow] \u00b6 NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a recipe for distributed computation over internal binary rows on Apache Spark ( RDD[InternalRow] ). doExecute ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | keySchema a| [[keySchema]] FIXME Used when...FIXME | valueSchema a| [[valueSchema]] FIXME Used when...FIXME |===","title":"StreamingGlobalLimitExec"},{"location":"physical-operators/StreamingGlobalLimitExec/#streaminggloballimitexec-unary-physical-operator","text":"StreamingGlobalLimitExec is a unary physical operator that represents a Limit logical operator of a streaming query at execution time.","title":"StreamingGlobalLimitExec Unary Physical Operator"},{"location":"physical-operators/StreamingGlobalLimitExec/#note","text":"A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator.","title":"[NOTE]"},{"location":"physical-operators/StreamingGlobalLimitExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-sparkplanhtmlunaryexecnode-and-physical-operators-in-general-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-book","text":"","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book."},{"location":"physical-operators/StreamingGlobalLimitExec/#note_1","text":"Limit logical operator represents Dataset.limit operator in a logical query plan.","title":"[NOTE]"},{"location":"physical-operators/StreamingGlobalLimitExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-logicalplan-limithtmllimit-logical-operator-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-book","text":"StreamingGlobalLimitExec is a < >. StreamingGlobalLimitExec supports < > output mode only. The optional properties, i.e. the < > and the < >, are initially undefined when StreamingGlobalLimitExec is < >. StreamingGlobalLimitExec is updated to hold execution-specific configuration when IncrementalExecution is requested to prepare the logical plan (of a streaming query) for execution (when the state preparation rule is executed).","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-Limit.html[Limit Logical Operator] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book."},{"location":"physical-operators/StreamingGlobalLimitExec/#creating-instance","text":"StreamingGlobalLimitExec takes the following to be created: [[streamLimit]] Streaming Limit [[child]] Child physical operator ( SparkPlan ) [[stateInfo]] StatefulOperatorStateInfo (default: None ) [[outputMode]] OutputMode (default: None ) StreamingGlobalLimitExec is created when StreamingGlobalLimitStrategy execution planning strategy is requested to plan a Limit logical operator (in the logical plan of a streaming query) for execution. === [[StateStoreWriter]] StreamingGlobalLimitExec as StateStoreWriter StreamingGlobalLimitExec is a stateful physical operator that can write to a state store . === [[metrics]] Performance Metrics StreamingGlobalLimitExec uses the performance metrics of the parent StateStoreWriter . === [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method","title":"Creating Instance"},{"location":"physical-operators/StreamingGlobalLimitExec/#source-scala","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingGlobalLimitExec/#doexecute-rddinternalrow","text":"NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a recipe for distributed computation over internal binary rows on Apache Spark ( RDD[InternalRow] ). doExecute ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | keySchema a| [[keySchema]] FIXME Used when...FIXME | valueSchema a| [[valueSchema]] FIXME Used when...FIXME |===","title":"doExecute(): RDD[InternalRow]"},{"location":"physical-operators/StreamingRelationExec/","text":"StreamingRelationExec Leaf Physical Operator \u00b6 StreamingRelationExec is a leaf physical operator (i.e. LeafExecNode ) that...FIXME [source, scala] \u00b6 scala> spark.version res0: String = 2.3.0-SNAPSHOT val rates = spark. readStream. format(\"rate\"). load // StreamingRelation logical operator scala> println(rates.queryExecution.logical.numberedTreeString) 00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@31ba0af0,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] // StreamingRelationExec physical operator (shown without \"Exec\" suffix) scala> rates.explain == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L] [[doExecute]] StreamingRelationExec is not supposed to be executed and is used...FIXME === [[creating-instance]] Creating StreamingRelationExec Instance StreamingRelationExec takes the following when created: [[sourceName]] The name of a streaming source [[output]] Output attributes StreamingRelationExec is created when StreamingRelationStrategy execution planning strategy is executed (to plan StreamingRelation and StreamingExecutionRelation logical operators).","title":"StreamingRelationExec"},{"location":"physical-operators/StreamingRelationExec/#streamingrelationexec-leaf-physical-operator","text":"StreamingRelationExec is a leaf physical operator (i.e. LeafExecNode ) that...FIXME","title":"StreamingRelationExec Leaf Physical Operator"},{"location":"physical-operators/StreamingRelationExec/#source-scala","text":"scala> spark.version res0: String = 2.3.0-SNAPSHOT val rates = spark. readStream. format(\"rate\"). load // StreamingRelation logical operator scala> println(rates.queryExecution.logical.numberedTreeString) 00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@31ba0af0,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] // StreamingRelationExec physical operator (shown without \"Exec\" suffix) scala> rates.explain == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L] [[doExecute]] StreamingRelationExec is not supposed to be executed and is used...FIXME === [[creating-instance]] Creating StreamingRelationExec Instance StreamingRelationExec takes the following when created: [[sourceName]] The name of a streaming source [[output]] Output attributes StreamingRelationExec is created when StreamingRelationStrategy execution planning strategy is executed (to plan StreamingRelation and StreamingExecutionRelation logical operators).","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/","text":"StreamingSymmetricHashJoinExec Binary Physical Operator \u00b6 StreamingSymmetricHashJoinExec is a binary physical operator for stream-stream equi-join at execution time. Note A binary physical operator ( BinaryExecNode ) is a physical operator with < > and < > child physical operators. Learn more about BinaryExecNode (and physical operators in general) in The Internals of Spark SQL online book. [[supported-join-types]][[joinType]] StreamingSymmetricHashJoinExec supports Inner , LeftOuter , and RightOuter join types (with the < > and the < > keys using the exact same data types). StreamingSymmetricHashJoinExec is < > exclusively when StreamingJoinStrategy execution planning strategy is requested to plan a logical query plan with a Join logical operator of two streaming queries with equality predicates ( EqualTo and EqualNullSafe ). StreamingSymmetricHashJoinExec is given execution-specific configuration (i.e. < >, < >, and < >) when IncrementalExecution is requested to plan a streaming query for execution (and uses the state preparation rule ). StreamingSymmetricHashJoinExec uses two OneSideHashJoiners (for the < > and < > sides of the join) to manage join state when < >. StreamingSymmetricHashJoinExec is a stateful physical operator that writes to a state store . Creating Instance \u00b6 StreamingSymmetricHashJoinExec takes the following to be created: [[leftKeys]] Left keys (Catalyst expressions of the keys on the left side) [[rightKeys]] Right keys (Catalyst expressions of the keys on the right side) Join type [[condition]] Join condition ( JoinConditionSplitPredicates ) [[stateInfo]] StatefulOperatorStateInfo Event-Time Watermark Watermark Predicates for State Removal [[left]] Physical operator on the left side ( SparkPlan ) [[right]] Physical operator on the right side ( SparkPlan ) StreamingSymmetricHashJoinExec initializes the < >. === [[output]] Output Schema -- output Method [source, scala] \u00b6 output: Seq[Attribute] \u00b6 NOTE: output is part of the QueryPlan Contract to describe the attributes of (the schema of) the output. output schema depends on the < >: For Cross and Inner ( InnerLike ) joins, it is the output schema of the < > and < > operators For LeftOuter joins, it is the output schema of the < > operator with the attributes of the < > operator with nullability flag enabled ( true ) For RightOuter joins, it is the output schema of the < > operator with the attributes of the < > operator with nullability flag enabled ( true ) output throws an IllegalArgumentException for other join types: [className] should not take [joinType] as the JoinType === [[outputPartitioning]] Output Partitioning -- outputPartitioning Method [source, scala] \u00b6 outputPartitioning: Partitioning \u00b6 NOTE: outputPartitioning is part of the SparkPlan Contract to specify how data should be partitioned across different nodes in the cluster. outputPartitioning depends on the < >: For Cross and Inner ( InnerLike ) joins, it is a PartitioningCollection of the output partitioning of the < > and < > operators For LeftOuter joins, it is a PartitioningCollection of the output partitioning of the < > operator For RightOuter joins, it is a PartitioningCollection of the output partitioning of the < > operator outputPartitioning throws an IllegalArgumentException for other join types: [className] should not take [joinType] as the JoinType === [[eventTimeWatermark]] Event-Time Watermark -- eventTimeWatermark Internal Property [source, scala] \u00b6 eventTimeWatermark: Option[Long] \u00b6 When < >, StreamingSymmetricHashJoinExec can be given the event-time watermark of the current streaming micro-batch. eventTimeWatermark is an optional property that is specified only after IncrementalExecution was requested to apply the state preparation rule to a physical query plan of a streaming query (to optimize (prepare) the physical plan of the streaming query once for ContinuousExecution and every trigger for MicroBatchExecution in the queryPlanning phase). eventTimeWatermark is used when: StreamingSymmetricHashJoinExec is requested to check out whether the last batch execution requires another non-data batch or not OneSideHashJoiner is requested to storeAndJoinWithOtherSide Watermark Predicates for State Removal \u00b6 stateWatermarkPredicates : JoinStateWatermarkPredicates When < >, StreamingSymmetricHashJoinExec is given a < > for the < > and < > join sides (using the StreamingSymmetricHashJoinHelper utility). stateWatermarkPredicates contains the left and right predicates only when IncrementalExecution is requested to apply the state preparation rule to a physical query plan of a streaming query (to optimize (prepare) the physical plan of the streaming query once for ContinuousExecution and every trigger for MicroBatchExecution in the queryPlanning phase). stateWatermarkPredicates is used when StreamingSymmetricHashJoinExec is requested for the following: Process partitions of the left and right sides of the stream-stream join (and creating OneSideHashJoiner s) Checking out whether the last batch execution requires another non-data batch or not === [[requiredChildDistribution]] Required Partition Requirements -- requiredChildDistribution Method [source, scala] \u00b6 requiredChildDistribution: Seq[Distribution] \u00b6 [NOTE] \u00b6 requiredChildDistribution is part of the SparkPlan Contract for the required partition requirements (aka required child distribution ) of the input data, i.e. how the output of the children physical operators is split across partitions before this operator can be executed. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[SparkPlan Contract]\u2009in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. \u00b6 requiredChildDistribution returns two HashClusteredDistributions for the < > and < > keys with the required number of partitions based on the StatefulOperatorStateInfo . [NOTE] \u00b6 requiredChildDistribution is used exclusively when EnsureRequirements physical query plan optimization is executed (and enforces partition requirements). Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-EnsureRequirements.html[EnsureRequirements Physical Query Optimization]\u2009in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. \u00b6 [NOTE] \u00b6 HashClusteredDistribution becomes HashPartitioning at execution that distributes rows across partitions (generates partition IDs of rows) based on Murmur3Hash of the join expressions (separately for the < > and < > keys) modulo the required number of partitions. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Distribution-HashClusteredDistribution.html[HashClusteredDistribution ]\u2009in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. \u00b6 === [[metrics]] Performance Metrics (SQLMetrics) StreamingSymmetricHashJoinExec uses the performance metrics as other stateful physical operators that write to a state store . The following table shows how the performance metrics are computed (and so their exact meaning). [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Name (in web UI) | Description | total time to update rows a| [[allUpdatesTimeMs]] Processing time of all rows | total time to remove rows a| [[allRemovalsTimeMs]] | time to commit changes a| [[commitTimeMs]] | number of output rows a| [[numOutputRows]] Total number of output rows | number of total state rows a| [[numTotalStateRows]] | number of updated state rows a| [[numUpdatedStateRows]] Number of updated state rows of the left and right OneSideHashJoiners | memory used by state a| [[stateMemory]] |=== === [[shouldRunAnotherBatch]] Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not -- shouldRunAnotherBatch Method [source, scala] \u00b6 shouldRunAnotherBatch( newMetadata: OffsetSeqMetadata): Boolean shouldRunAnotherBatch is positive ( true ) when all of the following are positive: Either the < > or < > join state watermark predicates are defined (in the < >) < > threshold (of the StreamingSymmetricHashJoinExec operator) is defined and the current event-time watermark threshold of the given OffsetSeqMetadata is above ( greater than ) it, i.e. moved above shouldRunAnotherBatch is negative ( false ) otherwise. shouldRunAnotherBatch is part of the StateStoreWriter abstraction. Executing Physical Operator \u00b6 doExecute (): RDD [ InternalRow ] doExecute is part of the SparkPlan abstraction ( Spark SQL ). doExecute first requests the StreamingQueryManager for the StateStoreCoordinatorRef to the StateStoreCoordinator RPC endpoint (for the driver). doExecute then uses SymmetricHashJoinStateManager utility to get the names of the state stores for the left and right sides of the streaming join. In the end, doExecute requests the < > and < > child physical operators to execute (generate an RDD) and then < > with < > (and with the StateStoreCoordinatorRef and the state stores). === [[processPartitions]] Processing Partitions of Left and Right Sides of Stream-Stream Join -- processPartitions Internal Method [source, scala] \u00b6 processPartitions( leftInputIter: Iterator[InternalRow], rightInputIter: Iterator[InternalRow]): Iterator[InternalRow] [[processPartitions-updateStartTimeNs]] processPartitions records the current time (as updateStartTimeNs for the < > performance metric in < >). [[processPartitions-postJoinFilter]] processPartitions creates a new predicate ( postJoinFilter ) based on the bothSides of the < > if defined or true literal. [[processPartitions-leftSideJoiner]] processPartitions creates a OneSideHashJoiner for the LeftSide and all other properties for the left-hand join side ( leftSideJoiner ). [[processPartitions-rightSideJoiner]] processPartitions creates a OneSideHashJoiner for the RightSide and all other properties for the right-hand join side ( rightSideJoiner ). [[processPartitions-leftOutputIter]][[processPartitions-rightOutputIter]] processPartitions requests the OneSideHashJoiner for the left-hand join side to storeAndJoinWithOtherSide with the right-hand side one (that creates a leftOutputIter row iterator) and the OneSideHashJoiner for the right-hand join side to do the same with the left-hand side one (and creates a rightOutputIter row iterator). [[processPartitions-innerOutputCompletionTimeNs]] processPartitions records the current time (as innerOutputCompletionTimeNs for the < > performance metric in < >). [[processPartitions-innerOutputIter]] processPartitions creates a CompletionIterator with the left and right output iterators (with the rows of the leftOutputIter first followed by rightOutputIter ). When no rows are left to process, the CompletionIterator records the completion time. [[processPartitions-outputIter]] processPartitions creates a join-specific output Iterator[InternalRow] of the output rows based on the < > (of the StreamingSymmetricHashJoinExec ): For Inner joins, processPartitions simply uses the < > For LeftOuter joins, processPartitions ... For RightOuter joins, processPartitions ... For other joins, processPartitions simply throws an IllegalArgumentException . [[processPartitions-outputIterWithMetrics]] processPartitions creates an UnsafeProjection for the < > (and the output of the < > and < > child operators) that counts all the rows of the < > (as the < > metric) and generate an output projection. In the end, processPartitions returns a CompletionIterator with with the < > and < > completion function. NOTE: processPartitions is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >. ==== [[processPartitions-onOutputCompletion]][[onOutputCompletion]] Calculating Performance Metrics (Output Completion Callback) -- onOutputCompletion Internal Method [source, scala] \u00b6 onOutputCompletion: Unit \u00b6 onOutputCompletion calculates the < > performance metric (that is the time since the < > was executed). onOutputCompletion adds the time for the inner join to complete (since < > time marker) to the < > performance metric. onOutputCompletion records the time to remove old state (per the join state watermark predicate for the < > and the < > streaming queries) and adds it to the < > performance metric. NOTE: onOutputCompletion triggers the old state removal eagerly by iterating over the state rows to be deleted. onOutputCompletion records the time for the < > and < > OneSideHashJoiners to commit any state changes that becomes the < > performance metric. onOutputCompletion calculates the < > performance metric (as the number of updated state rows of the < > and < > streaming queries). onOutputCompletion calculates the < > performance metric (as the sum of the < > in the KeyWithIndexToValueStore of the < > and < > streaming queries). onOutputCompletion calculates the < > performance metric (as the sum of the < > by the KeyToNumValuesStore and KeyWithIndexToValueStore of the < > and < > streams). In the end, onOutputCompletion calculates the < >. Internal Properties \u00b6 [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | hadoopConfBcast a| [[hadoopConfBcast]] Hadoop Configuration broadcast (to the Spark cluster) Used exclusively to < > | joinStateManager a| [[joinStateManager]] SymmetricHashJoinStateManager Used when OneSideHashJoiner is requested to storeAndJoinWithOtherSide , removeOldState , commitStateAndGetMetrics , and for the values for a given key | nullLeft a| [[nullLeft]] GenericInternalRow of the size of the output schema of the < > | nullRight a| [[nullRight]] GenericInternalRow of the size of the output schema of the < > | storeConf a| [[storeConf]] StateStoreConf Used exclusively to < > |===","title":"StreamingSymmetricHashJoinExec"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#streamingsymmetrichashjoinexec-binary-physical-operator","text":"StreamingSymmetricHashJoinExec is a binary physical operator for stream-stream equi-join at execution time. Note A binary physical operator ( BinaryExecNode ) is a physical operator with < > and < > child physical operators. Learn more about BinaryExecNode (and physical operators in general) in The Internals of Spark SQL online book. [[supported-join-types]][[joinType]] StreamingSymmetricHashJoinExec supports Inner , LeftOuter , and RightOuter join types (with the < > and the < > keys using the exact same data types). StreamingSymmetricHashJoinExec is < > exclusively when StreamingJoinStrategy execution planning strategy is requested to plan a logical query plan with a Join logical operator of two streaming queries with equality predicates ( EqualTo and EqualNullSafe ). StreamingSymmetricHashJoinExec is given execution-specific configuration (i.e. < >, < >, and < >) when IncrementalExecution is requested to plan a streaming query for execution (and uses the state preparation rule ). StreamingSymmetricHashJoinExec uses two OneSideHashJoiners (for the < > and < > sides of the join) to manage join state when < >. StreamingSymmetricHashJoinExec is a stateful physical operator that writes to a state store .","title":"StreamingSymmetricHashJoinExec Binary Physical Operator"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#creating-instance","text":"StreamingSymmetricHashJoinExec takes the following to be created: [[leftKeys]] Left keys (Catalyst expressions of the keys on the left side) [[rightKeys]] Right keys (Catalyst expressions of the keys on the right side) Join type [[condition]] Join condition ( JoinConditionSplitPredicates ) [[stateInfo]] StatefulOperatorStateInfo Event-Time Watermark Watermark Predicates for State Removal [[left]] Physical operator on the left side ( SparkPlan ) [[right]] Physical operator on the right side ( SparkPlan ) StreamingSymmetricHashJoinExec initializes the < >. === [[output]] Output Schema -- output Method","title":"Creating Instance"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#output-seqattribute","text":"NOTE: output is part of the QueryPlan Contract to describe the attributes of (the schema of) the output. output schema depends on the < >: For Cross and Inner ( InnerLike ) joins, it is the output schema of the < > and < > operators For LeftOuter joins, it is the output schema of the < > operator with the attributes of the < > operator with nullability flag enabled ( true ) For RightOuter joins, it is the output schema of the < > operator with the attributes of the < > operator with nullability flag enabled ( true ) output throws an IllegalArgumentException for other join types: [className] should not take [joinType] as the JoinType === [[outputPartitioning]] Output Partitioning -- outputPartitioning Method","title":"output: Seq[Attribute]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_1","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#outputpartitioning-partitioning","text":"NOTE: outputPartitioning is part of the SparkPlan Contract to specify how data should be partitioned across different nodes in the cluster. outputPartitioning depends on the < >: For Cross and Inner ( InnerLike ) joins, it is a PartitioningCollection of the output partitioning of the < > and < > operators For LeftOuter joins, it is a PartitioningCollection of the output partitioning of the < > operator For RightOuter joins, it is a PartitioningCollection of the output partitioning of the < > operator outputPartitioning throws an IllegalArgumentException for other join types: [className] should not take [joinType] as the JoinType === [[eventTimeWatermark]] Event-Time Watermark -- eventTimeWatermark Internal Property","title":"outputPartitioning: Partitioning"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_2","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#eventtimewatermark-optionlong","text":"When < >, StreamingSymmetricHashJoinExec can be given the event-time watermark of the current streaming micro-batch. eventTimeWatermark is an optional property that is specified only after IncrementalExecution was requested to apply the state preparation rule to a physical query plan of a streaming query (to optimize (prepare) the physical plan of the streaming query once for ContinuousExecution and every trigger for MicroBatchExecution in the queryPlanning phase). eventTimeWatermark is used when: StreamingSymmetricHashJoinExec is requested to check out whether the last batch execution requires another non-data batch or not OneSideHashJoiner is requested to storeAndJoinWithOtherSide","title":"eventTimeWatermark: Option[Long]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#watermark-predicates-for-state-removal","text":"stateWatermarkPredicates : JoinStateWatermarkPredicates When < >, StreamingSymmetricHashJoinExec is given a < > for the < > and < > join sides (using the StreamingSymmetricHashJoinHelper utility). stateWatermarkPredicates contains the left and right predicates only when IncrementalExecution is requested to apply the state preparation rule to a physical query plan of a streaming query (to optimize (prepare) the physical plan of the streaming query once for ContinuousExecution and every trigger for MicroBatchExecution in the queryPlanning phase). stateWatermarkPredicates is used when StreamingSymmetricHashJoinExec is requested for the following: Process partitions of the left and right sides of the stream-stream join (and creating OneSideHashJoiner s) Checking out whether the last batch execution requires another non-data batch or not === [[requiredChildDistribution]] Required Partition Requirements -- requiredChildDistribution Method","title":" Watermark Predicates for State Removal"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_3","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#requiredchilddistribution-seqdistribution","text":"","title":"requiredChildDistribution: Seq[Distribution]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#note","text":"requiredChildDistribution is part of the SparkPlan Contract for the required partition requirements (aka required child distribution ) of the input data, i.e. how the output of the children physical operators is split across partitions before this operator can be executed.","title":"[NOTE]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-sparkplanhtmlsparkplan-contract-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-online-book","text":"requiredChildDistribution returns two HashClusteredDistributions for the < > and < > keys with the required number of partitions based on the StatefulOperatorStateInfo .","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[SparkPlan Contract]\u2009in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book."},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#note_1","text":"requiredChildDistribution is used exclusively when EnsureRequirements physical query plan optimization is executed (and enforces partition requirements).","title":"[NOTE]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-ensurerequirementshtmlensurerequirements-physical-query-optimization-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-online-book","text":"","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-EnsureRequirements.html[EnsureRequirements Physical Query Optimization]\u2009in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book."},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#note_2","text":"HashClusteredDistribution becomes HashPartitioning at execution that distributes rows across partitions (generates partition IDs of rows) based on Murmur3Hash of the join expressions (separately for the < > and < > keys) modulo the required number of partitions.","title":"[NOTE]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-distribution-hashclustereddistributionhtmlhashclustereddistribution-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-online-book","text":"=== [[metrics]] Performance Metrics (SQLMetrics) StreamingSymmetricHashJoinExec uses the performance metrics as other stateful physical operators that write to a state store . The following table shows how the performance metrics are computed (and so their exact meaning). [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Name (in web UI) | Description | total time to update rows a| [[allUpdatesTimeMs]] Processing time of all rows | total time to remove rows a| [[allRemovalsTimeMs]] | time to commit changes a| [[commitTimeMs]] | number of output rows a| [[numOutputRows]] Total number of output rows | number of total state rows a| [[numTotalStateRows]] | number of updated state rows a| [[numUpdatedStateRows]] Number of updated state rows of the left and right OneSideHashJoiners | memory used by state a| [[stateMemory]] |=== === [[shouldRunAnotherBatch]] Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not -- shouldRunAnotherBatch Method","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Distribution-HashClusteredDistribution.html[HashClusteredDistribution]\u2009in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book."},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_4","text":"shouldRunAnotherBatch( newMetadata: OffsetSeqMetadata): Boolean shouldRunAnotherBatch is positive ( true ) when all of the following are positive: Either the < > or < > join state watermark predicates are defined (in the < >) < > threshold (of the StreamingSymmetricHashJoinExec operator) is defined and the current event-time watermark threshold of the given OffsetSeqMetadata is above ( greater than ) it, i.e. moved above shouldRunAnotherBatch is negative ( false ) otherwise. shouldRunAnotherBatch is part of the StateStoreWriter abstraction.","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#executing-physical-operator","text":"doExecute (): RDD [ InternalRow ] doExecute is part of the SparkPlan abstraction ( Spark SQL ). doExecute first requests the StreamingQueryManager for the StateStoreCoordinatorRef to the StateStoreCoordinator RPC endpoint (for the driver). doExecute then uses SymmetricHashJoinStateManager utility to get the names of the state stores for the left and right sides of the streaming join. In the end, doExecute requests the < > and < > child physical operators to execute (generate an RDD) and then < > with < > (and with the StateStoreCoordinatorRef and the state stores). === [[processPartitions]] Processing Partitions of Left and Right Sides of Stream-Stream Join -- processPartitions Internal Method","title":" Executing Physical Operator"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_5","text":"processPartitions( leftInputIter: Iterator[InternalRow], rightInputIter: Iterator[InternalRow]): Iterator[InternalRow] [[processPartitions-updateStartTimeNs]] processPartitions records the current time (as updateStartTimeNs for the < > performance metric in < >). [[processPartitions-postJoinFilter]] processPartitions creates a new predicate ( postJoinFilter ) based on the bothSides of the < > if defined or true literal. [[processPartitions-leftSideJoiner]] processPartitions creates a OneSideHashJoiner for the LeftSide and all other properties for the left-hand join side ( leftSideJoiner ). [[processPartitions-rightSideJoiner]] processPartitions creates a OneSideHashJoiner for the RightSide and all other properties for the right-hand join side ( rightSideJoiner ). [[processPartitions-leftOutputIter]][[processPartitions-rightOutputIter]] processPartitions requests the OneSideHashJoiner for the left-hand join side to storeAndJoinWithOtherSide with the right-hand side one (that creates a leftOutputIter row iterator) and the OneSideHashJoiner for the right-hand join side to do the same with the left-hand side one (and creates a rightOutputIter row iterator). [[processPartitions-innerOutputCompletionTimeNs]] processPartitions records the current time (as innerOutputCompletionTimeNs for the < > performance metric in < >). [[processPartitions-innerOutputIter]] processPartitions creates a CompletionIterator with the left and right output iterators (with the rows of the leftOutputIter first followed by rightOutputIter ). When no rows are left to process, the CompletionIterator records the completion time. [[processPartitions-outputIter]] processPartitions creates a join-specific output Iterator[InternalRow] of the output rows based on the < > (of the StreamingSymmetricHashJoinExec ): For Inner joins, processPartitions simply uses the < > For LeftOuter joins, processPartitions ... For RightOuter joins, processPartitions ... For other joins, processPartitions simply throws an IllegalArgumentException . [[processPartitions-outputIterWithMetrics]] processPartitions creates an UnsafeProjection for the < > (and the output of the < > and < > child operators) that counts all the rows of the < > (as the < > metric) and generate an output projection. In the end, processPartitions returns a CompletionIterator with with the < > and < > completion function. NOTE: processPartitions is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >. ==== [[processPartitions-onOutputCompletion]][[onOutputCompletion]] Calculating Performance Metrics (Output Completion Callback) -- onOutputCompletion Internal Method","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_6","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#onoutputcompletion-unit","text":"onOutputCompletion calculates the < > performance metric (that is the time since the < > was executed). onOutputCompletion adds the time for the inner join to complete (since < > time marker) to the < > performance metric. onOutputCompletion records the time to remove old state (per the join state watermark predicate for the < > and the < > streaming queries) and adds it to the < > performance metric. NOTE: onOutputCompletion triggers the old state removal eagerly by iterating over the state rows to be deleted. onOutputCompletion records the time for the < > and < > OneSideHashJoiners to commit any state changes that becomes the < > performance metric. onOutputCompletion calculates the < > performance metric (as the number of updated state rows of the < > and < > streaming queries). onOutputCompletion calculates the < > performance metric (as the sum of the < > in the KeyWithIndexToValueStore of the < > and < > streaming queries). onOutputCompletion calculates the < > performance metric (as the sum of the < > by the KeyToNumValuesStore and KeyWithIndexToValueStore of the < > and < > streams). In the end, onOutputCompletion calculates the < >.","title":"onOutputCompletion: Unit"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#internal-properties","text":"[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | hadoopConfBcast a| [[hadoopConfBcast]] Hadoop Configuration broadcast (to the Spark cluster) Used exclusively to < > | joinStateManager a| [[joinStateManager]] SymmetricHashJoinStateManager Used when OneSideHashJoiner is requested to storeAndJoinWithOtherSide , removeOldState , commitStateAndGetMetrics , and for the values for a given key | nullLeft a| [[nullLeft]] GenericInternalRow of the size of the output schema of the < > | nullRight a| [[nullRight]] GenericInternalRow of the size of the output schema of the < > | storeConf a| [[storeConf]] StateStoreConf Used exclusively to < > |===","title":"Internal Properties"},{"location":"physical-operators/WriteToContinuousDataSourceExec/","text":"WriteToContinuousDataSourceExec Unary Physical Operator \u00b6 [[children]] WriteToContinuousDataSourceExec is a unary physical operator that < >. [NOTE] \u00b6 A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode ] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. \u00b6 WriteToContinuousDataSourceExec is < > exclusively when DataSourceV2Strategy execution planning strategy is requested to plan a WriteToContinuousDataSource unary logical operator. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy-DataSourceV2Strategy.html[DataSourceV2Strategy Execution Planning Strategy] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. [[creating-instance]] WriteToContinuousDataSourceExec takes the following to be created: [[query]][[child]] Child physical operator ( SparkPlan ) [[output]] WriteToContinuousDataSourceExec uses empty output schema (which is exactly to say that no output is expected whatsoever). [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.continuous.WriteToContinuousDataSourceExec to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.continuous.WriteToContinuousDataSourceExec=ALL Refer to < >. \u00b6 === [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method [source, scala] \u00b6 doExecute(): RDD[InternalRow] \u00b6 NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. RDD[InternalRow] ). doExecute requests the < > to create a DataWriterFactory . doExecute then requests the < > to execute (that gives a RDD[InternalRow] ) and uses the RDD[InternalRow] and the DataWriterFactory to create a < >. doExecute prints out the following INFO message to the logs: Start processing data source writer: [writer]. The input RDD has [partitions] partitions. doExecute requests the EpochCoordinatorRef helper for a < > (using the < >). NOTE: The < > runs on the driver as the single point to coordinate epochs across partition tasks. doExecute requests the EpochCoordinator RPC endpoint reference to send out a < > message synchronously. In the end, doExecute requests the ContinuousWriteRDD to collect (which simply runs a Spark job on all partitions in an RDD and returns the results in an array). NOTE: Requesting the ContinuousWriteRDD to collect is how a Spark job is ran that in turn runs tasks (one per partition) that are described by the < > method. Since executing collect is meant to run a Spark job (with tasks on executors), it's in the discretion of the tasks themselves to decide when to finish (so if they want to run indefinitely, so be it). What a clever trick!","title":"WriteToContinuousDataSourceExec"},{"location":"physical-operators/WriteToContinuousDataSourceExec/#writetocontinuousdatasourceexec-unary-physical-operator","text":"[[children]] WriteToContinuousDataSourceExec is a unary physical operator that < >.","title":"WriteToContinuousDataSourceExec Unary Physical Operator"},{"location":"physical-operators/WriteToContinuousDataSourceExec/#note","text":"A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator.","title":"[NOTE]"},{"location":"physical-operators/WriteToContinuousDataSourceExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-sparkplanhtmlunaryexecnode-and-physical-operators-in-general-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-book","text":"WriteToContinuousDataSourceExec is < > exclusively when DataSourceV2Strategy execution planning strategy is requested to plan a WriteToContinuousDataSource unary logical operator. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy-DataSourceV2Strategy.html[DataSourceV2Strategy Execution Planning Strategy] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. [[creating-instance]] WriteToContinuousDataSourceExec takes the following to be created: [[query]][[child]] Child physical operator ( SparkPlan ) [[output]] WriteToContinuousDataSourceExec uses empty output schema (which is exactly to say that no output is expected whatsoever). [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.continuous.WriteToContinuousDataSourceExec to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.continuous.WriteToContinuousDataSourceExec=ALL","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book."},{"location":"physical-operators/WriteToContinuousDataSourceExec/#refer-to","text":"=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method","title":"Refer to &lt;&gt;."},{"location":"physical-operators/WriteToContinuousDataSourceExec/#source-scala","text":"","title":"[source, scala]"},{"location":"physical-operators/WriteToContinuousDataSourceExec/#doexecute-rddinternalrow","text":"NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. RDD[InternalRow] ). doExecute requests the < > to create a DataWriterFactory . doExecute then requests the < > to execute (that gives a RDD[InternalRow] ) and uses the RDD[InternalRow] and the DataWriterFactory to create a < >. doExecute prints out the following INFO message to the logs: Start processing data source writer: [writer]. The input RDD has [partitions] partitions. doExecute requests the EpochCoordinatorRef helper for a < > (using the < >). NOTE: The < > runs on the driver as the single point to coordinate epochs across partition tasks. doExecute requests the EpochCoordinator RPC endpoint reference to send out a < > message synchronously. In the end, doExecute requests the ContinuousWriteRDD to collect (which simply runs a Spark job on all partitions in an RDD and returns the results in an array). NOTE: Requesting the ContinuousWriteRDD to collect is how a Spark job is ran that in turn runs tasks (one per partition) that are described by the < > method. Since executing collect is meant to run a Spark job (with tasks on executors), it's in the discretion of the tasks themselves to decide when to finish (so if they want to run indefinitely, so be it). What a clever trick!","title":"doExecute(): RDD[InternalRow]"}]}